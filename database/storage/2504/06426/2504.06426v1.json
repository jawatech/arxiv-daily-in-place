{"2504.06426": {"publish_time": "2025-04-08", "title": "S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning", "paper_summary": "Fine-tuning pre-trained large language models (LLMs) presents a dual\nchallenge of balancing parameter efficiency and model capacity. Existing\nmethods like low-rank adaptations (LoRA) are efficient but lack flexibility,\nwhile Mixture-of-Experts (MoE) architectures enhance model capacity at the cost\nof more & under-utilized parameters. To address these limitations, we propose\nStructural Mixture of Residual Experts (S'MoRE), a novel framework that\nseamlessly integrates the efficiency of LoRA with the flexibility of MoE.\nSpecifically, S'MoRE employs hierarchical low-rank decomposition of expert\nweights, yielding residuals of varying orders interconnected in a multi-layer\nstructure. By routing input tokens through sub-trees of residuals, S'MoRE\nemulates the capacity of many experts by instantiating and assembling just a\nfew low-rank matrices. We craft the inter-layer propagation of S'MoRE's\nresiduals as a special type of Graph Neural Network (GNN), and prove that under\nsimilar parameter budget, S'MoRE improves \"structural flexibility\" of\ntraditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive\ntheoretical analysis and empirical results demonstrate that S'MoRE achieves\nsuperior fine-tuning performance, offering a transformative approach for\nefficient LLM adaptation.", "paper_summary_zh": "\u5fae\u8abf\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63d0\u51fa\u4e86\u4e00\u500b\u5e73\u8861\u53c3\u6578\u6548\u7387\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u96d9\u91cd\u6311\u6230\u3002\u73fe\u6709\u7684\u65b9\u6cd5\uff0c\u5982\u4f4e\u79e9\u9069\u61c9 (LoRA) \u6548\u7387\u9ad8\u4f46\u7f3a\u4e4f\u9748\u6d3b\u6027\uff0c\u800c\u6df7\u5408\u5c08\u5bb6 (MoE) \u67b6\u69cb\u4ee5\u66f4\u591a\u4e14\u672a\u5145\u5206\u5229\u7528\u7684\u53c3\u6578\u70ba\u4ee3\u50f9\u63d0\u9ad8\u4e86\u6a21\u578b\u5bb9\u91cf\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d50\u69cb\u5316\u6b98\u5dee\u5c08\u5bb6\u6df7\u5408 (S'MoRE)\uff0c\u9019\u662f\u4e00\u500b\u5c07 LoRA \u7684\u6548\u7387\u8207 MoE \u7684\u9748\u6d3b\u6027\u7121\u7e2b\u96c6\u6210\u7684\u65b0\u7a4e\u6846\u67b6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cS'MoRE \u63a1\u7528\u5c08\u5bb6\u6b0a\u91cd\u7684\u5206\u5c64\u4f4e\u79e9\u5206\u89e3\uff0c\u7522\u751f\u4e0d\u540c\u968e\u6578\u7684\u6b98\u5dee\uff0c\u9019\u4e9b\u6b98\u5dee\u5728\u591a\u5c64\u7d50\u69cb\u4e2d\u76f8\u4e92\u9023\u63a5\u3002\u901a\u904e\u5c07\u8f38\u5165\u6a19\u8a18\u8def\u7531\u5230\u6b98\u5dee\u5b50\u6a39\uff0cS'MoRE \u901a\u904e\u5be6\u4f8b\u5316\u548c\u7d44\u88dd\u5c11\u91cf\u4f4e\u79e9\u77e9\u9663\u4f86\u6a21\u64ec\u8a31\u591a\u5c08\u5bb6\u7684\u80fd\u529b\u3002\u6211\u5011\u5c07 S'MoRE \u6b98\u5dee\u7684\u5c64\u9593\u50b3\u64ad\u8a2d\u8a08\u6210\u4e00\u7a2e\u7279\u6b8a\u7684\u5716\u795e\u7d93\u7db2\u7d61 (GNN)\uff0c\u4e26\u8b49\u660e\u5728\u76f8\u4f3c\u7684\u53c3\u6578\u9810\u7b97\u4e0b\uff0cS'MoRE \u5c07\u50b3\u7d71 MoE\uff08\u6216 LoRA \u6df7\u5408\uff09\u7684\u300c\u7d50\u69cb\u9748\u6d3b\u6027\u300d\u63d0\u9ad8\u4e86\u6307\u6578\u7d1a\u3002\u5168\u9762\u7684\u7406\u8ad6\u5206\u6790\u548c\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cS'MoRE \u7372\u5f97\u4e86\u5353\u8d8a\u7684\u5fae\u8abf\u6027\u80fd\uff0c\u70ba\u9ad8\u6548\u7684 LLM \u9069\u61c9\u63d0\u4f9b\u4e86\u4e00\u7a2e\u8b8a\u9769\u6027\u7684\u65b9\u6cd5\u3002\n", "author": "Hanqing Zeng et.al.", "authors": "Hanqing Zeng, Yinglong Xia, Zhuokai Zhao, Gilbert Jiang, Qiang Zhang, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Benyu Zhang", "id": "2504.06426v1", "paper_url": "http://arxiv.org/abs/2504.06426v1", "repo": "null"}}