{"2409.09636": {"publish_time": "2024-09-15", "title": "Towards understanding evolution of science through language model series", "paper_summary": "We introduce AnnualBERT, a series of language models designed specifically to\ncapture the temporal evolution of scientific text. Deviating from the\nprevailing paradigms of subword tokenizations and \"one model to rule them all\",\nAnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model\npretrained from scratch on the full-text of 1.7 million arXiv papers published\nuntil 2008 and a collection of progressively trained models on arXiv papers at\nan annual basis. We demonstrate the effectiveness of AnnualBERT models by\nshowing that they not only have comparable performances in standard tasks but\nalso achieve state-of-the-art performances on domain-specific NLP tasks as well\nas link prediction tasks in the arXiv citation network. We then utilize probing\ntasks to quantify the models' behavior in terms of representation learning and\nforgetting as time progresses. Our approach enables the pretrained models to\nnot only improve performances on scientific text processing tasks but also to\nprovide insights into the development of scientific discourse over time. The\nseries of the models is available at https://huggingface.co/jd445/AnnualBERTs.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 AnnualBERT\uff0c\u9019\u662f\u4e00\u7cfb\u5217\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u6355\u6349\u79d1\u5b78\u6587\u672c\u6642\u9593\u6f14\u5316\u7684\u8a9e\u8a00\u6a21\u578b\u3002\u504f\u96e2\u5206\u8a5e\u6a19\u8a18\u548c\u300c\u4e00\u500b\u6a21\u578b\u7d71\u6cbb\u6240\u6709\u300d\u7684\u4e3b\u6d41\u6a21\u5f0f\uff0cAnnualBERT \u63a1\u7528\u6574\u500b\u55ae\u8a5e\u4f5c\u70ba\u6a19\u8a18\uff0c\u4e26\u7531\u4e00\u500b\u57fa\u790e RoBERTa \u6a21\u578b\u7d44\u6210\uff0c\u8a72\u6a21\u578b\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4\u5728 2008 \u5e74\u4e4b\u524d\u767c\u8868\u7684 170 \u842c\u7bc7 arXiv \u5168\u6587\u4e0a\uff0c\u4ee5\u53ca\u6bcf\u5e74\u5728 arXiv \u8ad6\u6587\u4e0a\u8a13\u7df4\u7684\u4e00\u7cfb\u5217\u6a21\u578b\u3002\u6211\u5011\u901a\u904e\u5c55\u793a AnnualBERT \u6a21\u578b\u4e0d\u50c5\u5728\u6a19\u6e96\u4efb\u52d9\u4e2d\u5177\u6709\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u7279\u5b9a\u9818\u57df\u7684 NLP \u4efb\u52d9\u4ee5\u53ca arXiv \u5f15\u6587\u7db2\u8def\u4e2d\u7684\u9023\u7d50\u9810\u6e2c\u4efb\u52d9\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u8b49\u660e\u4e86 AnnualBERT \u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5229\u7528\u63a2\u6e2c\u4efb\u52d9\u4f86\u91cf\u5316\u6a21\u578b\u5728\u8868\u793a\u5b78\u7fd2\u548c\u96a8\u8457\u6642\u9593\u63a8\u79fb\u800c\u907a\u5fd8\u65b9\u9762\u7684\u884c\u70ba\u3002\u6211\u5011\u7684\u505a\u6cd5\u4f7f\u9810\u8a13\u7df4\u6a21\u578b\u4e0d\u50c5\u53ef\u4ee5\u6539\u5584\u79d1\u5b78\u6587\u672c\u8655\u7406\u4efb\u52d9\u7684\u6027\u80fd\uff0c\u9084\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u79d1\u5b78\u8ad6\u8ff0\u7684\u767c\u5c55\u3002\u8a72\u7cfb\u5217\u6a21\u578b\u53ef\u5728 https://huggingface.co/jd445/AnnualBERTs \u4e2d\u7372\u5f97\u3002", "author": "Junjie Dong et.al.", "authors": "Junjie Dong, Zhuoqi Lyu, Qing Ke", "id": "2409.09636v1", "paper_url": "http://arxiv.org/abs/2409.09636v1", "repo": "null"}}