{"2406.11427": {"publish_time": "2024-06-17", "title": "DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer", "paper_summary": "Large-scale diffusion models have shown outstanding generative abilities\nacross multiple modalities including images, videos, and audio. However,\ntext-to-speech (TTS) systems typically involve domain-specific modeling factors\n(e.g., phonemes and phoneme-level durations) to ensure precise temporal\nalignments between text and speech, which hinders the efficiency and\nscalability of diffusion models for TTS. In this work, we present an efficient\nand scalable Diffusion Transformer (DiT) that utilizes off-the-shelf\npre-trained text and speech encoders. Our approach addresses the challenge of\ntext-speech alignment via cross-attention mechanisms with the prediction of the\ntotal length of speech representations. To achieve this, we enhance the DiT\narchitecture to suit TTS and improve the alignment by incorporating semantic\nguidance into the latent space of speech. We scale the training dataset and the\nmodel size to 82K hours and 790M parameters, respectively. Our extensive\nexperiments demonstrate that the large-scale diffusion model for TTS without\ndomain-specific modeling not only simplifies the training pipeline but also\nyields superior or comparable zero-shot performance to state-of-the-art TTS\nmodels in terms of naturalness, intelligibility, and speaker similarity. Our\nspeech samples are available at https://ditto-tts.github.io.", "paper_summary_zh": "\u5927\u578b\u64f4\u6563\u6a21\u578b\u5df2\u5c55\u73fe\u51fa\u8de8\u8d8a\u591a\u7a2e\u6a21\u614b\uff08\u5305\u542b\u5f71\u50cf\u3001\u5f71\u7247\u548c\u97f3\u8a0a\uff09\u7684\u51fa\u8272\u751f\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u6587\u5b57\u8f49\u8a9e\u97f3\uff08TTS\uff09\u7cfb\u7d71\u901a\u5e38\u5305\u542b\u7279\u5b9a\u65bc\u9818\u57df\u7684\u5efa\u6a21\u56e0\u5b50\uff08\u4f8b\u5982\u97f3\u7d20\u548c\u97f3\u7d20\u5c64\u7d1a\u7684\u6301\u7e8c\u6642\u9593\uff09\uff0c\u4ee5\u78ba\u4fdd\u6587\u5b57\u8207\u8a9e\u97f3\u4e4b\u9593\u7cbe\u78ba\u7684\u6642\u9593\u5c0d\u9f4a\uff0c\u9019\u6703\u963b\u7919\u64f4\u6563\u6a21\u578b\u5728 TTS \u4e2d\u7684\u6548\u7387\u548c\u53ef\u64f4\u5145\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6709\u6548\u4e14\u53ef\u64f4\u5145\u7684\u64f4\u6563Transformer\uff08DiT\uff09\uff0c\u5b83\u5229\u7528\u73fe\u6210\u7684\u9810\u8a13\u7df4\u6587\u5b57\u548c\u8a9e\u97f3\u7de8\u78bc\u5668\u3002\u6211\u5011\u7684\u505a\u6cd5\u900f\u904e\u4ea4\u53c9\u6ce8\u610f\u529b\u6a5f\u5236\u4ee5\u53ca\u9810\u6e2c\u8a9e\u97f3\u8868\u5fb5\u7684\u7e3d\u9577\u5ea6\uff0c\u4f86\u89e3\u6c7a\u6587\u5b57\u8a9e\u97f3\u5c0d\u9f4a\u7684\u6311\u6230\u3002\u70ba\u9054\u6210\u6b64\u76ee\u7684\uff0c\u6211\u5011\u589e\u5f37\u4e86 DiT \u67b6\u69cb\u4ee5\u9069\u7528\u65bc TTS\uff0c\u4e26\u900f\u904e\u5c07\u8a9e\u610f\u5f15\u5c0e\u7d0d\u5165\u8a9e\u97f3\u7684\u6f5b\u5728\u7a7a\u9593\u4e2d\u4f86\u6539\u5584\u5c0d\u9f4a\u3002\u6211\u5011\u5c07\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u5927\u5c0f\u5206\u5225\u64f4\u5145\u5230 82K \u5c0f\u6642\u548c 790M \u53c3\u6578\u3002\u6211\u5011\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u5927\u578b\u64f4\u6563\u6a21\u578b\u7528\u65bc TTS\uff0c\u5373\u4f7f\u6c92\u6709\u7279\u5b9a\u65bc\u9818\u57df\u7684\u5efa\u6a21\uff0c\u4e0d\u50c5\u7c21\u5316\u4e86\u8a13\u7df4\u6d41\u7a0b\uff0c\u9084\u80fd\u7522\u751f\u512a\u65bc\u6216\u5ab2\u7f8e\u73fe\u6709 TTS \u6a21\u578b\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\uff0c\u5728\u81ea\u7136\u5ea6\u3001\u6e05\u6670\u5ea6\u548c\u8aaa\u8a71\u8005\u76f8\u4f3c\u6027\u65b9\u9762\u7686\u662f\u5982\u6b64\u3002\u6211\u5011\u7684\u8a9e\u97f3\u7bc4\u4f8b\u53ef\u65bc https://ditto-tts.github.io/ \u53d6\u5f97\u3002", "author": "Keon Lee et.al.", "authors": "Keon Lee, Dong Won Kim, Jaehyeon Kim, Jaewoong Cho", "id": "2406.11427v1", "paper_url": "http://arxiv.org/abs/2406.11427v1", "repo": "null"}}