{"2501.08597": {"publish_time": "2025-01-15", "title": "Dynamic Knowledge Integration for Enhanced Vision-Language Reasoning", "paper_summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multimodal tasks, but their performance is often constrained by\nthe lack of external knowledge integration, limiting their ability to handle\nknowledge-intensive tasks such as visual question answering and reasoning. To\naddress this challenge, we propose a novel method, Adaptive Knowledge-Guided\nPretraining for Large Vision-Language Models (AKGP-LVLM), which dynamically\nincorporates structured and unstructured knowledge into LVLMs during\npretraining and fine-tuning. Our approach employs a knowledge encoder to\nrepresent external knowledge, a retrieval mechanism to select task-relevant\ninformation, and a dynamic adaptor to align multimodal and knowledge\nrepresentations effectively. We evaluate our method on four benchmark datasets,\ndemonstrating significant performance improvements over state-of-the-art\nmodels. Furthermore, human evaluations highlight the superior correctness and\nrelevance of our model's outputs. Extensive analyses confirm the robustness,\nefficiency, and scalability of AKGP-LVLM, making it a compelling solution for\nreal-world knowledge-intensive tasks.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5df2\u5728\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u5176\u6548\u80fd\u7d93\u5e38\u53d7\u5230\u5916\u90e8\u77e5\u8b58\u6574\u5408\u4e0d\u8db3\u7684\u9650\u5236\uff0c\u9019\u6703\u9650\u5236\u5b83\u5011\u8655\u7406\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\uff08\u4f8b\u5982\u8996\u89ba\u554f\u7b54\u548c\u63a8\u7406\uff09\u7684\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u81ea\u9069\u61c9\u77e5\u8b58\u5f15\u5c0e\u9810\u8a13\u7df4 (AKGP-LVLM)\uff0c\u5b83\u6703\u5728\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u671f\u9593\u52d5\u614b\u5730\u5c07\u7d50\u69cb\u5316\u548c\u975e\u7d50\u69cb\u5316\u77e5\u8b58\u7d0d\u5165 LVLMs\u3002\u6211\u5011\u7684\u505a\u6cd5\u63a1\u7528\u77e5\u8b58\u7de8\u78bc\u5668\u4f86\u8868\u793a\u5916\u90e8\u77e5\u8b58\uff0c\u63a1\u7528\u6aa2\u7d22\u6a5f\u5236\u4f86\u9078\u64c7\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u8cc7\u8a0a\uff0c\u4e26\u63a1\u7528\u52d5\u614b\u9069\u914d\u5668\u4f86\u6709\u6548\u5730\u5c0d\u9f4a\u591a\u6a21\u614b\u548c\u77e5\u8b58\u8868\u793a\u3002\u6211\u5011\u5728\u56db\u5927\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u8a55\u4f30\u4e86\u6211\u5011\u7684\u6a21\u578b\uff0c\u8b49\u660e\u5176\u6548\u80fd\u986f\u8457\u512a\u65bc\u73fe\u6709\u6700\u5148\u9032\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4eba\u5de5\u8a55\u4f30\u7a81\u986f\u4e86\u6211\u5011\u6a21\u578b\u8f38\u51fa\u7684\u6b63\u78ba\u6027\u548c\u76f8\u95dc\u6027\u7684\u512a\u8d8a\u6027\u3002\u5ee3\u6cdb\u7684\u5206\u6790\u8b49\u5be6\u4e86 AKGP-LVLM \u7684\u7a69\u5065\u6027\u3001\u6548\u7387\u548c\u53ef\u64f4\u5145\u6027\uff0c\u4f7f\u5176\u6210\u70ba\u73fe\u5be6\u4e16\u754c\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u7684\u5f37\u5927\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Julian Perry et.al.", "authors": "Julian Perry, Surasakdi Siripong, Thanakorn Phonchai", "id": "2501.08597v1", "paper_url": "http://arxiv.org/abs/2501.08597v1", "repo": "null"}}