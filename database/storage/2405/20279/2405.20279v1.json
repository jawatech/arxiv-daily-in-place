{"2405.20279": {"publish_time": "2024-05-30", "title": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models", "paper_summary": "Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.", "paper_summary_zh": "\u6642\u7a7a\u5f71\u7247\u58d3\u7e2e\uff0c\u5229\u7528\u8b8a\u7570\u81ea\u7de8\u78bc\u5668 (VAE) \u7b49\u7db2\u8def\uff0c\u5728 OpenAI \u7684 SORA \u548c\u8a31\u591a\u5176\u4ed6\u5f71\u7247\u751f\u6210\u6a21\u578b\u4e2d\u626e\u6f14\u95dc\u9375\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u8a31\u591a\u985e\u4f3c LLM \u7684\u5f71\u7247\u6a21\u578b\u6703\u5728 VQVAE \u67b6\u69cb\u4e2d\u5b78\u7fd2\u6e90\u81ea 3D VAE \u7684\u96e2\u6563\u4ee3\u78bc\u7684\u5206\u5e03\uff0c\u800c\u5927\u591a\u6578\u57fa\u65bc\u64f4\u6563\u7684\u5f71\u7247\u6a21\u578b\u6703\u64f7\u53d6\u672a\u7d93\u91cf\u5316\u7684 2D VAE \u8403\u53d6\u7684\u9023\u7e8c\u6f5b\u5728\u5206\u5e03\u3002\u6642\u5e8f\u58d3\u7e2e\u50c5\u900f\u904e\u5747\u52fb\u7684\u5f71\u683c\u53d6\u6a23\u5be6\u73fe\uff0c\u9019\u6703\u5c0e\u81f4\u9023\u7e8c\u5f71\u683c\u4e4b\u9593\u7684\u52d5\u4f5c\u4e0d\u9806\u66a2\u3002\u76ee\u524d\uff0c\u7814\u7a76\u793e\u7fa4\u4e2d\u7f3a\u4e4f\u4e00\u500b\u666e\u904d\u4f7f\u7528\u7684\u9023\u7e8c\u5f71\u7247 (3D) VAE\uff0c\u53ef\u4f9b\u6f5b\u5728\u64f4\u6563\u5f0f\u5f71\u7247\u6a21\u578b\u4f7f\u7528\u3002\u6b64\u5916\uff0c\u7531\u65bc\u76ee\u524d\u7684\u57fa\u65bc\u64f4\u6563\u7684\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u6587\u5b57\u8f49\u5f71\u50cf (T2I) \u6a21\u578b\u5be6\u4f5c\uff0c\u76f4\u63a5\u8a13\u7df4\u5f71\u7247 VAE \u800c\u672a\u8003\u91cf\u8207\u73fe\u6709 T2I \u6a21\u578b\u7684\u76f8\u5bb9\u6027\uff0c\u5c07\u5c0e\u81f4\u5b83\u5011\u4e4b\u9593\u7684\u6f5b\u5728\u7a7a\u9593\u5dee\u8ddd\uff0c\u5373\u4f7f\u4f7f\u7528 T2I \u6a21\u578b\u4f5c\u70ba\u521d\u59cb\u5316\uff0c\u586b\u88dc\u5dee\u8ddd\u4ecd\u9700\u8017\u8cbb\u9f90\u5927\u7684\u904b\u7b97\u8cc7\u6e90\u9032\u884c\u8a13\u7df4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7528\u65bc\u8a13\u7df4\u6f5b\u5728\u5f71\u7247\u6a21\u578b\u7684\u5f71\u7247 VAE \u7684\u65b9\u6cd5\uff0c\u5373 CV-VAE\uff0c\u5176\u6f5b\u5728\u7a7a\u9593\u8207\u7d66\u5b9a\u7684\u5f71\u50cf VAE \u76f8\u5bb9\uff0c\u4f8b\u5982 Stable Diffusion (SD) \u7684\u5f71\u50cf VAE\u3002\u76f8\u5bb9\u6027\u662f\u900f\u904e\u63d0\u51fa\u7684\u65b0\u7a4e\u6f5b\u5728\u7a7a\u9593\u6b63\u5247\u5316\u5be6\u73fe\u7684\uff0c\u5176\u4e2d\u6d89\u53ca\u4f7f\u7528\u5f71\u50cf VAE \u5236\u5b9a\u6b63\u5247\u5316\u640d\u5931\u3002\u53d7\u76ca\u65bc\u6f5b\u5728\u7a7a\u9593\u76f8\u5bb9\u6027\uff0c\u5f71\u7247\u6a21\u578b\u53ef\u4ee5\u5f9e\u9810\u5148\u8a13\u7df4\u597d\u7684 T2I \u6216\u5f71\u7247\u6a21\u578b\u5728\u771f\u6b63\u7684\u6642\u7a7a\u58d3\u7e2e\u6f5b\u5728\u7a7a\u9593\u4e2d\u7121\u7e2b\u8a13\u7df4\uff0c\u800c\u4e0d\u7528\u50c5\u4ee5\u76f8\u7b49\u7684\u9593\u9694\u53d6\u6a23\u5f71\u7247\u5f71\u683c\u3002\u900f\u904e\u6211\u5011\u7684 CV-VAE\uff0c\u73fe\u6709\u7684\u5f71\u7247\u6a21\u578b\u53ef\u4ee5\u7522\u751f\u591a\u56db\u500d\u7684\u5f71\u683c\uff0c\u5fae\u8abf\u5e45\u5ea6\u6975\u5c0f\u3002\u6211\u5011\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u8b49\u660e\u6240\u63d0\u51fa\u7684\u5f71\u7247 VAE \u7684\u6709\u6548\u6027\u3002", "author": "Sijie Zhao et.al.", "authors": "Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, Ying Shan", "id": "2405.20279v1", "paper_url": "http://arxiv.org/abs/2405.20279v1", "repo": "https://github.com/ailab-cvc/cv-vae"}}