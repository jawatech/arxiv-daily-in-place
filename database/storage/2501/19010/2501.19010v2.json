{"2501.19010": {"publish_time": "2025-01-31", "title": "DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition", "paper_summary": "Dysarthric speech recognition often suffers from performance degradation due\nto the intrinsic diversity of dysarthric severity and extrinsic disparity from\nnormal speech. To bridge these gaps, we propose a Dynamic Phoneme-level\nContrastive Learning (DyPCL) method, which leads to obtaining invariant\nrepresentations across diverse speakers. We decompose the speech utterance into\nphoneme segments for phoneme-level contrastive learning, leveraging dynamic\nconnectionist temporal classification alignment. Unlike prior studies focusing\non utterance-level embeddings, our granular learning allows discrimination of\nsubtle parts of speech. In addition, we introduce dynamic curriculum learning,\nwhich progressively transitions from easy negative samples to\ndifficult-to-distinguishable negative samples based on phonetic similarity of\nphoneme. Our approach to training by difficulty levels alleviates the inherent\nvariability of speakers, better identifying challenging speeches. Evaluated on\nthe UASpeech dataset, DyPCL outperforms baseline models, achieving an average\n22.10\\% relative reduction in word error rate (WER) across the overall\ndysarthria group.", "paper_summary_zh": "\u69cb\u97f3\u969c\u7919\u7684\u8a9e\u97f3\u8fa8\u8b58\u5e38\u5e38\u6703\u56e0\u70ba\u69cb\u97f3\u969c\u7919\u56b4\u91cd\u7a0b\u5ea6\u7684\u5167\u5728\u5dee\u7570\u548c\u8207\u6b63\u5e38\u8a9e\u97f3\u7684\u5916\u5728\u5dee\u7570\u800c\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u4e9b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u52d5\u614b\u97f3\u7d20\u5c64\u5c0d\u6bd4\u5b78\u7fd2 (DyPCL) \u65b9\u6cd5\uff0c\u9019\u6703\u5c0e\u81f4\u7372\u5f97\u4e0d\u540c\u8aaa\u8a71\u8005\u4e4b\u9593\u7684\u4e0d\u8b8a\u8868\u793a\u3002\u6211\u5011\u5c07\u8a9e\u97f3\u8a71\u8a9e\u5206\u89e3\u6210\u97f3\u7d20\u7247\u6bb5\uff0c\u4ee5\u9032\u884c\u97f3\u7d20\u5c64\u5c0d\u6bd4\u5b78\u7fd2\uff0c\u5229\u7528\u52d5\u614b\u9023\u63a5\u4e3b\u7fa9\u6642\u5e8f\u5206\u985e\u5c0d\u9f4a\u3002\u8207\u5c08\u6ce8\u65bc\u8a71\u8a9e\u5c64\u7d1a\u5d4c\u5165\u7684\u5148\u524d\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u5011\u7684\u7d30\u7c92\u5ea6\u5b78\u7fd2\u5141\u8a31\u5340\u5206\u8a9e\u97f3\u7684\u7d30\u5fae\u90e8\u5206\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u52d5\u614b\u8ab2\u7a0b\u5b78\u7fd2\uff0c\u5b83\u6703\u6839\u64da\u97f3\u7d20\u7684\u8a9e\u97f3\u76f8\u4f3c\u6027\uff0c\u5f9e\u5bb9\u6613\u7684\u8ca0\u9762\u6a23\u672c\u9010\u6f38\u904e\u6e21\u5230\u96e3\u4ee5\u5340\u5206\u7684\u8ca0\u9762\u6a23\u672c\u3002\u6211\u5011\u900f\u904e\u96e3\u5ea6\u7b49\u7d1a\u9032\u884c\u8a13\u7df4\u7684\u65b9\u6cd5\u6e1b\u8f15\u4e86\u8aaa\u8a71\u8005\u7684\u5167\u5728\u8b8a\u7570\u6027\uff0c\u80fd\u66f4\u597d\u5730\u8b58\u5225\u6709\u6311\u6230\u6027\u7684\u8a9e\u97f3\u3002\u5728 UASpeech \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\uff0cDyPCL \u512a\u65bc\u57fa\u7dda\u6a21\u578b\uff0c\u5728\u6574\u9ad4\u69cb\u97f3\u969c\u7919\u7d44\u4e2d\uff0c\u5b57\u5143\u932f\u8aa4\u7387 (WER) \u5e73\u5747\u964d\u4f4e\u4e86 22.10%\u3002", "author": "Wonjun Lee et.al.", "authors": "Wonjun Lee, Solee Im, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee", "id": "2501.19010v2", "paper_url": "http://arxiv.org/abs/2501.19010v2", "repo": "null"}}