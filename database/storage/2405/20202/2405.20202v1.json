{"2405.20202": {"publish_time": "2024-05-30", "title": "One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments", "paper_summary": "Large Language Models (LLMs) have advanced rapidly but face significant\nmemory demands. While quantization has shown promise for LLMs, current methods\ntypically require lengthy training to alleviate the performance degradation\nfrom quantization loss. However, deploying LLMs across diverse scenarios with\ndifferent resource constraints, e.g., servers and personal computers, requires\nrepeated training per application, which amplifies the lengthy training\nproblem. Given that, it is advantageous to train a once-for-all (OFA) supernet\ncapable of yielding diverse optimal subnets for downstream applications through\none-shot training. Nonetheless, the scale of current language models impedes\nefficiency and amplifies interference from weight sharing between subnets. We\nmake an initial attempt to extend the once-for-all framework to large language\nmodels. Specifically, we decouple shared weights to eliminate the interference\nand incorporate Low-Rank adapters for training efficiency. Furthermore, we\nobserve the imbalance allocation of training resources from the traditional\nuniform sampling. A non-parametric scheduler is introduced to adjust the\nsampling rate for each quantization configuration, achieving a more balanced\nallocation among subnets with varying demands. We validate the approach on\nLLaMA2 families, and downstream evaluation confirms our ability to maintain\nhigh performance while significantly reducing deployment time faced with\nmultiple scenarios.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u8fc5\u901f\u9032\u6b65\uff0c\u4f46\u9762\u81e8\u56b4\u5cfb\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u3002\u96d6\u7136\u91cf\u5316\u5df2\u986f\u793a\u51fa\u5c0d LLM \u7684\u524d\u666f\uff0c\u4f46\u76ee\u524d\u7684\u6280\u8853\u901a\u5e38\u9700\u8981\u5197\u9577\u7684\u8a13\u7df4\uff0c\u4ee5\u6e1b\u8f15\u91cf\u5316\u640d\u5931\u9020\u6210\u7684\u6548\u80fd\u4e0b\u964d\u3002\u7136\u800c\uff0c\u5728\u5177\u6709\u4e0d\u540c\u8cc7\u6e90\u9650\u5236\u7684\u4e0d\u540c\u60c5\u6cc1\u4e0b\u90e8\u7f72 LLM\uff0c\u4f8b\u5982\u4f3a\u670d\u5668\u548c\u500b\u4eba\u96fb\u8166\uff0c\u9700\u8981\u91dd\u5c0d\u6bcf\u500b\u61c9\u7528\u7a0b\u5f0f\u9032\u884c\u91cd\u8907\u8a13\u7df4\uff0c\u9019\u6703\u64f4\u5927\u5197\u9577\u7684\u8a13\u7df4\u554f\u984c\u3002\u6709\u9451\u65bc\u6b64\uff0c\u8a13\u7df4\u4e00\u6b21\u6027 (OFA) \u8d85\u7d1a\u7db2\u8def\u662f\u6709\u5229\u7684\uff0c\u5b83\u80fd\u5920\u900f\u904e\u4e00\u6b21\u6027\u8a13\u7df4\u70ba\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u7522\u751f\u591a\u6a23\u5316\u7684\u6700\u4f73\u5b50\u7db2\u8def\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u76ee\u524d\u8a9e\u8a00\u6a21\u578b\u7684\u898f\u6a21\u6703\u963b\u7919\u6548\u7387\uff0c\u4e26\u64f4\u5927\u5b50\u7db2\u8def\u4e4b\u9593\u6b0a\u91cd\u5171\u4eab\u7684\u5e72\u64fe\u3002\u6211\u5011\u6700\u521d\u5617\u8a66\u5c07\u4e00\u6b21\u6027\u67b6\u69cb\u64f4\u5c55\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u89e3\u8026\u5171\u4eab\u6b0a\u91cd\u4ee5\u6d88\u9664\u5e72\u64fe\uff0c\u4e26\u52a0\u5165\u4f4e\u79e9\u9069\u914d\u5668\u4ee5\u63d0\u9ad8\u8a13\u7df4\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u50b3\u7d71\u5747\u52fb\u53d6\u6a23\u8a13\u7df4\u8cc7\u6e90\u5206\u914d\u7684\u4e0d\u5e73\u8861\u3002\u5f15\u5165\u975e\u53c3\u6578\u6392\u7a0b\u5668\u4f86\u8abf\u6574\u6bcf\u500b\u91cf\u5316\u914d\u7f6e\u7684\u53d6\u6a23\u7387\uff0c\u5728\u9700\u6c42\u4e0d\u540c\u7684\u5b50\u7db2\u8def\u4e4b\u9593\u5be6\u73fe\u66f4\u5e73\u8861\u7684\u5206\u914d\u3002\u6211\u5011\u5728 LLaMA2 \u5bb6\u65cf\u4e2d\u9a57\u8b49\u4e86\u9019\u7a2e\u65b9\u6cd5\uff0c\u800c\u4e0b\u6e38\u8a55\u4f30\u8b49\u5be6\u4e86\u6211\u5011\u5728\u9762\u5c0d\u591a\u7a2e\u60c5\u6cc1\u6642\uff0c\u80fd\u5920\u5728\u986f\u8457\u6e1b\u5c11\u90e8\u7f72\u6642\u9593\u7684\u540c\u6642\uff0c\u7dad\u6301\u9ad8\u6027\u80fd\u3002", "author": "Ke Yi et.al.", "authors": "Ke Yi, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, Jia Li", "id": "2405.20202v1", "paper_url": "http://arxiv.org/abs/2405.20202v1", "repo": "null"}}