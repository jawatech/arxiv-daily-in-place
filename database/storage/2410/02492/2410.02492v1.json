{"2410.02492": {"publish_time": "2024-10-03", "title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM", "paper_summary": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u8ffd\u8e64 (VLT) \u5df2\u6210\u70ba\u5c16\u7aef\u7684\u7684\u7814\u7a76\u9818\u57df\uff0c\n\u5229\u7528\u8a9e\u8a00\u8cc7\u6599\u4f86\u589e\u5f37\u5e36\u6709\u591a\u6a21\u5f0f\u8f38\u5165\u7684\u6f14\u7b97\u6cd5\uff0c\u4e26\u64f4\u5c55\u50b3\u7d71\u55ae\u4e00\u7269\u4ef6\u8ffd\u8e64 (SOT) \u7684\u7bc4\u570d\uff0c\u4ee5\u6db5\u84cb\u5f71\u7247\u7406\u89e3\u61c9\u7528\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u5927\u591a\u6578 VLT \u8a55\u91cf\u57fa\u6e96\u4ecd\u7136\u4f9d\u8cf4\u6bcf\u500b\u5f71\u7247\u7684\u7c21\u6f54\u3001\u4eba\u5de5\u6a19\u8a3b\u7684\u6587\u5b57\u63cf\u8ff0\u3002\u9019\u4e9b\u63cf\u8ff0\u901a\u5e38\u7121\u6cd5\u6355\u6349\u5f71\u7247\u5167\u5bb9\u52d5\u614b\u7684\u7d30\u5fae\u5dee\u5225\uff0c\u4e14\u7f3a\u4e4f\u8a9e\u8a00\u98a8\u683c\u7684\u591a\u6a23\u6027\uff0c\u53d7\u5230\u5176\u5747\u52fb\u7684\u8a73\u7d30\u7a0b\u5ea6\u548c\u56fa\u5b9a\u7684\u6a19\u8a3b\u983b\u7387\u6240\u9650\u5236\u3002\u56e0\u6b64\uff0c\u6f14\u7b97\u6cd5\u50be\u5411\u65bc\u9810\u8a2d\u70ba\u300c\u8a18\u4f4f\u7b54\u6848\u300d\u7b56\u7565\uff0c\u504f\u96e2\u4e86\u6df1\u5165\u7406\u89e3\u5f71\u7247\u5167\u5bb9\u7684\u6838\u5fc3\u76ee\u6a19\u3002\u5e78\u904b\u7684\u662f\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u4f7f\u5f97\u80fd\u5920\u7522\u751f\u591a\u6a23\u5316\u7684\u6587\u5b57\u3002\u9019\u9805\u5de5\u4f5c\u5229\u7528 LLM \u70ba\u5177\u4ee3\u8868\u6027\u7684 SOT \u8a55\u91cf\u57fa\u6e96\u7522\u751f\u591a\u6a23\u7684\u8a9e\u7fa9\u6a19\u8a3b\uff08\u5c31\u6587\u5b57\u9577\u5ea6\u548c\u7c92\u5ea6\u800c\u8a00\uff09\uff0c\u5f9e\u800c\u5efa\u7acb\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u5f0f\u8a55\u91cf\u57fa\u6e96\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011 (1) \u63d0\u51fa\u4e00\u500b\u65b0\u7684\u5177\u6709\u591a\u6a23\u5316\u6587\u5b57\u7684\u8996\u89ba\u8a9e\u8a00\u8ffd\u8e64\u8a55\u91cf\u57fa\u6e96\uff0c\u7a31\u70ba DTVLT\uff0c\u57fa\u65bc\u4e94\u500b\u8457\u540d\u7684 VLT \u548c SOT \u8a55\u91cf\u57fa\u6e96\uff0c\u5305\u62ec\u4e09\u500b\u5b50\u4efb\u52d9\uff1a\u77ed\u671f\u8ffd\u8e64\u3001\u9577\u671f\u8ffd\u8e64\u548c\u5168\u5c40\u5be6\u4f8b\u8ffd\u8e64\u3002(2) \u6211\u5011\u5728\u8a55\u91cf\u57fa\u6e96\u4e2d\u63d0\u4f9b\u4e86\u56db\u500b\u7c92\u5ea6\u6587\u5b57\uff0c\u8003\u616e\u4e86\u8a9e\u7fa9\u8cc7\u8a0a\u7684\u7bc4\u570d\u548c\u5bc6\u5ea6\u3002\u6211\u5011\u9810\u671f\u9019\u7a2e\u591a\u7c92\u5ea6\u751f\u6210\u7b56\u7565\u5c07\u70ba VLT \u548c\u5f71\u7247\u7406\u89e3\u7814\u7a76\u71df\u9020\u4e00\u500b\u6709\u5229\u7684\u74b0\u5883\u3002(3) \u6211\u5011\u5c0d DTVLT \u9032\u884c\u4e86\u5168\u9762\u7684\u5be6\u9a57\u5206\u6790\uff0c\u8a55\u4f30\u4e86\u591a\u6a23\u5316\u6587\u5b57\u5c0d\u8ffd\u8e64\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u4e26\u5e0c\u671b\u73fe\u6709\u6f14\u7b97\u6cd5\u8b58\u5225\u51fa\u7684\u6548\u80fd\u74f6\u9838\u80fd\u5920\u652f\u6301 VLT \u548c\u5f71\u7247\u7406\u89e3\u7684\u9032\u4e00\u6b65\u7814\u7a76\u3002\u5efa\u8b70\u7684\u8a55\u91cf\u57fa\u6e96\u3001\u5be6\u9a57\u7d50\u679c\u548c\u5de5\u5177\u5305\u5c07\u9010\u6b65\u5728 http://videocube.aitestunion.com/ \u4e0a\u767c\u5e03\u3002", "author": "Xuchen Li et.al.", "authors": "Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang", "id": "2410.02492v1", "paper_url": "http://arxiv.org/abs/2410.02492v1", "repo": "null"}}