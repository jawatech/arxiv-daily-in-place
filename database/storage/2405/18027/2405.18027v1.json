{"2405.18027": {"publish_time": "2024-05-28", "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models", "paper_summary": "While Large Language Models (LLMs) can serve as agents to simulate human\nbehaviors (i.e., role-playing agents), we emphasize the importance of\npoint-in-time role-playing. This situates characters at specific moments in the\nnarrative progression for three main reasons: (i) enhancing users' narrative\nimmersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom\nrole-playing. To accurately represent characters at specific time points,\nagents must avoid character hallucination, where they display knowledge that\ncontradicts their characters' identities and historical timelines. We introduce\nTimeChara, a new benchmark designed to evaluate point-in-time character\nhallucination in role-playing LLMs. Comprising 10,895 instances generated\nthrough an automated pipeline, this benchmark reveals significant hallucination\nissues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this\nchallenge, we propose Narrative-Experts, a method that decomposes the reasoning\nsteps and utilizes narrative experts to reduce point-in-time character\nhallucinations effectively. Still, our findings with TimeChara highlight the\nongoing challenges of point-in-time character hallucination, calling for\nfurther study.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u96d6\u7136\u53ef\u4ee5\u4f5c\u70ba\u6a21\u64ec\u4eba\u985e\u884c\u70ba\u7684\u4ee3\u7406\uff08\u5373\u89d2\u8272\u626e\u6f14\u4ee3\u7406\uff09\uff0c\u4f46\u6211\u5011\u5f37\u8abf\u5373\u6642\u89d2\u8272\u626e\u6f14\u7684\u91cd\u8981\u6027\u3002\u9019\u8b93\u89d2\u8272\u8655\u65bc\u6558\u4e8b\u9032\u7a0b\u4e2d\u7684\u7279\u5b9a\u6642\u523b\uff0c\u539f\u56e0\u6709\u4e09\u500b\uff1a(i) \u589e\u5f37\u4f7f\u7528\u8005\u7684\u6558\u4e8b\u6c89\u6d78\u611f\uff0c(ii) \u907f\u514d\u5287\u900f\uff0c\u4ee5\u53ca (iii) \u4fc3\u9032\u7c89\u7d72\u89d2\u8272\u626e\u6f14\u7684\u53c3\u8207\u3002\u70ba\u4e86\u5728\u7279\u5b9a\u6642\u9593\u9ede\u6e96\u78ba\u5448\u73fe\u89d2\u8272\uff0c\u4ee3\u7406\u5fc5\u9808\u907f\u514d\u89d2\u8272\u5e7b\u89ba\uff0c\u5373\u5c55\u73fe\u8207\u89d2\u8272\u8eab\u4efd\u548c\u6b77\u53f2\u6642\u9593\u7dda\u76f8\u77db\u76fe\u7684\u77e5\u8b58\u3002\u6211\u5011\u5f15\u5165\u4e86 TimeChara\uff0c\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30\u89d2\u8272\u626e\u6f14 LLM \u4e2d\u7684\u5373\u6642\u89d2\u8272\u5e7b\u89ba\u3002\u9019\u500b\u57fa\u6e96\u5305\u542b\u900f\u904e\u81ea\u52d5\u5316\u7ba1\u9053\u7522\u751f\u7684 10,895 \u500b\u5be6\u4f8b\uff0c\u63ed\u793a\u4e86\u76ee\u524d\u6700\u5148\u9032\u7684 LLM\uff08\u4f8b\u5982 GPT-4o\uff09\u4e2d\u56b4\u91cd\u7684\u5e7b\u89ba\u554f\u984c\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6558\u4e8b\u5c08\u5bb6\uff0c\u4e00\u7a2e\u5206\u89e3\u63a8\u7406\u6b65\u9a5f\u4e26\u5229\u7528\u6558\u4e8b\u5c08\u5bb6\u4f86\u6709\u6548\u6e1b\u5c11\u5373\u6642\u89d2\u8272\u5e7b\u89ba\u7684\u65b9\u6cd5\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u6211\u5011\u5c0d TimeChara \u7684\u767c\u73fe\u7a81\u986f\u4e86\u5373\u6642\u89d2\u8272\u5e7b\u89ba\u7684\u6301\u7e8c\u6311\u6230\uff0c\u9700\u8981\u9032\u4e00\u6b65\u7814\u7a76\u3002", "author": "Jaewoo Ahn et.al.", "authors": "Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim", "id": "2405.18027v1", "paper_url": "http://arxiv.org/abs/2405.18027v1", "repo": "null"}}