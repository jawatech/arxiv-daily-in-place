{"2408.10602": {"publish_time": "2024-08-20", "title": "MV-MOS: Multi-View Feature Fusion for 3D Moving Object Segmentation", "paper_summary": "Effectively summarizing dense 3D point cloud data and extracting motion\ninformation of moving objects (moving object segmentation, MOS) is crucial to\nautonomous driving and robotics applications. How to effectively utilize motion\nand semantic features and avoid information loss during 3D-to-2D projection is\nstill a key challenge. In this paper, we propose a novel multi-view MOS model\n(MV-MOS) by fusing motion-semantic features from different 2D representations\nof point clouds. To effectively exploit complementary information, the motion\nbranches of the proposed model combines motion features from both bird's eye\nview (BEV) and range view (RV) representations. In addition, a semantic branch\nis introduced to provide supplementary semantic features of moving objects.\nFinally, a Mamba module is utilized to fuse the semantic features with motion\nfeatures and provide effective guidance for the motion branches. We validated\nthe effectiveness of the proposed multi-branch fusion MOS framework via\ncomprehensive experiments, and our proposed model outperforms existing\nstate-of-the-art models on the SemanticKITTI benchmark.", "paper_summary_zh": "\u6709\u6548\u6458\u8981\u5bc6\u96c6 3D \u9ede\u96f2\u8cc7\u6599\u548c\u8403\u53d6\u79fb\u52d5\u7269\u9ad4\u7684\u52d5\u4f5c\u8cc7\u8a0a\uff08\u79fb\u52d5\u7269\u9ad4\u5206\u5272\uff0cMOS\uff09\u5c0d\u65bc\u81ea\u52d5\u99d5\u99db\u548c\u6a5f\u5668\u4eba\u61c9\u7528\u7a0b\u5f0f\u81f3\u95dc\u91cd\u8981\u3002\u5982\u4f55\u6709\u6548\u5229\u7528\u52d5\u4f5c\u548c\u8a9e\u610f\u7279\u5fb5\uff0c\u4e26\u5728 3D \u5230 2D \u6295\u5f71\u671f\u9593\u907f\u514d\u8cc7\u8a0a\u907a\u5931\uff0c\u4ecd\u7136\u662f\u4e00\u9805\u95dc\u9375\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u591a\u8996\u5716 MOS \u6a21\u578b (MV-MOS)\uff0c\u900f\u904e\u878d\u5408\u4f86\u81ea\u9ede\u96f2\u4e0d\u540c 2D \u8868\u793a\u7684\u52d5\u4f5c\u8a9e\u610f\u7279\u5fb5\u3002\u70ba\u4e86\u6709\u6548\u5229\u7528\u4e92\u88dc\u8cc7\u8a0a\uff0c\u6240\u63d0\u51fa\u6a21\u578b\u7684\u52d5\u4f5c\u5206\u652f\u7d50\u5408\u4e86\u4f86\u81ea\u9ce5\u77b0\u5716 (BEV) \u548c\u7bc4\u570d\u8996\u5716 (RV) \u8868\u793a\u7684\u52d5\u4f5c\u7279\u5fb5\u3002\u6b64\u5916\uff0c\u9084\u5f15\u5165\u4e86\u4e00\u500b\u8a9e\u610f\u5206\u652f\u4f86\u63d0\u4f9b\u79fb\u52d5\u7269\u9ad4\u7684\u88dc\u5145\u8a9e\u610f\u7279\u5fb5\u3002\u6700\u5f8c\uff0c\u5229\u7528 Mamba \u6a21\u7d44\u5c07\u8a9e\u610f\u7279\u5fb5\u8207\u52d5\u4f5c\u7279\u5fb5\u878d\u5408\uff0c\u4e26\u70ba\u52d5\u4f5c\u5206\u652f\u63d0\u4f9b\u6709\u6548\u7684\u6307\u5c0e\u3002\u6211\u5011\u900f\u904e\u5168\u9762\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u7684\u591a\u5206\u652f\u878d\u5408 MOS \u67b6\u69cb\u7684\u6709\u6548\u6027\uff0c\u800c\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\u5728 SemanticKITTI \u57fa\u6e96\u6e2c\u8a66\u4e2d\u512a\u65bc\u73fe\u6709\u7684\u6700\u5148\u9032\u6a21\u578b\u3002", "author": "Jintao Cheng et.al.", "authors": "Jintao Cheng, Xingming Chen, Jinxin Liang, Xiaoyu Tang, Xieyuanli Chen, Dachuan Li", "id": "2408.10602v1", "paper_url": "http://arxiv.org/abs/2408.10602v1", "repo": "null"}}