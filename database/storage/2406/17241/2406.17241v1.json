{"2406.17241": {"publish_time": "2024-06-25", "title": "What Do the Circuits Mean? A Knowledge Edit View", "paper_summary": "In the field of language model interpretability, circuit discovery is gaining\npopularity. Despite this, the true meaning of these circuits remain largely\nunanswered. We introduce a novel method to learn their meanings as a holistic\nobject through the lens of knowledge editing. We extract circuits in the\nGPT2-XL model using diverse text classification datasets, and use hierarchical\nrelations datasets to explore knowledge editing in the circuits. Our findings\nindicate that these circuits contain entity knowledge but resist new knowledge\nmore than complementary circuits during knowledge editing. Additionally, we\nexamine the impact of circuit size, discovering that an ideal \"theoretical\ncircuit\" where essential knowledge is concentrated likely incorporates more\nthan 5% but less than 50% of the model's parameters. We also assess the overlap\nbetween circuits from different datasets, finding moderate similarities. What\nconstitutes these circuits, then? We find that up to 60% of the circuits\nconsist of layer normalization modules rather than attention or MLP modules,\nadding evidence to the ongoing debates regarding knowledge localization. In\nsummary, our findings offer new insights into the functions of the circuits,\nand introduce research directions for further interpretability and safety\nresearch of language models.", "paper_summary_zh": "\u5728\u8a9e\u8a00\u6a21\u578b\u53ef\u89e3\u91cb\u6027\u7684\u9818\u57df\u4e2d\uff0c\u96fb\u8def\u767c\u73fe\u6b63\u7372\u5f97\n\u666e\u53ca\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u9019\u4e9b\u96fb\u8def\u7684\u771f\u6b63\u542b\u7fa9\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u89e3\u7b54\u3002\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u901a\u904e\u77e5\u8b58\u7de8\u8f2f\u7684\u8996\u89d2\u5c07\u5b83\u5011\u7684\u542b\u7fa9\u4f5c\u70ba\u4e00\u500b\u6574\u9ad4\u5c0d\u8c61\u9032\u884c\u5b78\u7fd2\u3002\u6211\u5011\u4f7f\u7528\u591a\u6a23\u5316\u7684\u6587\u672c\u5206\u985e\u6578\u64da\u96c6\u63d0\u53d6 GPT2-XL \u6a21\u578b\u4e2d\u7684\u96fb\u8def\uff0c\u4e26\u4f7f\u7528\u5206\u5c64\u95dc\u4fc2\u6578\u64da\u96c6\u63a2\u7d22\u96fb\u8def\u4e2d\u7684\u77e5\u8b58\u7de8\u8f2f\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u9019\u4e9b\u96fb\u8def\u5305\u542b\u5be6\u9ad4\u77e5\u8b58\uff0c\u4f46\u5728\u77e5\u8b58\u7de8\u8f2f\u904e\u7a0b\u4e2d\u6bd4\u4e92\u88dc\u96fb\u8def\u66f4\u6297\u62d2\u65b0\u77e5\u8b58\u3002\u6b64\u5916\uff0c\u6211\u5011\u6aa2\u67e5\u4e86\u96fb\u8def\u5927\u5c0f\u7684\u5f71\u97ff\uff0c\u767c\u73fe\u4e86\u4e00\u500b\u7406\u60f3\u7684\u300c\u7406\u8ad6\u96fb\u8def\u300d\uff0c\u5176\u4e2d\u96c6\u4e2d\u4e86\u5fc5\u8981\u7684\u77e5\u8b58\uff0c\u53ef\u80fd\u5305\u542b\u8d85\u904e 5% \u4f46\u5c11\u65bc\u6a21\u578b\u53c3\u6578\u7684 50%\u3002\u6211\u5011\u9084\u8a55\u4f30\u4e86\u4f86\u81ea\u4e0d\u540c\u6578\u64da\u96c6\u7684\u96fb\u8def\u4e4b\u9593\u7684\u91cd\u758a\uff0c\u767c\u73fe\u6709\u4e2d\u7b49\u76f8\u4f3c\u6027\u3002\u90a3\u9ebc\uff0c\u9019\u4e9b\u96fb\u8def\u7531\u4ec0\u9ebc\u7d44\u6210\u5462\uff1f\u6211\u5011\u767c\u73fe\uff0c\u9ad8\u9054 60% \u7684\u96fb\u8def\u7531\u5c64\u6b78\u4e00\u5316\u6a21\u7d44\u7d44\u6210\uff0c\u800c\u4e0d\u662f\u6ce8\u610f\u529b\u6216 MLP \u6a21\u7d44\uff0c\u9019\u70ba\u95dc\u65bc\u77e5\u8b58\u672c\u5730\u5316\u7684\u6301\u7e8c\u722d\u8ad6\u63d0\u4f9b\u4e86\u8b49\u64da\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u70ba\u96fb\u8def\u7684\u529f\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\uff0c\u4e26\u70ba\u8a9e\u8a00\u6a21\u578b\u7684\u9032\u4e00\u6b65\u53ef\u89e3\u91cb\u6027\u548c\u5b89\u5168\u6027\u7814\u7a76\u5f15\u5165\u4e86\u7814\u7a76\u65b9\u5411\u3002", "author": "Huaizhi Ge et.al.", "authors": "Huaizhi Ge, Frank Rudzicz, Zining Zhu", "id": "2406.17241v1", "paper_url": "http://arxiv.org/abs/2406.17241v1", "repo": "null"}}