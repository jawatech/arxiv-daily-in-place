{"2412.15209": {"publish_time": "2024-12-19", "title": "PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation", "paper_summary": "Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u6709\u986f\u8457\u7684\u9032\u5c55\uff0c\u73fe\u6709\u7684\u50cf\u7d20\u57fa\u790e\u6a21\u578b\u5728\u55ae\u4e00\u5f71\u50cf\u8a2d\u5b9a\u4e2d\u904b\u4f5c\uff0c\u9650\u5236\u4e86\u5b83\u5011\u8de8\u591a\u500b\u5f71\u50cf\u57f7\u884c\u8a73\u7d30\u3001\u7d30\u5fae\u7684\u6bd4\u8f03\u7684\u80fd\u529b\u3002\u76f8\u53cd\u5730\uff0c\u76ee\u524d\u7684\u8a31\u591a\u5f71\u50cf\u7406\u89e3\u6a21\u578b\u7f3a\u4e4f\u50cf\u7d20\u5c64\u7d1a\u7684\u57fa\u790e\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u5f15\u5165\u591a\u5f71\u50cf\u50cf\u7d20\u57fa\u790e\u63a8\u7406\u5206\u5272\u7684\u4efb\u52d9\uff0c\u4ee5\u53ca PRIMA\uff0c\u4e00\u500b\u6574\u5408\u50cf\u7d20\u5c64\u7d1a\u57fa\u790e\u8207\u5f37\u5065\u7684\u591a\u5f71\u50cf\u63a8\u7406\u80fd\u529b\u7684\u65b0\u7a4e LVLM\uff0c\u4f86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u4ee5\u7522\u751f\u8108\u7d61\u8c50\u5bcc\u3001\u50cf\u7d20\u57fa\u790e\u7684\u89e3\u91cb\u3002PRIMA \u7684\u6838\u5fc3\u662f\u4e00\u500b\u9ad8\u6548\u7684\u8996\u89ba\u6a21\u7d44\uff0c\u5b83\u67e5\u8a62\u8de8\u591a\u500b\u5f71\u50cf\u7684\u7d30\u5fae\u8996\u89ba\u8868\u793a\uff0c\u5c07 TFLOP \u6e1b\u5c11\u4e86 25.3%\u3002\u70ba\u4e86\u652f\u63f4\u8a13\u7df4\u548c\u8a55\u4f30\uff0c\u6211\u5011\u7b56\u5283\u4e86 M4Seg\uff0c\u4e00\u500b\u65b0\u7684\u63a8\u7406\u5206\u5272\u57fa\u6e96\uff0c\u5305\u542b\u7d04 22.4 \u842c\u500b\u554f\u984c\u56de\u7b54\u5c0d\uff0c\u9019\u4e9b\u5c0d\u9700\u8981\u8de8\u591a\u500b\u5f71\u50cf\u7684\u7d30\u5fae\u8996\u89ba\u7406\u89e3\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e PRIMA \u512a\u65bc\u6700\u5148\u9032\u7684\u57fa\u6e96\u3002", "author": "Muntasir Wahed et.al.", "authors": "Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou", "id": "2412.15209v1", "paper_url": "http://arxiv.org/abs/2412.15209v1", "repo": "null"}}