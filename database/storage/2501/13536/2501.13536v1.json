{"2501.13536": {"publish_time": "2025-01-23", "title": "ReasVQA: Advancing VideoQA with Imperfect Reasoning Process", "paper_summary": "Video Question Answering (VideoQA) is a challenging task that requires\nunderstanding complex visual and temporal relationships within videos to answer\nquestions accurately. In this work, we introduce \\textbf{ReasVQA}\n(Reasoning-enhanced Video Question Answering), a novel approach that leverages\nreasoning processes generated by Multimodal Large Language Models (MLLMs) to\nimprove the performance of VideoQA models. Our approach consists of three\nphases: reasoning generation, reasoning refinement, and learning from\nreasoning. First, we generate detailed reasoning processes using additional\nMLLMs, and second refine them via a filtering step to ensure data quality.\nFinally, we use the reasoning data, which might be in an imperfect form, to\nguide the VideoQA model via multi-task learning, on how to interpret and answer\nquestions based on a given video. We evaluate ReasVQA on three popular\nbenchmarks, and our results establish new state-of-the-art performance with\nsignificant improvements of +2.9 on NExT-QA, +7.3 on STAR, and +5.9 on\nIntentQA. Our findings demonstrate the supervising benefits of integrating\nreasoning processes into VideoQA. Further studies validate each component of\nour method, also with different backbones and MLLMs, and again highlight the\nadvantages of this simple but effective method. We offer a new perspective on\nenhancing VideoQA performance by utilizing advanced reasoning techniques,\nsetting a new benchmark in this research field.", "paper_summary_zh": "\u5f71\u7247\u554f\u7b54 (VideoQA) \u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u9700\u8981\u4e86\u89e3\u5f71\u7247\u4e2d\u8907\u96dc\u7684\u8996\u89ba\u548c\u6642\u9593\u95dc\u4fc2\u624d\u80fd\u6e96\u78ba\u56de\u7b54\u554f\u984c\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 \\textbf{ReasVQA}\uff08\u63a8\u7406\u589e\u5f37\u5f71\u7247\u554f\u7b54\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u751f\u6210\u7684\u63a8\u7406\u7a0b\u5e8f\u4f86\u63d0\u5347 VideoQA \u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u542b\u4e09\u500b\u968e\u6bb5\uff1a\u63a8\u7406\u751f\u6210\u3001\u63a8\u7406\u7cbe\u7149\u548c\u5f9e\u63a8\u7406\u4e2d\u5b78\u7fd2\u3002\u9996\u5148\uff0c\u6211\u5011\u4f7f\u7528\u984d\u5916\u7684 MLLM \u751f\u6210\u8a73\u7d30\u7684\u63a8\u7406\u7a0b\u5e8f\uff0c\u7136\u5f8c\u900f\u904e\u904e\u6ffe\u6b65\u9a5f\u7cbe\u7149\u5b83\u5011\u4ee5\u78ba\u4fdd\u8cc7\u6599\u54c1\u8cea\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u63a8\u7406\u8cc7\u6599\uff08\u53ef\u80fd\u662f\u4e0d\u5b8c\u7f8e\u7684\u5f62\u5f0f\uff09\u900f\u904e\u591a\u4efb\u52d9\u5b78\u7fd2\u4f86\u5f15\u5c0e VideoQA \u6a21\u578b\uff0c\u4e86\u89e3\u5982\u4f55\u6839\u64da\u7d66\u5b9a\u7684\u5f71\u7247\u8a6e\u91cb\u548c\u56de\u7b54\u554f\u984c\u3002\u6211\u5011\u5728\u4e09\u500b\u71b1\u9580\u57fa\u6e96\u4e0a\u8a55\u4f30 ReasVQA\uff0c\u6211\u5011\u7684\u7d50\u679c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u9032\u6548\u80fd\uff0c\u5728 NExT-QA \u4e0a\u986f\u8457\u63d0\u5347 +2.9\u3001\u5728 STAR \u4e0a\u63d0\u5347 +7.3\uff0c\u4ee5\u53ca\u5728 IntentQA \u4e0a\u63d0\u5347 +5.9\u3002\u6211\u5011\u7684\u767c\u73fe\u8b49\u660e\u4e86\u5c07\u63a8\u7406\u7a0b\u5e8f\u6574\u5408\u5230 VideoQA \u4e2d\u7684\u76e3\u7763\u6548\u76ca\u3002\u9032\u4e00\u6b65\u7684\u7814\u7a76\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6bcf\u500b\u7d44\u6210\u90e8\u5206\uff0c\u4e5f\u9a57\u8b49\u4e86\u4e0d\u540c\u7684\u4e3b\u5e79\u548c MLLM\uff0c\u4e26\u518d\u6b21\u5f37\u8abf\u4e86\u9019\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u65b9\u6cd5\u7684\u512a\u9ede\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u5229\u7528\u5148\u9032\u63a8\u7406\u6280\u8853\u4f86\u63d0\u5347 VideoQA \u6548\u80fd\u7684\u65b0\u89c0\u9ede\uff0c\u70ba\u9019\u500b\u7814\u7a76\u9818\u57df\u8a2d\u5b9a\u4e86\u65b0\u7684\u57fa\u6e96\u3002", "author": "Jianxin Liang et.al.", "authors": "Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, Dongyan Zhao", "id": "2501.13536v1", "paper_url": "http://arxiv.org/abs/2501.13536v1", "repo": "null"}}