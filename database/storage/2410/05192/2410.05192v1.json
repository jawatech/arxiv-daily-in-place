{"2410.05192": {"publish_time": "2024-10-07", "title": "Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective", "paper_summary": "Training language models currently requires pre-determining a fixed compute\nbudget because the typical cosine learning rate schedule depends on the total\nnumber of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a\nconstant learning rate to produce a main branch of iterates that can in\nprinciple continue indefinitely without a pre-specified compute budget. Then,\ngiven any compute budget, one can branch out from the main branch at a proper\nat any time with a rapidly decaying learning rate to produce a strong model.\nEmpirically, WSD generates a non-traditional loss curve: the loss remains\nelevated during the stable phase but sharply declines during the decay phase.\nTowards explaining this phenomenon, we conjecture that pretraining loss\nexhibits a river valley landscape, which resembles a deep valley with a river\nat its bottom. Under this assumption, we show that during the stable phase, the\niterate undergoes large oscillations due to the high learning rate, yet it\nprogresses swiftly along the river. During the decay phase, the rapidly\ndropping learning rate minimizes the iterate's oscillations, moving it closer\nto the river and revealing true optimization progress. Therefore, the sustained\nhigh learning rate phase and fast decaying phase are responsible for progress\nin the river and the mountain directions respectively, and are both critical.\nOur analysis predicts phenomenons consistent with empirical observations and\nshows that this landscape can emerge from pretraining on a simple bi-gram\ndataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that\nreuses previous checkpoints' decay phases and keeps only one main branch, where\nwe resume from a decayed checkpoint. WSD-S empirically outperforms WSD and\nCyclic-Cosine in obtaining multiple language model checkpoints across various\ncompute budgets in a single run for parameters scaling from 0.1B to 1.2B.", "paper_summary_zh": "\u76ee\u524d\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u9700\u8981\u9810\u5148\u6c7a\u5b9a\u4e00\u500b\u56fa\u5b9a\u7684\u904b\u7b97\u9810\u7b97\uff0c\u56e0\u70ba\u5178\u578b\u7684\u9918\u5f26\u5b78\u7fd2\u7387\u6642\u7a0b\u53d6\u6c7a\u65bc\u6b65\u9a5f\u7684\u7e3d\u6578\u3002\u76f8\u53cd\u5730\uff0c\u71b1\u8eab-\u7a69\u5b9a-\u8870\u6e1b (WSD) \u6642\u7a0b\u4f7f\u7528\u4e00\u500b\u56fa\u5b9a\u7684\u5b78\u7fd2\u7387\u4f86\u7522\u751f\u4e00\u500b\u539f\u5247\u4e0a\u53ef\u4ee5\u7121\u9650\u6301\u7e8c\u800c\u7121\u9700\u9810\u5148\u6307\u5b9a\u904b\u7b97\u9810\u7b97\u7684\u8fed\u4ee3\u4e3b\u5206\u652f\u3002\u7136\u5f8c\uff0c\u5728\u7d66\u5b9a\u7684\u4efb\u4f55\u904b\u7b97\u9810\u7b97\u4e0b\uff0c\u4eba\u5011\u53ef\u4ee5\u5728\u9069\u7576\u7684\u4efb\u4f55\u6642\u9593\u5f9e\u4e3b\u5206\u652f\u5206\u652f\u51fa\u4e00\u500b\u5feb\u901f\u8870\u6e1b\u5b78\u7fd2\u7387\u4ee5\u7522\u751f\u4e00\u500b\u5f37\u5927\u7684\u6a21\u578b\u3002\u6839\u64da\u7d93\u9a57\uff0cWSD \u6703\u7522\u751f\u4e00\u500b\u975e\u50b3\u7d71\u7684\u640d\u5931\u66f2\u7dda\uff1a\u640d\u5931\u5728\u7a69\u5b9a\u968e\u6bb5\u4fdd\u6301\u5347\u9ad8\uff0c\u4f46\u5728\u8870\u6e1b\u968e\u6bb5\u6025\u5287\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u91cb\u9019\u7a2e\u73fe\u8c61\uff0c\u6211\u5011\u63a8\u6e2c\u9810\u8a13\u7df4\u640d\u5931\u8868\u73fe\u51fa\u4e00\u500b\u6cb3\u8c37\u666f\u89c0\uff0c\u5b83\u985e\u4f3c\u65bc\u4e00\u500b\u5e95\u90e8\u6709\u6cb3\u6d41\u7684\u6df1\u8c37\u3002\u5728\u9019\u500b\u5047\u8a2d\u4e0b\uff0c\u6211\u5011\u8868\u660e\u5728\u7a69\u5b9a\u968e\u6bb5\uff0c\u7531\u65bc\u9ad8\u5b78\u7fd2\u7387\uff0c\u8fed\u4ee3\u6703\u7d93\u6b77\u5927\u7684\u632f\u76ea\uff0c\u4f46\u5b83\u6cbf\u8457\u6cb3\u6d41\u8fc5\u901f\u63a8\u9032\u3002\u5728\u8870\u6e1b\u968e\u6bb5\uff0c\u5feb\u901f\u4e0b\u964d\u7684\u5b78\u7fd2\u7387\u6700\u5c0f\u5316\u8fed\u4ee3\u7684\u632f\u76ea\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u6cb3\u6d41\u4e26\u63ed\u793a\u771f\u6b63\u7684\u512a\u5316\u9032\u5ea6\u3002\u56e0\u6b64\uff0c\u6301\u7e8c\u7684\u9ad8\u5b78\u7fd2\u7387\u968e\u6bb5\u548c\u5feb\u901f\u8870\u6e1b\u968e\u6bb5\u5206\u5225\u8ca0\u8cac\u6cb3\u6d41\u548c\u5c71\u8108\u65b9\u5411\u7684\u9032\u5ea6\uff0c\u4e26\u4e14\u5169\u8005\u90fd\u5f88\u91cd\u8981\u3002\u6211\u5011\u7684\u5206\u6790\u9810\u6e2c\u4e86\u8207\u7d93\u9a57\u89c0\u5bdf\u4e00\u81f4\u7684\u73fe\u8c61\uff0c\u4e26\u8868\u660e\u9019\u7a2e\u666f\u89c0\u53ef\u4ee5\u5f9e\u4e00\u500b\u7c21\u55ae\u7684\u4e8c\u5143\u8a9e\u6599\u5eab\u7684\u9810\u8a13\u7df4\u4e2d\u51fa\u73fe\u3002\u53d7\u5230\u9019\u500b\u7406\u8ad6\u7684\u555f\u767c\uff0c\u6211\u5011\u5f15\u5165\u4e86 WSD-S\uff0c\u5b83\u662f WSD \u7684\u4e00\u500b\u8b8a\u9ad4\uff0c\u5b83\u91cd\u8907\u4f7f\u7528\u5148\u524d\u6aa2\u67e5\u9ede\u7684\u8870\u6e1b\u968e\u6bb5\u4e26\u53ea\u4fdd\u7559\u4e00\u500b\u4e3b\u5206\u652f\uff0c\u6211\u5011\u5f9e\u4e00\u500b\u8870\u6e1b\u7684\u6aa2\u67e5\u9ede\u6062\u5fa9\u3002WSD-S \u5728\u55ae\u6b21\u904b\u7b97\u4e2d\u8de8\u8d8a\u5404\u7a2e\u904b\u7b97\u9810\u7b97\u7372\u5f97\u591a\u500b\u8a9e\u8a00\u6a21\u578b\u6aa2\u67e5\u9ede\u65b9\u9762\uff0c\u5728\u7d93\u9a57\u4e0a\u512a\u65bc WSD \u548c\u5faa\u74b0\u9918\u5f26\uff0c\u53c3\u6578\u7bc4\u570d\u5f9e 0.1B \u5230 1.2B\u3002", "author": "Kaiyue Wen et.al.", "authors": "Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, Tengyu Ma", "id": "2410.05192v1", "paper_url": "http://arxiv.org/abs/2410.05192v1", "repo": "null"}}