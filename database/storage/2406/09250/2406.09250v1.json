{"2406.09250": {"publish_time": "2024-06-13", "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models", "paper_summary": "Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u6b63\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u653b\u64ca\uff0c\u56e0\u70ba\u91dd\u5c0d\u9019\u4e9b\u6a21\u578b\u63d0\u51fa\u4e86\u5404\u7a2e\u65b0\u7a4e\u7684\u653b\u64ca\u7b56\u7565\u3002\u96d6\u7136\u73fe\u6709\u7684\u9632\u79a6\u63aa\u65bd\u5728\u55ae\u6a21\u614b\u74b0\u5883\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b83\u5011\u76ee\u524d\u7121\u6cd5\u4fdd\u8b77 VLM \u514d\u53d7\u5c0d\u6297\u6027\u5a01\u8105\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u7a2e\u6f0f\u6d1e\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u4f46\u512a\u96c5\u7c21\u6f54\u7684\u65b9\u6cd5\u4f86\u5728 VLM \u4e2d\u5075\u6e2c\u5c0d\u6297\u6027\u6a23\u672c\u3002\u6211\u5011\u7684\u6a21\u578b\u5229\u7528\u6587\u5b57\u5230\u5f71\u50cf (T2I) \u6a21\u578b\u6839\u64da\u76ee\u6a19 VLM \u7522\u751f\u7684\u6a19\u984c\u4f86\u7522\u751f\u5f71\u50cf\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u8a08\u7b97\u7279\u5fb5\u7a7a\u9593\u4e2d\u8f38\u5165\u5f71\u50cf\u548c\u7522\u751f\u5f71\u50cf\u7684\u5d4c\u5165\u76f8\u4f3c\u6027\uff0c\u4ee5\u8b58\u5225\u5c0d\u6297\u6027\u6a23\u672c\u3002\u5728\u4e0d\u540c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u7d93\u9a57\u8a55\u4f30\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6548\u80fd\uff0c\u512a\u65bc\u5f9e\u5f71\u50cf\u5206\u985e\u9818\u57df\u6539\u7de8\u7684\u57fa\u6e96\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u6a21\u578b\u5ef6\u4f38\u5230\u5206\u985e\u4efb\u52d9\uff0c\u5c55\u793a\u5176\u9069\u61c9\u6027\u548c\u8207\u6a21\u578b\u7121\u95dc\u7684\u7279\u6027\u3002\u7406\u8ad6\u5206\u6790\u548c\u7d93\u9a57\u767c\u73fe\u4e5f\u986f\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5c0d\u6297\u9069\u61c9\u6027\u653b\u64ca\u7684\u97cc\u6027\uff0c\u4f7f\u5176\u6210\u70ba\u5c0d\u6297\u73fe\u5be6\u4e16\u754c\u5c0d\u6297\u6027\u5a01\u8105\u7684\u7d55\u4f73\u9632\u79a6\u6a5f\u5236\u3002", "author": "Samar Fares et.al.", "authors": "Samar Fares, Klea Ziu, Toluwani Aremu, Nikita Durasov, Martin Tak\u00e1\u010d, Pascal Fua, Karthik Nandakumar, Ivan Laptev", "id": "2406.09250v1", "paper_url": "http://arxiv.org/abs/2406.09250v1", "repo": "null"}}