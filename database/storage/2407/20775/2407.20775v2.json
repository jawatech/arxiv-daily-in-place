{"2407.20775": {"publish_time": "2024-07-30", "title": "Interpretable Pre-Trained Transformers for Heart Time-Series Data", "paper_summary": "Decoder-only transformers are the backbone of the popular generative\npre-trained transformer (GPT) series of large language models. In this work, we\nemploy this framework to the analysis of clinical heart time-series data, to\ncreate two pre-trained general purpose cardiac models, termed PPG-PT and\nECG-PT. We place a special emphasis on making both such pre-trained models\nfully interpretable. This is achieved firstly through aggregate attention maps\nwhich show that, in order to make predictions, the model focuses on similar\npoints in previous cardiac cycles and gradually broadens its attention in\ndeeper layers. Next, we show that tokens with the same value, which occur at\ndifferent distinct points in the electrocardiography (ECG) and\nphotoplethysmography (PPG) cycle, form separate clusters in high dimensional\nspace. The clusters form according to phase, as the tokens propagate through\nthe transformer blocks. Finally, we highlight that individual attention heads\nrespond to specific physiologically relevent features, such as the dicrotic\nnotch in PPG and the P-wave in ECG. It is also demonstrated that these\npre-trained models are straightforward to fine-tune for tasks such as\nclassification of atrial fibrillation (AF), and beat detection in\nphotoplethysmography. For the example of AF, the fine-tuning took 11 minutes of\ncomputer time, and achieved the respective leave-one-subject-out AUCs of 0.99\nand 0.93 for ECG and PPG within the MIMIC Perform AF dataset. In addition, the\nfine-tuned beat detector achieved a state-of-the-art F1 score of 98%, as well\nas uniquely providing a beat confidence level which acts as a signal quality\nestimator. Importantly, the fine-tuned models for AF screening are also fully\nexplainable, with attention shifting to regions in the context that are\nstrongly indicative of atrial fibrillation.", "paper_summary_zh": "<paragraph>\u50c5\u89e3\u78bc\u5668Transformer\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u71b1\u9580\u751f\u6210\u5f0f\u9810\u8a13\u7df4Transformer (GPT) \u7cfb\u5217\u7684\u6838\u5fc3\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c07\u6b64\u67b6\u69cb\u61c9\u7528\u65bc\u81e8\u5e8a\u5fc3\u81df\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u5206\u6790\uff0c\u4ee5\u5efa\u7acb\u5169\u500b\u9810\u8a13\u7df4\u901a\u7528\u5fc3\u81df\u6a21\u578b\uff0c\u7a31\u70ba PPG-PT \u548c ECG-PT\u3002\u6211\u5011\u7279\u5225\u5f37\u8abf\u8b93\u9019\u5169\u500b\u9810\u8a13\u7df4\u6a21\u578b\u5b8c\u5168\u53ef\u89e3\u91cb\u3002\u9019\u9996\u5148\u662f\u900f\u904e\u7e3d\u9ad4\u6ce8\u610f\u5716\u5be6\u73fe\u7684\uff0c\u5b83\u986f\u793a\u6a21\u578b\u70ba\u4e86\u505a\u51fa\u9810\u6e2c\uff0c\u6703\u5c08\u6ce8\u65bc\u5148\u524d\u5fc3\u81df\u9031\u671f\u4e2d\u7684\u985e\u4f3c\u9ede\uff0c\u4e26\u5728\u66f4\u6df1\u5c64\u7684\u5c64\u7d1a\u9010\u6f38\u64f4\u5927\u5176\u6ce8\u610f\u7bc4\u570d\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u986f\u793a\u5728\u5fc3\u96fb\u5716 (ECG) \u548c\u5149\u96fb\u5bb9\u7a4d\u63cf\u8a18\u6cd5 (PPG) \u9031\u671f\u4e2d\u65bc\u4e0d\u540c\u7279\u5b9a\u9ede\u51fa\u73fe\u7684\u5177\u6709\u76f8\u540c\u503c\u7684\u4ee3\u5e63\uff0c\u6703\u5728\u9ad8\u7dad\u7a7a\u9593\u4e2d\u5f62\u6210\u7368\u7acb\u7684\u7fa4\u96c6\u3002\u7fa4\u96c6\u6703\u6839\u64da\u76f8\u4f4d\u5f62\u6210\uff0c\u56e0\u70ba\u4ee3\u5e63\u6703\u900f\u904eTransformer\u5340\u584a\u50b3\u64ad\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5f37\u8abf\u500b\u5225\u6ce8\u610f\u6b0a\u91cd\u6703\u5c0d\u7279\u5b9a\u7684\u751f\u7406\u76f8\u95dc\u7279\u5fb5\u505a\u51fa\u56de\u61c9\uff0c\u4f8b\u5982 PPG \u4e2d\u7684\u4e8c\u5c16\u7f3a\u53e3\u548c ECG \u4e2d\u7684 P \u6ce2\u3002\u4e5f\u5df2\u8b49\u5be6\u9019\u4e9b\u9810\u8a13\u7df4\u6a21\u578b\u5f88\u5bb9\u6613\u91dd\u5c0d\u4efb\u52d9\u9032\u884c\u5fae\u8abf\uff0c\u4f8b\u5982\u5fc3\u623f\u986b\u52d5 (AF) \u5206\u985e\u548c\u5149\u96fb\u5bb9\u7a4d\u63cf\u8a18\u6cd5\u4e2d\u7684\u7bc0\u62cd\u5075\u6e2c\u3002\u4ee5 AF \u70ba\u4f8b\uff0c\u5fae\u8abf\u8017\u6642 11 \u5206\u9418\u7684\u96fb\u8166\u6642\u9593\uff0c\u4e26\u5728 MIMIC Perform AF \u8cc7\u6599\u96c6\u4e2d\u5206\u5225\u9054\u6210 ECG \u548c PPG \u7684\u7559\u4e00\u6cd5 AUC \u70ba 0.99 \u548c 0.93\u3002\u6b64\u5916\uff0c\u5fae\u8abf\u7bc0\u62cd\u5075\u6e2c\u5668\u9054\u5230\u4e86 98% \u7684\u6700\u65b0 F1 \u5206\u6578\uff0c\u4e26\u7368\u7279\u5730\u63d0\u4f9b\u4e86\u4e00\u500b\u7bc0\u62cd\u4fe1\u5fc3\u7b49\u7d1a\uff0c\u4f5c\u70ba\u8a0a\u865f\u54c1\u8cea\u4f30\u8a08\u5668\u3002\u91cd\u8981\u7684\u662f\uff0c\u91dd\u5c0d AF \u7be9\u6aa2\u9032\u884c\u5fae\u8abf\u7684\u6a21\u578b\u4e5f\u5b8c\u5168\u53ef\u89e3\u91cb\uff0c\u6ce8\u610f\u529b\u6703\u8f49\u79fb\u5230\u6587\u4e2d\u5f37\u70c8\u6307\u793a\u5fc3\u623f\u986b\u52d5\u7684\u5340\u57df\u3002</paragraph>", "author": "Harry J. Davies et.al.", "authors": "Harry J. Davies, James Monsen, Danilo P. Mandic", "id": "2407.20775v2", "paper_url": "http://arxiv.org/abs/2407.20775v2", "repo": "https://github.com/harryjdavies/heartgpt"}}