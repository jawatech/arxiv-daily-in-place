{"2501.13302": {"publish_time": "2025-01-23", "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers", "paper_summary": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models.", "paper_summary_zh": "AI \u5b89\u5168\u5be9\u6838 (ASM) \u5206\u985e\u5668\u65e8\u5728\u5be9\u6838\u793e\u7fa4\u5a92\u9ad4\u5e73\u53f0\u4e0a\u7684\u5167\u5bb9\uff0c\u4e26\u4f5c\u70ba\u8b77\u6b04\uff0c\u9632\u6b62\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u4e0d\u5b89\u5168\u7684\u8f38\u5165\u4e0a\u9032\u884c\u5fae\u8abf\u3002\u7531\u65bc\u5b83\u5011\u5177\u6709\u7522\u751f\u4e0d\u540c\u5f71\u97ff\u7684\u6f5b\u529b\uff0c\u56e0\u6b64\u78ba\u4fdd\u9019\u4e9b\u5206\u985e\u5668\u81f3\u95dc\u91cd\u8981\uff1a(1) \u8207\u591a\u6578\u7fa4\u9ad4\u76f8\u6bd4\uff0c\u4e0d\u6703\u4e0d\u516c\u5e73\u5730\u5c07\u5c11\u6578\u7fa4\u9ad4\u4f7f\u7528\u8005\u7684\u5167\u5bb9\u5206\u985e\u70ba\u4e0d\u5b89\u5168\uff0c(2) \u5b83\u5011\u7684\u884c\u70ba\u5728\u985e\u4f3c\u7684\u8f38\u5165\u4e2d\u4fdd\u6301\u7a69\u5065\u4e14\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u6aa2\u67e5\u4e86\u56db\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u9589\u6e90 ASM \u5206\u985e\u5668\u7684\u516c\u5e73\u6027\u548c\u7a69\u5065\u6027\uff1aOpenAI Moderation API\u3001Perspective API\u3001Google Cloud Natural Language (GCNL) API \u548c Clarifai API\u3002\u6211\u5011\u4f7f\u7528\u4eba\u53e3\u7d71\u8a08\u540c\u8cea\u6027\u548c\u689d\u4ef6\u7d71\u8a08\u540c\u8cea\u6027\u7b49\u6307\u6a19\u8a55\u4f30\u516c\u5e73\u6027\uff0c\u4e26\u5c07\u5176\u6548\u80fd\u8207 ASM \u6a21\u578b\u548c\u50c5\u516c\u5e73\u7684\u57fa\u6e96\u9032\u884c\u6bd4\u8f03\u3002\u6b64\u5916\uff0c\u6211\u5011\u901a\u904e\u6e2c\u8a66\u5206\u985e\u5668\u5c0d\u5c0f\u578b\u4e14\u81ea\u7136\u7684\u8f38\u5165\u64fe\u52d5\u7684\u654f\u611f\u6027\u4f86\u5206\u6790\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u6f5b\u5728\u7684\u516c\u5e73\u6027\u548c\u7a69\u5065\u6027\u5dee\u8ddd\uff0c\u7a81\u986f\u4e86\u5728\u9019\u4e9b\u6a21\u578b\u7684\u672a\u4f86\u7248\u672c\u4e2d\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\u7684\u5fc5\u8981\u6027\u3002", "author": "Akshit Achara et.al.", "authors": "Akshit Achara, Anshuman Chhabra", "id": "2501.13302v1", "paper_url": "http://arxiv.org/abs/2501.13302v1", "repo": "null"}}