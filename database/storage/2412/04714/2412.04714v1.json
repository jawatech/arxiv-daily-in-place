{"2412.04714": {"publish_time": "2024-12-06", "title": "PCTreeS: 3D Point Cloud Tree Species Classification Using Airborne LiDAR Images", "paper_summary": "Reliable large-scale data on the state of forests is crucial for monitoring\necosystem health, carbon stock, and the impact of climate change. Current\nknowledge of tree species distribution relies heavily on manual data collection\nin the field, which often takes years to complete, resulting in limited\ndatasets that cover only a small subset of the world's forests. Recent works\nshow that state-of-the-art deep learning models using Light Detection and\nRanging (LiDAR) images enable accurate and scalable classification of tree\nspecies in various ecosystems. While LiDAR images contain rich 3D information,\nmost previous works flatten the 3D images into 2D projections to use\nConvolutional Neural Networks (CNNs). This paper offers three significant\ncontributions: (1) we apply the deep learning framework for tree classification\nin tropical savannas; (2) we use Airborne LiDAR images, which have a lower\nresolution but greater scalability than Terrestrial LiDAR images used in most\nprevious works; (3) we introduce the approach of directly feeding 3D point\ncloud images into a vision transformer model (PCTreeS). Our results show that\nthe PCTreeS approach outperforms current CNN baselines with 2D projections in\nAUC (0.81), overall accuracy (0.72), and training time (~45 mins). This paper\nalso motivates further LiDAR image collection and validation for accurate\nlarge-scale automatic classification of tree species.", "paper_summary_zh": "\u53ef\u9760\u7684\u5927\u898f\u6a21\u68ee\u6797\u72c0\u614b\u8cc7\u6599\u5c0d\u65bc\u76e3\u6e2c\u751f\u614b\u7cfb\u7d71\u5065\u5eb7\u3001\u78b3\u5132\u91cf\u548c\u6c23\u5019\u8b8a\u9077\u7684\u5f71\u97ff\u81f3\u95dc\u91cd\u8981\u3002\u76ee\u524d\u5c0d\u6a39\u7a2e\u5206\u5e03\u7684\u4e86\u89e3\u6975\u5ea6\u4f9d\u8cf4\u65bc\u5be6\u5730\u624b\u52d5\u6536\u96c6\u8cc7\u6599\uff0c\u9019\u901a\u5e38\u9700\u8981\u82b1\u8cbb\u6578\u5e74\u624d\u80fd\u5b8c\u6210\uff0c\u5c0e\u81f4\u53ea\u80fd\u6db5\u84cb\u5168\u7403\u5c11\u6578\u68ee\u6797\u7684\u6709\u9650\u8cc7\u6599\u96c6\u3002\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0c\u4f7f\u7528\u5149\u63a2\u6e2c\u548c\u6e2c\u8ddd (LiDAR) \u5f71\u50cf\u7684\u6700\u65b0\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u5404\u7a2e\u751f\u614b\u7cfb\u7d71\u4e2d\u5c0d\u6a39\u7a2e\u9032\u884c\u6e96\u78ba\u4e14\u53ef\u64f4\u5145\u7684\u5206\u985e\u3002\u5118\u7ba1 LiDAR \u5f71\u50cf\u5305\u542b\u8c50\u5bcc\u7684 3D \u8cc7\u8a0a\uff0c\u4f46\u5927\u591a\u6578\u5148\u524d\u7684\u7814\u7a76\u6703\u5c07 3D \u5f71\u50cf\u58d3\u7e2e\u6210 2D \u6295\u5f71\uff0c\u4ee5\u4f7f\u7528\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN)\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u4e09\u9805\u91cd\u8981\u7684\u8ca2\u737b\uff1a(1) \u6211\u5011\u5c07\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u61c9\u7528\u65bc\u71b1\u5e36\u7a00\u6a39\u8349\u539f\u7684\u6a39\u7a2e\u5206\u985e\uff1b(2) \u6211\u5011\u4f7f\u7528\u6a5f\u8f09 LiDAR \u5f71\u50cf\uff0c\u5176\u89e3\u6790\u5ea6\u8f03\u4f4e\uff0c\u4f46\u53ef\u64f4\u5145\u6027\u6bd4\u5927\u591a\u6578\u5148\u524d\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u5730\u9762 LiDAR \u5f71\u50cf\u66f4\u9ad8\uff1b(3) \u6211\u5011\u5f15\u5165\u4e86\u76f4\u63a5\u5c07 3D \u9ede\u96f2\u5f71\u50cf\u8f38\u5165\u5230\u8996\u89baTransformer\u6a21\u578b (PCTreeS) \u7684\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0cPCTreeS \u65b9\u6cd5\u5728 AUC (0.81)\u3001\u6574\u9ad4\u6e96\u78ba\u5ea6 (0.72) \u548c\u8a13\u7df4\u6642\u9593 (~45 \u5206\u9418) \u65b9\u9762\u512a\u65bc\u7576\u524d\u4f7f\u7528 2D \u6295\u5f71\u7684 CNN \u57fa\u6e96\u3002\u672c\u6587\u4e5f\u6fc0\u52f5\u9032\u4e00\u6b65\u6536\u96c6\u548c\u9a57\u8b49 LiDAR \u5f71\u50cf\uff0c\u4ee5\u9032\u884c\u6e96\u78ba\u7684\u5927\u898f\u6a21\u6a39\u7a2e\u81ea\u52d5\u5206\u985e\u3002", "author": "Hongjin Lin et.al.", "authors": "Hongjin Lin, Matthew Nazari, Derek Zheng", "id": "2412.04714v1", "paper_url": "http://arxiv.org/abs/2412.04714v1", "repo": "null"}}