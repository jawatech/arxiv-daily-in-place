{"2407.10817": {"publish_time": "2024-07-15", "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "paper_summary": "As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9032\u6b65\uff0c\u7531\u65bc\u4eba\u5de5\u8a55\u4f30\u6210\u672c\u9ad8\u6602\uff0c\u53ef\u9760\u8a55\u4f30\u5176\u8f38\u51fa\u7684\u96e3\u5ea6\u4e5f\u96a8\u4e4b\u589e\u52a0\u3002\u70ba\u4e86\u5728\u6539\u5584 LLM \u81ea\u52d5\u8a55\u5206\u5668\u65b9\u9762\u53d6\u5f97\u9032\u5c55\uff0c\u6211\u5011\u63a8\u51fa\u4e86 FLAMe\uff0c\u9019\u662f\u4e00\u500b\u57fa\u790e\u5927\u578b\u81ea\u52d5\u8a55\u5206\u5668\u6a21\u578b\u7cfb\u5217\u3002FLAMe \u7d93\u904e\u6211\u5011\u9f90\u5927\u4e14\u591a\u6a23\u5316\u7684 100 \u591a\u9805\u54c1\u8cea\u8a55\u4f30\u4efb\u52d9\u8a13\u7df4\uff0c\u5305\u542b 500 \u591a\u842c\u500b\u4eba\u985e\u5224\u65b7\uff0c\u4e26\u4f7f\u7528\u5148\u524d\u7814\u7a76\u4e2d\u516c\u958b\u767c\u5e03\u7684\u4eba\u985e\u8a55\u4f30\u9032\u884c\u6574\u7406\u548c\u6a19\u6e96\u5316\u3002FLAMe \u5927\u5e45\u63d0\u5347\u4e86\u5c0d\u5404\u7a2e\u4fdd\u7559\u4efb\u52d9\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u8a31\u591a\u4efb\u52d9\u4e0a\u512a\u65bc\u8a13\u7df4\u65bc\u5c08\u6709\u6578\u64da\uff08\u4f8b\u5982 GPT-4 \u548c Claude-3\uff09\u7684 LLM\u3002\u6211\u5011\u5c55\u793a FLAMe \u4e5f\u53ef\u4ee5\u4f5c\u70ba\u9032\u4e00\u6b65\u4e0b\u6e38\u5fae\u8abf\u7684\u6709\u529b\u8d77\u9ede\uff0c\u4f7f\u7528\u734e\u52f5\u5efa\u6a21\u8a55\u4f30\u4f5c\u70ba\u6848\u4f8b\u7814\u7a76 (FLAMe-RM)\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 RewardBench \u4e0a\uff0c\u6211\u5011\u7684 FLAMe-RM-24B \u6a21\u578b\uff08\u6e96\u78ba\u7387\u70ba 87.8%\uff09\u662f\u7d93\u904e\u8a31\u53ef\u6578\u64da\u8a13\u7df4\u7684\u8868\u73fe\u6700\u4f73\u7684\u751f\u6210\u6a21\u578b\uff0c\u512a\u65bc GPT-4-0125 (85.9%) \u548c GPT-4o (84.7%)\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u7d22\u4e86\u4e00\u7a2e\u4f7f\u7528\u65b0\u7a4e\u7684\u5c3e\u90e8\u4fee\u88dc\u5fae\u8abf\u7b56\u7565\u7684\u66f4\u5177\u904b\u7b97\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u4ee5\u512a\u5316\u6211\u5011\u7684 FLAMe \u591a\u4efb\u52d9\u6df7\u5408\u9032\u884c\u734e\u52f5\u5efa\u6a21\u8a55\u4f30 (FLAMe-Opt-RM)\uff0c\u5728\u9700\u8981\u5927\u7d04\u5c11 25 \u500d\u7684\u8a13\u7df4\u6578\u64da\u9ede\u7684\u540c\u6642\uff0c\u63d0\u4f9b\u6709\u7af6\u722d\u529b\u7684 RewardBench \u6027\u80fd\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u6211\u5011\u7684 FLAMe \u8b8a\u9ad4\u5728 12 \u500b\u81ea\u52d5\u8a55\u5206\u5668\u8a55\u4f30\u57fa\u6e96\u4e2d\u7684 8 \u500b\u57fa\u6e96\u4e0a\u512a\u65bc\u6240\u6709\u6211\u5011\u8003\u616e\u7684\u6d41\u884c\u5c08\u6709 LLM \u4f5c\u70ba\u8a55\u5206\u5668\u6a21\u578b\uff0c\u5305\u542b 53 \u9805\u54c1\u8cea\u8a55\u4f30\u4efb\u52d9\uff0c\u5305\u62ec RewardBench \u548c LLM-AggreFact\u3002\u6700\u5f8c\uff0c\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u5728 CoBBLEr \u81ea\u52d5\u8a55\u5206\u5668\u504f\u5dee\u57fa\u6e96\u4e0a\uff0cFLAMe \u7684\u504f\u5dee\u986f\u8457\u4f4e\u65bc\u9019\u4e9b LLM \u4f5c\u70ba\u8a55\u5206\u5668\u6a21\u578b\uff0c\u540c\u6642\u6709\u6548\u5730\u8b58\u5225\u51fa\u7a0b\u5f0f\u78bc\u751f\u6210\u7684\u9ad8\u54c1\u8cea\u56de\u61c9\u3002", "author": "Tu Vu et.al.", "authors": "Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung", "id": "2407.10817v1", "paper_url": "http://arxiv.org/abs/2407.10817v1", "repo": "null"}}