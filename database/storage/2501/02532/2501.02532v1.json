{"2501.02532": {"publish_time": "2025-01-05", "title": "Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm", "paper_summary": "In the era of rapid digital communication, vast amounts of textual data are\ngenerated daily, demanding efficient methods for latent content analysis to\nextract meaningful insights. Large Language Models (LLMs) offer potential for\nautomating this process, yet comprehensive assessments comparing their\nperformance to human annotators across multiple dimensions are lacking. This\nstudy evaluates the reliability, consistency, and quality of seven\nstate-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and\nMixtral, relative to human annotators in analyzing sentiment, political\nleaning, emotional intensity, and sarcasm detection. A total of 33 human\nannotators and eight LLM variants assessed 100 curated textual items,\ngenerating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across\nthree time points to examine temporal consistency. Inter-rater reliability was\nmeasured using Krippendorff's alpha, and intra-class correlation coefficients\nassessed consistency over time. The results reveal that both humans and LLMs\nexhibit high reliability in sentiment analysis and political leaning\nassessments, with LLMs demonstrating higher internal consistency than humans.\nIn emotional intensity, LLMs displayed higher agreement compared to humans,\nthough humans rated emotional intensity significantly higher. Both groups\nstruggled with sarcasm detection, evidenced by low agreement. LLMs showed\nexcellent temporal consistency across all dimensions, indicating stable\nperformance over time. This research concludes that LLMs, especially GPT-4, can\neffectively replicate human analysis in sentiment and political leaning,\nalthough human expertise remains essential for emotional intensity\ninterpretation. The findings demonstrate the potential of LLMs for consistent\nand high-quality performance in certain areas of latent content analysis.", "paper_summary_zh": "<paragraph>\u5728\u5feb\u901f\u6578\u4f4d\u901a\u8a0a\u7684\u6642\u4ee3\uff0c\u6bcf\u5929\u7522\u751f\u5927\u91cf\u6587\u5b57\u8cc7\u6599\uff0c\u9700\u8981\u6709\u6548\u7387\u7684\u6f5b\u5728\u5167\u5bb9\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u8403\u53d6\u6709\u610f\u7fa9\u7684\u898b\u89e3\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63d0\u4f9b\u81ea\u52d5\u5316\u6b64\u7a0b\u5e8f\u7684\u6f5b\u529b\uff0c\u4f46\u7f3a\u4e4f\u7d9c\u5408\u8a55\u91cf\uff0c\u6bd4\u8f03\u5b83\u5011\u5728\u591a\u9762\u5411\u8868\u73fe\u8207\u4eba\u985e\u8a3b\u89e3\u8005\u3002\u6b64\u7814\u7a76\u8a55\u4f30\u4e03\u7a2e\u6700\u5148\u9032 LLM \u7684\u53ef\u9760\u6027\u3001\u4e00\u81f4\u6027\u548c\u54c1\u8cea\uff0c\u5305\u62ec OpenAI \u7684 GPT-4\u3001Gemini\u3001Llama \u548c Mixtral\uff0c\u76f8\u8f03\u65bc\u4eba\u985e\u8a3b\u89e3\u8005\u5728\u5206\u6790\u60c5\u7dd2\u3001\u653f\u6cbb\u50be\u5411\u3001\u60c5\u7dd2\u5f37\u5ea6\u548c\u8af7\u523a\u5075\u6e2c\u3002\u7e3d\u8a08 33 \u4f4d\u4eba\u985e\u8a3b\u89e3\u8005\u548c\u516b\u7a2e LLM \u8b8a\u9ad4\u8a55\u4f30 100 \u9805\u7b56\u5283\u7684\u6587\u5b57\u9805\u76ee\uff0c\u7522\u751f 3,300 \u9805\u4eba\u985e\u8a3b\u89e3\u548c 19,200 \u9805 LLM \u8a3b\u89e3\uff0c\u5728\u4e09\u500b\u6642\u9593\u9ede\u8a55\u4f30 LLM\uff0c\u4ee5\u6aa2\u8996\u6642\u9593\u4e00\u81f4\u6027\u3002\u4f7f\u7528 Krippendorff's alpha \u8861\u91cf\u8a55\u5206\u8005\u9593\u4fe1\u5ea6\uff0c\u4e26\u4f7f\u7528\u985e\u5167\u76f8\u95dc\u4fc2\u6578\u8a55\u4f30\u6642\u9593\u4e00\u81f4\u6027\u3002\u7d50\u679c\u986f\u793a\uff0c\u4eba\u985e\u548c LLM \u5728\u60c5\u7dd2\u5206\u6790\u548c\u653f\u6cbb\u50be\u5411\u8a55\u4f30\u4e2d\u90fd\u5c55\u73fe\u9ad8\u4fe1\u5ea6\uff0cLLM \u986f\u793a\u51fa\u6bd4\u4eba\u985e\u66f4\u9ad8\u7684\u5167\u90e8\u4e00\u81f4\u6027\u3002\u5728\u60c5\u7dd2\u5f37\u5ea6\u65b9\u9762\uff0cLLM \u986f\u793a\u51fa\u6bd4\u4eba\u985e\u66f4\u9ad8\u7684\u5171\u8b58\uff0c\u5118\u7ba1\u4eba\u985e\u5c0d\u60c5\u7dd2\u5f37\u5ea6\u7684\u8a55\u5206\u986f\u8457\u66f4\u9ad8\u3002\u5169\u7d44\u5728\u8af7\u523a\u5075\u6e2c\u65b9\u9762\u90fd\u9762\u81e8\u56f0\u96e3\uff0c\u9019\u7531\u4f4e\u5171\u8b58\u6240\u8b49\u5be6\u3002LLM \u5728\u6240\u6709\u9762\u5411\u90fd\u986f\u793a\u51fa\u6975\u4f73\u7684\u6642\u9593\u4e00\u81f4\u6027\uff0c\u8868\u793a\u96a8\u8457\u6642\u9593\u63a8\u79fb\uff0c\u8868\u73fe\u7a69\u5b9a\u3002\u672c\u7814\u7a76\u7d50\u8ad6\u662f\uff0cLLM\uff0c\u7279\u5225\u662f GPT-4\uff0c\u53ef\u4ee5\u5728\u60c5\u7dd2\u548c\u653f\u6cbb\u50be\u5411\u6709\u6548\u8907\u88fd\u4eba\u985e\u5206\u6790\uff0c\u5118\u7ba1\u4eba\u985e\u5c08\u5bb6\u77e5\u8b58\u5c0d\u65bc\u60c5\u7dd2\u5f37\u5ea6\u8a6e\u91cb\u4ecd\u7136\u81f3\u95dc\u91cd\u8981\u3002\u9019\u4e9b\u767c\u73fe\u8b49\u5be6\u4e86 LLM \u5728\u6f5b\u5728\u5167\u5bb9\u5206\u6790\u7279\u5b9a\u9818\u57df\u5177\u6709\u4e00\u81f4\u4e14\u9ad8\u54c1\u8cea\u8868\u73fe\u7684\u6f5b\u529b\u3002</paragraph>", "author": "Ljubisa Bojic et.al.", "authors": "Ljubisa Bojic, Olga Zagovora, Asta Zelenkauskaite, Vuk Vukovic, Milan Cabarkapa, Selma Veseljevi\u0107 Jerkovic, Ana Jovan\u010devic", "id": "2501.02532v1", "paper_url": "http://arxiv.org/abs/2501.02532v1", "repo": "null"}}