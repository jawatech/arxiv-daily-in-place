{"2407.21770": {"publish_time": "2024-07-31", "title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts", "paper_summary": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39 MoMa\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u6a21\u614b\u611f\u77e5\u6df7\u5408\u5c08\u5bb6 (MoE) \u67b6\u69cb\uff0c\u5c08\u70ba\u6df7\u5408\u6a21\u614b\u3001\u65e9\u671f\u878d\u5408\u8a9e\u8a00\u6a21\u578b\u7684\u9810\u8a13\u7df4\u800c\u8a2d\u8a08\u3002MoMa \u900f\u904e\u5c07\u5c08\u5bb6\u6a21\u7d44\u5206\u70ba\u6a21\u614b\u7279\u5b9a\u7fa4\u7d44\uff0c\u4ee5\u4efb\u610f\u9806\u5e8f\u8655\u7406\u5f71\u50cf\u548c\u6587\u5b57\u3002\u9019\u4e9b\u7fa4\u7d44\u6703\u7368\u81ea\u8655\u7406\u6307\u5b9a\u7684\u4ee3\u78bc\uff0c\u540c\u6642\u5728\u6bcf\u500b\u7fa4\u7d44\u5167\u4f7f\u7528\u5df2\u5b78\u7fd2\u7684\u8def\u7531\uff0c\u4ee5\u7dad\u6301\u8a9e\u7fa9\u9069\u61c9\u6027\u3002\u6211\u5011\u7684\u5be6\u8b49\u7d50\u679c\u986f\u793a\uff0c\u900f\u904e\u9019\u7a2e\u6a21\u614b\u7279\u5b9a\u53c3\u6578\u914d\u7f6e\uff0c\u53ef\u5927\u5e45\u63d0\u5347\u9810\u8a13\u7df4\u6548\u7387\u3002\u5728 1 \u5146\u500b\u4ee3\u78bc\u7684\u8a13\u7df4\u9810\u7b97\u4e0b\uff0cMoMa 1.4B \u6a21\u578b\u914d\u5099 4 \u500b\u6587\u5b57\u5c08\u5bb6\u548c 4 \u500b\u5f71\u50cf\u5c08\u5bb6\uff0c\u53ef\u7bc0\u7701\u4ee4\u4eba\u9a5a\u8c54\u7684 FLOP\uff1a\u6574\u9ad4\u800c\u8a00\u70ba 3.7 \u500d\uff0c\u6587\u5b57\u8655\u7406\u70ba 2.6 \u500d\uff0c\u5f71\u50cf\u8655\u7406\u70ba 5.2 \u500d\uff0c\u9019\u662f\u4ee5\u9810\u8a13\u7df4\u640d\u5931\u6e2c\u91cf\uff0c\u4e26\u8207\u904b\u7b97\u7b49\u6548\u7684\u5bc6\u96c6\u57fa\u6e96\u7dda\u76f8\u6bd4\u8f03\u3002\u9019\u512a\u65bc\u6a19\u6e96\u7684\u5c08\u5bb6\u9078\u64c7 MoE\uff0c\u5f8c\u8005\u914d\u5099 8 \u500b\u6df7\u5408\u6a21\u614b\u5c08\u5bb6\uff0c\u53ef\u7bc0\u7701\u6574\u9ad4 FLOP 3 \u500d\uff08\u6587\u5b57\u70ba 3 \u500d\uff0c\u5f71\u50cf\u70ba 2.8 \u500d\uff09\u3002\u5c07 MoMa \u8207\u6df7\u5408\u6df1\u5ea6 (MoD) \u7d50\u5408\uff0c\u53ef\u9032\u4e00\u6b65\u5c07\u9810\u8a13\u7df4 FLOP \u7bc0\u7701\u63d0\u5347\u81f3\u6574\u9ad4 4.2 \u500d\uff08\u6587\u5b57\uff1a3.4 \u500d\uff0c\u5f71\u50cf\uff1a5.3 \u500d\uff09\uff0c\u5118\u7ba1\u9019\u7a2e\u7d44\u5408\u6703\u56e0\u8def\u7531\u5668\u7cbe\u78ba\u5ea6\u654f\u611f\u5ea6\u589e\u52a0\u800c\u640d\u5bb3\u56e0\u679c\u63a8\u7406\u7684\u6548\u80fd\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86 MoMa \u5728\u63d0\u5347\u6df7\u5408\u6a21\u614b\u3001\u65e9\u671f\u878d\u5408\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u6548\u7387\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u70ba\u66f4\u5177\u8cc7\u6e90\u6548\u7387\u4e14\u529f\u80fd\u5f37\u5927\u7684\u591a\u6a21\u614b AI \u7cfb\u7d71\u92ea\u8def\u3002</paragraph>", "author": "Xi Victoria Lin et.al.", "authors": "Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan", "id": "2407.21770v1", "paper_url": "http://arxiv.org/abs/2407.21770v1", "repo": "null"}}