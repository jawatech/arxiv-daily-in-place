{"2501.09804": {"publish_time": "2025-01-16", "title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models", "paper_summary": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging\nnatural language process problem yet highly desirable in many real-life\napplications. Existing CoT knowledge distillation methods often suffer from\noverly conservative memorization in smaller LLMs, leading to low generalization\nconfidence. As fully preserving the CoT ability of teacher model is impossible,\nwe hypothesize that adversarial CoT fine-tuning is crucial for developing\nsmaller LLM with robust CoT generalization. To this end, we propose\n\\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled\nfine-tuning framework that integrates diverse CoT domains. Specifically, PRADA\npioneers two CoT improvements in smaller LLM: (1) Recovering the\ndomain-invariant feature insight which typically lost during distillation with\ndomain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT\nprompt engineering by employing domain-adversarial approaches. We theoretically\ndemonstrate the effectiveness of our approach and empirically show that it\nsignificantly outperforms the state of the arts in a wide range of tasks.\nMoreover, our empirical findings reveal that the smaller LLM, when leveraging\nPRADA, aligns closely with domain knowledge, thereby improving the\nexplainability of our approach.", "paper_summary_zh": "\u93c8\u5f0f\u601d\u8003 (CoT) \u63a8\u7406\u5728\u8f03\u5c0f\u7684\u8a9e\u8a00\u6a21\u578b\u4e2d\u662f\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u554f\u984c\uff0c\u4f46\u5728\u8a31\u591a\u5be6\u969b\u61c9\u7528\u4e2d\u537b\u975e\u5e38\u9700\u8981\u3002\u73fe\u6709\u7684 CoT \u77e5\u8b58\u84b8\u993e\u65b9\u6cd5\u901a\u5e38\u6703\u5c0e\u81f4\u8f03\u5c0f\u7684 LLM \u904e\u65bc\u4fdd\u5b88\u7684\u8a18\u61b6\uff0c\u5f9e\u800c\u964d\u4f4e\u6cdb\u5316\u4fe1\u5fc3\u3002\u7531\u65bc\u7121\u6cd5\u5b8c\u5168\u4fdd\u7559\u6559\u5e2b\u6a21\u578b\u7684 CoT \u80fd\u529b\uff0c\u6211\u5011\u5047\u8a2d\u5c0d\u6297\u6027\u7684 CoT \u5fae\u8abf\u5c0d\u65bc\u958b\u767c\u5177\u6709\u5f37\u5065 CoT \u6cdb\u5316\u7684\u8f03\u5c0f LLM \u81f3\u95dc\u91cd\u8981\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 \\textit{\u63d0\u793a\u8f14\u52a9\u9818\u57df\u5c0d\u6297\u5fae\u8abf} (PRADA)\uff0c\u9019\u662f\u4e00\u500b\u6574\u5408\u4e86\u4e0d\u540c CoT \u9818\u57df\u7684\u539f\u5247\u6027\u5fae\u8abf\u6846\u67b6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cPRADA \u5728\u8f03\u5c0f\u7684 LLM \u4e2d\u958b\u5275\u4e86\u5169\u9805 CoT \u6539\u9032\uff1a(1) \u6062\u5fa9\u5728\u84b8\u993e\u904e\u7a0b\u4e2d\u901a\u5e38\u6703\u4e1f\u5931\u7684\u8207\u9818\u57df\u7121\u95dc\u7684\u7279\u5fb5\u898b\u89e3\uff0c\u4e26\u63a1\u7528\u9818\u57df\u5c0d\u6297\u5fae\u8abf\uff1b(2) \u901a\u904e\u63a1\u7528\u9818\u57df\u5c0d\u6297\u65b9\u6cd5\u4f86\u589e\u5f37 CoT \u63d0\u793a\u5de5\u7a0b\u7684\u9818\u57df\u9069\u61c9\u6027\u3002\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e26\u901a\u904e\u5be6\u8b49\u8868\u660e\uff0c\u5b83\u5728\u5ee3\u6cdb\u7684\u4efb\u52d9\u4e2d\u986f\u8457\u512a\u65bc\u73fe\u6709\u6280\u8853\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u5be6\u8b49\u7d50\u679c\u8868\u660e\uff0c\u8f03\u5c0f\u7684 LLM \u5728\u5229\u7528 PRADA \u6642\u8207\u9818\u57df\u77e5\u8b58\u7dca\u5bc6\u7d50\u5408\uff0c\u5f9e\u800c\u63d0\u9ad8\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u53ef\u89e3\u91cb\u6027\u3002", "author": "Maxwell J. Yin et.al.", "authors": "Maxwell J. Yin, Dingyi Jiang, Yongbing Chen, Boyu Wang, Charles Ling", "id": "2501.09804v1", "paper_url": "http://arxiv.org/abs/2501.09804v1", "repo": "null"}}