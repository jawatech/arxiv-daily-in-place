{"2501.15688": {"publish_time": "2025-01-26", "title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts", "paper_summary": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning.", "paper_summary_zh": "\u591a\u6a21\u614b\u77e5\u8b58\u5716\u8b5c\u88dc\u5168 (MMKGC) \u65e8\u5728\u900f\u904e\u5229\u7528\u4f86\u81ea\u5404\u7a2e\u6a21\u614b\u8207\u7d50\u69cb\u5316\u8cc7\u6599\u7684\u8cc7\u8a0a\uff0c\u4f86\u9810\u6e2c\u591a\u6a21\u614b\u77e5\u8b58\u5716\u8b5c (MMKG) \u4e2d\u7684\u7f3a\u5931\u9023\u7d50\u3002\u73fe\u6709\u7684 MMKGC \u65b9\u6cd5\u4e3b\u8981\u64f4\u5145\u50b3\u7d71\u7684\u77e5\u8b58\u5716\u8b5c\u5d4c\u5165 (KGE) \u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u9700\u8981\u70ba\u6bcf\u500b\u5be6\u9ad4\u5efa\u7acb\u4e00\u500b\u5d4c\u5165\u3002\u9019\u6703\u5c0e\u81f4\u6a21\u578b\u5c3a\u5bf8\u904e\u5927\uff0c\u4e14\u5728\u6574\u5408\u591a\u6a21\u614b\u8cc7\u8a0a\u6642\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u771f\u5be6\u4e16\u754c\u7684\u5716\u8b5c\u3002\u8207\u6b64\u540c\u6642\uff0c\u57fa\u65bc Transformer \u7684\u6a21\u578b\u5df2\u5728\u77e5\u8b58\u5716\u8b5c\u88dc\u5168 (KGC) \u4e2d\u5c55\u73fe\u51fa\u7af6\u722d\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u8457\u91cd\u65bc\u55ae\u6a21\u614b\u77e5\u8b58\uff0c\u9650\u5236\u4e86\u5b83\u5011\u5229\u7528\u8de8\u6a21\u614b\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5df2\u5728\u8de8\u6a21\u614b\u4efb\u52d9\u4e2d\u5c55\u73fe\u6f5b\u529b\uff0c\u4f46\u53d7\u9650\u65bc\u8a13\u7df4\u6210\u672c\u904e\u9ad8\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u5b83\u5c07\u57fa\u65bc Transformer \u7684 KGE \u6a21\u578b\u8207\u9810\u5148\u8a13\u7df4\u7684 VLM \u6240\u7522\u751f\u7684\u8de8\u6a21\u614b\u5167\u5bb9\u6574\u5408\u5728\u4e00\u8d77\uff0c\u5f9e\u800c\u64f4\u5c55\u5b83\u5011\u5728 MMKGC \u4e2d\u7684\u9069\u7528\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a1\u7528\u9810\u5148\u8a13\u7df4\u7684 VLM\uff0c\u5c07\u5be6\u9ad4\u53ca\u5176\u9130\u5c45\u76f8\u95dc\u7684\u8996\u89ba\u8cc7\u8a0a\u8f49\u63db\u6210\u6587\u5b57\u5e8f\u5217\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07 KGC \u67b6\u69cb\u6210\u4e00\u500b\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u4efb\u52d9\uff0c\u4e26\u4f7f\u7528\u7522\u751f\u7684\u8de8\u6a21\u614b\u5167\u5bb9\u5fae\u8abf\u6a21\u578b\u3002\u9019\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u8207\u50b3\u7d71\u7684 KGE \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5927\u5e45\u6e1b\u5c11\u4e86\u6a21\u578b\u5c3a\u5bf8\uff0c\u540c\u6642\u5728\u591a\u500b\u5927\u578b\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86\u7af6\u722d\u529b\u7684\u6548\u80fd\uff0c\u4e14\u53ea\u9700\u6700\u5c11\u7684\u8d85\u53c3\u6578\u8abf\u6574\u3002", "author": "Haodi Ma et.al.", "authors": "Haodi Ma, Dzmitry Kasinets, Daisy Zhe Wang", "id": "2501.15688v1", "paper_url": "http://arxiv.org/abs/2501.15688v1", "repo": "null"}}