{"2412.05270": {"publish_time": "2024-12-06", "title": "APOLLO: SGD-like Memory, AdamW-level Performance", "paper_summary": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a13\u7df4\u671f\u9593\u4ee5\u6d88\u8017\u5927\u91cf\u8a18\u61b6\u9ad4\u800c\u60e1\u540d\u662d\u5f70\uff0c\u7279\u5225\u662f\u4f7f\u7528\u5ee3\u53d7\u6b61\u8fce\u7684 AdamW \u6700\u4f73\u5316\u5668\u6642\u3002\u9019\u7a2e\u8a18\u61b6\u9ad4\u8ca0\u64d4\u9700\u8981\u4f7f\u7528\u66f4\u591a\u6216\u66f4\u9ad8\u968e\u7684 GPU \u6216\u6e1b\u5c11\u6279\u6b21\u5927\u5c0f\uff0c\u9650\u5236\u8a13\u7df4\u53ef\u64f4\u5145\u6027\u548c\u541e\u5410\u91cf\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5df2\u7d93\u63d0\u51fa\u5404\u7a2e\u7701\u8a18\u61b6\u9ad4\u7684\u6700\u4f73\u5316\u5668\u4f86\u6e1b\u5c11\u6700\u4f73\u5316\u5668\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u7136\u800c\uff0c\u5b83\u5011\u9762\u81e8\u56b4\u5cfb\u7684\u6311\u6230\uff1a(i) \u4f9d\u8cf4\u6602\u8cb4\u7684 SVD \u904b\u7b97\uff1b(ii) \u8207 AdamW \u76f8\u6bd4\u6709\u986f\u8457\u7684\u6548\u80fd\u53d6\u6368\uff1b(iii) \u4ecd\u7136\u9700\u8981\u5927\u91cf\u7684\u6700\u4f73\u5316\u5668\u8a18\u61b6\u9ad4\u958b\u92b7\u4f86\u7dad\u6301\u7af6\u722d\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe AdamW \u7684\u5b78\u7fd2\u7387\u9069\u61c9\u898f\u5247\u53ef\u4ee5\u6709\u6548\u5730\u7c97\u7565\u5316\u70ba\u7d50\u69cb\u5316\u7684\u5b78\u7fd2\u7387\u66f4\u65b0\u3002\u6839\u64da\u9019\u500b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8a18\u61b6\u9ad4\u9ad8\u6548 LLM \u6700\u4f73\u5316\u8fd1\u4f3c\u68af\u5ea6\u7e2e\u653e (APOLLO)\uff0c\u5b83\u4f7f\u7528\u57fa\u65bc\u7d14\u96a8\u6a5f\u6295\u5f71\u7684\u8f14\u52a9\u4f4e\u79e9\u6700\u4f73\u5316\u5668\u72c0\u614b\u4f86\u8fd1\u4f3c\u5b78\u7fd2\u7387\u7e2e\u653e\u3002\u9019\u7a2e\u7d50\u69cb\u5316\u7684\u5b78\u7fd2\u7387\u66f4\u65b0\u898f\u5247\u8b93 APOLLO \u975e\u5e38\u8010\u53d7\u9032\u4e00\u6b65\u7684\u8a18\u61b6\u9ad4\u6e1b\u5c11\uff0c\u540c\u6642\u63d0\u4f9b\u53ef\u6bd4\u8f03\u7684\u9810\u8a13\u7df4\u6548\u80fd\u3002\u5373\u4f7f\u662f\u5b83\u7684\u79e9 1 \u8b8a\u9ad4 APOLLO-Mini\uff0c\u8207 SGD \u7b49\u7d1a\u7684\u8a18\u61b6\u9ad4\u6210\u672c\u76f8\u6bd4\uff0c\u4e5f\u9054\u5230\u4e86\u512a\u7570\u7684\u9810\u8a13\u7df4\u6548\u80fd\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0cAPOLLO \u7cfb\u5217\u7684\u6548\u80fd\u8207 AdamW \u76f8\u7576\u6216\u512a\u65bc AdamW\uff0c\u540c\u6642\u900f\u904e\u5e7e\u4e4e\u6d88\u9664 AdamW \u7684\u6700\u4f73\u5316\u72c0\u614b\u4f86\u7bc0\u7701\u66f4\u591a\u8a18\u61b6\u9ad4\u3002\u9019\u4e9b\u7bc0\u7701\u63d0\u4f9b\u4e86\u986f\u8457\u7684\u7cfb\u7d71\u5c64\u7d1a\u512a\u9ede\uff1a(1) \u589e\u5f37\u7684\u541e\u5410\u91cf\uff1a\u8207 AdamW \u76f8\u6bd4\uff0c\u5728 8xA100-80GB \u8a2d\u5b9a\u4e0a\u63d0\u9ad8\u4e86 3 \u500d\u7684\u541e\u5410\u91cf\uff0c\u652f\u63f4 4 \u500d\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\u3002(2) \u6539\u5584\u6a21\u578b\u53ef\u64f4\u5145\u6027\uff1a\u5728\u6c92\u6709\u7cfb\u7d71\u5c64\u7d1a\u6700\u4f73\u5316\u7684\u60c5\u6cc1\u4e0b\uff0c\u4f7f\u7528 A100-80GB GPU \u9810\u8a13\u7df4 LLaMA-13B \u6642\u63a1\u7528\u55ae\u7d14\u7684 DDP\u3002(3) \u4f4e\u968e GPU \u53cb\u5584\u7684\u9810\u8a13\u7df4\uff1a\u4f7f\u7528\u4f4e\u65bc 12 GB \u7684\u8a18\u61b6\u9ad4\u548c\u6b0a\u91cd\u91cf\u5316\u5728\u55ae\u4e00 GPU \u4e0a\u9810\u8a13\u7df4 LLaMA-7B\u3002", "author": "Hanqing Zhu et.al.", "authors": "Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z. Pan, Zhangyang Wang, Jinwon Lee", "id": "2412.05270v1", "paper_url": "http://arxiv.org/abs/2412.05270v1", "repo": "null"}}