{"2407.03320": {"publish_time": "2024-07-03", "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output", "paper_summary": "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa InternLM-XComposer-2.5 (IXC-2.5)\uff0c\u4e00\u500b\u591a\u529f\u80fd\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u652f\u63f4\u9577\u8108\u7d61\u8f38\u5165\u548c\u8f38\u51fa\u3002IXC-2.5 \u5728\u5404\u7a2e\u6587\u672c\u5f71\u50cf\u7406\u89e3\u548c\u7de8\u5beb\u61c9\u7528\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u50c5\u4f7f\u7528 7B LLM \u5f8c\u7aef\uff0c\u4fbf\u80fd\u9054\u5230 GPT-4V \u7b49\u7d1a\u7684\u80fd\u529b\u3002\u900f\u904e 24K \u4ea4\u932f\u5f71\u50cf\u6587\u5b57\u8108\u7d61\u8a13\u7df4\uff0c\u5b83\u53ef\u4ee5\u900f\u904e RoPE \u5916\u63a8\u7121\u7e2b\u64f4\u5c55\u5230 96K \u9577\u8108\u7d61\u3002\u9019\u7a2e\u9577\u8108\u7d61\u80fd\u529b\u8b93 IXC-2.5 \u80fd\u5728\u9700\u8981\u5927\u91cf\u8f38\u5165\u548c\u8f38\u51fa\u8108\u7d61\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u8207\u5148\u524d\u7684 2.0 \u7248\u672c\u76f8\u6bd4\uff0cInternLM-XComposer-2.5 \u5728\u8996\u89ba\u8a9e\u8a00\u7406\u89e3\u65b9\u9762\u6709\u4e09\u5927\u4e3b\u8981\u5347\u7d1a\uff1a(1) \u8d85\u9ad8\u89e3\u6790\u5ea6\u7406\u89e3\uff0c(2) \u7d30\u7dfb\u5f71\u7247\u7406\u89e3\uff0c(3) \u591a\u8f2a\u591a\u5f71\u50cf\u5c0d\u8a71\u3002\u9664\u4e86\u7406\u89e3\u4e4b\u5916\uff0cIXC-2.5 \u9084\u64f4\u5c55\u5230\u5169\u500b\u5f15\u4eba\u6ce8\u76ee\u7684\u61c9\u7528\uff0c\u4f7f\u7528\u984d\u5916\u7684 LoRA \u53c3\u6578\u9032\u884c\u6587\u672c\u5f71\u50cf\u7de8\u5beb\uff1a(1) \u5efa\u69cb\u7db2\u9801\uff0c(2) \u7de8\u5beb\u9ad8\u54c1\u8cea\u6587\u672c\u5f71\u50cf\u6587\u7ae0\u3002IXC-2.5 \u5df2\u5728 28 \u500b\u57fa\u6e96\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u5728 16 \u500b\u57fa\u6e96\u4e0a\u512a\u65bc\u73fe\u6709\u7684\u958b\u6e90\u6700\u5148\u9032\u6a21\u578b\u3002\u5b83\u4e5f\u5728 16 \u500b\u4e3b\u8981\u4efb\u52d9\u4e0a\u8d85\u8d8a\u6216\u63a5\u8fd1 GPT-4V \u548c Gemini Pro\u3002InternLM-XComposer-2.5 \u5df2\u516c\u958b\u767c\u5e03\u65bc https://github.com/InternLM/InternLM-XComposer\u3002</paragraph>", "author": "Pan Zhang et.al.", "authors": "Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang", "id": "2407.03320v1", "paper_url": "http://arxiv.org/abs/2407.03320v1", "repo": "https://github.com/internlm/internlm-xcomposer"}}