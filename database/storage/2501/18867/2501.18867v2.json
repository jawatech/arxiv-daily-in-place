{"2501.18867": {"publish_time": "2025-01-31", "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent", "paper_summary": "Recent advancements in Vision-Language-Action (VLA) models have leveraged\npre-trained Vision-Language Models (VLMs) to improve the generalization\ncapabilities. VLMs, typically pre-trained on vision-language understanding\ntasks, provide rich semantic knowledge and reasoning abilities. However, prior\nresearch has shown that VLMs often focus on high-level semantic content and\nneglect low-level features, limiting their ability to capture detailed spatial\ninformation and understand physical dynamics. These aspects, which are crucial\nfor embodied control tasks, remain underexplored in existing pre-training\nparadigms. In this paper, we investigate the training paradigm for VLAs, and\nintroduce \\textbf{UP-VLA}, a \\textbf{U}nified VLA model training with both\nmulti-modal \\textbf{U}nderstanding and future \\textbf{P}rediction objectives,\nenhancing both high-level semantic comprehension and low-level spatial\nunderstanding. Experimental results show that UP-VLA achieves a 33% improvement\non the Calvin ABC-D benchmark compared to the previous state-of-the-art method.\nAdditionally, UP-VLA demonstrates improved success rates in real-world\nmanipulation tasks, particularly those requiring precise spatial information.", "paper_summary_zh": "\u8fd1\u671f\u5728\u8996\u89ba\u8a9e\u8a00\u52d5\u4f5c (VLA) \u6a21\u578b\u7684\u9032\u5c55\u4e2d\uff0c\u5229\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f86\u63d0\u5347\u6982\u5316\u80fd\u529b\u3002VLM \u901a\u5e38\u9810\u5148\u8a13\u7df4\u65bc\u8996\u89ba\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\uff0c\u63d0\u4f9b\u8c50\u5bcc\u7684\u8a9e\u7fa9\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff0cVLM \u901a\u5e38\u5c08\u6ce8\u65bc\u9ad8\u5c64\u7d1a\u7684\u8a9e\u7fa9\u5167\u5bb9\uff0c\u800c\u5ffd\u7565\u4f4e\u5c64\u7d1a\u7279\u5fb5\uff0c\u9650\u5236\u4e86\u5176\u64f7\u53d6\u8a73\u7d30\u7a7a\u9593\u8cc7\u8a0a\u548c\u7406\u89e3\u7269\u7406\u52d5\u614b\u7684\u80fd\u529b\u3002\u9019\u4e9b\u9762\u5411\u5c0d\u65bc\u5177\u9ad4\u63a7\u5236\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u5728\u73fe\u6709\u7684\u9810\u5148\u8a13\u7df4\u7bc4\u4f8b\u4e2d\u4ecd\u672a\u7372\u5f97\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e VLA \u7684\u8a13\u7df4\u7bc4\u4f8b\uff0c\u4e26\u4ecb\u7d39 UP-VLA\uff0c\u4e00\u500b\u7d71\u4e00\u7684 VLA \u6a21\u578b\u8a13\u7df4\uff0c\u540c\u6642\u5177\u5099\u591a\u6a21\u614b\u7406\u89e3\u548c\u672a\u4f86\u9810\u6e2c\u76ee\u6a19\uff0c\u63d0\u5347\u9ad8\u5c64\u7d1a\u8a9e\u7fa9\u7406\u89e3\u548c\u4f4e\u5c64\u7d1a\u7a7a\u9593\u7406\u89e3\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u8207\u5148\u524d\u7684\u6700\u5148\u9032\u65b9\u6cd5\u76f8\u6bd4\uff0cUP-VLA \u5728 Calvin ABC-D \u57fa\u6e96\u4e0a\u7372\u5f97\u4e86 33% \u7684\u63d0\u5347\u3002\u6b64\u5916\uff0cUP-VLA \u5728\u771f\u5be6\u4e16\u754c\u64cd\u4f5c\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u9700\u8981\u7cbe\u78ba\u7a7a\u9593\u8cc7\u8a0a\u7684\u4efb\u52d9\u3002", "author": "Jianke Zhang et.al.", "authors": "Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen", "id": "2501.18867v2", "paper_url": "http://arxiv.org/abs/2501.18867v2", "repo": "null"}}