{"2412.08110": {"publish_time": "2024-12-11", "title": "Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic Losses", "paper_summary": "Vision-Language Models (VLMs) achieved strong performance on a variety of\ntasks (e.g., image-text retrieval, visual question answering). However, most\nVLMs rely on coarse-grained image-caption pairs for alignment, relying on data\nvolume to resolve ambiguities and ground linguistic concepts in images. The\nricher semantic and syntactic structure within text is largely overlooked. To\naddress this, we propose HIerarchically STructured Learning (HIST) that\nenhances VLM training without any additional supervision, by hierarchically\ndecomposing captions into the constituent Subject, Noun Phrases, and Composite\nPhrases. Entailment between these constituent components allows us to formulate\nadditional regularization constraints on the VLM attention maps. Specifically,\nwe introduce two novel loss functions: (1) Subject Loss, which aligns image\ncontent with the subject of corresponding phrase, acting as an entailment of\nstandard contrastive/matching losses at the Phrase level; (2) Addition Loss, to\nbalance attention across multiple objects. HIST is general, and can be applied\nto any VLM for which attention between vision and language can be computed; we\nillustrate its efficacy on BLIP and ALBEF. HIST outperforms baseline VLMs,\nachieving up to +9.8% improvement in visual grounding, +6.3% in multi-object\nreferring segmentation, +1.1% in image-text retrieval, and +0.2% in visual\nquestion answering, underscoring the value of structuring learning in VLMs.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e0a (\u4f8b\u5982\uff0c\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u3001\u8996\u89ba\u554f\u984c\u89e3\u7b54) \u7686\u6709\u5f37\u52c1\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u5927\u591a\u6578 VLM \u4f9d\u8cf4\u7c97\u7565\u7684\u5f71\u50cf\u6a19\u984c\u914d\u5c0d\u4f86\u5c0d\u9f4a\uff0c\u4ef0\u8cf4\u8cc7\u6599\u91cf\u4f86\u89e3\u6c7a\u6b67\u7fa9\u4e26\u5728\u5f71\u50cf\u4e2d\u5efa\u7acb\u8a9e\u8a00\u6982\u5ff5\u3002\u6587\u672c\u4e2d\u66f4\u8c50\u5bcc\u7684\u8a9e\u610f\u548c\u8a9e\u6cd5\u7d50\u69cb\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u7565\u4e86\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u5206\u5c64\u7d50\u69cb\u5b78\u7fd2 (HIST)\uff0c\u900f\u904e\u5c07\u6a19\u984c\u5206\u5c64\u5206\u89e3\u70ba\u69cb\u6210\u4e3b\u8a5e\u3001\u540d\u8a5e\u7247\u8a9e\u548c\u8907\u5408\u7247\u8a9e\uff0c\u5728\u6c92\u6709\u4efb\u4f55\u984d\u5916\u76e3\u7763\u7684\u60c5\u6cc1\u4e0b\u589e\u5f37 VLM \u8a13\u7df4\u3002\u9019\u4e9b\u69cb\u6210\u6210\u5206\u4e4b\u9593\u7684\u860a\u6db5\u95dc\u4fc2\u8b93\u6211\u5011\u80fd\u5920\u5236\u5b9a VLM \u6ce8\u610f\u529b\u5716\u4e0a\u7684\u984d\u5916\u6b63\u5247\u5316\u7d04\u675f\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5169\u500b\u65b0\u7a4e\u7684\u640d\u5931\u51fd\u6578\uff1a(1) \u4e3b\u8a5e\u640d\u5931\uff0c\u5c07\u5f71\u50cf\u5167\u5bb9\u8207\u5c0d\u61c9\u7247\u8a9e\u7684\u4e3b\u8a5e\u5c0d\u9f4a\uff0c\u4f5c\u70ba\u7247\u8a9e\u5c64\u7d1a\u6a19\u6e96\u5c0d\u6bd4/\u5339\u914d\u640d\u5931\u7684\u860a\u6db5\u95dc\u4fc2\uff1b(2) \u52a0\u6cd5\u640d\u5931\uff0c\u5e73\u8861\u591a\u500b\u7269\u4ef6\u7684\u6ce8\u610f\u529b\u3002HIST \u662f\u901a\u7528\u7684\uff0c\u4e26\u4e14\u53ef\u4ee5\u61c9\u7528\u65bc\u4efb\u4f55\u53ef\u4ee5\u8a08\u7b97\u8996\u89ba\u548c\u8a9e\u8a00\u4e4b\u9593\u6ce8\u610f\u529b\u7684 VLM\uff1b\u6211\u5011\u8aaa\u660e\u4e86\u5b83\u5728 BLIP \u548c ALBEF \u4e0a\u7684\u529f\u6548\u3002HIST \u512a\u65bc\u57fa\u6e96 VLM\uff0c\u5728\u8996\u89ba\u57fa\u790e\u4e0a\u63d0\u5347\u4e86 +9.8%\uff0c\u5728\u591a\u7269\u4ef6\u53c3\u8003\u5206\u5272\u4e0a\u63d0\u5347\u4e86 +6.3%\uff0c\u5728\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u4e0a\u63d0\u5347\u4e86 +1.1%\uff0c\u5728\u8996\u89ba\u554f\u984c\u89e3\u7b54\u4e0a\u63d0\u5347\u4e86 +0.2%\uff0c\u5f37\u8abf\u4e86\u5728 VLM \u4e2d\u5efa\u69cb\u5b78\u7fd2\u7684\u50f9\u503c\u3002", "author": "Jiayun Luo et.al.", "authors": "Jiayun Luo, Mir Rayat Imtiaz Hossain, Boyang Li, Leonid Sigal", "id": "2412.08110v1", "paper_url": "http://arxiv.org/abs/2412.08110v1", "repo": "null"}}