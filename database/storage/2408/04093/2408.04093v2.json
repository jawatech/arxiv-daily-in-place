{"2408.04093": {"publish_time": "2024-08-07", "title": "Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters", "paper_summary": "Self-attention is the core mathematical operation of modern transformer\narchitectures and is also a significant computational bottleneck due to its\nquadratic complexity in the sequence length. In this work, we derive the scalar\nenergy function whose gradient computes the self-attention block, thus\nelucidating the theoretical underpinnings of self-attention, providing a\nBayesian interpretation of the operation and linking it closely with\nenergy-based models such as Hopfield Networks. Our formulation reveals that the\nreduction across the sequence axis can be efficiently computed in parallel\nthrough a tree reduction. Our algorithm, for parallelizing attention\ncomputation across multiple GPUs enables cross-device decoding to be performed\nasymptotically faster (up to 8x faster in our experiments) than alternative\napproaches such as Ring Attention, while also requiring significantly less\ncommunication volume and incurring 2x less peak memory. Our code is publicly\navailable here: \\url{https://github.com/Zyphra/tree_attention}.", "paper_summary_zh": "\u81ea\u6ce8\u610f\u529b\u662f\u73fe\u4ee3Transformer\u67b6\u69cb\u7684\u6838\u5fc3\u6578\u5b78\u904b\u7b97\uff0c\u4e26\u4e14\u7531\u65bc\u5176\u5728\u5e8f\u5217\u9577\u5ea6\u4e0a\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\uff0c\u4e5f\u662f\u91cd\u8981\u7684\u8a08\u7b97\u74f6\u9838\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a8\u5c0e\u51fa\u68af\u5ea6\u8a08\u7b97\u81ea\u6ce8\u610f\u529b\u5340\u584a\u7684\u6a19\u91cf\u80fd\u91cf\u51fd\u6578\uff0c\u5f9e\u800c\u95e1\u660e\u81ea\u6ce8\u610f\u529b\u7684\u7406\u8ad6\u57fa\u790e\uff0c\u63d0\u4f9b\u8a72\u904b\u7b97\u7684\u8c9d\u6c0f\u89e3\u91cb\uff0c\u4e26\u5c07\u5176\u8207\u57fa\u65bc\u80fd\u91cf\u7684\u6a21\u578b\uff08\u4f8b\u5982 Hopfield \u7db2\u8def\uff09\u7dca\u5bc6\u806f\u7e6b\u8d77\u4f86\u3002\u6211\u5011\u7684\u516c\u5f0f\u63ed\u793a\u4e86\u5e8f\u5217\u8ef8\u4e0a\u7684\u7d04\u7c21\u53ef\u4ee5\u901a\u904e\u6a39\u7d04\u7c21\u6709\u6548\u5730\u4e26\u884c\u8a08\u7b97\u3002\u6211\u5011\u7684\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u5728\u591a\u500b GPU \u4e0a\u4e26\u884c\u5316\u6ce8\u610f\u529b\u8a08\u7b97\uff0c\u4f7f\u8de8\u88dd\u7f6e\u89e3\u78bc\u80fd\u5920\u6bd4 Ring Attention \u7b49\u66ff\u4ee3\u65b9\u6cd5\u57f7\u884c\u6f38\u8fd1\u66f4\u5feb\uff08\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\u5feb 8 \u500d\uff09\uff0c\u540c\u6642\u4e5f\u9700\u8981\u986f\u8457\u66f4\u5c11\u7684\u901a\u4fe1\u91cf\uff0c\u4e26\u7522\u751f 2 \u500d\u66f4\u5c11\u7684\u5cf0\u503c\u8a18\u61b6\u9ad4\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5728\u6b64\u516c\u958b\uff1a\\url{https://github.com/Zyphra/tree_attention}\u3002", "author": "Vasudev Shyam et.al.", "authors": "Vasudev Shyam, Jonathan Pilault, Emily Shepperd, Quentin Anthony, Beren Millidge", "id": "2408.04093v2", "paper_url": "http://arxiv.org/abs/2408.04093v2", "repo": "https://github.com/zyphra/tree_attention"}}