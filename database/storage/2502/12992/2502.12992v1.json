{"2502.12992": {"publish_time": "2025-02-18", "title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "paper_summary": "Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural models. Meanwhile, B-cos networks have been introduced to\nimprove model explainability through architectural and computational\nadaptations, but their application has so far been limited to computer vision\nmodels and their associated training pipelines. In this work, we introduce\nB-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly\ntransforms pre-trained language models into B-cos LMs by combining B-cos\nconversion and task fine-tuning, improving efficiency compared to previous\nB-cos methods. Our automatic and human evaluation results demonstrate that\nB-cos LMs produce more faithful and human interpretable explanations than post\nhoc methods, while maintaining task performance comparable to conventional\nfine-tuning. Our in-depth analysis explores how B-cos LMs differ from\nconventionally fine-tuned models in their learning processes and explanation\npatterns. Finally, we provide practical guidelines for effectively building\nB-cos LMs based on our findings. Our code is available at\nhttps://anonymous.4open.science/r/bcos_lm.", "paper_summary_zh": "\u9ed1\u76d2\u6a21\u578b\u7684\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u4f1a\u56e0\u4e3a\u5f53\u524d\u795e\u7ecf\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u800c\u96be\u4ee5\u505a\u5230\u5fe0\u5b9e\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u3002\u4e0e\u6b64\u540c\u65f6\uff0cB-cos \u7f51\u7edc\u5df2\u88ab\u5f15\u5165\uff0c\u4ee5\u901a\u8fc7\u67b6\u6784\u548c\u8ba1\u7b97\u6539\u7f16\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u5b83\u4eec\u7684\u5e94\u7528\u4ec5\u9650\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u53ca\u5176\u76f8\u5173\u7684\u8bad\u7ec3\u7ba1\u9053\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 B-cos LM\uff0c\u5373\u9488\u5bf9 NLP \u4efb\u52a1\u589e\u5f3a\u7684 B-cos \u7f51\u7edc\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408 B-cos \u8f6c\u6362\u548c\u4efb\u52a1\u5fae\u8c03\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a B-cos LM\uff0c\u4e0e\u4ee5\u524d B-cos \u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002\u6211\u4eec\u7684\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e8b\u540e\u65b9\u6cd5\u76f8\u6bd4\uff0cB-cos LM \u4ea7\u751f\u4e86\u66f4\u5fe0\u5b9e\u548c\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4f20\u7edf\u5fae\u8c03\u76f8\u5f53\u7684\u4efb\u52a1\u6027\u80fd\u3002\u6211\u4eec\u7684\u6df1\u5165\u5206\u6790\u63a2\u8ba8\u4e86 B-cos LM \u5728\u5176\u5b66\u4e60\u8fc7\u7a0b\u548c\u89e3\u91ca\u6a21\u5f0f\u4e2d\u4e0e\u4f20\u7edf\u5fae\u8c03\u6a21\u578b\u6709\u4f55\u4e0d\u540c\u3002\u6700\u540e\uff0c\u6211\u4eec\u6839\u636e\u6211\u4eec\u7684\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u6784\u5efa B-cos LM \u7684\u5b9e\u7528\u6307\u5357\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://anonymous.4open.science/r/bcos_lm \u83b7\u5f97\u3002", "author": "Yifan Wang et.al.", "authors": "Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg", "id": "2502.12992v1", "paper_url": "http://arxiv.org/abs/2502.12992v1", "repo": "null"}}