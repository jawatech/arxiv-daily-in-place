{"2501.01805": {"publish_time": "2025-01-03", "title": "End-to-End Long Document Summarization using Gradient Caching", "paper_summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.", "paper_summary_zh": "<paragraph>\u91dd\u5c0d\u9577\u7bc7\u6587\u4ef6\u6458\u8981\u8a13\u7df4\u57fa\u65bc\u8f49\u63db\u5668\u7684\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\uff0c\u7531\u65bc\u8a13\u7df4\u671f\u9593\u4e8c\u6b21\u8a18\u61b6\u9ad4\u6d88\u8017\uff0c\u56e0\u6b64\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u5df2\u63d0\u51fa\u591a\u7a2e\u65b9\u6cd5\u4f86\u64f4\u5145\u6e2c\u8a66\u671f\u9593\u7684\u8f38\u5165\u9577\u5ea6\uff0c\u4f46\u4f7f\u7528\u9019\u4e9b\u65b9\u6cd5\u8a13\u7df4\u4ecd\u7136\u56f0\u96e3\uff0c\u9700\u8981\u622a\u65b7\u8f38\u5165\u6587\u4ef6\uff0c\u4e26\u5c0e\u81f4\u8a13\u7df4\u548c\u6e2c\u8a66\u689d\u4ef6\u4e0d\u5339\u914d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa CachED\uff08\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\u7684\u68af\u5ea6\u5feb\u53d6\uff09\uff0c\u4e00\u7a2e\u65b9\u6cd5\uff0c\u80fd\u4f7f\u7528\u6574\u500b\u6587\u4ef6\u800c\u4e0d\u9032\u884c\u622a\u65b7\uff0c\u5c0d\u73fe\u6709\u7684\u57fa\u65bc\u8f49\u63db\u5668\u7684\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\u9032\u884c\u7aef\u5230\u7aef\u8a13\u7df4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u4e0d\u91cd\u758a\u7684\u6ed1\u52d5\u8996\u7a97\u5957\u7528\u5728\u8f38\u5165\u6587\u4ef6\uff0c\u7136\u5f8c\u5728\u89e3\u78bc\u5668\u4e2d\u9032\u884c\u878d\u5408\u3002\u5728\u53cd\u5411\u50b3\u64ad\u671f\u9593\uff0c\u68af\u5ea6\u6703\u5feb\u53d6\u5728\u89e3\u78bc\u5668\u4e2d\uff0c\u4e26\u900f\u904e\u91cd\u65b0\u8a08\u7b97\u96b1\u85cf\u5411\u91cf\uff0c\u5206\u6279\u50b3\u905e\u5230\u7de8\u78bc\u5668\u4e2d\uff0c\u985e\u4f3c\u65bc\u68af\u5ea6\u6aa2\u67e5\u9ede\u3002\u5728\u9577\u7bc7\u6587\u4ef6\u6458\u8981\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5c07 BART \u5ef6\u4f38\u5230 CachED BART\uff0c\u5728\u8a13\u7df4\u671f\u9593\u8655\u7406\u8d85\u904e 50 \u842c\u500b\u7b26\u865f\uff0c\u4e26\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u984d\u5916\u53c3\u6578\u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u6210\u5353\u8d8a\u7684\u6548\u80fd\u3002</paragraph>", "author": "Rohit Saxena et.al.", "authors": "Rohit Saxena, Hao Tang, Frank Keller", "id": "2501.01805v1", "paper_url": "http://arxiv.org/abs/2501.01805v1", "repo": "null"}}