
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-12**|**ICE-G: Image Conditional Editing of 3D Gaussian Splats**|Vishnu Jaganathan et.al.|[2406.08488v1](http://arxiv.org/abs/2406.08488v1)|null|
|**2024-06-12**|**Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation**|Raphael Tang et.al.|[2406.08482v1](http://arxiv.org/abs/2406.08482v1)|null|
|**2024-06-12**|**What If We Recaption Billions of Web Images with LLaMA-3?**|Xianhang Li et.al.|[2406.08478v1](http://arxiv.org/abs/2406.08478v1)|null|
|**2024-06-12**|**Real2Code: Reconstruct Articulated Objects via Code Generation**|Zhao Mandi et.al.|[2406.08474v1](http://arxiv.org/abs/2406.08474v1)|null|
|**2024-06-12**|**Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]**|Imran Khan et.al.|[2406.08471v1](http://arxiv.org/abs/2406.08471v1)|null|
|**2024-06-12**|**DafnyBench: A Benchmark for Formal Software Verification**|Chloe Loughridge et.al.|[2406.08467v1](http://arxiv.org/abs/2406.08467v1)|[link](https://github.com/sun-wendy/dafnybench)|
|**2024-06-12**|**Scaling Laws in Linear Regression: Compute, Parameters, and Data**|Licong Lin et.al.|[2406.08466v1](http://arxiv.org/abs/2406.08466v1)|null|
|**2024-06-12**|**Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**|Zhangchen Xu et.al.|[2406.08464v1](http://arxiv.org/abs/2406.08464v1)|null|
|**2024-06-12**|**The Impact of Initialization on LoRA Finetuning Dynamics**|Soufiane Hayou et.al.|[2406.08447v1](http://arxiv.org/abs/2406.08447v1)|null|
|**2024-06-12**|**OLMES: A Standard for Language Model Evaluations**|Yuling Gu et.al.|[2406.08446v1](http://arxiv.org/abs/2406.08446v1)|null|
|**2024-06-12**|**TasTe: Teaching Large Language Models to Translate through Self-Reflection**|Yutong Wang et.al.|[2406.08434v1](http://arxiv.org/abs/2406.08434v1)|null|
|**2024-06-12**|**Diffusion Soup: Model Merging for Text-to-Image Diffusion Models**|Benjamin Biggs et.al.|[2406.08431v1](http://arxiv.org/abs/2406.08431v1)|null|
|**2024-06-12**|**Improving Noise Robustness through Abstractions and its Impact on Machine Learning**|Alfredo Ibias et.al.|[2406.08428v1](http://arxiv.org/abs/2406.08428v1)|null|
|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|Zijin Hong et.al.|[2406.08426v1](http://arxiv.org/abs/2406.08426v1)|null|
|**2024-06-12**|**AWGUNET: Attention-Aided Wavelet Guided U-Net for Nuclei Segmentation in Histopathology Images**|Ayush Roy et.al.|[2406.08425v1](http://arxiv.org/abs/2406.08425v1)|[link](https://github.com/ayushroy2001/awgunet)|
|**2024-06-12**|**State Soup: In-Context Skill Learning, Retrieval and Mixing**|Maciej Pióro et.al.|[2406.08423v1](http://arxiv.org/abs/2406.08423v1)|null|
|**2024-06-12**|**OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text**|Qingyun Li et.al.|[2406.08418v1](http://arxiv.org/abs/2406.08418v1)|[link](https://github.com/opengvlab/omnicorpus)|
|**2024-06-12**|**MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos**|Xuehai He et.al.|[2406.08407v1](http://arxiv.org/abs/2406.08407v1)|[link](https://github.com/eric-ai-lab/mmworld)|
|**2024-06-12**|**Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**|Chun-Yi Kuan et.al.|[2406.08402v1](http://arxiv.org/abs/2406.08402v1)|null|
|**2024-06-12**|**cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers**|Anirudh Sundar et.al.|[2406.08398v1](http://arxiv.org/abs/2406.08398v1)|null|
|**2024-06-12**|**Neural Blind Source Separation and Diarization for Distant Speech Recognition**|Yoshiaki Bando et.al.|[2406.08396v1](http://arxiv.org/abs/2406.08396v1)|null|
|**2024-06-12**|**Large Language Models Must Be Taught to Know What They Don't Know**|Sanyam Kapoor et.al.|[2406.08391v1](http://arxiv.org/abs/2406.08391v1)|[link](https://github.com/activatedgeek/calibration-tuning)|
|**2024-06-12**|**Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models**|Javier Nistal et.al.|[2406.08384v1](http://arxiv.org/abs/2406.08384v1)|null|
|**2024-06-12**|**Towards Unsupervised Speech Recognition Without Pronunciation Models**|Junrui Ni et.al.|[2406.08380v1](http://arxiv.org/abs/2406.08380v1)|null|
|**2024-06-12**|**2.5D Multi-view Averaging Diffusion Model for 3D Medical Image Translation: Application to Low-count PET Reconstruction with CT-less Attenuation Correction**|Tianqi Chen et.al.|[2406.08374v1](http://arxiv.org/abs/2406.08374v1)|null|
|**2024-06-12**|**From a Social Cognitive Perspective: Context-aware Visual Social Relationship Recognition**|Shiwei Wu et.al.|[2406.08358v1](http://arxiv.org/abs/2406.08358v1)|null|
|**2024-06-12**|**DocSynthv2: A Practical Autoregressive Modeling for Document Generation**|Sanket Biswas et.al.|[2406.08354v1](http://arxiv.org/abs/2406.08354v1)|null|
|**2024-06-12**|**Continuous-Time Digital Twin with Analogue Memristive Neural Ordinary Differential Equation Solver**|Hegan Chen et.al.|[2406.08343v1](http://arxiv.org/abs/2406.08343v1)|null|
|**2024-06-12**|**ProTrain: Efficient LLM Training via Memory-Aware Techniques**|Hanmei Yang et.al.|[2406.08334v1](http://arxiv.org/abs/2406.08334v1)|null|
|**2024-06-12**|**It's all about PR -- Smart Benchmarking AI Accelerators using Performance Representatives**|Alexander Louis-Ferdinand Jung et.al.|[2406.08330v1](http://arxiv.org/abs/2406.08330v1)|null|
|**2024-06-12**|**Is Programming by Example solved by LLMs?**|Wen-Ding Li et.al.|[2406.08316v1](http://arxiv.org/abs/2406.08316v1)|null|
|**2024-06-12**|**Causality for Tabular Data Synthesis: A High-Order Structure Causal Benchmark Framework**|Ruibo Tu et.al.|[2406.08311v1](http://arxiv.org/abs/2406.08311v1)|[link](https://github.com/turuibo/cautabbench)|
|**2024-06-12**|**Analyzing constrained LLM through PDFA-learning**|Matías Carrasco et.al.|[2406.08269v1](http://arxiv.org/abs/2406.08269v1)|null|
|**2024-06-12**|**A deep cut into Split Federated Self-supervised Learning**|Marcin Przewięźlikowski et.al.|[2406.08267v1](http://arxiv.org/abs/2406.08267v1)|[link](https://github.com/gmum/monacosfl)|
|**2024-06-12**|**Leveraging Large Language Models for Web Scraping**|Aman Ahluwalia et.al.|[2406.08246v1](http://arxiv.org/abs/2406.08246v1)|null|
|**2024-06-12**|**Using Deep Convolutional Neural Networks to Detect Rendered Glitches in Video Games**|Carlos Garcia Ling et.al.|[2406.08231v1](http://arxiv.org/abs/2406.08231v1)|null|
|**2024-06-12**|**DistilDoc: Knowledge Distillation for Visually-Rich Document Applications**|Jordy Van Landeghem et.al.|[2406.08226v1](http://arxiv.org/abs/2406.08226v1)|null|
|**2024-06-12**|**Research Trends for the Interplay between Large Language Models and Knowledge Graphs**|Hanieh Khorashadizadeh et.al.|[2406.08223v1](http://arxiv.org/abs/2406.08223v1)|null|
|**2024-06-12**|**A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion**|Sha Luo et.al.|[2406.08222v1](http://arxiv.org/abs/2406.08222v1)|null|
|**2024-06-12**|**Figuratively Speaking: Authorship Attribution via Multi-Task Figurative Language Modeling**|Gregorios A Katsios et.al.|[2406.08218v1](http://arxiv.org/abs/2406.08218v1)|null|
|**2024-06-12**|**Transformer-based Model for ASR N-Best Rescoring and Rewriting**|Iwen E. Kang et.al.|[2406.08207v1](http://arxiv.org/abs/2406.08207v1)|null|
|**2024-06-12**|**A Dialogue Game for Eliciting Balanced Collaboration**|Isidora Jeknić et.al.|[2406.08202v1](http://arxiv.org/abs/2406.08202v1)|null|
|**2024-06-12**|**MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents**|Luyuan Wang et.al.|[2406.08184v1](http://arxiv.org/abs/2406.08184v1)|null|
|**2024-06-12**|**Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction**|Micol Spitale et.al.|[2406.08183v1](http://arxiv.org/abs/2406.08183v1)|null|
|**2024-06-12**|**Semi-Supervised Spoken Language Glossification**|Huijie Yao et.al.|[2406.08173v1](http://arxiv.org/abs/2406.08173v1)|[link](https://github.com/yaohj11/s3lg)|
|**2024-06-12**|**Continuous fake media detection: adapting deepfake detectors to new generative techniques**|Francesco Tassone et.al.|[2406.08171v1](http://arxiv.org/abs/2406.08171v1)|null|
|**2024-06-12**|**Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark**|Pingzhi Li et.al.|[2406.08155v1](http://arxiv.org/abs/2406.08155v1)|null|
|**2024-06-12**|**Making AI Intelligible: Philosophical Foundations**|Herman Cappelen et.al.|[2406.08134v1](http://arxiv.org/abs/2406.08134v1)|null|
|**2024-06-12**|**Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets**|Duanyu Feng et.al.|[2406.08124v1](http://arxiv.org/abs/2406.08124v1)|[link](https://github.com/colfeng/legend)|
|**2024-06-12**|**Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling**|Zile Qiao et.al.|[2406.08116v1](http://arxiv.org/abs/2406.08116v1)|null|
|**2024-06-12**|**Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey**|Feng Liang et.al.|[2406.08115v1](http://arxiv.org/abs/2406.08115v1)|null|
|**2024-06-12**|**Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio**|Yi Lu et.al.|[2406.08112v1](http://arxiv.org/abs/2406.08112v1)|null|
|**2024-06-12**|**CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems**|Qianli Wang et.al.|[2406.08101v1](http://arxiv.org/abs/2406.08101v1)|null|
|**2024-06-12**|**Multimodal Table Understanding**|Mingyu Zheng et.al.|[2406.08100v1](http://arxiv.org/abs/2406.08100v1)|[link](https://github.com/spursgozmy/table-llava)|
|**2024-06-12**|**Languages Transferred Within the Encoder: On Representation Transfer in Zero-Shot Multilingual Translation**|Zhi Qu et.al.|[2406.08092v1](http://arxiv.org/abs/2406.08092v1)|null|
|**2024-06-12**|**AustroTox: A Dataset for Target-Based Austrian German Offensive Language Detection**|Pia Pachinger et.al.|[2406.08080v1](http://arxiv.org/abs/2406.08080v1)|null|
|**2024-06-12**|**A Concept-Based Explainability Framework for Large Multimodal Models**|Jayneel Parekh et.al.|[2406.08074v1](http://arxiv.org/abs/2406.08074v1)|null|
|**2024-06-12**|**CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models**|Hyungjin Chung et.al.|[2406.08070v1](http://arxiv.org/abs/2406.08070v1)|null|
|**2024-06-12**|**Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey**|Hao Yang et.al.|[2406.08068v1](http://arxiv.org/abs/2406.08068v1)|null|
|**2024-06-12**|**Adversarial Evasion Attack Efficiency against Large Language Models**|João Vitorino et.al.|[2406.08050v1](http://arxiv.org/abs/2406.08050v1)|null|
|**2024-06-12**|**LVBench: An Extreme Long Video Understanding Benchmark**|Weihan Wang et.al.|[2406.08035v1](http://arxiv.org/abs/2406.08035v1)|null|
|**2024-06-12**|**Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models**|Shimin Chen et.al.|[2406.08024v1](http://arxiv.org/abs/2406.08024v1)|null|
|**2024-06-12**|**SHACL2FOL: An FOL Toolkit for SHACL Decision Problems**|Paolo Pareti et.al.|[2406.08018v1](http://arxiv.org/abs/2406.08018v1)|[link](https://github.com/paolo7/shacl2fol)|
|**2024-06-12**|**OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding**|Yinan Deng et.al.|[2406.08009v1](http://arxiv.org/abs/2406.08009v1)|[link](https://github.com/BIT-DYN/OpenObj)|
|**2024-06-12**|**Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning**|Yizhe Huang et.al.|[2406.08002v1](http://arxiv.org/abs/2406.08002v1)|null|
|**2024-06-12**|**It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF**|Taiming Lu et.al.|[2406.07971v1](http://arxiv.org/abs/2406.07971v1)|[link](https://github.com/taiminglu/seamless)|
|**2024-06-12**|**Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation**|Javad Pourmostafa Roshan Sharami et.al.|[2406.07970v1](http://arxiv.org/abs/2406.07970v1)|null|
|**2024-06-12**|**LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning**|Masaya Kawamura et.al.|[2406.07969v1](http://arxiv.org/abs/2406.07969v1)|[link](https://github.com/line/libritts-p)|
|**2024-06-12**|**Toward a Method to Generate Capability Ontologies from Natural Language Descriptions**|Luis Miguel Vieira da Silva et.al.|[2406.07962v1](http://arxiv.org/abs/2406.07962v1)|null|
|**2024-06-12**|**Accurate Explanation Model for Image Classifiers using Class Association Embedding**|Ruitao Xie et.al.|[2406.07961v1](http://arxiv.org/abs/2406.07961v1)|[link](https://github.com/xrt11/xai-code)|
|**2024-06-12**|**Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition**|Edoardo Debenedetti et.al.|[2406.07954v1](http://arxiv.org/abs/2406.07954v1)|null|
|**2024-06-12**|**Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization**|Guopeng Lin et.al.|[2406.07948v1](http://arxiv.org/abs/2406.07948v1)|null|
|**2024-06-12**|**DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis**|Meiziniu Li et.al.|[2406.07944v1](http://arxiv.org/abs/2406.07944v1)|null|
|**2024-06-12**|**Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation**|Jie Ruan et.al.|[2406.07935v1](http://arxiv.org/abs/2406.07935v1)|null|
|**2024-06-12**|**Large Language Model Unlearning via Embedding-Corrupted Prompts**|Chris Yuhao Liu et.al.|[2406.07933v1](http://arxiv.org/abs/2406.07933v1)|null|
|**2024-06-12**|**A Generic Layer Pruning Method for Signal Modulation Recognition Deep Learning Models**|Yao Lu et.al.|[2406.07929v1](http://arxiv.org/abs/2406.07929v1)|null|
|**2024-06-12**|**Efficient Neural Common Neighbor for Temporal Graph Link Prediction**|Xiaohui Zhang et.al.|[2406.07926v1](http://arxiv.org/abs/2406.07926v1)|null|
|**2024-06-12**|**CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting**|Sichen Jin et.al.|[2406.07923v1](http://arxiv.org/abs/2406.07923v1)|null|
|**2024-06-12**|**Automated Information Extraction from Thyroid Operation Narrative: A Comparative Study of GPT-4 and Fine-tuned KoELECTRA**|Dongsuk Jang et.al.|[2406.07922v1](http://arxiv.org/abs/2406.07922v1)|null|
|**2024-06-12**|**Near-Optimal Learning and Planning in Separated Latent MDPs**|Fan Chen et.al.|[2406.07920v1](http://arxiv.org/abs/2406.07920v1)|null|
|**2024-06-12**|**Graph Transductive Defense: a Two-Stage Defense for Graph Membership Inference Attacks**|Peizhi Niu et.al.|[2406.07917v1](http://arxiv.org/abs/2406.07917v1)|null|
|**2024-06-12**|**DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning**|Yuxi Feng et.al.|[2406.07913v1](http://arxiv.org/abs/2406.07913v1)|null|
|**2024-06-12**|**Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation**|Eungbeom Kim et.al.|[2406.07909v1](http://arxiv.org/abs/2406.07909v1)|null|
|**2024-06-12**|**Ablation Based Counterfactuals**|Zheng Dai et.al.|[2406.07908v1](http://arxiv.org/abs/2406.07908v1)|null|
|**2024-06-12**|**Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations**|Bulat Khaertdinov et.al.|[2406.07900v1](http://arxiv.org/abs/2406.07900v1)|null|
|**2024-06-12**|**Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions**|Anfeng Xu et.al.|[2406.07890v1](http://arxiv.org/abs/2406.07890v1)|null|
|**2024-06-12**|**Classification Modeling with RNN-Based, Random Forest, and XGBoost for Imbalanced Data: A Case of Early Crash Detection in ASEAN-5 Stock Markets**|Deri Siswara et.al.|[2406.07888v1](http://arxiv.org/abs/2406.07888v1)|null|
|**2024-06-12**|**An Empirical Study of Mamba-based Language Models**|Roger Waleffe et.al.|[2406.07887v1](http://arxiv.org/abs/2406.07887v1)|null|
|**2024-06-12**|**Label-aware Hard Negative Sampling Strategies with Momentum Contrastive Learning for Implicit Hate Speech Detection**|Jaehoon Kim et.al.|[2406.07886v1](http://arxiv.org/abs/2406.07886v1)|[link](https://github.com/hanyang-hcc-lab/lahn)|
|**2024-06-12**|**Designing a Dashboard for Transparency and Control of Conversational AI**|Yida Chen et.al.|[2406.07882v1](http://arxiv.org/abs/2406.07882v1)|[link](https://github.com/yc015/talktuner-chatbot-llm-dashboard)|
|**2024-06-12**|**KernelWarehouse: Rethinking the Design of Dynamic Convolution**|Chao Li et.al.|[2406.07879v1](http://arxiv.org/abs/2406.07879v1)|[link](https://github.com/osvai/kernelwarehouse)|
|**2024-06-12**|**Hierarchical Reinforcement Learning for Swarm Confrontation with High Uncertainty**|Qizhen Wu et.al.|[2406.07877v1](http://arxiv.org/abs/2406.07877v1)|null|
|**2024-06-12**|**Small Scale Data-Free Knowledge Distillation**|He Liu et.al.|[2406.07876v1](http://arxiv.org/abs/2406.07876v1)|null|
|**2024-06-12**|**Carbon Market Simulation with Adaptive Mechanism Design**|Han Wang et.al.|[2406.07875v1](http://arxiv.org/abs/2406.07875v1)|[link](https://github.com/xwanghan/carbon-simulator)|
|**2024-06-12**|**Unveiling the Power of Wavelets: A Wavelet-based Kolmogorov-Arnold Network for Hyperspectral Image Classification**|Seyd Teymoor Seydi et.al.|[2406.07869v1](http://arxiv.org/abs/2406.07869v1)|null|
|**2024-06-12**|**Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation**|Se Jin Park et.al.|[2406.07867v1](http://arxiv.org/abs/2406.07867v1)|null|
|**2024-06-12**|**Self-Distillation Learning Based on Temporal-Spatial Consistency for Spiking Neural Networks**|Lin Zuo et.al.|[2406.07862v1](http://arxiv.org/abs/2406.07862v1)|null|
|**2024-06-12**|**BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain**|Rahul Kumar et.al.|[2406.07860v1](http://arxiv.org/abs/2406.07860v1)|[link](https://github.com/exploration-lab/booksql)|
|**2024-06-12**|**VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment**|Bing Han et.al.|[2406.07855v1](http://arxiv.org/abs/2406.07855v1)|null|
|**2024-06-12**|**Dynamic Stochastic Decoding Strategy for Open-Domain Dialogue Generation**|Yiwei Li et.al.|[2406.07850v1](http://arxiv.org/abs/2406.07850v1)|null|

#### Abstracts
##### **ICE-G: Image Conditional Editing of 3D Gaussian Splats**
2406.08488v1 by Vishnu Jaganathan, Hannah Hanyun Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira

Recently many techniques have emerged to create high quality 3D assets and
scenes. When it comes to editing of these objects, however, existing approaches
are either slow, compromise on quality, or do not provide enough customization.
We introduce a novel approach to quickly edit a 3D model from a single
reference view. Our technique first segments the edit image, and then matches
semantically corresponding regions across chosen segmented dataset views using
DINO features. A color or texture change from a particular region of the edit
image can then be applied to other views automatically in a semantically
sensible manner. These edited views act as an updated dataset to further train
and re-style the 3D scene. The end-result is therefore an edited 3D model. Our
framework enables a wide variety of editing tasks such as manual local edits,
correspondence based style transfer from any example image, and a combination
of different styles from multiple example images. We use Gaussian Splats as our
primary 3D representation due to their speed and ease of local editing, but our
technique works for other methods such as NeRFs as well. We show through
multiple examples that our method produces higher quality results while
offering fine-grained control of editing. Project page: ice-gaussian.github.io

摘要：最近出现了许多技术来创建高质量的 3D 资产和场景。然而，在编辑这些对象时，现有方法要么速度慢、要么在质量上妥协，要么无法提供足够的定制。我们引入了一种新方法，可以根据单个参考视图快速编辑 3D 模型。我们的技术首先对编辑图像进行分割，然后使用 DINO 特征匹配跨所选分割数据集视图的语义对应区域。然后可以将编辑图像的特定区域的颜色或纹理更改自动应用于其他视图，以语义上合理的方式进行。这些编辑后的视图充当更新后的数据集，以进一步训练和重新调整 3D 场景的风格。因此，最终结果是一个经过编辑的 3D 模型。我们的框架支持各种编辑任务，例如手动局部编辑、基于对应关系的任意示例图像的风格迁移以及多个示例图像的不同风格的组合。我们使用高斯斑点作为我们的主要 3D 表示，因为它们速度快且易于局部编辑，但我们的技术也适用于其他方法，例如 NeRF。我们通过多个示例展示了我们的方法在提供编辑的细粒度控制的同时产生了更高质量的结果。项目页面：ice-gaussian.github.io

##### **Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation**
2406.08482v1 by Raphael Tang, Xinyu Zhang, Lixinyu Xu, Yao Lu, Wenyan Li, Pontus Stenetorp, Jimmy Lin, Ferhan Ture

Diffusion models are the state of the art in text-to-image generation, but
their perceptual variability remains understudied. In this paper, we examine
how prompts affect image variability in black-box diffusion-based models. We
propose W1KP, a human-calibrated measure of variability in a set of images,
bootstrapped from existing image-pair perceptual distances. Current datasets do
not cover recent diffusion models, thus we curate three test sets for
evaluation. Our best perceptual distance outperforms nine baselines by up to 18
points in accuracy, and our calibration matches graded human judgements 78% of
the time. Using W1KP, we study prompt reusability and show that Imagen prompts
can be reused for 10-50 random seeds before new images become too similar to
already generated images, while Stable Diffusion XL and DALL-E 3 can be reused
50-200 times. Lastly, we analyze 56 linguistic features of real prompts,
finding that the prompt's length, CLIP embedding norm, concreteness, and word
senses influence variability most. As far as we are aware, we are the first to
analyze diffusion variability from a visuolinguistic perspective. Our project
page is at http://w1kp.com

摘要：擴散模型是文字轉圖像生成中的最新技術，但其感知變異性仍未得到充分研究。在本文中，我們探討提示如何影響基於黑盒擴散模型中的影像變異性。我們提出 W1KP，這是一種基於現有影像對感知距離自舉的人類校準變異性測量。目前的資料集未涵蓋最近的擴散模型，因此我們策劃了三個測試集進行評估。我們最佳的感知距離在準確度上比九個基準高出 18 個百分點，而我們的校準在 78% 的時間內與分級的人類判斷相符。使用 W1KP，我們研究提示的可重複使用性，並顯示 Imagen 提示可以在 10-50 個隨機種子中重複使用，然後新影像才會變得與已生成的影像過於相似，而 Stable Diffusion XL 和 DALL-E 3 可以重複使用 50-200 次。最後，我們分析了 56 個真實提示的語言特徵，發現提示的長度、CLIP 嵌入範數、具體性和詞義最能影響變異性。據我們所知，我們是第一個從視覺語言學的角度分析擴散變異性的人。我們的專案頁面位於 http://w1kp.com

##### **What If We Recaption Billions of Web Images with LLaMA-3?**
2406.08478v1 by Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie

Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate
that semantically aligning and enriching textual descriptions of these pairs
can significantly enhance model training across various vision-language tasks,
particularly text-to-image generation. However, large-scale investigations in
this area remain predominantly closed-source. Our paper aims to bridge this
community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a
GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a
LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images
from the DataComp-1B dataset. Our empirical results confirm that this enhanced
dataset, Recap-DataComp-1B, offers substantial benefits in training advanced
vision-language models. For discriminative models like CLIP, we observe
enhanced zero-shot performance in cross-modal retrieval tasks. For generative
models like text-to-image Diffusion Transformers, the generated images exhibit
a significant improvement in alignment with users' text instructions,
especially in following complex queries. Our project page is
https://www.haqtu.me/Recap-Datacomp-1B/

摘要：網路爬取的影像文字配對本身就存在雜訊。先前的研究證明，對這些配對的文字描述進行語意對齊和豐富化，可以大幅增強各種視覺語言任務的模型訓練，尤其是文字轉影像生成。然而，這方面的規模化調查仍以閉源為主。我們的論文旨在彌合社群的努力，利用功能強大且\textit{開放原始碼}的 LLaMA-3，一個 GPT-4 等級的 LLM。我們的重新標題處理流程很簡單：首先，我們微調一個由 LLaMA-3-8B 驅動的 LLaVA-1.5，然後使用它重新標題來自 DataComp-1B 資料集的 13 億張影像。我們的經驗結果證實，這個增強的資料集 Recap-DataComp-1B，在訓練進階視覺語言模型方面提供了大量的優點。對於像 CLIP 這樣的判別模型，我們觀察到在跨模態檢索任務中增強的零次學習效能。對於像文字轉影像 Diffusion Transformers 這樣的生成模型，生成的影像在與使用者文字指令對齊方面有顯著的改善，特別是在遵循複雜查詢時。我們的專案頁面是 https://www.haqtu.me/Recap-Datacomp-1B/

##### **Real2Code: Reconstruct Articulated Objects via Code Generation**
2406.08474v1 by Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song

We present Real2Code, a novel approach to reconstructing articulated objects
via code generation. Given visual observations of an object, we first
reconstruct its part geometry using an image segmentation model and a shape
completion model. We then represent the object parts with oriented bounding
boxes, which are input to a fine-tuned large language model (LLM) to predict
joint articulation as code. By leveraging pre-trained vision and language
models, our approach scales elegantly with the number of articulated parts, and
generalizes from synthetic training data to real world objects in unstructured
environments. Experimental results demonstrate that Real2Code significantly
outperforms previous state-of-the-art in reconstruction accuracy, and is the
first approach to extrapolate beyond objects' structural complexity in the
training set, and reconstructs objects with up to 10 articulated parts. When
incorporated with a stereo reconstruction model, Real2Code also generalizes to
real world objects from a handful of multi-view RGB images, without the need
for depth or camera information.

摘要：我們提出 Real2Code，一種透過產生程式碼來重建關節物件的新穎方法。給定物件的視覺觀察結果，我們首先使用影像分割模型和形狀完成模型來重建其部分幾何。然後我們用有向邊界框表示物件部分，作為輸入到微調過的大語言模型 (LLM) 中，以預測作為程式碼的關節關節。透過利用預先訓練好的視覺和語言模型，我們的做法可以優雅地隨著關節部分數量的增加而擴展，並且可以從合成訓練資料推廣到非結構化環境中的真實世界物件。實驗結果表明，Real2Code 在重建準確度方面顯著優於先前的最新技術，並且是第一個超越訓練集中物件結構複雜性的外推方法，並且重建具有多達 10 個關節部分的物件。當與立體重建模型結合使用時，Real2Code 也能推廣到真實世界的物件，這些物件來自少數多視角 RGB 影像，而不需要深度或相機資訊。

##### **Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]**
2406.08471v1 by Imran Khan, Robert Lowe

Allostasis proposes that long-term viability of a living system is achieved
through anticipatory adjustments of its physiology and behaviour: emphasising
physiological and affective stress as an adaptive state of adaptation that
minimizes long-term prediction errors. More recently, the active inference
framework (AIF) has also sought to explain action and long-term adaptation
through the minimization of future errors (free energy), through the learning
of statistical contingencies of the world, offering a formalism for allostatic
regulation. We suggest that framing prediction errors through the lens of
biological hormonal dynamics proposed by allostasis offers a way to integrate
these two models together in a biologically-plausible manner. In this paper, we
describe our initial work in developing a model that grounds prediction errors
(surprisal) into the secretion of a physiological stress hormone (cortisol)
acting as an adaptive, allostatic mediator on a homeostatically-controlled
physiology. We evaluate this using a computational model in simulations using
an active inference agent endowed with an artificial physiology, regulated
through homeostatic and allostatic control in a stochastic environment. Our
results find that allostatic functions of cortisol (stress), secreted as a
function of prediction errors, provide adaptive advantages to the agent's
long-term physiological regulation. We argue that the coupling of
information-theoretic prediction errors to low-level, biological hormonal
dynamics of stress can provide a computationally efficient model to long-term
regulation for embodied intelligent systems.

摘要：同質異構提出，生物系統的長期可行性是透過預測調整其生理和行為來實現的：強調生理和情感壓力作為適應狀態，以最小化長期預測誤差。最近，主動推論架構 (AIF) 也試圖透過最小化未來誤差（自由能）來解釋動作和長期適應，透過學習世界的統計或然率，提供同質異構調節的公式。我們建議透過同質異構提出的生物荷爾蒙動態來建構預測誤差，提供一種以生物學上合理的方式整合這兩個模型的方法。在本文中，我們描述我們在開發一個模型中的初步工作，該模型將預測誤差（驚訝）基礎到生理壓力荷爾蒙（皮質醇）的分泌中，該荷爾蒙作為一個適應性的、同質異構的調節劑，作用於一個受恆定狀態控制的生理機制。我們使用一個計算模型在模擬中評估這一點，使用一個具有人工生理結構的主動推論代理，在一個隨機環境中透過恆定狀態和同質異構控制進行調節。我們的結果發現，皮質醇（壓力）的同質異構功能，作為預測誤差的函數分泌，為代理人的長期生理調節提供適應性優勢。我們認為，將資訊理論預測誤差與低層級的生物荷爾蒙壓力動態相結合，可以為具身智能系統的長期調節提供一個計算上有效率的模型。

##### **DafnyBench: A Benchmark for Formal Software Verification**
2406.08467v1 by Chloe Loughridge, Qinyi Sun, Seth Ahrenbach, Federico Cassano, Chuyue Sun, Ying Sheng, Anish Mudide, Md Rakib Hossain Misu, Nada Amin, Max Tegmark

We introduce DafnyBench, the largest benchmark of its kind for training and
evaluating machine learning systems for formal software verification. We test
the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints
for the Dafny formal verification engine to successfully verify over 750
programs with about 53,000 lines of code. The best model and prompting scheme
achieved 68% success rate, and we quantify how this rate improves when retrying
with error message feedback and how it deteriorates with the amount of required
code and hints. We hope that DafnyBench will enable rapid improvements from
this baseline as LLMs and verification techniques grow in quality.

摘要：我們介紹了 DafnyBench，這是同類中規模最大的基準，用於訓練和評估用於形式化軟體驗證的機器學習系統。我們測試了 GPT-4 和 Claude 3 等 LLM 自動產生足夠提示的能力，讓 Dafny 形式驗證引擎成功驗證超過 750 個程式，程式碼行數約為 53,000 行。最佳模型和提示方案達到了 68% 的成功率，我們量化了在使用錯誤訊息回饋重試時此成功率如何提升，以及在需要程式碼和提示的數量增加時如何下降。我們希望 DafnyBench 能夠在 LLM 和驗證技術品質提升時，從此基線快速改進。

##### **Scaling Laws in Linear Regression: Compute, Parameters, and Data**
2406.08466v1 by Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee

Empirically, large-scale deep learning models often satisfy a neural scaling
law: the test error of the trained model improves polynomially as the model
size and data size grow. However, conventional wisdom suggests the test error
consists of approximation, bias, and variance errors, where the variance error
increases with model size. This disagrees with the general form of neural
scaling laws, which predict that increasing model size monotonically improves
performance.
  We study the theory of scaling laws in an infinite dimensional linear
regression setup. Specifically, we consider a model with $M$ parameters as a
linear function of sketched covariates. The model is trained by one-pass
stochastic gradient descent (SGD) using $N$ data. Assuming the optimal
parameter satisfies a Gaussian prior and the data covariance matrix has a
power-law spectrum of degree $a>1$, we show that the reducible part of the test
error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which
increases with $M$, is dominated by the other errors due to the implicit
regularization of SGD, thus disappearing from the bound. Our theory is
consistent with the empirical neural scaling laws and verified by numerical
simulation.

摘要：经验上，大规模深度学习模型通常满足神经网络缩放定律：随着模型大小和数据大小的增长，训练模型的测试误差以多项式方式得到改善。然而，传统观点认为测试误差由近似误差、偏差误差和方差误差组成，其中方差误差随着模型大小的增加而增加。这与神经网络缩放定律的一般形式相矛盾，后者预测增加模型大小会单调地提高性能。我们研究了无限维线性回归设置中的缩放定律理论。具体来说，我们考虑了一个模型，其中 $M$ 个参数作为素描协变量的线性函数。该模型通过使用 $N$ 个数据的一遍随机梯度下降 (SGD) 进行训练。假设最优参数满足高斯先验，并且数据协方差矩阵具有 $a>1$ 阶的幂律谱，我们表明测试误差的可约部分为 $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$。方差误差随着 $M$ 的增加而增加，由于 SGD 的隐式正则化，它被其他误差所支配，因此从界限中消失。我们的理论与经验神经网络缩放定律一致，并通过数值模拟得到验证。

##### **Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**
2406.08464v1 by Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin

High-quality instruction data is critical for aligning large language models
(LLMs). Although some models, such as Llama-3-Instruct, have open weights,
their alignment data remain private, which hinders the democratization of AI.
High human labor costs and a limited, predefined scope for prompting prevent
existing open-source data creation methods from scaling effectively,
potentially limiting the diversity and quality of public alignment datasets. Is
it possible to synthesize high-quality instruction data at scale by extracting
it directly from an aligned LLM? We present a self-synthesis method for
generating large-scale alignment data named Magpie. Our key observation is that
aligned LLMs like Llama-3-Instruct can generate a user query when we input only
the left-side templates up to the position reserved for user messages, thanks
to their auto-regressive nature. We use this method to prompt Llama-3-Instruct
and generate 4 million instructions along with their corresponding responses.
We perform a comprehensive analysis of the extracted data and select 300K
high-quality instances. To compare Magpie data with other public instruction
datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the
performance of the fine-tuned models. Our results indicate that in some tasks,
models fine-tuned with Magpie perform comparably to the official
Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data
points through supervised fine-tuning (SFT) and subsequent feedback learning.
We also show that using Magpie solely for SFT can surpass the performance of
previous public datasets utilized for both SFT and preference optimization,
such as direct preference optimization with UltraFeedback. This advantage is
evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.

摘要：高质量的指令数据对于对齐大型语言模型 (LLM) 至关重要。尽管某些模型（如 Llama-3-Instruct）具有开放权重，但其对齐数据仍然是私有的，这阻碍了人工智能的民主化。高昂的人工成本和有限的预定义提示范围阻碍了现有的开源数据创建方法有效扩展，从而可能限制公共对齐数据集的多样性和质量。是否可以通过直接从对齐的 LLM 中提取来大规模合成高质量的指令数据？我们提出了一种名为 Magpie 的自合成方法，用于生成大规模的对齐数据。我们的关键观察是，像 Llama-3-Instruct 这样的对齐 LLM 可以在我们仅输入左侧模板（直到为用户消息保留的位置）时生成用户查询，这要归功于它们的自动回归特性。我们使用这种方法提示 Llama-3-Instruct 并生成 400 万条指令及其相应的响应。我们对提取的数据进行了全面分析，并选择了 30 万个高质量的实例。为了将 Magpie 数据与其他公共指令数据集进行比较，我们使用每个数据集对 Llama-3-8B-Base 进行了微调，并评估了微调后的模型的性能。我们的结果表明，在某些任务中，使用 Magpie 微调的模型的性能与官方的 Llama-3-8B-Instruct 相当，尽管后者通过监督微调 (SFT) 和后续反馈学习得到了 1000 万个数据点的增强。我们还表明，仅将 Magpie 用于 SFT 可以超越以前用于 SFT 和偏好优化的公共数据集的性能，例如使用 UltraFeedback 的直接偏好优化。这种优势在对齐基准（如 AlpacaEval、ArenaHard 和 WildBench）中很明显。

##### **The Impact of Initialization on LoRA Finetuning Dynamics**
2406.08447v1 by Soufiane Hayou, Nikhil Ghosh, Bin Yu

In this paper, we study the role of initialization in Low Rank Adaptation
(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from
the pretrained model as initialization for finetuning, one can either
initialize B to zero and A to random (default initialization in PEFT package),
or vice-versa. In both cases, the product BA is equal to zero at
initialization, which makes finetuning starts from the pretrained model. These
two initialization schemes are seemingly similar. They should in-principle
yield the same performance and share the same optimal learning rate. We
demonstrate that this is an incorrect intuition and that the first scheme
(initializing B to zero and A to random) on average yields better performance
compared to the other scheme. Our theoretical analysis shows that the reason
behind this might be that the first initialization allows the use of larger
learning rates (without causing output instability) compared to the second
initialization, resulting in more efficient learning of the first scheme. We
validate our results with extensive experiments on LLMs.

摘要：在本文中，我们研究了初始化在低秩适应 (LoRA) 中的角色，正如 Hu 等人 (2021) 最初提出的那样。从本质上讲，为了从预训练模型开始作为微调的初始化，可以将 B 初始化为零，将 A 初始化为随机值（PEFT 包中的默认初始化），或者反之亦然。在这两种情况下，产品 BA 在初始化时等于零，这使得微调从预训练模型开始。这两个初始化方案看似相似。它们在原则上应该产生相同的性能并共享相同的最佳学习率。我们证明这是一个不正确的直觉，并且第一个方案（将 B 初始化为零，将 A 初始化为随机值）平均产生比另一个方案更好的性能。我们的理论分析表明，其背后的原因可能是第一个初始化允许使用更大的学习率（不会导致输出不稳定），与第二个初始化相比，这导致第一个方案的学习效率更高。我们通过对 LLM 的广泛实验验证了我们的结果。

##### **OLMES: A Standard for Language Model Evaluations**
2406.08446v1 by Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, Hannaneh Hajishirzi

Progress in AI is often demonstrated by new models claiming improved
performance on tasks measuring model capabilities. Evaluating language models
in particular is challenging, as small changes to how a model is evaluated on a
task can lead to large changes in measured performance. There is no common
standard setup, so different models are evaluated on the same tasks in
different ways, leading to claims about which models perform best not being
reproducible. We propose OLMES, a completely documented, practical, open
standard for reproducible LLM evaluations. In developing this standard, we
identify and review the varying factors in evaluation practices adopted by the
community - such as details of prompt formatting, choice of in-context
examples, probability normalizations, and task formulation. In particular,
OLMES supports meaningful comparisons between smaller base models that require
the unnatural "cloze" formulation of multiple-choice questions against larger
models that can utilize the original formulation. OLMES includes
well-considered recommendations guided by results from existing literature as
well as new experiments investigating open questions.

摘要：人工智能的進展通常由新的模型證明，這些模型聲稱在衡量模型能力的任務中改進了效能。特別是評估語言模型具有挑戰性，因為在任務中評估模型的方式的微小變化可能會導致測量效能發生巨大變化。沒有通用的標準設定，因此不同的模型會以不同的方式評估相同的任務，導致關於哪個模型效能最佳的說法無法重現。我們提出 OLMES，這是一個完全記錄、實用、開放的標準，用於可重現的 LLM 評估。在制定此標準時，我們識別並檢視社群採用的評估實務中變動的因素，例如提示格式的詳細資料、脈絡中範例的選擇、機率正規化和任務表述。特別是，OLMES 支援在較小的基礎模型（需要多選題的不自然「完形填空」表述）與能夠使用原始表述的較大模型之間進行有意義的比較。OLMES 包含經過深思熟慮的建議，這些建議由現有文獻的結果以及調查開放性問題的新實驗指導。

##### **TasTe: Teaching Large Language Models to Translate through Self-Reflection**
2406.08434v1 by Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang

Large language models (LLMs) have exhibited remarkable performance in various
natural language processing tasks. Techniques like instruction tuning have
effectively enhanced the proficiency of LLMs in the downstream task of machine
translation. However, the existing approaches fail to yield satisfactory
translation outputs that match the quality of supervised neural machine
translation (NMT) systems. One plausible explanation for this discrepancy is
that the straightforward prompts employed in these methodologies are unable to
fully exploit the acquired instruction-following capabilities. To this end, we
propose the TasTe framework, which stands for translating through
self-reflection. The self-reflection process includes two stages of inference.
In the first stage, LLMs are instructed to generate preliminary translations
and conduct self-assessments on these translations simultaneously. In the
second stage, LLMs are tasked to refine these preliminary translations
according to the evaluation results. The evaluation results in four language
directions on the WMT22 benchmark reveal the effectiveness of our approach
compared to existing methods. Our work presents a promising approach to unleash
the potential of LLMs and enhance their capabilities in MT. The codes and
datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務中展現出卓越的表現。像指令微調的技術已有效提升 LLM 在機器翻譯的下游任務中的能力。然而，現有的方法無法產生令人滿意的翻譯輸出，以符合監督式神經機器翻譯 (NMT) 系統的品質。對此差異一個合理的解釋是，這些方法中所採用的直接提示無法充分利用已習得的遵循指令的能力。為此，我們提出 TasTe 框架，其代表透過自我反省進行翻譯。自我反省的過程包含兩個推論階段。在第一階段，LLM 接受指示，同時產生初步翻譯並對這些翻譯進行自我評估。在第二階段，LLM 被指派根據評估結果修正這些初步翻譯。在 WMT22 基準上的四個語言方向的評估結果顯示，與現有方法相比，我們的方法更有效。我們的成果提出了一個有前途的方法，可以釋放 LLM 的潛力，並增強其在機器翻譯中的能力。程式碼和資料集已開放原始碼，網址為 https://github.com/YutongWang1216/ReflectionLLMMT。

##### **Diffusion Soup: Model Merging for Text-to-Image Diffusion Models**
2406.08431v1 by Benjamin Biggs, Arjun Seshadri, Yang Zou, Achin Jain, Aditya Golatkar, Yusheng Xie, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto

We present Diffusion Soup, a compartmentalization method for Text-to-Image
Generation that averages the weights of diffusion models trained on sharded
data. By construction, our approach enables training-free continual learning
and unlearning with no additional memory or inference costs, since models
corresponding to data shards can be added or removed by re-averaging. We show
that Diffusion Soup samples from a point in weight space that approximates the
geometric mean of the distributions of constituent datasets, which offers
anti-memorization guarantees and enables zero-shot style mixing. Empirically,
Diffusion Soup outperforms a paragon model trained on the union of all data
shards and achieves a 30% improvement in Image Reward (.34 $\to$ .44) on domain
sharded data, and a 59% improvement in IR (.37 $\to$ .59) on aesthetic data. In
both cases, souping also prevails in TIFA score (respectively, 85.5 $\to$ 86.5
and 85.6 $\to$ 86.8). We demonstrate robust unlearning -- removing any
individual domain shard only lowers performance by 1% in IR (.45 $\to$ .44) --
and validate our theoretical insights on anti-memorization using real data.
Finally, we showcase Diffusion Soup's ability to blend the distinct styles of
models finetuned on different shards, resulting in the zero-shot generation of
hybrid styles.

摘要：<paragraph>我們提出 Diffusion Soup，這是一種針對文字轉圖像生成的分區方法，它會對在分片資料上訓練的擴散模型權重取平均。根據建構，我們的做法能進行免訓練的持續學習和取消學習，而無需額外記憶體或推論成本，因為對應於資料分片的模型可以透過重新取平均來新增或移除。我們展示 Diffusion Soup 從權重空間中的一個點進行取樣，此點近似於組成資料集分佈的幾何平均值，這提供了反記憶保證並能進行零次學習風格混合。根據經驗，Diffusion Soup 優於在所有資料分片的聯集上訓練的範例模型，並且在網域分片資料上影像獎勵方面提升了 30%（.34 $\to$ .44），在美學資料上 IR 提升了 59%（.37 $\to$ .59）。在兩種情況下，souping 也在 TIFA 分數中勝出（分別為 85.5 $\to$ 86.5 和 85.6 $\to$ 86.8）。我們展示了強大的取消學習能力——移除任何個別網域分片只會使 IR 效能降低 1%（.45 $\to$ .44）——並使用真實資料驗證了我們在反記憶方面的理論見解。最後，我們展示了 Diffusion Soup 混合在不同分片上微調的模型的不同風格的能力，產生了零次學習的混合風格。</paragraph>

##### **Improving Noise Robustness through Abstractions and its Impact on Machine Learning**
2406.08428v1 by Alfredo Ibias, Karol Capala, Varun Ravi Varma, Anna Drozdz, Jose Sousa

Noise is a fundamental problem in learning theory with huge effects in the
application of Machine Learning (ML) methods, due to real world data tendency
to be noisy. Additionally, introduction of malicious noise can make ML methods
fail critically, as is the case with adversarial attacks. Thus, finding and
developing alternatives to improve robustness to noise is a fundamental problem
in ML. In this paper, we propose a method to deal with noise: mitigating its
effect through the use of data abstractions. The goal is to reduce the effect
of noise over the model's performance through the loss of information produced
by the abstraction. However, this information loss comes with a cost: it can
result in an accuracy reduction due to the missing information. First, we
explored multiple methodologies to create abstractions, using the training
dataset, for the specific case of numerical data and binary classification
tasks. We also tested how these abstractions can affect robustness to noise
with several experiments that explore the robustness of an Artificial Neural
Network to noise when trained using raw data \emph{vs} when trained using
abstracted data. The results clearly show that using abstractions is a viable
approach for developing noise robust ML methods.

摘要：噪聲是學習理論中一個基本的問題，它對機器學習 (ML) 方法的應用有很大的影響，這是因為現實世界的資料傾向於有雜訊。此外，引入惡意的雜訊可能會導致 ML 方法嚴重失敗，就像對抗攻擊的情況一樣。因此，尋找和開發替代方案以提高對雜訊的魯棒性是 ML 中的一個基本問題。在本文中，我們提出了一種處理雜訊的方法：透過使用資料抽象來減輕其影響。目標是透過抽象產生的資訊損失來減少雜訊對模型效能的影響。然而，這種資訊損失是有代價的：它可能會因為缺少資訊而導致準確度下降。首先，我們探索了多種方法來建立抽象，使用訓練資料集，針對數值資料和二元分類任務的特定案例。我們還測試了這些抽象如何影響對雜訊的魯棒性，進行了多項實驗來探討人工神經網路在使用原始資料訓練時與使用抽象化資料訓練時的抗雜訊性。結果清楚地表明，使用抽象化是一種可行的途徑，可開發出抗雜訊的 ML 方法。

##### **Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**
2406.08426v1 by Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, Xiao Huang

Generating accurate SQL according to natural language questions (text-to-SQL)
is a long-standing problem since it is challenging in user question
understanding, database schema comprehension, and SQL generation. Conventional
text-to-SQL systems include human engineering and deep neural networks.
Subsequently, pre-trained language models (PLMs) have been developed and
utilized for text-to-SQL tasks, achieving promising performance. As modern
databases become more complex and corresponding user questions more
challenging, PLMs with limited comprehension capabilities can lead to incorrect
SQL generation. This necessitates more sophisticated and tailored optimization
methods, which, in turn, restricts the applications of PLM-based systems. Most
recently, large language models (LLMs) have demonstrated significant abilities
in natural language understanding as the model scale remains increasing.
Therefore, integrating the LLM-based implementation can bring unique
opportunities, challenges, and solutions to text-to-SQL research. In this
survey, we present a comprehensive review of LLM-based text-to-SQL.
Specifically, we propose a brief overview of the current challenges and the
evolutionary process of text-to-SQL. Then, we provide a detailed introduction
to the datasets and metrics designed to evaluate text-to-SQL systems. After
that, we present a systematic analysis of recent advances in LLM-based
text-to-SQL. Finally, we discuss the remaining challenges in this field and
propose expectations for future directions.

摘要：根據自然語言問題（文本轉 SQL）產生準確的 SQL 是一個長期存在的問題，因為這在使用者問題理解、資料庫架構理解和 SQL 產生方面具有挑戰性。傳統的文本轉 SQL 系統包含人工工程和深度神經網路。隨後，預訓練語言模型 (PLM) 已被開發並用於文本轉 SQL 任務，達到了有希望的效能。由於現代資料庫變得越來越複雜，對應的使用者問題也更具挑戰性，具有理解能力有限的 PLM 可能導致不正確的 SQL 產生。這需要更精緻和量身定制的最佳化方法，這反過來又限制了基於 PLM 的系統的應用。最近，大型語言模型 (LLM) 在自然語言理解方面展示了顯著的能力，因為模型規模持續增加。因此，整合基於 LLM 的實作可以為文本轉 SQL 研究帶來獨特的機會、挑戰和解決方案。在本次調查中，我們對基於 LLM 的文本轉 SQL 進行了全面的回顧。具體來說，我們簡要概述了當前的挑戰和文本轉 SQL 的演化過程。然後，我們詳細介紹了用於評估文本轉 SQL 系統的資料集和指標。在那之後，我們對基於 LLM 的文本轉 SQL 的最新進展進行了系統分析。最後，我們討論了該領域的剩餘挑戰，並對未來的方向提出期望。

##### **AWGUNET: Attention-Aided Wavelet Guided U-Net for Nuclei Segmentation in Histopathology Images**
2406.08425v1 by Ayush Roy, Payel Pramanik, Dmitrii Kaplun, Sergei Antonov, Ram Sarkar

Accurate nuclei segmentation in histopathological images is crucial for
cancer diagnosis. Automating this process offers valuable support to clinical
experts, as manual annotation is time-consuming and prone to human errors.
However, automating nuclei segmentation presents challenges due to uncertain
cell boundaries, intricate staining, and diverse structures. In this paper, we
present a segmentation approach that combines the U-Net architecture with a
DenseNet-121 backbone, harnessing the strengths of both to capture
comprehensive contextual and spatial information. Our model introduces the
Wavelet-guided channel attention module to enhance cell boundary delineation,
along with a learnable weighted global attention module for channel-specific
attention. The decoder module, composed of an upsample block and convolution
block, further refines segmentation in handling staining patterns. The
experimental results conducted on two publicly accessible histopathology
datasets, namely Monuseg and TNBC, underscore the superiority of our proposed
model, demonstrating its potential to advance histopathological image analysis
and cancer diagnosis. The code is made available at:
https://github.com/AyushRoy2001/AWGUNET.

摘要：組織病理學影像中精確的細胞核分割對於癌症診斷至關重要。自動化此程序可為臨床專家提供有價值的支援，因為手動註解既耗時又容易發生人為錯誤。然而，由於細胞邊界不確定、染色複雜且結構多樣，自動化細胞核分割會產生挑戰。在本文中，我們提出了一種分割方法，將 U-Net 架構與 DenseNet-121 主幹結合，利用兩者的優勢來擷取全面的上下文和空間資訊。我們的模型引入了小波導向通道注意模組，以增強細胞邊界的描繪，以及一個可學習的加權全局注意模組，用於特定通道的注意。解碼器模組由上採樣區塊和卷積區塊組成，進一步優化了處理染色模式的分割。在兩個公開可用的組織病理學資料集（即 Monuseg 和 TNBC）上進行的實驗結果，突顯了我們提出的模型的優越性，證明了其在推進組織病理學影像分析和癌症診斷方面的潛力。程式碼可在以下位置取得：https://github.com/AyushRoy2001/AWGUNET。

##### **State Soup: In-Context Skill Learning, Retrieval and Mixing**
2406.08423v1 by Maciej Pióro, Maciej Wołczyk, Razvan Pascanu, Johannes von Oswald, João Sacramento

A new breed of gated-linear recurrent neural networks has reached
state-of-the-art performance on a range of sequence modeling problems. Such
models naturally handle long sequences efficiently, as the cost of processing a
new input is independent of sequence length. Here, we explore another advantage
of these stateful sequence models, inspired by the success of model merging
through parameter interpolation. Building on parallels between fine-tuning and
in-context learning, we investigate whether we can treat internal states as
task vectors that can be stored, retrieved, and then linearly combined,
exploiting the linearity of recurrence. We study this form of fast model
merging on Mamba-2.8b, a pretrained recurrent model, and present preliminary
evidence that simple linear state interpolation methods suffice to improve
next-token perplexity as well as downstream in-context learning task
performance.

摘要：新一代門控線性遞迴神經網路在各種序列建模問題上已達到最先進的效能。由於處理新輸入的成本與序列長度無關，因此此類模型自然可以有效率地處理長序列。在此，我們探討這些有狀態序列模型的另一項優點，靈感來自透過參數插值進行模型合併的成功。建立在微調和情境中學習之間的相似性之上，我們探討是否能將內部狀態視為可以儲存、擷取，然後線性組合的任務向量，利用遞迴的線性。我們在預先訓練的遞迴模型 Mamba-2.8b 上研究這種快速模型合併形式，並提出初步證據，證明簡單的線性狀態插值方法足以改善下一個代幣的困惑度以及下游情境中學習任務的效能。

##### **OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text**
2406.08418v1 by Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai

Image-text interleaved data, consisting of multiple images and texts arranged
in a natural document format, aligns with the presentation paradigm of internet
data and closely resembles human reading habits. Recent studies have shown that
such data aids multimodal in-context learning and maintains the capabilities of
large language models during multimodal fine-tuning. However, the limited scale
and diversity of current image-text interleaved data restrict the development
of multimodal large language models. In this paper, we introduce OmniCorpus, a
10 billion-scale image-text interleaved dataset. Using an efficient data
engine, we filter and extract large-scale high-quality documents, which contain
8.6 billion images and 1,696 billion text tokens. Compared to counterparts
(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while
maintaining good data quality; 2) features more diverse sources, including both
English and non-English websites as well as video-centric websites; 3) is more
flexible, easily degradable from an image-text interleaved format to pure text
corpus and image-text pairs. Through comprehensive analysis and experiments, we
validate the quality, usability, and effectiveness of the proposed dataset. We
hope this could provide a solid data foundation for future multimodal model
research. Code and data are released at
https://github.com/OpenGVLab/OmniCorpus.

摘要：影像文字穿插資料，由多個影像和文字組成，以自然的文件格式排列，與網際網路資料的呈現範例相符，且與人類的閱讀習慣十分類似。最近的研究顯示，此類資料有助於多模態脈絡學習，並在多模態微調過程中維持大型語言模型的能力。然而，目前影像文字穿插資料的規模和多樣性有限，限制了多模態大型語言模型的發展。在本文中，我們介紹 OmniCorpus，一個 100 億規模的影像文字穿插資料集。我們使用高效能的資料引擎，篩選並擷取大量高品質的文件，其中包含 86 億張影像和 1,6960 億個文字符號。與同類型資料集（例如 MMC4、OBELICS）相比，我們的資料集 1) 規模大 15 倍，同時維持良好的資料品質；2) 來源更多元，包含英文和非英文網站，以及以影片為中心的網站；3) 更具彈性，可輕易從影像文字穿插格式轉換為純文字語料庫和影像文字配對。透過全面的分析和實驗，我們驗證了所提出資料集的品質、可用性和有效性。我們希望這能為未來的多模態模型研究提供穩固的資料基礎。程式碼和資料已於 https://github.com/OpenGVLab/OmniCorpus 發布。

##### **MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos**
2406.08407v1 by Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, Xin Eric Wang

Multimodal Language Language Models (MLLMs) demonstrate the emerging
abilities of "world models" -- interpreting and reasoning about complex
real-world dynamics. To assess these abilities, we posit videos are the ideal
medium, as they encapsulate rich representations of real-world dynamics and
causalities. To this end, we introduce MMWorld, a new benchmark for
multi-discipline, multi-faceted multimodal video understanding. MMWorld
distinguishes itself from previous video understanding benchmarks with two
unique advantages: (1) multi-discipline, covering various disciplines that
often require domain expertise for comprehensive understanding; (2)
multi-faceted reasoning, including explanation, counterfactual thinking, future
prediction, etc. MMWorld consists of a human-annotated dataset to evaluate
MLLMs with questions about the whole videos and a synthetic dataset to analyze
MLLMs within a single modality of perception. Together, MMWorld encompasses
1,910 videos across seven broad disciplines and 69 subdisciplines, complete
with 6,627 question-answer pairs and associated captions. The evaluation
includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld
(e.g., GPT-4V performs the best with only 52.3\% accuracy), showing large room
for improvement. Further ablation studies reveal other interesting findings
such as models' different skill sets from humans. We hope MMWorld can serve as
an essential step towards world model evaluation in videos.

摘要：多模態語言模型 (MLLM) 展示了「世界模型」的新興能力，包括對複雜的真實世界動態進行詮釋和推理。為了評估這些能力，我們假設影片是理想的媒介，因為它們囊括了豐富的真實世界動態和因果關係表徵。為此，我們引入了 MMWorld，一個多學科、多面向多模態影片理解的新基準。MMWorld 憑藉兩個獨特的優勢，與先前的影片理解基準區分開來：(1) 多學科，涵蓋各種通常需要領域專業知識才能全面理解的學科；(2) 多面向推理，包括解釋、反事實思考、未來預測等。MMWorld 包含一個人工標註的資料集，用於評估 MLLM 對整個影片的問題，以及一個用於分析 MLLM 在單一感知模式中的合成資料集。MMWorld 總共包含橫跨七個廣泛學科和 69 個子學科的 1,910 個影片，並附有 6,627 個問答配對和相關字幕。評估包括 2 個專有和 10 個開源 MLLM，它們在 MMWorld 中表現不佳（例如，GPT-4V 表現最佳，但準確率僅有 52.3%），顯示有很大的改進空間。進一步的消融研究揭示了其他有趣的發現，例如模型與人類不同的技能組。我們希望 MMWorld 能成為影片中世界模型評估的重要一步。

##### **Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**
2406.08402v1 by Chun-Yi Kuan, Wei-Ping Huang, Hung-yi Lee

Large audio-language models (LALMs) enhance traditional large language models
by integrating audio perception capabilities, allowing them to tackle
audio-related tasks. Previous research has primarily focused on assessing the
performance of LALMs across various tasks, yet overlooking their reliability,
particularly concerning issues like object hallucination. In our study, we
introduce methods to assess the extent of object hallucination of publicly
available LALMs. Our findings reveal that LALMs are comparable to specialized
audio captioning models in their understanding of audio content, but struggle
to answer discriminative questions, specifically those requiring the
identification of the presence of particular object sounds within an audio
clip. This limitation highlights a critical weakness in current LALMs: their
inadequate understanding of discriminative queries. Moreover, we explore the
potential of prompt engineering to enhance LALMs' performance on discriminative
questions.

摘要：大型音訊語言模型 (LALM) 整合音訊感知能力，強化傳統大型語言模型，讓它們能處理音訊相關任務。先前的研究主要著重於評估 LALM 在各種任務中的表現，卻忽略了它們的可靠性，特別是關於物件幻覺等問題。在我們的研究中，我們引進方法來評估公開可用的 LALM 的物件幻覺程度。我們的研究結果顯示，在理解音訊內容方面，LALM 可與專業音訊字幕模型相提並論，但難以回答辨別性問題，特別是那些需要辨識音訊片段中特定物件聲音是否存在的問題。此限制突顯了當前 LALM 的一個重大弱點：它們對辨別性查詢的理解不足。此外，我們探討了提示工程在提升 LALM 執行辨別性問題的表現方面的潛力。

##### **cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers**
2406.08398v1 by Anirudh Sundar, Jin Xu, William Gay, Christopher Richardson, Larry Heck

An emerging area of research in situated and multimodal interactive
conversations (SIMMC) includes interactions in scientific papers. Since
scientific papers are primarily composed of text, equations, figures, and
tables, SIMMC methods must be developed specifically for each component to
support the depth of inquiry and interactions required by research scientists.
This work introduces Conversational Papers (cPAPERS), a dataset of
conversational question-answer pairs from reviews of academic papers grounded
in these paper components and their associated references from scientific
documents available on arXiv. We present a data collection strategy to collect
these question-answer pairs from OpenReview and associate them with contextual
information from LaTeX source files. Additionally, we present a series of
baseline approaches utilizing Large Language Models (LLMs) in both zero-shot
and fine-tuned configurations to address the cPAPERS dataset.

摘要：情境式和多模式互动对话 (SIMMC) 研究的新兴领域包括科学论文中的互动。由于科学论文主要由文本、方程式、图形和表格组成，因此必须针对每个部分专门开发 SIMMC 方法，以支持研究人员所需的深入探究和互动。这项工作引入了会话论文 (cPAPERS)，这是一个会话问答对数据集，来自对学术论文的评论，这些评论基于论文组件及其来自 arXiv 上可用科学文档的相关参考。我们提出了一种数据收集策略，从 OpenReview 收集这些问答对，并将它们与来自 LaTeX 源文件的内容信息关联起来。此外，我们提出了一系列利用大语言模型 (LLM) 的基线方法，既采用零样本又采用微调配置来解决 cPAPERS 数据集。

##### **Neural Blind Source Separation and Diarization for Distant Speech Recognition**
2406.08396v1 by Yoshiaki Bando, Tomohiko Nakamura, Shinji Watanabe

This paper presents a neural method for distant speech recognition (DSR) that
jointly separates and diarizes speech mixtures without supervision by isolated
signals. A standard separation method for multi-talker DSR is a statistical
multichannel method called guided source separation (GSS). While GSS does not
require signal-level supervision, it relies on speaker diarization results to
handle unknown numbers of active speakers. To overcome this limitation, we
introduce and train a neural inference model in a weakly-supervised manner,
employing the objective function of a statistical separation method. This
training requires only multichannel mixtures and their temporal annotations of
speaker activities. In contrast to GSS, the trained model can jointly separate
and diarize speech mixtures without any auxiliary information. The experiments
with the AMI corpus show that our method outperforms GSS with oracle
diarization results regarding word error rates. The code is available online.

摘要：本文提出了一种用于远距离语音识别 (DSR) 的神经网络方法，该方法在没有孤立信号监督的情况下联合分离并整理语音混合。多说话者 DSR 的标准分离方法是一种称为引导源分离 (GSS) 的统计多通道方法。虽然 GSS 不需要信号级监督，但它依赖于说话者日记结果来处理未知数量的活动说话者。为了克服这一限制，我们以弱监督的方式引入并训练了一个神经推理模型，采用了统计分离方法的目标函数。此训练仅需要多通道混合及其说话者活动的时间注释。与 GSS 相比，训练后的模型可以在没有任何辅助信息的情况下联合分离和整理语音混合。使用 AMI 语料库进行的实验表明，我们的方法在单词错误率方面优于带有 oracle 整理结果的 GSS。该代码可在网上获得。

##### **Large Language Models Must Be Taught to Know What They Don't Know**
2406.08391v1 by Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, Andrew Gordon Wilson

When using large language models (LLMs) in high-stakes applications, we need
to know when we can trust their predictions. Some works argue that prompting
high-performance LLMs is sufficient to produce calibrated uncertainties, while
others introduce sampling methods that can be prohibitively expensive. In this
work, we first argue that prompting on its own is insufficient to achieve good
calibration and then show that fine-tuning on a small dataset of correct and
incorrect answers can create an uncertainty estimate with good generalization
and small computational overhead. We show that a thousand graded examples are
sufficient to outperform baseline methods and that training through the
features of a model is necessary for good performance and tractable for large
open-source models when using LoRA. We also investigate the mechanisms that
enable reliable LLM uncertainty estimation, finding that many models can be
used as general-purpose uncertainty estimators, applicable not just to their
own uncertainties but also the uncertainty of other models. Lastly, we show
that uncertainty estimates inform human use of LLMs in human-AI collaborative
settings through a user study.

摘要：在高風險應用中使用大型語言模型 (LLM) 時，我們需要知道何時可以信任其預測。一些研究認為，提示高性能 LLM 足以產生校準的不確定性，而另一些研究則引入了可能非常昂貴的抽樣方法。在這項研究中，我們首先論證單獨提示不足以實現良好的校準，然後表明在正確和不正確答案的小型數據集上進行微調可以創建具有良好泛化性和較小計算開銷的不確定性估計。我們表明，一千個分級示例足以優於基線方法，並且通過模型的特徵進行訓練對於良好的性能是必要的，並且在使用 LoRA 時對於大型開源模型來說是可行的。我們還研究了實現可靠 LLM 不確定性估計的機制，發現許多模型可以用作通用不確定性估計器，不僅適用於它們自己的不確定性，也適用於其他模型的不確定性。最後，我們通過用戶研究表明，不確定性估計通過人機協作設置告知人類使用 LLM。

##### **Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models**
2406.08384v1 by Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, Stefan Lattner

Recent advancements in deep generative models present new opportunities for
music production but also pose challenges, such as high computational demands
and limited audio quality. Moreover, current systems frequently rely solely on
text input and typically focus on producing complete musical pieces, which is
incompatible with existing workflows in music production. To address these
issues, we introduce "Diff-A-Riff," a Latent Diffusion Model designed to
generate high-quality instrumental accompaniments adaptable to any musical
context. This model offers control through either audio references, text
prompts, or both, and produces 48kHz pseudo-stereo audio while significantly
reducing inference time and memory usage. We demonstrate the model's
capabilities through objective metrics and subjective listening tests, with
extensive examples available on the accompanying website:
sonycslparis.github.io/diffariff-companion/

摘要：深度生成模型的最新进展为音乐制作带来了新机遇，但也带来了挑战，例如高计算需求和有限的音频质量。此外，当前系统通常仅依赖文本输入，并且通常专注于制作完整的音乐片段，这与音乐制作中的现有工作流程不兼容。为了解决这些问题，我们引入了“Diff-A-Riff”，这是一种潜在扩散模型，旨在生成高质量的器乐伴奏，可适应任何音乐环境。此模型通过音频参考、文本提示或两者提供控制，并生成 48kHz 的伪立体声音频，同时显著减少推理时间和内存使用。我们通过客观指标和主观听力测试展示了该模型的能力，并提供了广泛的示例，可在随附网站上找到：sonycslparis.github.io/diffariff-companion/

##### **Towards Unsupervised Speech Recognition Without Pronunciation Models**
2406.08380v1 by Junrui Ni, Liming Wang, Yang Zhang, Kaizhi Qian, Heting Gao, Mark Hasegawa-Johnson, Chang D. Yoo

Recent advancements in supervised automatic speech recognition (ASR) have
achieved remarkable performance, largely due to the growing availability of
large transcribed speech corpora. However, most languages lack sufficient
paired speech and text data to effectively train these systems. In this
article, we tackle the challenge of developing ASR systems without paired
speech and text corpora by proposing the removal of reliance on a phoneme
lexicon. We explore a new research direction: word-level unsupervised ASR.
Using a curated speech corpus containing only high-frequency English words, our
system achieves a word error rate of nearly 20% without parallel transcripts or
oracle word boundaries. Furthermore, we experimentally demonstrate that an
unsupervised speech recognizer can emerge from joint speech-to-speech and
text-to-text masked token-infilling. This innovative model surpasses the
performance of previous unsupervised ASR models trained with direct
distribution matching.

摘要：最近在監督式自動語音辨識 (ASR) 方面的進展已取得顯著的成效，這在很大程度上要歸功於大量轉錄語音語料庫的日益普及。然而，大多數語言都缺乏足夠的配對語音和文字資料，無法有效地訓練這些系統。在本文中，我們透過提議不再依賴音素詞彙表，來應對在沒有配對語音和文字語料庫的情況下開發 ASR 系統的挑戰。我們探索了一個新的研究方向：字元級別的非監督式 ASR。我們的系統使用一個經過整理的語料庫，其中僅包含高頻率的英文單字，在沒有平行轉錄或神諭字元邊界的條件下，即可達到近 20% 的字元錯誤率。此外，我們透過實驗證明，一個非監督式語音辨識器可以從聯合語音到語音和文字到文字的遮蔽符號填入中出現。這個創新的模型超越了以前使用直接分佈匹配訓練的非監督式 ASR 模型的效能。

##### **2.5D Multi-view Averaging Diffusion Model for 3D Medical Image Translation: Application to Low-count PET Reconstruction with CT-less Attenuation Correction**
2406.08374v1 by Tianqi Chen, Jun Hou, Yinchi Zhou, Huidong Xie, Xiongchao Chen, Qiong Liu, Xueqi Guo, Menghua Xia, James S. Duncan, Chi Liu, Bo Zhou

Positron Emission Tomography (PET) is an important clinical imaging tool but
inevitably introduces radiation hazards to patients and healthcare providers.
Reducing the tracer injection dose and eliminating the CT acquisition for
attenuation correction can reduce the overall radiation dose, but often results
in PET with high noise and bias. Thus, it is desirable to develop 3D methods to
translate the non-attenuation-corrected low-dose PET (NAC-LDPET) into
attenuation-corrected standard-dose PET (AC-SDPET). Recently, diffusion models
have emerged as a new state-of-the-art deep learning method for image-to-image
translation, better than traditional CNN-based methods. However, due to the
high computation cost and memory burden, it is largely limited to 2D
applications. To address these challenges, we developed a novel 2.5D Multi-view
Averaging Diffusion Model (MADM) for 3D image-to-image translation with
application on NAC-LDPET to AC-SDPET translation. Specifically, MADM employs
separate diffusion models for axial, coronal, and sagittal views, whose outputs
are averaged in each sampling step to ensure the 3D generation quality from
multiple views. To accelerate the 3D sampling process, we also proposed a
strategy to use the CNN-based 3D generation as a prior for the diffusion model.
Our experimental results on human patient studies suggested that MADM can
generate high-quality 3D translation images, outperforming previous CNN-based
and Diffusion-based baseline methods.

摘要：正子發射斷層掃描 (PET) 是一種重要的臨床影像工具，但不可避免地會對患者和醫療保健提供者造成輻射危害。降低示蹤劑注射劑量並消除電腦斷層掃描以進行衰減校正可以降低整體輻射劑量，但通常會導致 PET 產生高雜訊和偏差。因此，開發 3D 方法將非衰減校正低劑量 PET (NAC-LDPET) 轉換為衰減校正標準劑量 PET (AC-SDPET) 是很重要的。最近，擴散模型已成為一種新的最先進的深度學習方法，用於影像轉換影像，優於傳統的基於 CNN 的方法。然而，由於高運算成本和記憶體負擔，它在很大程度上僅限於 2D 應用程式。為了應對這些挑戰，我們開發了一種新穎的 2.5D 多視圖平均擴散模型 (MADM)，用於 3D 影像轉換影像，並應用於 NAC-LDPET 轉換為 AC-SDPET。具體來說，MADM 為軸向、冠狀和矢狀視圖採用了單獨的擴散模型，其輸出在每個採樣步驟中取平均值，以確保從多個視圖中進行 3D 生成品質。為了加速 3D 採樣過程，我們還提出了一種策略，將基於 CNN 的 3D 生成用作擴散模型的先驗。我們在人體患者研究中得到的實驗結果表明，MADM 可以生成高品質的 3D 轉換影像，優於先前的基於 CNN 和基於擴散的基線方法。

##### **From a Social Cognitive Perspective: Context-aware Visual Social Relationship Recognition**
2406.08358v1 by Shiwei Wu, Chao Zhang, Joya Chen, Tong Xu, Likang Wu, Yao Hu, Enhong Chen

People's social relationships are often manifested through their
surroundings, with certain objects or interactions acting as symbols for
specific relationships, e.g., wedding rings, roses, hugs, or holding hands.
This brings unique challenges to recognizing social relationships, requiring
understanding and capturing the essence of these contexts from visual
appearances. However, current methods of social relationship understanding rely
on the basic classification paradigm of detected persons and objects, which
fails to understand the comprehensive context and often overlooks decisive
social factors, especially subtle visual cues. To highlight the social-aware
context and intricate details, we propose a novel approach that recognizes
\textbf{Con}textual \textbf{So}cial \textbf{R}elationships (\textbf{ConSoR})
from a social cognitive perspective. Specifically, to incorporate social-aware
semantics, we build a lightweight adapter upon the frozen CLIP to learn social
concepts via our novel multi-modal side adapter tuning mechanism. Further, we
construct social-aware descriptive language prompts (e.g., scene, activity,
objects, emotions) with social relationships for each image, and then compel
ConSoR to concentrate more intensively on the decisive visual social factors
via visual-linguistic contrasting. Impressively, ConSoR outperforms previous
methods with a 12.2\% gain on the People-in-Social-Context (PISC) dataset and a
9.8\% increase on the People-in-Photo-Album (PIPA) benchmark. Furthermore, we
observe that ConSoR excels at finding critical visual evidence to reveal social
relationships.

摘要：人們的社交關係通常透過他們的周遭環境體現出來，某些物件或互動作為特定關係的象徵，例如結婚戒指、玫瑰花、擁抱或牽手。這為識別社交關係帶來了獨特的挑戰，需要從視覺外觀中理解和捕捉這些脈絡的本質。然而，當前社交關係理解方法依賴於已偵測人物和物件的基本分類範例，這無法理解全面的脈絡，且經常忽略決定性的社會因素，特別是微妙的視覺線索。為了強調具有社會意識的脈絡和複雜細節，我們提出了一種新穎的方法，從社會認知的角度識別**情境****社**會**關**係（**ConSoR**）。具體來說，為了納入具有社會意識的語義，我們在凍結的 CLIP 上建立了一個輕量級適配器，以透過我們新穎的多模式側適配器調整機制來學習社會概念。此外，我們為每張影像建立具有社會關係的社會意識描述性語言提示（例如場景、活動、物件、情緒），然後迫使 ConSoR 透過視覺語言對比更專注於決定性的視覺社會因素。令人印象深刻的是，ConSoR 在 People-in-Social-Context (PISC) 資料集上以 12.2% 的增益優於先前的各種方法，並在 People-in-Photo-Album (PIPA) 基準上增加了 9.8%。此外，我們觀察到 ConSoR 擅長找到關鍵的視覺證據來揭示社會關係。

##### **DocSynthv2: A Practical Autoregressive Modeling for Document Generation**
2406.08354v1 by Sanket Biswas, Rajiv Jain, Vlad I. Morariu, Jiuxiang Gu, Puneet Mathur, Curtis Wigington, Tong Sun, Josep Lladós

While the generation of document layouts has been extensively explored,
comprehensive document generation encompassing both layout and content presents
a more complex challenge. This paper delves into this advanced domain,
proposing a novel approach called DocSynthv2 through the development of a
simple yet effective autoregressive structured model. Our model, distinct in
its integration of both layout and textual cues, marks a step beyond existing
layout-generation approaches. By focusing on the relationship between the
structural elements and the textual content within documents, we aim to
generate cohesive and contextually relevant documents without any reliance on
visual components. Through experimental studies on our curated benchmark for
the new task, we demonstrate the ability of our model combining layout and
textual information in enhancing the generation quality and relevance of
documents, opening new pathways for research in document creation and automated
design. Our findings emphasize the effectiveness of autoregressive models in
handling complex document generation tasks.

摘要：雖然文件版面生成已廣泛探討，
但包含版面和內容的全面文件生成
是一個更複雜的挑戰。這篇論文深入探討這個進階領域，
提出一個稱為 DocSynthv2 的新方法，透過開發一個
簡單但有效的自迴歸結構模型。我們的模型不同於
整合版面和文字提示，標誌著超越現有
版面生成方法的一步。透過專注於文件中的
結構元素和文字內容之間的關係，我們旨在
生成有凝聚力和符合脈絡的文件，而無需依賴
視覺組件。透過對我們為新任務策展的基準進行實驗研究，我們展示了我們的模型結合版面和
文字資訊的能力，以提升文件生成品質和相關性，為文件建立和自動
設計的研究開啟新途徑。我們的發現強調了自迴歸模型在
處理複雜文件生成任務中的有效性。

##### **Continuous-Time Digital Twin with Analogue Memristive Neural Ordinary Differential Equation Solver**
2406.08343v1 by Hegan Chen, Jichang Yang, Jia Chen, Songqi Wang, Shaocong Wang, Dingchen Wang, Xinyu Tian, Yifei Yu, Xi Chen, Yinan Lin, Yangu He, Xiaoshan Wu, Yi Li, Xinyuan Zhang, Ning Lin, Meng Xu, Yi Li, Xumeng Zhang, Zhongrui Wang, Han Wang, Dashan Shang, Qi Liu, Kwang-Ting Cheng, Ming Liu

Digital twins, the cornerstone of Industry 4.0, replicate real-world entities
through computer models, revolutionising fields such as manufacturing
management and industrial automation. Recent advances in machine learning
provide data-driven methods for developing digital twins using discrete-time
data and finite-depth models on digital computers. However, this approach fails
to capture the underlying continuous dynamics and struggles with modelling
complex system behaviour. Additionally, the architecture of digital computers,
with separate storage and processing units, necessitates frequent data
transfers and Analogue-Digital (A/D) conversion, thereby significantly
increasing both time and energy costs. Here, we introduce a memristive neural
ordinary differential equation (ODE) solver for digital twins, which is capable
of capturing continuous-time dynamics and facilitates the modelling of complex
systems using an infinite-depth model. By integrating storage and computation
within analogue memristor arrays, we circumvent the von Neumann bottleneck,
thus enhancing both speed and energy efficiency. We experimentally validate our
approach by developing a digital twin of the HP memristor, which accurately
extrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and
a 41.4-fold projected decrease in energy consumption compared to
state-of-the-art digital hardware, while maintaining an acceptable error
margin. Additionally, we demonstrate scalability through experimentally
grounded simulations of Lorenz96 dynamics, exhibiting projected performance
improvements of 12.6-fold in speed and 189.7-fold in energy efficiency relative
to traditional digital approaches. By harnessing the capabilities of fully
analogue computing, our breakthrough accelerates the development of digital
twins, offering an efficient and rapid solution to meet the demands of Industry
4.0.

摘要：數位孿生為工業 4.0 的基石，透過電腦模型複製真實世界的實體，革新製造管理與工業自動化等領域。機器學習的最新進展提供了資料驅動的方法，使用離散時間資料和數位電腦上的有限深度模型來開發數位孿生。然而，此方法無法擷取基礎的連續動態，並難以建模複雜的系統行為。此外，數位電腦的架構具有分開的儲存和處理單元，需要頻繁的資料傳輸和類比數位 (A/D) 轉換，進而顯著增加時間和能源成本。在此，我們介紹了一種用於數位孿生的記憶電阻神經常微分方程式 (ODE) 求解器，它能夠擷取連續時間動態，並使用無限深度模型促進複雜系統的建模。透過在類比記憶電阻陣列中整合儲存和運算，我們避開了馮紐曼瓶頸，從而提升速度和能源效率。我們透過開發惠普記憶電阻的數位孿生來驗證我們的實驗方法，它準確地外推其非線性動態，與最先進的數位硬體相比，實現了 4.2 倍的預測速度提升和 41.4 倍的預測能耗降低，同時維持可接受的誤差範圍。此外，我們透過 Lorenz96 動態的實驗基礎模擬展示了可擴充性，相較於傳統數位方法，展現了速度提升 12.6 倍和能源效率提升 189.7 倍的預測效能改善。透過利用全類比運算的能力，我們的突破加速了數位孿生的開發，提供了一種有效且快速的解決方案，以滿足工業 4.0 的需求。

##### **ProTrain: Efficient LLM Training via Memory-Aware Techniques**
2406.08334v1 by Hanmei Yang, Jin Zhou, Yao Fu, Xiaoqun Wang, Ramine Roane, Hui Guan, Tongping Liu

It is extremely memory-hungry to train Large Language Models (LLM). To solve
this problem, existing work exploits the combination of CPU and GPU for the
training process, such as ZeRO-Offload. Such a technique largely democratizes
billion-scale model training, making it possible to train with few consumer
graphics cards. However, based on our observation, existing frameworks often
provide coarse-grained memory management and require experienced experts in
configuration tuning, leading to suboptimal hardware utilization and
performance. This paper proposes ProTrain, a novel training system that
intelligently balances memory usage and performance by coordinating memory,
computation, and IO. ProTrain achieves adaptive memory management through
Chunk-Based Model State Management and Block-Wise Activation Management, guided
by a Memory-Aware Runtime Profiler without user intervention. ProTrain does not
change the training algorithm and thus does not compromise accuracy.
Experiments show that ProTrain improves training throughput by 1.43$\times$ to
2.71$\times$ compared to the SOTA training systems.

摘要：訓練大型語言模型 (LLM) 極度耗費記憶體。為了解決這個問題，現有工作利用 CPU 和 GPU 的組合進行訓練過程，例如 ZeRO-Offload。這種技術在很大程度上民主化了十億規模的模型訓練，讓使用少數消費者級顯示卡進行訓練成為可能。然而，根據我們的觀察，現有框架通常提供粗略的記憶體管理，並需要經驗豐富的專家進行配置調整，導致硬體利用率和效能不佳。本文提出 ProTrain，這是一個新穎的訓練系統，透過協調記憶體、運算和 IO，在記憶體使用和效能之間取得平衡。ProTrain 透過區塊化模型狀態管理和區塊化激活管理，在無需使用者介入的情況下，透過記憶體感知執行時期剖析器，達成適應性記憶體管理。ProTrain 沒有改變訓練演算法，因此不會影響準確性。實驗顯示，與 SOTA 訓練系統相比，ProTrain 將訓練處理量提升了 1.43 倍至 2.71 倍。

##### **It's all about PR -- Smart Benchmarking AI Accelerators using Performance Representatives**
2406.08330v1 by Alexander Louis-Ferdinand Jung, Jannik Steinmetz, Jonathan Gietz, Konstantin Lübeck, Oliver Bringmann

Statistical models are widely used to estimate the performance of commercial
off-the-shelf (COTS) AI hardware accelerators. However, training of statistical
performance models often requires vast amounts of data, leading to a
significant time investment and can be difficult in case of limited hardware
availability. To alleviate this problem, we propose a novel performance
modeling methodology that significantly reduces the number of training samples
while maintaining good accuracy. Our approach leverages knowledge of the target
hardware architecture and initial parameter sweeps to identify a set of
Performance Representatives (PR) for deep neural network (DNN) layers. These
PRs are then used for benchmarking, building a statistical performance model,
and making estimations. This targeted approach drastically reduces the number
of training samples needed, opposed to random sampling, to achieve a better
estimation accuracy. We achieve a Mean Absolute Percentage Error (MAPE) of as
low as 0.02% for single-layer estimations and 0.68% for whole DNN estimations
with less than 10000 training samples. The results demonstrate the superiority
of our method for single-layer estimations compared to models trained with
randomly sampled datasets of the same size.

摘要：統計模型廣泛用於估計商用現成 (COTS) AI 硬體加速器的效能。然而，統計效能模型的訓練通常需要大量的資料，導致顯著的時間投資，且在硬體可用性有限的情況下可能很困難。為了緩解此問題，我們提出了一種新穎的效能建模方法，可大幅減少訓練樣本的數量，同時維持良好的準確度。我們的做法利用目標硬體架構和初始參數掃描的知識，為深度神經網路 (DNN) 層識別一組效能代表 (PR)。這些 PR 隨後用於基準測試、建立統計效能模型和進行估計。這種有針對性的方法大幅減少了與隨機抽樣相比所需的訓練樣本數量，以實現更好的估計準確度。我們針對單層估計達到了低至 0.02% 的平均絕對百分比誤差 (MAPE)，而針對整個 DNN 估計達到了 0.68%，訓練樣本少於 10000 個。結果證明了我們的方法在單層估計方面的優越性，與使用相同大小的隨機抽樣資料集訓練的模型相比。

##### **Is Programming by Example solved by LLMs?**
2406.08316v1 by Wen-Ding Li, Kevin Ellis

Programming-by-Examples (PBE) aims to generate an algorithm from input-output
examples. Such systems are practically and theoretically important: from an
end-user perspective, they are deployed to millions of people, and from an AI
perspective, PBE corresponds to a very general form of few-shot inductive
inference. Given the success of Large Language Models (LLMs) in code-generation
tasks, we investigate here the extent to which LLMs can be said to have
`solved' PBE. We experiment on classic domains such as lists and strings, and
an uncommon graphics programming domain not well represented in typical
pretraining data. We find that pretrained models are not effective at PBE, but
that they can be fine-tuned for much higher performance, provided the test
problems are in-distribution. We analyze empirically what causes these models
to succeed and fail, and take steps toward understanding how to achieve better
out-of-distribution generalization. Collectively these results suggest that
LLMs make strong progress toward solving the typical suite of PBE tasks,
potentially increasing the flexibility and applicability of PBE systems, while
also identifying ways in which LLMs still fall short.

摘要：程式碼範例 (PBE) 目標是從輸入輸出範例中產生演算法。這些系統在實務上和理論上都非常重要：從最終使用者的觀點來看，它們被部署到數百萬人身上，而從 AI 的觀點來看，PBE 對應到非常通用的少量次數歸納式推論。考量到大型語言模型 (LLM) 在程式碼產生任務中的成功，我們在此探討 LLM 在何種程度上可以被視為「解決」了 PBE。我們在清單和字串等經典領域，以及在典型預訓練資料中未充分呈現的罕見圖形程式設計領域進行實驗。我們發現預訓練模型在 PBE 中並未發揮作用，但它們可以微調以獲得更高的效能，前提是測試問題在分佈中。我們透過經驗分析導致這些模型成功和失敗的原因，並採取措施了解如何達成更好的非分佈概化。這些結果共同表明 LLM 在解決典型的 PBE 任務套件方面取得顯著進展，這可能會增加 PBE 系統的靈活性和適用性，同時也找出 LLM 仍有不足之處。

##### **Causality for Tabular Data Synthesis: A High-Order Structure Causal Benchmark Framework**
2406.08311v1 by Ruibo Tu, Zineb Senane, Lele Cao, Cheng Zhang, Hedvig Kjellström, Gustav Eje Henter

Tabular synthesis models remain ineffective at capturing complex
dependencies, and the quality of synthetic data is still insufficient for
comprehensive downstream tasks, such as prediction under distribution shifts,
automated decision-making, and cross-table understanding. A major challenge is
the lack of prior knowledge about underlying structures and high-order
relationships in tabular data. We argue that a systematic evaluation on
high-order structural information for tabular data synthesis is the first step
towards solving the problem. In this paper, we introduce high-order structural
causal information as natural prior knowledge and provide a benchmark framework
for the evaluation of tabular synthesis models. The framework allows us to
generate benchmark datasets with a flexible range of data generation processes
and to train tabular synthesis models using these datasets for further
evaluation. We propose multiple benchmark tasks, high-order metrics, and causal
inference tasks as downstream tasks for evaluating the quality of synthetic
data generated by the trained models. Our experiments demonstrate to leverage
the benchmark framework for evaluating the model capability of capturing
high-order structural causal information. Furthermore, our benchmarking results
provide an initial assessment of state-of-the-art tabular synthesis models.
They have clearly revealed significant gaps between ideal and actual
performance and how baseline methods differ. Our benchmark framework is
available at URL https://github.com/TURuibo/CauTabBench.

摘要：表格合成模型在捕捉复杂依存关系方面仍然无效，并且合成数据的质量对于全面的下游任务来说仍然不够，例如分布转换下的预测、自动化决策和跨表理解。一个主要挑战是缺乏关于表格数据中底层结构和高阶关系的先验知识。我们认为，对表格数据合成的更高阶结构信息的系统评估是解决该问题的第一步。在本文中，我们引入了高阶结构因果信息作为自然先验知识，并为表格合成模型的评估提供了一个基准框架。该框架允许我们使用灵活的数据生成过程生成基准数据集，并使用这些数据集训练表格合成模型以进行进一步评估。我们提出了多个基准任务、高阶指标和因果推理任务作为下游任务，以评估训练模型生成的合成数据的质量。我们的实验表明，利用基准框架评估模型捕获高阶结构因果信息的能力。此外，我们的基准测试结果对最先进的表格合成模型进行了初步评估。它们清楚地揭示了理想性能和实际性能之间的显着差距，以及基线方法之间的差异。我们的基准框架可在 URL https://github.com/TURuibo/CauTabBench 获得。

##### **Analyzing constrained LLM through PDFA-learning**
2406.08269v1 by Matías Carrasco, Franz Mayr, Sergio Yovine, Johny Kidd, Martín Iturbide, Juan Pedro da Silva, Alejo Garat

We define a congruence that copes with null next-symbol probabilities that
arise when the output of a language model is constrained by some means during
text generation. We develop an algorithm for efficiently learning the quotient
with respect to this congruence and evaluate it on case studies for analyzing
statistical properties of LLM.

摘要：我們定義了一個同餘，它應對在文本生成期間語言模型的輸出受到某種方式約束時出現的空下一個符號機率。我們開發了一個演算法，以便有效率地學習關於此同餘的商，並在案例研究中評估它，以分析 LLM 的統計性質。

##### **A deep cut into Split Federated Self-supervised Learning**
2406.08267v1 by Marcin Przewięźlikowski, Marcin Osial, Bartosz Zieliński, Marek Śmieja

Collaborative self-supervised learning has recently become feasible in highly
distributed environments by dividing the network layers between client devices
and a central server. However, state-of-the-art methods, such as MocoSFL, are
optimized for network division at the initial layers, which decreases the
protection of the client data and increases communication overhead. In this
paper, we demonstrate that splitting depth is crucial for maintaining privacy
and communication efficiency in distributed training. We also show that MocoSFL
suffers from a catastrophic quality deterioration for the minimal communication
overhead. As a remedy, we introduce Momentum-Aligned contrastive Split
Federated Learning (MonAcoSFL), which aligns online and momentum client models
during training procedure. Consequently, we achieve state-of-the-art accuracy
while significantly reducing the communication overhead, making MonAcoSFL more
practical in real-world scenarios.

摘要：協作式自我監督學習最近已透過在用戶端裝置與中央伺服器之間分割網路層，在高度分散式環境中變得可行。然而，例如 MocoSFL 等最先進的方法最佳化為在初始層進行網路分割，這會降低用戶端資料的保護並增加通訊負擔。在本文中，我們證明深度分割對於在分散式訓練中維護隱私和通訊效率至關重要。我們也顯示 MocoSFL 會因最小的通訊負擔而遭受災難性的品質惡化。作為補救措施，我們引進動量對齊對比式分割聯邦學習 (MonAcoSFL)，它會在訓練程序中對齊線上和動量用戶端模型。因此，我們達到了最先進的準確度，同時大幅降低通訊負擔，讓 MonAcoSFL 在現實世界情境中更實用。

##### **Leveraging Large Language Models for Web Scraping**
2406.08246v1 by Aman Ahluwalia, Suhrud Wani

Large Language Models (LLMs) demonstrate remarkable capabilities in
replicating human tasks and boosting productivity. However, their direct
application for data extraction presents limitations due to a prioritisation of
fluency over factual accuracy and a restricted ability to manipulate specific
information. Therefore to overcome these limitations, this research leverages
the knowledge representation power of pre-trained LLMs and the targeted
information access enabled by RAG models, this research investigates a
general-purpose accurate data scraping recipe for RAG models designed for
language generation. To capture knowledge in a more modular and interpretable
way, we use pre trained language models with a latent knowledge retriever,
which allows the model to retrieve and attend over documents from a large
corpus. We utilised RAG model architecture and did an in-depth analysis of
their capabilities under three tasks: (i) Semantic Classification of HTML
elements, (ii) Chunking HTML text for effective understanding, and (iii)
comparing results from different LLMs and ranking algorithms. While previous
work has developed dedicated architectures and training procedures for HTML
understanding and extraction, we show that LLMs pre-trained on standard natural
language with an addition of effective chunking, searching and ranking
algorithms, can prove to be efficient data scraping tool to extract complex
data from unstructured text. Future research directions include addressing the
challenges of provenance tracking and dynamic knowledge updates within the
proposed RAG-based data extraction framework. By overcoming these limitations,
this approach holds the potential to revolutionise data extraction from vast
repositories of textual information.

摘要：大型語言模型 (LLM) 在複製人類任務和提升生產力方面展現了非凡的能力。然而，由於優先考慮流暢性而非事實準確性，以及操縱特定資訊的能力受限，它們在資料萃取方面的直接應用存在限制。因此，為了克服這些限制，本研究利用預先訓練 LLM 的知識表示能力和 RAG 模型所實現的目標資訊存取，探討一種針對語言生成而設計的 RAG 模型的通用且準確的資料擷取配方。為了以更模組化且可解釋的方式擷取知識，我們使用具有潛在知識檢索器的預先訓練語言模型，這允許模型從大型語料庫中檢索文件並關注這些文件。我們利用了 RAG 模型架構，並對它們在三項任務下的能力進行了深入分析：(i) HTML 元素的語義分類，(ii) 有效理解 HTML 文字的區塊化，以及 (iii) 比較不同 LLM 和排序演算法的結果。儘管先前的研究已經開發出專門的架構和訓練程序來理解和萃取 HTML，我們展示了在標準自然語言上預先訓練的 LLM，加上有效的區塊化、搜尋和排序演算法，可以證明是一種有效率的資料擷取工具，可從非結構化文字中萃取複雜的資料。未來的研究方向包括解決建議的基於 RAG 的資料萃取架構中，來源追蹤和動態知識更新的挑戰。透過克服這些限制，此方法有潛力革新從大量文字資訊儲存庫中萃取資料的方式。

##### **Using Deep Convolutional Neural Networks to Detect Rendered Glitches in Video Games**
2406.08231v1 by Carlos Garcia Ling, Konrad Tollmar, Linus Gisslen

In this paper, we present a method using Deep Convolutional Neural Networks
(DCNNs) to detect common glitches in video games. The problem setting consists
of an image (800x800 RGB) as input to be classified into one of five defined
classes, normal image, or one of four different kinds of glitches (stretched,
low resolution, missing and placeholder textures). Using a supervised approach,
we train a ShuffleNetV2 using generated data. This work focuses on detecting
texture graphical anomalies achieving arguably good performance with an
accuracy of 86.8\%, detecting 88\% of the glitches with a false positive rate
of 8.7\%, and with the models being able to generalize and detect glitches even
in unseen objects. We apply a confidence measure as well to tackle the issue
with false positives as well as an effective way of aggregating images to
achieve better detection in production. The main use of this work is the
partial automatization of graphical testing in the final stages of video game
development.

摘要：在本文中，我们提出一种使用深度卷积神经网络 (DCNN) 来检测视频游戏中常见故障的方法。问题设置包括一张图像 (800x800 RGB)，作为输入被分类为五个已定义的类别之一，即正常图像或四种不同类型的故障（拉伸、低分辨率、缺失和占位纹理）。使用监督方法，我们使用生成的数据训练 ShuffleNetV2。这项工作重点是检测纹理图形异常，以 86.8% 的准确度实现可以说是良好的性能，以 8.7% 的误报率检测 88% 的故障，并且模型能够泛化和检测故障，即使在看不见的对象中。我们还应用置信度测量来解决误报问题，以及一种聚合图像的有效方法，以在生产中实现更好的检测。这项工作的用途主要是视频游戏开发最后阶段图形测试的部分自动化。

##### **DistilDoc: Knowledge Distillation for Visually-Rich Document Applications**
2406.08226v1 by Jordy Van Landeghem, Subhajit Maity, Ayan Banerjee, Matthew Blaschko, Marie-Francine Moens, Josep Lladós, Sanket Biswas

This work explores knowledge distillation (KD) for visually-rich document
(VRD) applications such as document layout analysis (DLA) and document image
classification (DIC). While VRD research is dependent on increasingly
sophisticated and cumbersome models, the field has neglected to study
efficiency via model compression. Here, we design a KD experimentation
methodology for more lean, performant models on document understanding (DU)
tasks that are integral within larger task pipelines. We carefully selected KD
strategies (response-based, feature-based) for distilling knowledge to and from
backbones with different architectures (ResNet, ViT, DiT) and capacities (base,
small, tiny). We study what affects the teacher-student knowledge gap and find
that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can
consistently outperform supervised student training. Furthermore, we design
downstream task setups to evaluate covariate shift and the robustness of
distilled DLA models on zero-shot layout-aware document visual question
answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,
which unpredictably translates to downstream robustness, accentuating the need
to further explore how to efficiently obtain more semantic document layout
awareness.

摘要：这项工作探索了视觉丰富的文档 (VRD) 应用（例如文档布局分析 (DLA) 和文档图像分类 (DIC)）的知识蒸馏 (KD)。虽然 VRD 研究依赖于越来越复杂且繁琐的模型，但该领域忽略了通过模型压缩来研究效率。在此，我们设计了一种 KD 实验方法，用于在更大任务管道中不可或缺的文档理解 (DU) 任务上获得更精简、性能更高的模型。我们仔细选择了 KD 策略（基于响应、基于特征），用于将知识从具有不同架构（ResNet、ViT、DiT）和容量（基础、小型、微型）的主干网络蒸馏到主干网络，反之亦然。我们研究了影响师生知识差距的因素，发现一些方法（调整后的香草 KD、MSE、带有适当投影仪的 SimKD）可以持续优于监督式学生训练。此外，我们设计了下游任务设置，以评估协变量偏移和蒸馏 DLA 模型在零样本布局感知文档视觉问题解答 (DocVQA) 中的鲁棒性。DLA-KD 实验导致较大的 mAP 知识差距，该差距不可预测地转化为下游鲁棒性，突出了进一步探索如何有效获得更多语义文档布局感知的必要性。

##### **Research Trends for the Interplay between Large Language Models and Knowledge Graphs**
2406.08223v1 by Hanieh Khorashadizadeh, Fatima Zahra Amara, Morteza Ezzabady, Frédéric Ieng, Sanju Tiwari, Nandana Mihindukulasooriya, Jinghua Groppe, Soror Sahri, Farah Benamara, Sven Groppe

This survey investigates the synergistic relationship between Large Language
Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's
capabilities in understanding, reasoning, and language processing. It aims to
address gaps in current research by exploring areas such as KG Question
Answering, ontology generation, KG validation, and the enhancement of KG
accuracy and consistency through LLMs. The paper further examines the roles of
LLMs in generating descriptive texts and natural language queries for KGs.
Through a structured analysis that includes categorizing LLM-KG interactions,
examining methodologies, and investigating collaborative uses and potential
biases, this study seeks to provide new insights into the combined potential of
LLMs and KGs. It highlights the importance of their interaction for improving
AI applications and outlines future research directions.

摘要：這項調查探討大型語言模型 (LLM) 與知識圖譜 (KG) 之間的協同關係，這對提升 AI 在理解、推理和語言處理方面的能力至關重要。它旨在透過探討知識圖譜問答、本体生成、知識圖譜驗證，以及透過 LLM 增強知識圖譜的準確性和一致性等領域，來解決當前研究中的差距。本文進一步探討 LLM 在為知識圖譜產生描述性文字和自然語言查詢方面的作用。透過一項結構化分析，包括分類 LLM-KG 互動、檢視方法、以及探討協作用途和潛在偏差，本研究旨在提供 LLM 和知識圖譜結合潛力的新見解。它強調了它們互動對於改善 AI 應用程式的重要性，並概述了未來的研究方向。

##### **A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion**
2406.08222v1 by Sha Luo, Sang Jung Kim, Zening Duan, Kaiping Chen

In the evolving landscape of computer vision (CV) technologies, the automatic
detection and interpretation of gender and emotion in images is a critical area
of study. This paper investigates social biases in CV models, emphasizing the
limitations of traditional evaluation metrics such as precision, recall, and
accuracy. These metrics often fall short in capturing the complexities of
gender and emotion, which are fluid and culturally nuanced constructs. Our
study proposes a sociotechnical framework for evaluating CV models,
incorporating both technical performance measures and considerations of social
fairness. Using a dataset of 5,570 images related to vaccination and climate
change, we empirically compared the performance of various CV models, including
traditional models like DeepFace and FER, and generative models like GPT-4
Vision. Our analysis involved manually validating the gender and emotional
expressions in a subset of images to serve as benchmarks. Our findings reveal
that while GPT-4 Vision outperforms other models in technical accuracy for
gender classification, it exhibits discriminatory biases, particularly in
response to transgender and non-binary personas. Furthermore, the model's
emotion detection skew heavily towards positive emotions, with a notable bias
towards associating female images with happiness, especially when prompted by
male personas. These findings underscore the necessity of developing more
comprehensive evaluation criteria that address both validity and discriminatory
biases in CV models. Our proposed framework provides guidelines for researchers
to critically assess CV tools, ensuring their application in communication
research is both ethical and effective. The significant contribution of this
study lies in its emphasis on a sociotechnical approach, advocating for CV
technologies that support social good and mitigate biases rather than
perpetuate them.

摘要：在不斷演進的電腦視覺 (CV) 技術領域中，自動偵測和解讀圖像中的性別和情緒是一項重要的研究領域。本文探討 CV 模型中的社會偏見，強調傳統評估指標（例如準確度、召回率和準確性）的局限性。這些指標通常無法捕捉性別和情緒的複雜性，而性別和情緒是流動且具有文化差異的建構。我們的研究提出了一個社會技術框架來評估 CV 模型，結合技術性能指標和社會公平性考量。我們使用一個包含 5,570 張與疫苗接種和氣候變遷相關的圖像的資料集，對各種 CV 模型的效能進行實證比較，包括 DeepFace 和 FER 等傳統模型，以及 GPT-4 Vision 等生成模型。我們的分析涉及手動驗證部分圖像中的性別和情緒表達，作為基準。我們的研究結果顯示，儘管 GPT-4 Vision 在性別分類的技術準確度方面優於其他模型，但它表現出歧視性偏見，特別是在回應跨性別和非二元性別角色時。此外，該模型的情緒偵測嚴重偏向正面情緒，特別是在男性角色提示時，將女性圖像與快樂聯繫起來。這些發現強調了制定更全面的評量標準的必要性，以解決 CV 模型中的有效性和歧視性偏見。我們提出的框架為研究人員提供準則，以批判性地評估 CV 工具，確保它們在傳播研究中的應用既符合倫理道德，又有效。本研究的重要貢獻在於強調社會技術方法，倡導 CV 技術支持社會公益並減輕偏見，而不是讓偏見永存。

##### **Figuratively Speaking: Authorship Attribution via Multi-Task Figurative Language Modeling**
2406.08218v1 by Gregorios A Katsios, Ning Sa, Tomek Strzalkowski

The identification of Figurative Language (FL) features in text is crucial
for various Natural Language Processing (NLP) tasks, where understanding of the
author's intended meaning and its nuances is key for successful communication.
At the same time, the use of a specific blend of various FL forms most
accurately reflects a writer's style, rather than the use of any single
construct, such as just metaphors or irony. Thus, we postulate that FL features
could play an important role in Authorship Attribution (AA) tasks. We believe
that our is the first computational study of AA based on FL use. Accordingly,
we propose a Multi-task Figurative Language Model (MFLM) that learns to detect
multiple FL features in text at once. We demonstrate, through detailed
evaluation across multiple test sets, that the our model tends to perform
equally or outperform specialized binary models in FL detection. Subsequently,
we evaluate the predictive capability of joint FL features towards the AA task
on three datasets, observing improved AA performance through the integration of
MFLM embeddings.

摘要：在文本中識別比喻語言 (FL) 特徵對於各種自然語言處理 (NLP) 任務至關重要，其中了解作者的預期含義及其細微差別是成功溝通的關鍵。同時，使用各種 FL 形式的特定組合最能準確反映作者的風格，而不是使用任何單一結構，例如比喻或反諷。因此，我們假設 FL 特徵可以在作者歸屬 (AA) 任務中發揮重要作用。我們相信這是第一個基於 FL 使用的 AA 計算研究。因此，我們提出了一個多任務比喻語言模型 (MFLM)，它可以一次學習檢測文本中的多個 FL 特徵。我們通過多個測試集的詳細評估證明，我們的模型在 FL 檢測中往往表現得與專門的二元模型一樣好，甚至表現得更好。隨後，我們評估了聯合 FL 特徵對三個數據集上 AA 任務的預測能力，觀察到通過整合 MFLM 嵌入改進了 AA 性能。

##### **Transformer-based Model for ASR N-Best Rescoring and Rewriting**
2406.08207v1 by Iwen E. Kang, Christophe Van Gysel, Man-Hung Siu

Voice assistants increasingly use on-device Automatic Speech Recognition
(ASR) to ensure speed and privacy. However, due to resource constraints on the
device, queries pertaining to complex information domains often require further
processing by a search engine. For such applications, we propose a novel
Transformer based model capable of rescoring and rewriting, by exploring full
context of the N-best hypotheses in parallel. We also propose a new
discriminative sequence training objective that can work well for both rescore
and rewrite tasks. We show that our Rescore+Rewrite model outperforms the
Rescore-only baseline, and achieves up to an average 8.6% relative Word Error
Rate (WER) reduction over the ASR system by itself.

摘要：語音助理越來越常使用裝置上的自動語音辨識 (ASR) 來確保速度和隱私。然而，由於裝置上的資源限制，與複雜資訊領域相關的查詢通常需要搜尋引擎進一步處理。對於此類應用程式，我們提出一個新穎的 Transformer 基礎模型，能夠透過並行探索 N 個最佳假設的完整背景，來重新評分和改寫。我們也提出一個新的判別序列訓練目標，適用於重新評分和改寫任務。我們展示我們的 Rescore+Rewrite 模型優於僅 Rescore 的基準，且可將 ASR 系統本身的平均字元錯誤率 (WER) 降低多達 8.6%。

##### **A Dialogue Game for Eliciting Balanced Collaboration**
2406.08202v1 by Isidora Jeknić, David Schlangen, Alexander Koller

Collaboration is an integral part of human dialogue. Typical task-oriented
dialogue games assign asymmetric roles to the participants, which limits their
ability to elicit naturalistic role-taking in collaboration and its
negotiation. We present a novel and simple online setup that favors balanced
collaboration: a two-player 2D object placement game in which the players must
negotiate the goal state themselves. We show empirically that human players
exhibit a variety of role distributions, and that balanced collaboration
improves task performance. We also present an LLM-based baseline agent which
demonstrates that automatic playing of our game is an interesting challenge for
artificial systems.

摘要：協作是人類對話中不可或缺的一部分。典型的任務導向對話遊戲會將不對等的職責分配給參與者，這限制了他們在協作與協商中引發自然角色扮演的能力。我們提出一個新穎且簡單的線上設置，有利於平衡的協作：一個雙人 2D 物件放置遊戲，玩家必須自行協商目標狀態。我們以經驗方式證明，人類玩家展現出各種角色分配，而平衡的協作改善了任務表現。我們也提出一個基於 LLM 的基準代理，證明自動玩我們的遊戲對人工系統而言是一個有趣的挑戰。

##### **MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents**
2406.08184v1 by Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen, Shoufa Chen

Large language model (LLM)-based mobile agents are increasingly popular due
to their capability to interact directly with mobile phone Graphic User
Interfaces (GUIs) and their potential to autonomously manage daily tasks.
Despite their promising prospects in both academic and industrial sectors,
little research has focused on benchmarking the performance of existing mobile
agents, due to the inexhaustible states of apps and the vague definition of
feasible action sequences. To address this challenge, we propose an efficient
and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden
of extensive manual testing. We initially define 100 tasks across 10
open-source apps, categorized by multiple levels of difficulty. Subsequently,
we evaluate several existing mobile agents, including AppAgent and MobileAgent,
to thoroughly and systematically compare their performance. All materials are
accessible on our project webpage: https://MobileAgentBench.github.io,
contributing to the advancement of both academic and industrial fields.

摘要：大型語言模型 (LLM) 行動代理日益普及，因為它們能直接與行動電話圖形使用者介面 (GUI) 互動，並具備自主管理日常任務的潛力。儘管它們在學術和產業領域都具有良好的前景，但由於應用程式的狀態取之不盡，而且可行動作序列的定義模糊，因此很少有研究專注於評量現有行動代理的效能。為了應對這項挑戰，我們提出一個有效且使用者友善的基準測試 MobileAgentBench，旨在減輕廣泛手動測試的負擔。我們最初定義了 10 個開源應用程式中的 100 個任務，並依據多個難度層級進行分類。隨後，我們評估了幾個現有的行動代理，包括 AppAgent 和 MobileAgent，以全面且系統性地比較它們的效能。所有材料都可以在我們的專案網頁上取得：https://MobileAgentBench.github.io，為學術和產業領域的進步做出貢獻。

##### **Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction**
2406.08183v1 by Micol Spitale, Jiaee Cheong, Hatice Gunes

Recent studies show bias in many machine learning models for depression
detection, but bias in LLMs for this task remains unexplored. This work
presents the first attempt to investigate the degree of gender bias present in
existing LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and
qualitative approaches. From our quantitative evaluation, we found that ChatGPT
performs the best across various performance metrics and LLaMA 2 outperforms
other LLMs in terms of group fairness metrics. As qualitative fairness
evaluation remains an open research question we propose several strategies
(e.g., word count, thematic analysis) to investigate whether and how a
qualitative evaluation can provide valuable insights for bias analysis beyond
what is possible with quantitative evaluation. We found that ChatGPT
consistently provides a more comprehensive, well-reasoned explanation for its
prediction compared to LLaMA 2. We have also identified several themes adopted
by LLMs to qualitatively evaluate gender fairness. We hope our results can be
used as a stepping stone towards future attempts at improving qualitative
evaluation of fairness for LLMs especially for high-stakes tasks such as
depression detection.

摘要：最近的研究顯示許多機器學習模型在憂鬱症偵測上存在偏見，但 LLM 在這項任務上的偏見仍未被探討。這項工作首次嘗試使用量化和質化方法研究現有 LLM（ChatGPT、LLaMA 2 和 Bard）中存在的性別偏見程度。從我們的量化評估中，我們發現 ChatGPT 在各種效能指標上表現最佳，而 LLaMA 2 在群組公平性指標方面優於其他 LLM。由於質化公平性評估仍是一個開放的研究問題，我們提出了幾種策略（例如字數統計、主題分析）來探討質化評估是否以及如何能為偏見分析提供有價值的見解，超越量化評估的可能性。我們發現，與 LLaMA 2 相比，ChatGPT 持續提供更全面、更有條理的預測說明。我們也找出 LLM 採用來質化評估性別公平性的幾個主題。我們希望我們的結果可用作未來嘗試改善 LLM 公平性質化評估的踏腳石，特別是對於高風險任務，例如憂鬱症偵測。

##### **Semi-Supervised Spoken Language Glossification**
2406.08173v1 by Huijie Yao, Wengang Zhou, Hao Zhou, Houqiang Li

Spoken language glossification (SLG) aims to translate the spoken language
text into the sign language gloss, i.e., a written record of sign language. In
this work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage
$G$lossification ($S^3$LG) for SLG. To tackle the bottleneck of limited
parallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken
language text into SLG training. The proposed framework follows the
self-training structure that iteratively annotates and learns from pseudo
labels. Considering the lexical similarity and syntactic difference between
sign language and spoken language, our $S^3$LG adopts both the rule-based
heuristic and model-based approach for auto-annotation. During training, we
randomly mix these complementary synthetic datasets and mark their differences
with a special token. As the synthetic data may be less quality, the $S^3$LG
further leverages consistency regularization to reduce the negative impact of
noise in the synthetic data. Extensive experiments are conducted on public
benchmarks to demonstrate the effectiveness of the $S^3$LG. Our code is
available at \url{https://github.com/yaohj11/S3LG}.

摘要：口語化標記 (SLG) 旨在將口語文本翻譯成手語標記，即手語的書面記錄。在這項工作中，我們提出了一個名為 $S$emi-$S$upervised $S$poken $L$anguage $G$lossification ($S^3$LG) 的 SLG 框架。為了解決 SLG 中並行數據有限的瓶頸，我們的 $S^3$LG 將大規模單語口語文本納入 SLG 訓練中。所提出的框架遵循自訓練結構，反覆標記並從偽標籤中學習。考慮到手語和口語之間的詞彙相似性和句法差異，我們的 $S^3$LG 採用基於規則的啟發式方法和基於模型的方法進行自動標記。在訓練期間，我們隨機混合這些互補的合成數據集，並用特殊令牌標記它們的差異。由於合成數據的品質可能較差，因此 $S^3$LG 進一步利用一致性正則化來減少合成數據中雜訊的負面影響。在公共基準上進行了廣泛的實驗，以證明 $S^3$LG 的有效性。我們的程式碼可在 \url{https://github.com/yaohj11/S3LG} 取得。

##### **Continuous fake media detection: adapting deepfake detectors to new generative techniques**
2406.08171v1 by Francesco Tassone, Luca Maiano, Irene Amerini

Generative techniques continue to evolve at an impressively high rate, driven
by the hype about these technologies. This rapid advancement severely limits
the application of deepfake detectors, which, despite numerous efforts by the
scientific community, struggle to achieve sufficiently robust performance
against the ever-changing content. To address these limitations, in this paper,
we propose an analysis of two continuous learning techniques on a Short and a
Long sequence of fake media. Both sequences include a complex and heterogeneous
range of deepfakes generated from GANs, computer graphics techniques, and
unknown sources. Our study shows that continual learning could be important in
mitigating the need for generalizability. In fact, we show that, although with
some limitations, continual learning methods help to maintain good performance
across the entire training sequence. For these techniques to work in a
sufficiently robust way, however, it is necessary that the tasks in the
sequence share similarities. In fact, according to our experiments, the order
and similarity of the tasks can affect the performance of the models over time.
To address this problem, we show that it is possible to group tasks based on
their similarity. This small measure allows for a significant improvement even
in longer sequences. This result suggests that continual techniques can be
combined with the most promising detection methods, allowing them to catch up
with the latest generative techniques. In addition to this, we propose an
overview of how this learning approach can be integrated into a deepfake
detection pipeline for continuous integration and continuous deployment
(CI/CD). This allows you to keep track of different funds, such as social
networks, new generative tools, or third-party datasets, and through the
integration of continuous learning, allows constant maintenance of the
detectors.

摘要：生成式技術持續以驚人的速度演進，這股技術的熱潮推動了其發展。這種快速的進步嚴重限制了深度偽造偵測器的應用，儘管科學界已做出許多努力，但仍難以對不斷變化的內容做出足夠穩健的表現。為了解決這些限制，我們在本文中提出對兩個連續學習技術的分析，針對一段短時間和一段長時間的虛假媒體序列。這兩個序列都包含了從 GAN、電腦繪圖技術和未知來源生成的複雜且異質的深度偽造範圍。我們的研究表明，持續學習對於減輕對泛化性的需求可能很重要。事實上，我們表明，儘管存在一些限制，但持續學習方法有助於在整個訓練序列中維持良好的表現。然而，要讓這些技術以足夠穩健的方式發揮作用，序列中的任務必須具有相似性。事實上，根據我們的實驗，任務的順序和相似性會影響模型在時間上的表現。為了解決這個問題，我們表明可以根據任務的相似性對任務進行分組。這個小措施即使在較長的序列中也能帶來顯著的改進。這個結果表明，持續技術可以與最有希望的偵測方法結合，讓它們趕上最新的生成式技術。除此之外，我們概述了如何將這種學習方法整合到深度偽造偵測管道中，以進行持續整合和持續部署 (CI/CD)。這能讓你追蹤不同的資金，例如社群網路、新的生成工具或第三方資料集，並透過整合持續學習，讓偵測器持續維護。

##### **Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark**
2406.08155v1 by Pingzhi Li, Xiaolong Jin, Yu Cheng, Tianlong Chen

Large Language Models~(LLMs) have become foundational in the realm of natural
language processing, demonstrating performance improvements as model sizes
increase. The Mixture-of-Experts~(MoE) approach offers a promising way to scale
LLMs more efficiently by using fewer computational FLOPs through sparse
activation. However, it suffers from significant memory overheads,
necessitating model compression techniques. Post-training quantization, a
popular method for model compression, proves less effective when directly
applied to MoE models due to MoE's overlooked inherent sparsity. This paper
explores several MoE structure-aware quantization heuristics, ranging from
coarse to fine granularity, from MoE block to individual linear weight. Our
investigations reveal critical principles: different MoE structures (i.e.,
blocks, experts, linear layers) require varying numbers of weight bits for
effective and efficient quantization. Conclusions are supported by extensive
benchmarking across two representative MoE models and six tasks. We further
introduce novel enhancements to more accurately identify the most critical
weights in MoE quantization that necessitate higher bit allocations, including
the linear weight outlier scorer and MoE block scorer. Additionally, subsequent
experiments validate our findings in the context of both weight and activation
quantization.

摘要：大型語言模型 (LLM) 已成為自然語言處理領域的基礎，隨著模型規模的增加，效能表現也有所提升。混合專家 (MoE) 方法提供了一種更有效率的 LLM 擴充方式，透過稀疏激活使用較少的運算浮點運算 (FLOP)。然而，它會造成顯著的記憶體開銷，因此需要模型壓縮技術。訓練後量化是模型壓縮的熱門方法，但由於忽略了 MoE 的內在稀疏性，直接應用於 MoE 模型時效果較差。本文探討了多種考量 MoE 結構的量化啟發式方法，範圍從粗略到精細粒度，從 MoE 區塊到個別線性權重。我們的調查揭露了重要的原則：不同的 MoE 結構（即區塊、專家、線性層）需要不同數量的權重位元，才能進行有效且高效的量化。結論獲得了兩個具代表性的 MoE 模型和六項任務的廣泛基準測試支持。我們進一步引入了新穎的強化功能，以更準確地識別 MoE 量化中需要較高位元配置的最關鍵權重，包括線性權重異常值評分器和 MoE 區塊評分器。此外，後續實驗驗證了我們的發現，同時考量了權重和激活量化。

##### **Making AI Intelligible: Philosophical Foundations**
2406.08134v1 by Herman Cappelen, Josh Dever

Can humans and artificial intelligences share concepts and communicate?
'Making AI Intelligible' shows that philosophical work on the metaphysics of
meaning can help answer these questions. Herman Cappelen and Josh Dever use the
externalist tradition in philosophy to create models of how AIs and humans can
understand each other. In doing so, they illustrate ways in which that
philosophical tradition can be improved.
  The questions addressed in the book are not only theoretically interesting,
but the answers have pressing practical implications. Many important decisions
about human life are now influenced by AI. In giving that power to AI, we
presuppose that AIs can track features of the world that we care about (for
example, creditworthiness, recidivism, cancer, and combatants). If AIs can
share our concepts, that will go some way towards justifying this reliance on
AI. This ground-breaking study offers insight into how to take some first steps
towards achieving Interpretable AI.

摘要：人類與人工智慧能共享概念並溝通嗎？
「讓人工智慧易於理解」一書表明，關於意義的形上學哲學著作有助於回答這些問題。赫爾曼·卡佩倫和喬希·德弗利用哲學中的外在主義傳統，建立了關於人工智慧和人類如何理解彼此的模型。在這樣做的過程中，他們說明了哲學傳統可以得到改善的方式。
書中探討的問題不僅在理論上很有趣，而且答案具有迫切的實際意義。許多關於人類生活的重要決定現在都受到人工智慧的影響。在將這種力量賦予人工智慧時，我們預設人工智慧可以追蹤我們關心的世界特徵（例如，信用評分、累犯率、癌症和戰鬥人員）。如果人工智慧能共享我們的概念，這將有助於證明依賴人工智慧的合理性。這項開創性的研究提供了如何採取一些第一步來實現可解釋人工智慧的見解。

##### **Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets**
2406.08124v1 by Duanyu Feng, Bowen Qin, Chen Huang, Youcheng Huang, Zheng Zhang, Wenqiang Lei

The success of the reward model in distinguishing between responses with
subtle safety differences depends critically on the high-quality preference
dataset, which should capture the fine-grained nuances of harmful and harmless
responses. This motivates the need to develop a dataset involving preference
margins, which accurately quantify how harmless one response is compared to
another. In this paper, we take the first step to propose an effective and
cost-efficient framework to promote the margin-enhanced preference dataset
development. Our framework, Legend, Leverages representation engineering to
annotate preference datasets. It constructs the specific direction within the
LLM's embedding space that represents safety. By leveraging this safety
direction, Legend can then leverage the semantic distances of paired responses
along this direction to annotate margins automatically. We experimentally
demonstrate our effectiveness in both reward modeling and harmless alignment
for LLMs. Legend also stands out for its efficiency, requiring only the
inference time rather than additional training. This efficiency allows for
easier implementation and scalability, making Legend particularly valuable for
practical applications in aligning LLMs with safe conversations.

摘要：獎勵模型在區分具有細微安全性差異的回應方面的成功，關鍵取決於高品質的偏好資料集，該資料集應捕捉有害和無害回應的細微差別。這促使我們需要開發一個涉及偏好邊際的資料集，該資料集準確地量化一個回應與另一個回應相比的無害程度。在本文中，我們首先提出了一個有效且具有成本效益的框架，以促進邊際增強偏好資料集的開發。我們的框架 Legend 透過表示工程來註解偏好資料集。它在 LLM 的嵌入空間中建構表示安全性的特定方向。透過利用此安全性方向，Legend 然後可以利用沿此方向配對回應的語義距離來自動註解邊際。我們在獎勵建模和 LLM 的無害對齊中以實驗方式證明了我們的有效性。Legend 還以其效率而著稱，只需要推理時間，而不需要額外的訓練。這種效率允許更輕鬆地實作和擴充套件，使 Legend 對於將 LLM 與安全對話對齊的實際應用特別有價值。

##### **Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling**
2406.08116v1 by Zile Qiao, Wei Ye, Yong Jiang, Tong Mo, Pengjun Xie, Weiping Li, Fei Huang, Shikun Zhang

Retrieval-augmented language models (RALMs) have recently shown great
potential in mitigating the limitations of implicit knowledge in LLMs, such as
untimely updating of the latest expertise and unreliable retention of long-tail
knowledge. However, since the external knowledge base, as well as the
retriever, can not guarantee reliability, potentially leading to the knowledge
retrieved not being helpful or even misleading for LLM generation. In this
paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust
and pluggable knowledge rewriter inherently optimized for LLM generation.
Specifically, we introduce the novel concept of "supportiveness"--which
represents how effectively a knowledge piece facilitates downstream tasks--by
considering the perplexity impact of augmented knowledge on the response text
of a white-box LLM. Based on knowledge supportiveness, we first design a
training data curation strategy for our rewriter model, effectively identifying
and filtering out poor or irrelevant rewrites (e.g., with low supportiveness
scores) to improve data efficacy. We then introduce the direct preference
optimization (DPO) algorithm to align the generated rewrites to optimal
supportiveness, guiding the rewriter model to summarize augmented content that
better improves the final response. Comprehensive evaluations across six
popular knowledge-intensive tasks and four LLMs have demonstrated the
effectiveness and superiority of SKR. With only 7B parameters, SKR has shown
better knowledge rewriting capability over GPT-4, the current state-of-the-art
general-purpose LLM.

摘要：<paragraph>擷取增強語言模型 (RALM) 近期已展現出極佳潛力，可減輕隱含知識在大型語言模型 (LLM) 中的限制，例如最新專業知識的更新不即時，以及長尾知識的保留不可靠。然而，由於外部知識庫和擷取器無法保證可靠性，可能會導致擷取的知識對 LLM 生成沒有幫助，甚至會產生誤導。在本文中，我們介紹了基於支援度的知識改寫 (SKR)，這是一個強健且可插入的知識改寫器，本質上經過最佳化以進行 LLM 生成。具體來說，我們引入了「支援度」的新穎概念，它表示知識片段促進下游任務的成效，方法是考量增強知識對白盒 LLM 回應文字的困惑度影響。根據知識支援度，我們首先為我們的改寫器模型設計一個訓練資料策展策略，有效地找出並過濾掉不良或不相關的改寫（例如，支援度評分低），以提升資料效能。接著，我們引入了直接偏好最佳化 (DPO) 演算法，以將產生的改寫與最佳支援度對齊，引導改寫器模型摘要增強的內容，進而改善最終的回應。在六項廣泛使用的知識密集型任務和四個 LLM 中進行的全面評估，已證明 SKR 的效能和優越性。SKR 僅有 7B 個參數，已展現出比 GPT-4 更佳的知識改寫能力，GPT-4 是目前最先進的通用 LLM。</paragraph>

##### **Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey**
2406.08115v1 by Feng Liang, Zhen Zhang, Haifeng Lu, Chengming Li, Victor C. M. Leung, Yanyi Guo, Xiping Hu

With rapidly increasing distributed deep learning workloads in large-scale
data centers, efficient distributed deep learning framework strategies for
resource allocation and workload scheduling have become the key to
high-performance deep learning. The large-scale environment with large volumes
of datasets, models, and computational and communication resources raises
various unique challenges for resource allocation and workload scheduling in
distributed deep learning, such as scheduling complexity, resource and workload
heterogeneity, and fault tolerance. To uncover these challenges and
corresponding solutions, this survey reviews the literature, mainly from 2019
to 2024, on efficient resource allocation and workload scheduling strategies
for large-scale distributed DL. We explore these strategies by focusing on
various resource types, scheduling granularity levels, and performance goals
during distributed training and inference processes. We highlight critical
challenges for each topic and discuss key insights of existing technologies. To
illustrate practical large-scale resource allocation and workload scheduling in
real distributed deep learning scenarios, we use a case study of training large
language models. This survey aims to encourage computer science, artificial
intelligence, and communications researchers to understand recent advances and
explore future research directions for efficient framework strategies for
large-scale distributed deep learning.

摘要：<paragraph>隨著大型資料中心中快速增加的分布式深度學習工作負載，資源配置和工作負載排程的高效分布式深度學習架構策略已成為高性能深度學習的關鍵。大型環境中大量的資料集、模型以及運算和通訊資源，為分布式深度學習中的資源配置和工作負載排程帶來了各種獨特挑戰，例如排程複雜性、資源和工作負載異質性以及容錯能力。為了找出這些挑戰和對應的解決方案，本調查回顧了文獻，主要從 2019 年到 2024 年，探討了大型規模分布式深度學習的高效資源配置和工作負載排程策略。我們透過專注於各種資源類型、排程粒度層級和效能目標，在分布式訓練和推論過程中探討這些策略。我們重點指出每個主題的關鍵挑戰，並討論現有技術的主要見解。為了說明實際的大規模資源配置和工作負載排程在真實的分布式深度學習場景中，我們使用訓練大型語言模型的案例研究。本調查旨在鼓勵電腦科學、人工智慧和通訊研究人員了解最近的進展，並探討大型規模分布式深度學習的高效架構策略的未來研究方向。</paragraph>

##### **Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio**
2406.08112v1 by Yi Lu, Yuankun Xie, Ruibo Fu, Zhengqi Wen, Jianhua Tao, Zhiyong Wang, Xin Qi, Xuefei Liu, Yongwei Li, Yukun Liu, Xiaopeng Wang, Shuchen Shi

With the proliferation of Large Language Model (LLM) based deepfake audio,
there is an urgent need for effective detection methods. Previous deepfake
audio generation methods typically involve a multi-step generation process,
with the final step using a vocoder to predict the waveform from handcrafted
features. However, LLM-based audio is directly generated from discrete neural
codecs in an end-to-end generation process, skipping the final step of vocoder
processing. This poses a significant challenge for current audio deepfake
detection (ADD) models based on vocoder artifacts. To effectively detect
LLM-based deepfake audio, we focus on the core of the generation process, the
conversion from neural codec to waveform. We propose Codecfake dataset, which
is generated by seven representative neural codec methods. Experiment results
show that codec-trained ADD models exhibit a 41.406% reduction in average equal
error rate compared to vocoder-trained ADD models on the Codecfake test set.

摘要：隨著基於大型語言模型 (LLM) 的深度偽造音訊的激增，
迫切需要有效的檢測方法。先前的深度偽造音訊產生方法通常涉及多步驟產生過程，
最後一步使用語音編碼器從手工特徵預測波形。然而，基於 LLM 的音訊是從離散神經編碼器中直接產生的
在端到端生成過程中，跳過語音編碼器處理的最後一步。這對基於語音編碼器人工製品的當前音訊深度偽造
檢測 (ADD) 模型構成重大挑戰。為了有效檢測基於 LLM 的深度偽造音訊，我們專注於生成過程的核心，
從神經編碼器轉換為波形。我們提出 Codecfake 資料集，它是由七種代表性神經編碼器方法產生的。實驗結果
表明，與在 Codecfake 測試集中訓練的語音編碼器 ADD 模型相比，訓練編碼器的 ADD 模型的平均等誤差率降低了 41.406%。

##### **CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems**
2406.08101v1 by Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller

Conversational explainable artificial intelligence (ConvXAI) systems based on
large language models (LLMs) have garnered significant interest from the
research community in natural language processing (NLP) and human-computer
interaction (HCI). Such systems can provide answers to user questions about
explanations, have the potential to enhance users' comprehension and offer more
information about the decision-making and generation processes of LLMs.
Currently available ConvXAI systems are based on intent recognition rather than
free chat. Thus, reliably grasping users' intentions in ConvXAI systems still
presents a challenge, because there is a broad range of XAI methods to map
requests onto and each of them can have multiple slots to take care of. In
order to bridge this gap, we present CoXQL, the first dataset for user intent
recognition in ConvXAI, covering 31 intents, seven of which require filling
additional slots. Subsequently, we enhance an existing parsing approach by
incorporating template validations, and conduct an evaluation of several LLMs
on CoXQL using different parsing strategies. We conclude that the improved
parsing approach (MP+) surpasses the performance of previous approaches. We
also discover that intents with multiple slots remain highly challenging for
LLMs.

摘要：基於大型語言模型 (LLM) 的對話式可解釋人工智慧 (ConvXAI) 系統在自然語言處理 (NLP) 和人機互動 (HCI) 的研究社群中獲得極大的關注。此類系統可以提供使用者關於解釋的問題解答，有潛力增強使用者的理解力，並提供更多有關 LLM 決策制定和產生程序的資訊。目前可用的 ConvXAI 系統是基於意圖辨識，而非自由對話。因此，在 ConvXAI 系統中可靠地掌握使用者的意圖仍是一個挑戰，因為有廣泛的 XAI 方法可以將請求對應到，而且每個方法都可以有多個時段需要處理。為了彌合這個差距，我們提出了 CoXQL，這是 ConvXAI 中第一個使用者意圖辨識的資料集，涵蓋 31 個意圖，其中七個需要填寫額外的時段。隨後，我們透過整合範本驗證來增強現有的剖析方法，並使用不同的剖析策略對 CoXQL 上的幾個 LLM 進行評估。我們得出結論，改良的剖析方法 (MP+) 超越了先前方法的效能。我們也發現，具有多個時段的意圖對於 LLM 來說仍然極具挑戰性。

##### **Multimodal Table Understanding**
2406.08100v1 by Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, Weiping Wang

Although great progress has been made by previous table understanding methods
including recent approaches based on large language models (LLMs), they rely
heavily on the premise that given tables must be converted into a certain text
sequence (such as Markdown or HTML) to serve as model input. However, it is
difficult to access such high-quality textual table representations in some
real-world scenarios, and table images are much more accessible. Therefore, how
to directly understand tables using intuitive visual information is a crucial
and urgent challenge for developing more practical applications. In this paper,
we propose a new problem, multimodal table understanding, where the model needs
to generate correct responses to various table-related requests based on the
given table image. To facilitate both the model training and evaluation, we
construct a large-scale dataset named MMTab, which covers a wide spectrum of
table images, instructions and tasks. On this basis, we develop Table-LLaVA, a
generalist tabular multimodal large language model (MLLM), which significantly
outperforms recent open-source MLLM baselines on 23 benchmarks under held-in
and held-out settings. The code and data is available at this
https://github.com/SpursGoZmy/Table-LLaVA

摘要：儘管先前的表格理解方法已取得重大進展，包括基於大型語言模型 (LLM) 的最新方法，但它們嚴重依賴於以下前提：給定的表格必須轉換為某個文字序列（例如 Markdown 或 HTML）才能作為模型輸入。然而，在某些實際場景中難以存取此類高品質的文字表格表示，而表格影像則更容易存取。因此，如何直接使用直覺的視覺資訊理解表格，是開發更多實用應用程式的一項關鍵且迫切的挑戰。在本文中，我們提出一個新的問題，多模態表格理解，其中模型需要根據給定的表格影像，對各種與表格相關的請求產生正確的回應。為了促進模型訓練和評估，我們建構了一個名為 MMTab 的大型資料集，涵蓋了廣泛的表格影像、說明和任務。在此基礎上，我們開發了 Table-LLaVA，一個通用的表格多模態大型語言模型 (MLLM)，在 23 個基準測試中，在內部和外部設定下，其效能顯著優於最近的開源 MLLM 基準。程式碼和資料可在此取得：https://github.com/SpursGoZmy/Table-LLaVA

##### **Languages Transferred Within the Encoder: On Representation Transfer in Zero-Shot Multilingual Translation**
2406.08092v1 by Zhi Qu, Chenchen Ding, Taro Watanabe

Understanding representation transfer in multilingual neural machine
translation can reveal the representational issue causing the zero-shot
translation deficiency. In this work, we introduce the identity pair, a
sentence translated into itself, to address the lack of the base measure in
multilingual investigations, as the identity pair represents the optimal state
of representation among any language transfers. In our analysis, we demonstrate
that the encoder transfers the source language to the representational subspace
of the target language instead of the language-agnostic state. Thus, the
zero-shot translation deficiency arises because representations are entangled
with other languages and are not transferred effectively to the target
language. Based on our findings, we propose two methods: 1) low-rank
language-specific embedding at the encoder, and 2) language-specific
contrastive learning of the representation at the decoder. The experimental
results on Europarl-15, TED-19, and OPUS-100 datasets show that our methods
substantially enhance the performance of zero-shot translations by improving
language transfer capacity, thereby providing practical evidence to support our
conclusions.

摘要：了解多語言神經機器翻譯中的表示轉移，可以揭示導致零次學習翻譯缺陷的表示問題。在這項工作中，我們引入了同一對，一個翻譯成自身的句子，以解決多語言研究中缺乏基準測量，因為同一對表示任何語言轉換之間表示的最佳狀態。在我們的分析中，我們證明編碼器將源語言轉移到目標語言的表示子空間，而不是與語言無關的狀態。因此，零次學習翻譯缺陷是因為表示與其他語言糾纏在一起，並且沒有有效地轉移到目標語言。根據我們的發現，我們提出了兩種方法：1) 編碼器中的低秩語言特定嵌入，以及 2) 解碼器中表示的語言特定對比學習。在 Europarl-15、TED-19 和 OPUS-100 資料集上的實驗結果表明，我們的語言轉移能力方法顯著提高了零次學習翻譯的性能，從而提供了實際證據來支持我們的結論。

##### **AustroTox: A Dataset for Target-Based Austrian German Offensive Language Detection**
2406.08080v1 by Pia Pachinger, Janis Goldzycher, Anna Maria Planitzer, Wojciech Kusa, Allan Hanbury, Julia Neidhardt

Model interpretability in toxicity detection greatly profits from token-level
annotations. However, currently such annotations are only available in English.
We introduce a dataset annotated for offensive language detection sourced from
a news forum, notable for its incorporation of the Austrian German dialect,
comprising 4,562 user comments. In addition to binary offensiveness
classification, we identify spans within each comment constituting vulgar
language or representing targets of offensive statements. We evaluate
fine-tuned language models as well as large language models in a zero- and
few-shot fashion. The results indicate that while fine-tuned models excel in
detecting linguistic peculiarities such as vulgar dialect, large language
models demonstrate superior performance in detecting offensiveness in
AustroTox. We publish the data and code.

摘要：毒性檢測中的模型可解釋性從代幣層級標註中獲益良多。然而，目前此類標註僅以英文提供。我們引入一個標註有攻擊性語言檢測的資料集，其來源於一個新聞論壇，其特色是納入了奧地利德語方言，包含 4,562 則使用者留言。除了二元攻擊性分類之外，我們會識別每則留言中構成粗俗語言或代表攻擊性陳述目標的區間。我們以零次和少量樣本的方式評估微調語言模型以及大型語言模型。結果顯示，雖然微調模型在檢測語言特殊性（例如粗俗方言）方面表現出色，但大型語言模型在檢測 AustroTox 中的攻擊性方面表現出優越的效能。我們會發布資料和程式碼。

##### **A Concept-Based Explainability Framework for Large Multimodal Models**
2406.08074v1 by Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Alasdair Newson, Matthieu Cord

Large multimodal models (LMMs) combine unimodal encoders and large language
models (LLMs) to perform multimodal tasks. Despite recent advancements towards
the interpretability of these models, understanding internal representations of
LMMs remains largely a mystery. In this paper, we present a novel framework for
the interpretation of LMMs. We propose a dictionary learning based approach,
applied to the representation of tokens. The elements of the learned dictionary
correspond to our proposed concepts. We show that these concepts are well
semantically grounded in both vision and text. Thus we refer to these as
"multi-modal concepts". We qualitatively and quantitatively evaluate the
results of the learnt concepts. We show that the extracted multimodal concepts
are useful to interpret representations of test samples. Finally, we evaluate
the disentanglement between different concepts and the quality of grounding
concepts visually and textually. We will publicly release our code.

摘要：大型多模態模型（LMM）結合了單模態編碼器和大型語言模型（LLM）來執行多模態任務。儘管最近在這些模型的可解釋性方面取得了進展，但理解 LMM 的內部表示在很大程度上仍然是一個謎。在本文中，我們提出了一個解釋 LMM 的新框架。我們提出了一種基於字典學習的方法，應用於令牌的表示。學習到的字典的元素對應於我們提出的概念。我們表明這些概念在視覺和文本中都具有良好的語義基礎。因此，我們將這些稱為「多模態概念」。我們對學習到的概念的結果進行了定性和定量評估。我們表明提取的多模態概念有助於解釋測試樣本的表示。最後，我們評估了不同概念之間的解糾纏以及視覺和文本上接地概念的質量。我們將公開發布我們的代碼。

##### **CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models**
2406.08070v1 by Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, Jong Chul Ye

Classifier-free guidance (CFG) is a fundamental tool in modern diffusion
models for text-guided generation. Although effective, CFG has notable
drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image
editing; furthermore, high guidance scales, essential for high-quality outputs,
frequently result in issues like mode collapse. Contrary to the widespread
belief that these are inherent limitations of diffusion models, this paper
reveals that the problems actually stem from the off-manifold phenomenon
associated with CFG, rather than the diffusion models themselves. More
specifically, inspired by the recent advancements of diffusion model-based
inverse problem solvers (DIS), we reformulate text-guidance as an inverse
problem with a text-conditioned score matching loss, and develop CFG++, a novel
approach that tackles the off-manifold challenges inherent in traditional CFG.
CFG++ features a surprisingly simple fix to CFG, yet it offers significant
improvements, including better sample quality for text-to-image generation,
invertibility, smaller guidance scales, reduced mode collapse, etc.
Furthermore, CFG++ enables seamless interpolation between unconditional and
conditional sampling at lower guidance scales, consistently outperforming
traditional CFG at all scales. Experimental results confirm that our method
significantly enhances performance in text-to-image generation, DDIM inversion,
editing, and solving inverse problems, suggesting a wide-ranging impact and
potential applications in various fields that utilize text guidance. Project
Page: https://cfgpp-diffusion.github.io/.

摘要：<paragraph>無分類器引導 (CFG) 是現代擴散模型中用於文字引導生成的基礎工具。雖然有效，但 CFG 有顯著的缺點。例如，帶有 CFG 的 DDIM 缺乏可逆性，使影像編輯複雜化；此外，對於高品質輸出至關重要的高引導比例，經常導致模式崩潰等問題。與廣泛認為這些是擴散模型的固有限制相反，本文揭示這些問題實際上源自與 CFG 相關的流形外現象，而不是擴散模型本身。更具體地說，受到基於擴散模型的逆問題求解器 (DIS) 的最新進展的啟發，我們將文字引導重新表述為一個具有文字條件分數匹配損失的逆問題，並開發了 CFG++，這是一種新的方法，可以解決傳統 CFG 中固有的流形外挑戰。CFG++ 對 CFG 有一個驚人地簡單的修復，但它提供了顯著的改進，包括文字轉影像生成的更好範本品質、可逆性、更小的引導比例、減少模式崩潰等。此外，CFG++ 能夠在較低的引導比例下在無條件和條件抽樣之間進行無縫插值，在所有比例下都持續優於傳統 CFG。實驗結果證實，我們的模型顯著增強了文字轉影像生成、DDIM 反演、編輯和解決逆問題的效能，這表明在利用文字引導的各個領域中具有廣泛的影響和潛在應用。專案頁面：https://cfgpp-diffusion.github.io/。</paragraph>

##### **Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey**
2406.08068v1 by Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Wanxiang Che, Bing Qin

Compared to traditional sentiment analysis, which only considers text,
multimodal sentiment analysis needs to consider emotional signals from
multimodal sources simultaneously and is therefore more consistent with the way
how humans process sentiment in real-world scenarios. It involves processing
emotional information from various sources such as natural language, images,
videos, audio, physiological signals, etc. However, although other modalities
also contain diverse emotional cues, natural language usually contains richer
contextual information and therefore always occupies a crucial position in
multimodal sentiment analysis. The emergence of ChatGPT has opened up immense
potential for applying large language models (LLMs) to text-centric multimodal
tasks. However, it is still unclear how existing LLMs can adapt better to
text-centric multimodal sentiment analysis tasks. This survey aims to (1)
present a comprehensive review of recent research in text-centric multimodal
sentiment analysis tasks, (2) examine the potential of LLMs for text-centric
multimodal sentiment analysis, outlining their approaches, advantages, and
limitations, (3) summarize the application scenarios of LLM-based multimodal
sentiment analysis technology, and (4) explore the challenges and potential
research directions for multimodal sentiment analysis in the future.

摘要：相較於傳統只考慮文字的情感分析，多模態情感分析需要同時考慮來自多模態來源的情緒信號，因此更符合人類在真實世界場景中處理情緒的方式。它涉及處理來自自然語言、影像、影片、音訊、生理訊號等各種來源的情緒資訊。然而，儘管其他模態也包含多元的情緒線索，但自然語言通常包含更豐富的脈絡資訊，因此在多模態情感分析中始終佔有關鍵地位。ChatGPT 的出現為將大型語言模型 (LLM) 應用於以文字為中心的多模態任務開啟了巨大的潛力。然而，現有 LLM 如何能更好地適應以文字為中心的多模態情感分析任務仍不清楚。本綜述旨在：(1) 全面回顧近期以文字為中心的多模態情感分析任務的研究，(2) 探討 LLM 在以文字為中心的多模態情感分析中的潛力，概述其方法、優點和限制，(3) 總結基於 LLM 的多模態情感分析技術的應用情境，以及 (4) 探討未來多模態情感分析的挑戰和潛在研究方向。

##### **Adversarial Evasion Attack Efficiency against Large Language Models**
2406.08050v1 by João Vitorino, Eva Maia, Isabel Praça

Large Language Models (LLMs) are valuable for text classification, but their
vulnerabilities must not be disregarded. They lack robustness against
adversarial examples, so it is pertinent to understand the impacts of different
types of perturbations, and assess if those attacks could be replicated by
common users with a small amount of perturbations and a small number of queries
to a deployed LLM. This work presents an analysis of the effectiveness,
efficiency, and practicality of three different types of adversarial attacks
against five different LLMs in a sentiment classification task. The obtained
results demonstrated the very distinct impacts of the word-level and
character-level attacks. The word attacks were more effective, but the
character and more constrained attacks were more practical and required a
reduced number of perturbations and queries. These differences need to be
considered during the development of adversarial defense strategies to train
more robust LLMs for intelligent text classification applications.

摘要：大型語言模型 (LLM) 對於文本分類很有價值，但不能忽視其漏洞。它們缺乏對抗範例的健壯性，因此了解不同類型擾動的影響，並評估普通用戶是否可以用少量的擾動和少量的查詢對已部署的 LLM 複製這些攻擊非常重要。這項工作分析了在情緒分類任務中對抗五個不同 LLM 的三種類型對抗攻擊的有效性、效率和實用性。獲得的結果證明了字級和字元級攻擊的影響非常不同。字元攻擊更有效，但字元和更受限的攻擊更實用，並且需要較少的擾動和查詢。在開發對抗防禦策略以訓練更強大的 LLM 以用於智慧文本分類應用程式時，需要考慮這些差異。

##### **LVBench: An Extreme Long Video Understanding Benchmark**
2406.08035v1 by Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang

Recent progress in multimodal large language models has markedly enhanced the
understanding of short videos (typically under one minute), and several
evaluation datasets have emerged accordingly. However, these advancements fall
short of meeting the demands of real-world applications such as embodied
intelligence for long-term decision-making, in-depth movie reviews and
discussions, and live sports commentary, all of which require comprehension of
long videos spanning several hours. To address this gap, we introduce LVBench,
a benchmark specifically designed for long video understanding. Our dataset
comprises publicly sourced videos and encompasses a diverse set of tasks aimed
at long video comprehension and information extraction. LVBench is designed to
challenge multimodal models to demonstrate long-term memory and extended
comprehension capabilities. Our extensive evaluations reveal that current
multimodal models still underperform on these demanding long video
understanding tasks. Through LVBench, we aim to spur the development of more
advanced models capable of tackling the complexities of long video
comprehension. Our data and code are publicly available at:
https://lvbench.github.io.

摘要：最近多模态大型语言模型的进展显著增强了对短视频（通常在 1 分钟内）的理解，因此出现了几个评估数据集。然而，这些进步未能满足现实世界应用程序的需求，例如面向长期决策制定的具身智能、深入的电影评论和讨论以及现场体育评论，所有这些都需要理解跨越数小时的长视频。为了解决这一差距，我们引入了 LVBench，这是一个专门为理解长视频而设计的基准。我们的数据集包括公开获取的视频，并包含一组针对长视频理解和信息提取的任务。LVBench 旨在挑战多模态模型，以展示长期记忆和扩展理解能力。我们广泛的评估表明，当前的多模态模型在这些要求苛刻的长视频理解任务上仍然表现不佳。通过 LVBench，我们旨在促进能够应对长视频理解的复杂性的更高级模型的开发。我们的数据和代码可公开获取：https://lvbench.github.io。

##### **Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models**
2406.08024v1 by Shimin Chen, Yitian Yuan, Shaoxiang Chen, Zequn Jie, Lin Ma

Amidst the advancements in image-based Large Vision-Language Models
(image-LVLM), the transition to video-based models (video-LVLM) is hindered by
the limited availability of quality video data. This paper addresses the
challenge by leveraging the visual commonalities between images and videos to
efficiently evolve image-LVLMs into video-LVLMs. We present a cost-effective
video-LVLM that enhances model architecture, introduces innovative training
strategies, and identifies the most effective types of video instruction data.
Our innovative weighted token sampler significantly compresses the visual token
numbers of each video frame, effectively cutting computational expenses. We
also find that judiciously using just 10% of the video data, compared to prior
video-LVLMs, yields impressive results during various training phases.
Moreover, we delve into the influence of video instruction data in
limited-resource settings, highlighting the significance of incorporating video
training data that emphasizes temporal understanding to enhance model
performance. The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM)
exhibits exceptional performance across video and image benchmarks, validating
our model's design and training approaches.

摘要：在基於影像的大型視覺語言模型 (image-LVLM) 進展中，轉換為基於影片的模型 (video-LVLM) 受到影片資料品質有限的阻礙。本文透過利用影像和影片之間的視覺共性，提出一個解決方案，有效地將 image-LVLM 轉變為 video-LVLM。我們提出一個具有成本效益的 video-LVLM，它增強了模型架構、引進創新的訓練策略，並找出最有效的影片教學資料類型。我們創新的加權代幣取樣器大幅壓縮每個影片畫格的視覺代幣數量，有效地降低運算成本。我們也發現，與先前的 video-LVLM 相比，明智地只使用 10% 的影片資料，就能在各種訓練階段產生令人印象深刻的結果。此外，我們深入探討影片教學資料在資源有限的設定中的影響，強調納入強調時間理解的影片訓練資料，以增強模型效能的重要性。產生的更少代幣和更少影片 LVLM (FTFV-LVLM) 在影片和影像基準上展現出傑出的效能，驗證了我們模型的設計和訓練方法。

##### **SHACL2FOL: An FOL Toolkit for SHACL Decision Problems**
2406.08018v1 by Paolo Pareti

Recent studies on the Shapes Constraint Language (SHACL), a W3C specification
for validating RDF graphs, rely on translating the language into first-order
logic in order to provide formally-grounded solutions to the validation,
containment and satisfiability decision problems. Continuing on this line of
research, we introduce SHACL2FOL, the first automatic tool that (i) translates
SHACL documents into FOL sentences and (ii) computes the answer to the two
static analysis problems of satisfiability and containment; it also allow to
test the validity of a graph with respect to a set of constraints. By
integrating with existing theorem provers, such as E and Vampire, the tool
computes the answer to the aforementioned decision problems and outputs the
corresponding first-order logic theories in the standard TPTP format. We
believe this tool can contribute to further theoretical studies of SHACL, by
providing an automatic first-order logic interpretation of its semantics, while
also benefiting SHACL practitioners, by supplying static analysis capabilities
to help the creation and management of SHACL constraints.

摘要：最近對形狀約束語言 (SHACL) 的研究，W3C 規範用於驗證 RDF 圖形，依賴於將語言轉譯成一階邏輯，以便針對驗證、包含和可滿足性決策問題提供正式依據的解決方案。延續這條研究路線，我們介紹 SHACL2FOL，第一個自動工具：(i) 將 SHACL 文件轉譯成 FOL 句子，以及 (ii) 計算可滿足性和包含這兩個靜態分析問題的答案；它也允許測試圖形相對於一組約束的有效性。藉由整合現有的定理證明器，例如 E 和 Vampire，此工具計算前述決策問題的答案，並輸出標準 TPTP 格式中對應的一階邏輯理論。我們相信此工具有助於進一步的 SHACL 理論研究，藉由提供其語意的自動一階邏輯詮釋，同時也讓 SHACL 從業人員受益，藉由提供靜態分析功能來協助建立和管理 SHACL 約束。

##### **OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding**
2406.08009v1 by Yinan Deng, Jiahui Wang, Jingyu Zhao, Jianyu Dou, Yi Yang, Yufeng Yue

In recent years, there has been a surge of interest in open-vocabulary 3D
scene reconstruction facilitated by visual language models (VLMs), which
showcase remarkable capabilities in open-set retrieval. However, existing
methods face some limitations: they either focus on learning point-wise
features, resulting in blurry semantic understanding, or solely tackle
object-level reconstruction, thereby overlooking the intricate details of the
object's interior. To address these challenges, we introduce OpenObj, an
innovative approach to build open-vocabulary object-level Neural Radiance
Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes
a robust framework for efficient and watertight scene modeling and
comprehension at the object-level. Moreover, we incorporate part-level features
into the neural fields, enabling a nuanced representation of object interiors.
This approach captures object-level instances while maintaining a fine-grained
understanding. The results on multiple datasets demonstrate that OpenObj
achieves superior performance in zero-shot semantic segmentation and retrieval
tasks. Additionally, OpenObj supports real-world robotics tasks at multiple
scales, including global movement and local manipulation.

摘要：近年来，人们对开放式词汇 3D 场景重建产生了浓厚的兴趣，视觉语言模型 (VLM) 促进了这一发展，它在开放式检索方面展示了非凡的能力。然而，现有方法面临一些限制：它们要么专注于学习逐点特征，导致语义理解模糊，要么只解决对象级别的重建，从而忽略了对象内部的复杂细节。为了应对这些挑战，我们引入了 OpenObj，这是一种创新的方法，用于构建具有细粒度理解的开放式词汇对象级神经辐射场 (NeRF)。从本质上讲，OpenObj 为高效且严谨的对象级场景建模和理解建立了一个稳健的框架。此外，我们将部分级特征纳入神经场，从而能够对对象内部进行细致入微的表示。这种方法在保持细粒度理解的同时捕获了对象级别的实例。在多个数据集上的结果表明，OpenObj 在零样本语义分割和检索任务中实现了卓越的性能。此外，OpenObj 支持多种规模的现实世界机器人任务，包括全局运动和局部操作。

##### **Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning**
2406.08002v1 by Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, Xue Feng

Despite the recent successes of multi-agent reinforcement learning (MARL)
algorithms, efficiently adapting to co-players in mixed-motive environments
remains a significant challenge. One feasible approach is to hierarchically
model co-players' behavior based on inferring their characteristics. However,
these methods often encounter difficulties in efficient reasoning and
utilization of inferred information. To address these issues, we propose
Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent
decision-making algorithm that enables few-shot adaptation to unseen policies
in mixed-motive environments. HOP is hierarchically composed of two modules: an
opponent modeling module that infers others' goals and learns corresponding
goal-conditioned policies, and a planning module that employs Monte Carlo Tree
Search (MCTS) to identify the best response. Our approach improves efficiency
by updating beliefs about others' goals both across and within episodes and by
using information from the opponent modeling module to guide planning.
Experimental results demonstrate that in mixed-motive environments, HOP
exhibits superior few-shot adaptation capabilities when interacting with
various unseen agents, and excels in self-play scenarios. Furthermore, the
emergence of social intelligence during our experiments underscores the
potential of our approach in complex multi-agent environments.

摘要：儘管多重代理強化學習 (MARL) 演算法最近獲得成功，在混合動機環境中有效適應共同玩家仍然是一個重大的挑戰。一種可行的做法是根據推斷共同玩家的特徵，以階層方式對共同玩家的行為進行建模。然而，這些方法通常會在有效推理和利用推論資訊方面遇到困難。為了解決這些問題，我們提出階層對手建模和規劃 (HOP)，這是一種新穎的多重代理決策演算法，可以在混合動機環境中對未見過的政策進行少次適應。HOP 由兩個模組階層組成：對手建模模組，用於推論他人的目標並學習相應的目標條件政策，以及規劃模組，用於採用蒙地卡羅樹搜尋 (MCTS) 來識別最佳回應。我們的做法透過在各個情境中和情境內更新對他人目標的信念，以及使用來自對手建模模組的資訊來引導規劃，進而提高效率。實驗結果表明，在混合動機環境中，HOP 在與各種未見過的代理互動時展現出優異的少次適應能力，並在自玩情境中表現出色。此外，在我們的實驗中出現的社交智慧突顯了我們的方法在複雜的多重代理環境中的潛力。

##### **It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF**
2406.07971v1 by Taiming Lu, Lingfeng Shen, Xinyu Yang, Weiting Tan, Beidi Chen, Huaxiu Yao

Reinforcement Learning from Human Feedback (RLHF) involves training policy
models (PMs) and reward models (RMs) to align language models with human
preferences. Instead of focusing solely on PMs and RMs independently, we
propose to examine their interactions during fine-tuning, introducing the
concept of seamlessness. Our study starts with observing the saturation
phenomenon, where continual improvements in RM and PM do not translate into
RLHF progress. Our analysis shows that RMs fail to assign proper scores to PM
responses, resulting in a 35% mismatch rate with human preferences,
highlighting a significant discrepancy between PM and RM. To measure
seamlessness between PM and RM without human effort, we propose an automatic
metric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments
induced by data samples. We validate the effectiveness of SEAM in data
selection and model augmentation. Our experiments demonstrate that (1) using
SEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2)
SEAM-guided model augmentation results in a 4% performance improvement over
standard augmentation methods.

摘要：人类反馈强化学习（RLHF）涉及训练策略模型（PM）和奖励模型（RM），以使语言模型与人类偏好保持一致。我们不只专注于独立的 PM 和 RM，还建议在微调期间检查它们的交互，引入无缝性的概念。我们的研究从观察饱和现象开始，其中 RM 和 PM 的持续改进并不能转化为 RLHF 的进步。我们的分析表明，RM 无法为 PM 响应分配适当的分数，导致与人类偏好不符的比率达到 35%，突出了 PM 和 RM 之间的显着差异。为了在没有人工干预的情况下衡量 PM 和 RM 之间的无缝性，我们提出了一个自动指标 SEAM。SEAM 量化了由数据样本引起的 PM 和 RM 判断之间的差异。我们在数据选择和模型增强中验证了 SEAM 的有效性。我们的实验表明，（1）使用 SEAM 过滤的数据进行 RL 训练可将 RLHF 性能提高 4.5%，（2）SEAM 指导的模型增强比标准增强方法提高了 4% 的性能。

##### **Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation**
2406.07970v1 by Javad Pourmostafa Roshan Sharami, Dimitar Shterionov, Pieter Spronck

The quality of output from large language models (LLMs), particularly in
machine translation (MT), is closely tied to the quality of in-context examples
(ICEs) provided along with the query, i.e., the text to translate. The
effectiveness of these ICEs is influenced by various factors, such as the
domain of the source text, the order in which the ICEs are presented, the
number of these examples, and the prompt templates used. Naturally, selecting
the most impactful ICEs depends on understanding how these affect the resulting
translation quality, which ultimately relies on translation references or human
judgment. This paper presents a novel methodology for in-context learning (ICL)
that relies on a search algorithm guided by domain-specific quality estimation
(QE). Leveraging the XGLM model, our methodology estimates the resulting
translation quality without the need for translation references, selecting
effective ICEs for MT to maximize translation quality. Our results demonstrate
significant improvements over existing ICL methods and higher translation
performance compared to fine-tuning a pre-trained language model (PLM),
specifically mBART-50.

摘要：大型語言模型 (LLM) 的輸出品質，特別是在機器翻譯 (MT) 中，與查詢中提供的語境範例 (ICE) 品質密切相關，亦即要翻譯的文字。這些 ICE 的有效性受到各種因素影響，例如來源文本的領域、ICE 呈現的順序、這些範例的數量以及使用的提示範本。自然地，選擇影響最大的 ICE 取決於了解這些 ICE 如何影響最終的翻譯品質，這最終取決於翻譯參考或人類判斷。本文提出了一種新的語境學習 (ICL) 方法，該方法依賴於由特定領域品質估計 (QE) 指導的搜尋演算法。運用 XGLM 模型，我們的技術在不需要翻譯參考的情況下估計最終的翻譯品質，為機器翻譯選擇有效的 ICE 以最大化翻譯品質。我們的結果證明了與現有的 ICL 方法相比有顯著的進步，並且與微調預先訓練的語言模型 (PLM)，特別是 mBART-50 相比，翻譯效能更高。

##### **LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning**
2406.07969v1 by Masaya Kawamura, Ryuichi Yamamoto, Yuma Shirahata, Takuya Hasumi, Kentaro Tachibana

We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes
utterance-level descriptions (i.e., prompts) of speaking style and
speaker-level prompts of speaker characteristics. We employ a hybrid approach
to construct prompt annotations: (1) manual annotations that capture human
perceptions of speaker characteristics and (2) synthetic annotations on
speaking style. Compared to existing English prompt datasets, our corpus
provides more diverse prompt annotations for all speakers of LibriTTS-R.
Experimental results for prompt-based controllable TTS demonstrate that the TTS
model trained with LibriTTS-P achieves higher naturalness than the model using
the conventional dataset. Furthermore, the results for style captioning tasks
show that the model utilizing LibriTTS-P generates 2.5 times more accurate
words than the model using a conventional dataset. Our corpus, LibriTTS-P, is
available at https://github.com/line/LibriTTS-P.

摘要：我們推出 LibriTTS-P，這是一個基於 LibriTTS-R 的新語料庫，其中包含說話風格的語句級描述（即提示）和說話者特徵的說話者級提示。我們採用混合方法來建構提示註解：(1) 捕捉說話者特徵的人類感知的手動註解，以及 (2) 關於說話風格的合成註解。與現有的英語提示資料集相比，我們的語料庫為 LibriTTS-R 的所有說話者提供了更多樣化的提示註解。基於提示的可控 TTS 的實驗結果表明，使用 LibriTTS-P 訓練的 TTS 模型比使用傳統資料集的模型獲得更高的自然度。此外，風格標題任務的結果表明，利用 LibriTTS-P 的模型產生的準確字詞數比使用傳統資料集的模型多 2.5 倍。我們的語料庫 LibriTTS-P 可在 https://github.com/line/LibriTTS-P 取得。

##### **Toward a Method to Generate Capability Ontologies from Natural Language Descriptions**
2406.07962v1 by Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff, Alexander Fay

To achieve a flexible and adaptable system, capability ontologies are
increasingly leveraged to describe functions in a machine-interpretable way.
However, modeling such complex ontological descriptions is still a manual and
error-prone task that requires a significant amount of effort and ontology
expertise. This contribution presents an innovative method to automate
capability ontology modeling using Large Language Models (LLMs), which have
proven to be well suited for such tasks. Our approach requires only a natural
language description of a capability, which is then automatically inserted into
a predefined prompt using a few-shot prompting technique. After prompting an
LLM, the resulting capability ontology is automatically verified through
various steps in a loop with the LLM to check the overall correctness of the
capability ontology. First, a syntax check is performed, then a check for
contradictions, and finally a check for hallucinations and missing ontology
elements. Our method greatly reduces manual effort, as only the initial natural
language description and a final human review and possible correction are
necessary, thereby streamlining the capability ontology generation process.

摘要：為了實現彈性且適應性高的系統，能力本体論正越來越廣泛地用於以機器可解譯的方式描述功能。
然而，建模這種複雜的本体論描述仍然是一項手動且容易出錯的任務，需要大量的精力和本体論專業知識。這項貢獻提出了一種創新的方法，使用已被證明非常適合此類任務的大型語言模型 (LLM) 來自動化能力本体論建模。我們的做法只需要一種能力的自然語言描述，然後使用少次提示技術自動插入到預定義的提示中。在提示 LLM 之後，會透過 LLM 在迴圈中進行各種步驟自動驗證產生的能力本体論，以檢查能力本体論的整體正確性。首先執行語法檢查，然後檢查矛盾，最後檢查幻覺和遺漏的本体論元素。我們的做法大幅減少了手動工作，因為只需要最初的自然語言描述和最後的人工審查及可能的更正，從而簡化了能力本体論生成過程。

##### **Accurate Explanation Model for Image Classifiers using Class Association Embedding**
2406.07961v1 by Ruitao Xie, Jingbang Chen, Limai Jiang, Rui Xiao, Yi Pan, Yunpeng Cai

Image classification is a primary task in data analysis where explainable
models are crucially demanded in various applications. Although amounts of
methods have been proposed to obtain explainable knowledge from the black-box
classifiers, these approaches lack the efficiency of extracting global
knowledge regarding the classification task, thus is vulnerable to local traps
and often leads to poor accuracy. In this study, we propose a generative
explanation model that combines the advantages of global and local knowledge
for explaining image classifiers. We develop a representation learning method
called class association embedding (CAE), which encodes each sample into a pair
of separated class-associated and individual codes. Recombining the individual
code of a given sample with altered class-associated code leads to a synthetic
real-looking sample with preserved individual characters but modified
class-associated features and possibly flipped class assignments. A
building-block coherency feature extraction algorithm is proposed that
efficiently separates class-associated features from individual ones. The
extracted feature space forms a low-dimensional manifold that visualizes the
classification decision patterns. Explanation on each individual sample can be
then achieved in a counter-factual generation manner which continuously
modifies the sample in one direction, by shifting its class-associated code
along a guided path, until its classification outcome is changed. We compare
our method with state-of-the-art ones on explaining image classification tasks
in the form of saliency maps, demonstrating that our method achieves higher
accuracies. The code is available at https://github.com/xrt11/XAI-CODE.

摘要：影像分類是資料分析中一項主要任務，其中可解釋模型在各種應用中至關重要。儘管已提出大量方法從黑盒分類器中取得可解釋知識，但這些方法缺乏提取分類任務中整體知識的效率，因此容易陷入局部陷阱，且經常導致精確度不佳。在本研究中，我們提出一個生成式解釋模型，結合整體和局部知識的優點來解釋影像分類器。我們開發一種稱為類別關聯嵌入 (CAE) 的表示學習方法，將每個範例編碼成一對分離的類別關聯和個別代碼。將特定範例的個別代碼與變更的類別關聯代碼重新組合，會產生一個保留個別特徵但修改類別關聯特徵，且可能翻轉類別指派的合成真實範例。我們提出一個建構區塊相干性特徵提取演算法，可有效將類別關聯特徵與個別特徵分開。提取的特徵空間形成一個低維流形，可視化分類決策模式。然後，每個個別範例的解釋可以在反事實生成方式中達成，該方式透過沿著引導路徑移動其類別關聯代碼，持續地朝一個方向修改範例，直到其分類結果改變。我們以顯著度圖的形式將我們的模型與現有技術進行比較，以解釋影像分類任務，證明我們的模型可達成更高的精確度。程式碼可在 https://github.com/xrt11/XAI-CODE 取得。

##### **Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition**
2406.07954v1 by Edoardo Debenedetti, Javier Rando, Daniel Paleka, Silaghi Fineas Florin, Dragos Albastroiu, Niv Cohen, Yuval Lemberg, Reshmi Ghosh, Rui Wen, Ahmed Salem, Giovanni Cherubin, Santiago Zanella-Beguelin, Robin Schmid, Victor Klemm, Takahiro Miki, Chenhao Li, Stefan Kraft, Mario Fritz, Florian Tramèr, Sahar Abdelnabi, Lea Schönherr

Large language model systems face important security risks from maliciously
crafted messages that aim to overwrite the system's original instructions or
leak private data. To study this problem, we organized a capture-the-flag
competition at IEEE SaTML 2024, where the flag is a secret string in the LLM
system prompt. The competition was organized in two phases. In the first phase,
teams developed defenses to prevent the model from leaking the secret. During
the second phase, teams were challenged to extract the secrets hidden for
defenses proposed by the other teams. This report summarizes the main insights
from the competition. Notably, we found that all defenses were bypassed at
least once, highlighting the difficulty of designing a successful defense and
the necessity for additional research to protect LLM systems. To foster future
research in this direction, we compiled a dataset with over 137k multi-turn
attack chats and open-sourced the platform.

摘要：大型語言模型系統面臨惡意製作訊息的重要安全風險，旨在覆寫系統的原始指令或洩露私人數據。為了研究這個問題，我們在 IEEE SaTML 2024 舉辦了一場攻防競賽，其中旗幟是 LLM 系統提示中的秘密字串。比賽分為兩個階段。在第一階段，團隊開發了防禦措施來防止模型洩露秘密。在第二階段，團隊面臨挑戰，必須找出其他團隊提出的防禦措施中隱藏的秘密。本報告總結了比賽的主要見解。值得注意的是，我們發現所有防禦措施都至少被繞過一次，這凸顯了設計成功防禦措施的難度，以及保護 LLM 系統的額外研究必要性。為了促進未來在這個方向的研究，我們編制了一個包含超過 137k 個多輪攻擊聊天記錄的資料集，並開放了平台的原始碼。

##### **Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization**
2406.07948v1 by Guopeng Lin, Weili Han, Wenqiang Ruan, Ruisheng Zhou, Lushan Song, Bingshuai Li, Yunfeng Shao

Multi-party training frameworks for decision trees based on secure
multi-party computation enable multiple parties to train high-performance
models on distributed private data with privacy preservation. The training
process essentially involves frequent dataset splitting according to the
splitting criterion (e.g. Gini impurity). However, existing multi-party
training frameworks for decision trees demonstrate communication inefficiency
due to the following issues: (1) They suffer from huge communication overhead
in securely splitting a dataset with continuous attributes. (2) They suffer
from huge communication overhead due to performing almost all the computations
on a large ring to accommodate the secure computations for the splitting
criterion.
  In this paper, we are motivated to present an efficient three-party training
framework, namely Ents, for decision trees by communication optimization. For
the first issue, we present a series of training protocols based on the secure
radix sort protocols to efficiently and securely split a dataset with
continuous attributes. For the second issue, we propose an efficient share
conversion protocol to convert shares between a small ring and a large ring to
reduce the communication overhead incurred by performing almost all the
computations on a large ring. Experimental results from eight widely used
datasets show that Ents outperforms state-of-the-art frameworks by $5.5\times
\sim 9.3\times$ in communication sizes and $3.9\times \sim 5.3\times$ in
communication rounds. In terms of training time, Ents yields an improvement of
$3.5\times \sim 6.7\times$. To demonstrate its practicality, Ents requires less
than three hours to securely train a decision tree on a widely used real-world
dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.

摘要：<paragraph>基於安全多方計算的多方訓練架構，讓多方能夠在具有隱私保護的分布式私人資料上訓練高性能模型。訓練過程基本上涉及根據分割準則（例如，基尼不純度）頻繁分割資料集。然而，現有的多方決策樹訓練架構由於以下問題而表現出通訊效率低下：(1) 在安全分割具有連續屬性的資料集時，它們會產生巨大的通訊開銷。(2) 它們會產生巨大的通訊開銷，因為幾乎所有運算都在一個大環上執行，以適應分割準則的安全運算。在本文中，我們有動力提出一個高效的三方訓練架構，即 Ents，用於通過通訊最佳化來進行決策樹訓練。對於第一個問題，我們提出了一系列基於安全基數排序協定的訓練協定，以有效且安全地分割具有連續屬性的資料集。對於第二個問題，我們提出了一個高效的共享轉換協定，以在小環和大環之間轉換共享，以減少執行幾乎所有運算在大環上所產生的通訊開銷。來自八個廣泛使用的資料集的實驗結果表明，Ents 在通訊大小上優於最先進的框架 $5.5\times \sim 9.3\times$，在通訊回合數上優於 $3.9\times \sim 5.3\times$。在訓練時間方面，Ents 的改進為 $3.5\times \sim 6.7\times$。為了證明其實用性，Ents 在 WAN 設定中使用超過 245,000 個樣本的廣泛使用的真實世界資料集（皮膚分割）安全訓練決策樹所需的時間不到三小時。</paragraph>

##### **DLLens: Testing Deep Learning Libraries via LLM-aided Synthesis**
2406.07944v1 by Meiziniu Li, Dongze Li, Jianmeng Liu, Jialun Cao, Yongqiang Tian, Shing-Chi Cheung

Testing is a major approach to ensuring the quality of deep learning (DL)
libraries. Existing testing techniques commonly adopt differential testing to
relieve the need for test oracle construction. However, these techniques are
limited in finding implementations that offer the same functionality and
generating diverse test inputs for differential testing. This paper introduces
DLLens, a novel differential testing technique for DL library testing. Our
insight is that APIs in different DL libraries are commonly designed to
accomplish various computations for the same set of published DL algorithms.
Although the mapping of these APIs is not often one-to-one, we observe that
their computations can be mutually simulated after proper composition and
adaptation. The use of these simulation counterparts facilitates differential
testing for the detection of functional DL library bugs. Leveraging the
insight, we propose DLLens as a novel mechanism that utilizes a large language
model (LLM) to synthesize valid counterparts of DL library APIs. To generate
diverse test inputs, DLLens incorporates a static analysis method aided by LLM
to extract path constraints from all execution paths in each API and its
counterpart's implementations. These path constraints are then used to guide
the generation of diverse test inputs. We evaluate DLLens on two popular DL
libraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can
synthesize counterparts for more than twice as many APIs found by
state-of-the-art techniques on these libraries. Moreover, DLLens can extract
26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art
techniques. DLLens has successfully found 56 bugs in recent TensorFlow and
PyTorch libraries. Among them, 41 are previously unknown, 39 of which have been
confirmed by developers after reporting, and 19 of those confirmed bugs have
been fixed by developers.

摘要：<paragraph>測試是確保深度學習 (DL) 函式庫品質的主要方法。現有的測試技術通常採用差異測試來減輕測試預言建構的需求。然而，這些技術在尋找提供相同功能和產生差異測試的多樣化測試輸入方面受到限制。本文介紹 DLLens，一種用於 DL 函式庫測試的新穎差異測試技術。我們的見解是，不同 DL 函式庫中的 API 通常被設計為針對同一組已發布的 DL 演算法執行各種運算。儘管這些 API 的對應通常不是一對一的，但我們觀察到在適當的組合和調整後，它們的運算可以相互模擬。使用這些模擬對應物有助於進行差異測試，以偵測功能性 DL 函式庫錯誤。利用此見解，我們提出 DLLens 作為一種新穎的機制，它利用大型語言模型 (LLM) 來綜合 DL 函式庫 API 的有效對應物。為了產生多樣化的測試輸入，DLLens 結合了靜態分析方法，並借助 LLM 從每個 API 及其對應實作中的所有執行路徑中擷取路徑約束。然後使用這些路徑約束來引導多樣化測試輸入的產生。我們在兩個流行的 DL 函式庫 TensorFlow 和 PyTorch 上評估 DLLens。我們的評估顯示，DLLens 可以為這些函式庫中由最先進技術找到的 API 數量的兩倍以上綜合對應物。此外，DLLens 可以擷取多 26.7% 的約束，並偵測多 2.5 倍的錯誤，如同最先進的技術。DLLens 已成功在最近的 TensorFlow 和 PyTorch 函式庫中找到 56 個錯誤。其中，41 個以前未知，在報告後有 39 個已獲得開發人員確認，而這些已確認的錯誤中有 19 個已獲得開發人員修復。</paragraph>

##### **Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation**
2406.07935v1 by Jie Ruan, Wenqing Wang, Xiaojun Wan

Human evaluation serves as the gold standard for assessing the quality of
Natural Language Generation (NLG) systems. Nevertheless, the evaluation
guideline, as a pivotal element ensuring reliable and reproducible human
assessment, has received limited attention.Our investigation revealed that only
29.84% of recent papers involving human evaluation at top conferences release
their evaluation guidelines, with vulnerabilities identified in 77.09% of these
guidelines. Unreliable evaluation guidelines can yield inaccurate assessment
outcomes, potentially impeding the advancement of NLG in the right direction.
To address these challenges, we take an initial step towards reliable
evaluation guidelines and propose the first human evaluation guideline dataset
by collecting annotations of guidelines extracted from existing papers as well
as generated via Large Language Models (LLMs). We then introduce a taxonomy of
eight vulnerabilities and formulate a principle for composing evaluation
guidelines. Furthermore, a method for detecting guideline vulnerabilities has
been explored using LLMs, and we offer a set of recommendations to enhance
reliability in human evaluation. The annotated human evaluation guideline
dataset and code for the vulnerability detection method are publicly available
online.

摘要：人類評估是評估自然語言生成 (NLG) 系統品質的黃金標準。儘管如此，評估準則作為確保可靠且可重製人類評估的關鍵要素，卻鮮少受到重視。我們的調查顯示，在頂尖會議中涉及人類評估的近期論文中，只有 29.84% 發布了評估準則，而這些準則中有 77.09% 被找出漏洞。不可靠的評估準則可能會產生不準確的評估結果，進而阻礙 NLG 朝正確的方向前進。為了應對這些挑戰，我們朝著可靠的評估準則邁出第一步，並透過收集從現有論文中萃取的準則註解，以及透過大型語言模型 (LLM) 生成的準則，提出了第一個人類評估準則資料集。接著，我們介紹了八項漏洞的分類法，並制定了撰寫評估準則的原則。此外，我們已使用 LLM 探索了一種偵測準則漏洞的方法，並提供了一組建議，以增強人類評估的可靠性。已註解的人類評估準則資料集和漏洞偵測方法的程式碼已公開於線上。

##### **Large Language Model Unlearning via Embedding-Corrupted Prompts**
2406.07933v1 by Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, Yang Liu

Large language models (LLMs) have advanced to encompass extensive knowledge
across diverse domains. Yet controlling what a large language model should not
know is important for ensuring alignment and thus safe use. However, accurately
and efficiently unlearning knowledge from an LLM remains challenging due to the
potential collateral damage caused by the fuzzy boundary between retention and
forgetting, and the large computational requirements for optimization across
state-of-the-art models with hundreds of billions of parameters. In this work,
we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning
framework for large language models to address both the challenges of knowledge
entanglement and unlearning efficiency. Instead of relying on the LLM itself to
unlearn, we enforce an unlearned state during inference by employing a prompt
classifier to identify and safeguard prompts to forget. We learn corruptions
added to prompt embeddings via zeroth order optimization toward the unlearning
objective offline and corrupt prompts flagged by the classifier during
inference. We find that these embedding-corrupted prompts not only lead to
desirable outputs that satisfy the unlearning objective but also closely
approximate the output from a model that has never been trained on the data
intended for forgetting. Through extensive experiments on unlearning, we
demonstrate the superiority of our method in achieving promising unlearning at
nearly zero side effects in general domains and domains closely related to the
unlearned ones. Additionally, we highlight the scalability of our method to 100
LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the
number of parameters increases.

摘要：大型語言模型 (LLM) 已進步到涵蓋各個領域的廣泛知識。然而，控制大型語言模型不應知道的內容對於確保對齊並因此安全使用非常重要。然而，由於保留和遺忘之間的模糊界限造成的潛在附帶損害，以及優化數百億個參數的最新模型所需的龐大計算需求，準確且有效地從 LLM 中遺忘知識仍然具有挑戰性。在這項工作中，我們提出了嵌入式損壞 (ECO) 提示，這是一種輕量級的遺忘框架，適用於大型語言模型，以應對知識糾纏和遺忘效率的挑戰。我們並非依賴 LLM 本身來遺忘，而是通過使用提示分類器來識別和保護提示以遺忘，從而強制執行推論期間的未學習狀態。我們通過零階優化學習添加到提示嵌入的損壞，以實現離線遺忘目標，並損壞推論期間分類器標記的提示。我們發現，這些嵌入式損壞的提示不僅會導致滿足遺忘目標的理想輸出，而且還非常接近從未針對打算遺忘的數據進行訓練的模型的輸出。通過對遺忘進行廣泛的實驗，我們證明了我們的方法在一般領域和與未學習領域密切相關的領域實現有希望的遺忘時，副作用幾乎為零。此外，我們強調了我們的方法對 100 個 LLM 的可擴展性，範圍從 0.5B 到 236B 參數，隨著參數數量的增加，不會產生額外的成本。

##### **A Generic Layer Pruning Method for Signal Modulation Recognition Deep Learning Models**
2406.07929v1 by Yao Lu, Yutao Zhu, Yuqi Li, Dongwei Xu, Yun Lin, Qi Xuan, Xiaoniu Yang

With the successful application of deep learning in communications systems,
deep neural networks are becoming the preferred method for signal
classification. Although these models yield impressive results, they often come
with high computational complexity and large model sizes, which hinders their
practical deployment in communication systems. To address this challenge, we
propose a novel layer pruning method. Specifically, we decompose the model into
several consecutive blocks, each containing consecutive layers with similar
semantics. Then, we identify layers that need to be preserved within each block
based on their contribution. Finally, we reassemble the pruned blocks and
fine-tune the compact model. Extensive experiments on five datasets demonstrate
the efficiency and effectiveness of our method over a variety of
state-of-the-art baselines, including layer pruning and channel pruning
methods.

摘要：隨著深度學習在通訊系統中的成功應用，深度神經網路正成為訊號分類的首選方法。儘管這些模型產生令人印象深刻的結果，但它們通常伴隨著高運算複雜度和大模型大小，這會阻礙它們在通訊系統中的實際部署。為了應對這一挑戰，我們提出了一種新穎的層修剪方法。具體來說，我們將模型分解為幾個連續的區塊，每個區塊包含具有類似語義的連續層。然後，我們根據它們的貢獻來識別需要在每個區塊內保留的層。最後，我們重新組裝修剪後的區塊並微調緊湊模型。在五個資料集上的大量實驗證明了我們的方法在各種最先進的基準上的效率和有效性，包括層修剪和通道修剪方法。

##### **Efficient Neural Common Neighbor for Temporal Graph Link Prediction**
2406.07926v1 by Xiaohui Zhang, Yanbo Wang, Xiyuan Wang, Muhan Zhang

Temporal graphs are ubiquitous in real-world scenarios, such as social
network, trade and transportation. Predicting dynamic links between nodes in a
temporal graph is of vital importance. Traditional methods usually leverage the
temporal neighborhood of interaction history to generate node embeddings first
and then aggregate the source and target node embeddings to predict the link.
However, such methods focus on learning individual node representations, but
overlook the pairwise representation learning nature of link prediction and
fail to capture the important pairwise features of links such as common
neighbors (CN). Motivated by the success of Neural Common Neighbor (NCN) for
static graph link prediction, we propose TNCN, a temporal version of NCN for
link prediction in temporal graphs. TNCN dynamically updates a temporal
neighbor dictionary for each node, and utilizes multi-hop common neighbors
between the source and target node to learn a more effective pairwise
representation. We validate our model on five large-scale real-world datasets
from the Temporal Graph Benchmark (TGB), and find that it achieves new
state-of-the-art performance on three of them. Additionally, TNCN demonstrates
excellent scalability on large datasets, outperforming popular GNN baselines by
up to 6.4 times in speed. Our code is available at https:
//github.com/GraphPKU/TNCN.

摘要：時序圖在真實世界的場景中無處不在，例如社交網路、貿易和運輸。預測時序圖中節點之間的動態連結至關重要。傳統方法通常利用互動歷史的時間鄰域來生成節點嵌入，然後聚合來源和目標節點嵌入以預測連結。然而，這種方法專注於學習個別節點表示，但忽視了連結預測的成對表示學習性質，並且無法捕捉連結的重要成對特徵，例如共同鄰居 (CN)。受用於靜態圖連結預測的神經共同鄰居 (NCN) 的成功啟發，我們提出了 TNCN，它是 NCN 的時間版本，用於時序圖中的連結預測。TNCN 動態更新每個節點的時間鄰居字典，並利用來源和目標節點之間的多跳共同鄰居來學習更有效的成對表示。我們在時序圖基準 (TGB) 中的五個大型真實世界資料集上驗證了我們的模型，並發現它在其中三個資料集上達到了新的最先進性能。此外，TNCN 在大型資料集上展示了出色的可擴充性，速度比流行的 GNN 基準高出 6.4 倍。我們的程式碼可在 https://github.com/GraphPKU/TNCN 取得。

##### **CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword Spotting**
2406.07923v1 by Sichen Jin, Youngmoon Jung, Seungjin Lee, Jaeyoung Roh, Changwoo Han, Hoonyoung Cho

This paper introduces a novel approach for streaming openvocabulary keyword
spotting (KWS) with text-based keyword enrollment. For every input frame, the
proposed method finds the optimal alignment ending at the frame using
connectionist temporal classification (CTC) and aggregates the frame-level
acoustic embedding (AE) to obtain higher-level (i.e., character, word, or
phrase) AE that aligns with the text embedding (TE) of the target keyword text.
After that, we calculate the similarity of the aggregated AE and the TE. To the
best of our knowledge, this is the first attempt to dynamically align the audio
and the keyword text on-the-fly to attain the joint audio-text embedding for
KWS. Despite operating in a streaming fashion, our approach achieves
competitive performance on the LibriPhrase dataset compared to the
non-streaming methods with a mere 155K model parameters and a decoding
algorithm with time complexity O(U), where U is the length of the target
keyword at inference time.

摘要：本文提出了一個創新的方法，用基於文字的關鍵字註冊串流開放詞彙關鍵字偵測 (KWS)。對於每個輸入幀，建議的方法使用連接主義時序分類 (CTC) 找出結束於幀的最佳對齊，並彙總幀級聲學嵌入 (AE)，以取得與目標關鍵字文字的文字嵌入 (TE) 對齊的更高層級 (即字元、單字或詞組) AE。之後，我們計算彙總 AE 和 TE 的相似度。據我們所知，這是首次嘗試動態對齊音訊和關鍵字文字，以取得用於 KWS 的聯合音訊文字嵌入。儘管以串流方式操作，但我們的方法在 LibriPhrase 資料集上達到了與非串流方法相當的效能，僅使用 155K 模型參數和時間複雜度為 O(U) 的解碼演算法，其中 U 是推論時間目標關鍵字的長度。

##### **Automated Information Extraction from Thyroid Operation Narrative: A Comparative Study of GPT-4 and Fine-tuned KoELECTRA**
2406.07922v1 by Dongsuk Jang, Hyeryun Park, Jiye Son, Hyeonuk Hwang, Sujin Kim, Jinwook Choi

In the rapidly evolving field of healthcare, the integration of artificial
intelligence (AI) has become a pivotal component in the automation of clinical
workflows, ushering in a new era of efficiency and accuracy. This study focuses
on the transformative capabilities of the fine-tuned KoELECTRA model in
comparison to the GPT-4 model, aiming to facilitate automated information
extraction from thyroid operation narratives. The current research landscape is
dominated by traditional methods heavily reliant on regular expressions, which
often face challenges in processing free-style text formats containing critical
details of operation records, including frozen biopsy reports. Addressing this,
the study leverages advanced natural language processing (NLP) techniques to
foster a paradigm shift towards more sophisticated data processing systems.
Through this comparative study, we aspire to unveil a more streamlined,
precise, and efficient approach to document processing in the healthcare
domain, potentially revolutionizing the way medical data is handled and
analyzed.

摘要：在快速發展的醫療保健領域中，人工智慧 (AI) 的整合已成為臨床工作流程自動化的關鍵組成部分，引領著效率和準確性的新紀元。本研究專注於微調後的 KoELECTRA 模型的轉換能力，並與 GPT-4 模型進行比較，旨在促進自動化資訊從甲狀腺手術敘述中擷取。目前的的研究現況是由高度依賴正規表示式的傳統方法所主導，在處理包含手術記錄關鍵細節的自由式文字格式時，通常會面臨挑戰，包括冷凍切片報告。針對此問題，本研究利用進階自然語言處理 (NLP) 技術，促進典範轉移，朝向更精密的資料處理系統。透過這項比較研究，我們希望揭示一種更簡化、更精確、更有效率的方法，用於醫療領域的文件處理，並有可能徹底改變醫療資料的處理和分析方式。

##### **Near-Optimal Learning and Planning in Separated Latent MDPs**
2406.07920v1 by Fan Chen, Constantinos Daskalakis, Noah Golowich, Alexander Rakhlin

We study computational and statistical aspects of learning Latent Markov
Decision Processes (LMDPs). In this model, the learner interacts with an MDP
drawn at the beginning of each epoch from an unknown mixture of MDPs. To
sidestep known impossibility results, we consider several notions of separation
of the constituent MDPs. The main thrust of this paper is in establishing a
nearly-sharp *statistical threshold* for the horizon length necessary for
efficient learning. On the computational side, we show that under a weaker
assumption of separability under the optimal policy, there is a
quasi-polynomial algorithm with time complexity scaling in terms of the
statistical threshold. We further show a near-matching time complexity lower
bound under the exponential time hypothesis.

摘要：我們研究潛在馬可夫決策過程 (LMDP) 的計算和統計面向。在這個模型中，學習者與在每個時期開始時從未知的 MDP 混合中抽取的 MDP 互動。為了避開已知的不可能結果，我們考慮了構成 MDP 的幾個分離概念。本文的主要重點在於建立一個接近尖銳的 *統計閾值*，以了解有效學習所需的水平長度。在計算方面，我們表明在最優策略下較弱的可分離性假設下，存在一個準多項式演算法，其時間複雜度會隨著統計閾值而縮放。我們進一步表明在指數時間假設下，接近匹配的時間複雜度下限。

##### **Graph Transductive Defense: a Two-Stage Defense for Graph Membership Inference Attacks**
2406.07917v1 by Peizhi Niu, Chao Pan, Siheng Chen, Olgica Milenkovic

Graph neural networks (GNNs) have become instrumental in diverse real-world
applications, offering powerful graph learning capabilities for tasks such as
social networks and medical data analysis. Despite their successes, GNNs are
vulnerable to adversarial attacks, including membership inference attacks
(MIA), which threaten privacy by identifying whether a record was part of the
model's training data. While existing research has explored MIA in GNNs under
graph inductive learning settings, the more common and challenging graph
transductive learning setting remains understudied in this context. This paper
addresses this gap and proposes an effective two-stage defense, Graph
Transductive Defense (GTD), tailored to graph transductive learning
characteristics. The gist of our approach is a combination of a train-test
alternate training schedule and flattening strategy, which successfully reduces
the difference between the training and testing loss distributions. Extensive
empirical results demonstrate the superior performance of our method (a
decrease in attack AUROC by $9.42\%$ and an increase in utility performance by
$18.08\%$ on average compared to LBP), highlighting its potential for seamless
integration into various classification models with minimal overhead.

摘要：圖形神經網路 (GNN) 已成為各種真實世界應用中不可或缺的工具，為社交網路和醫療數據分析等任務提供強大的圖形學習能力。儘管取得成功，GNN 仍容易受到對抗性攻擊，包括成員推論攻擊 (MIA)，這會透過辨識記錄是否為模型訓練資料的一部分來威脅隱私。雖然現有研究已探討圖形歸納學習設定下的 GNN 中的 MIA，但更常見且更具挑戰性的圖形轉導學習設定在此背景下仍未獲得充分研究。本文探討此差距，並提出一個有效的分兩階段防禦機制，圖形轉導防禦 (GTD)，專門針對圖形轉導學習特性量身打造。我們方法的要點是結合訓練測試交替訓練時程和扁平化策略，成功縮小訓練和測試損失分佈之間的差異。廣泛的實證結果證明我們方法的優異效能（與 LBP 相比，攻擊 AUROC 減少 9.42%，效用效能平均增加 18.08%），突顯其在各種分類模型中無縫整合的潛力，且開銷極小。

##### **DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning**
2406.07913v1 by Yuxi Feng, Raymond Li, Zhenan Fan, Giuseppe Carenini, Mohammadreza Pourreza, Weiwei Zhang, Yong Zhang

While in-context Learning (ICL) has proven to be an effective technique to
improve the performance of Large Language Models (LLMs) in a variety of complex
tasks, notably in translating natural language questions into Structured Query
Language (NL2SQL), the question of how to select the most beneficial
demonstration examples remains an open research problem. While prior works
often adapted off-the-shelf encoders to retrieve examples dynamically, an
inherent discrepancy exists in the representational capacities between the
external retrievers and the LLMs. Further, optimizing the selection of examples
is a non-trivial task, since there are no straightforward methods to assess the
relative benefits of examples without performing pairwise inference. To address
these shortcomings, we propose DeTriever, a novel demonstration retrieval
framework that learns a weighted combination of LLM hidden states, where rich
semantic information is encoded. To train the model, we propose a proxy score
that estimates the relative benefits of examples based on the similarities
between output queries. Experiments on two popular NL2SQL benchmarks
demonstrate that our method significantly outperforms the state-of-the-art
baselines on one-shot NL2SQL tasks.

摘要：儘管情境學習 (ICL) 已被證明是一種有效技術，可提升大型語言模型 (LLM) 在各種複雜任務中的效能，特別是在將自然語言問題轉換為結構化查詢語言 (NL2SQL) 時，如何選擇最有益的示範範例的問題仍然是一個開放的研究問題。儘管先前的研究通常採用現成的編碼器動態擷取範例，但外部擷取器和 LLM 之間的表示能力存在固有的差異。此外，最佳化範例的選擇並非易事，因為沒有直接的方法可以在不執行成對推論的情況下評估範例的相對好處。為了解決這些缺點，我們提出 DeTriever，這是一個新穎的示範擷取架構，可學習 LLM 隱藏狀態的加權組合，其中編碼了豐富的語義資訊。為了訓練模型，我們提出一個代理評分，根據輸出查詢之間的相似性估計範例的相對好處。在兩個熱門的 NL2SQL 基準上的實驗證明，我們的模型在一次性 NL2SQL 任務上顯著優於最先進的基線。

##### **Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation**
2406.07909v1 by Eungbeom Kim, Hantae Kim, Kyogu Lee

Transformer encoder with connectionist temporal classification (CTC)
framework is widely used for automatic speech recognition (ASR). However,
knowledge distillation (KD) for ASR displays a problem of disagreement between
teacher-student models in frame-level alignment which ultimately hinders it
from improving the student model's performance. In order to resolve this
problem, this paper introduces a self-knowledge distillation (SKD) method that
guides the frame-level alignment during the training time. In contrast to the
conventional method using separate teacher and student models, this study
introduces a simple and effective method sharing encoder layers and applying
the sub-model as the student model. Overall, our approach is effective in
improving both the resource efficiency as well as performance. We also
conducted an experimental analysis of the spike timings to illustrate that the
proposed method improves performance by reducing the alignment disagreement.

摘要：連接主義時序分類 (CTC) 框架的 Transformer 編碼器廣泛用於自動語音辨識 (ASR)。然而，ASR 的知識蒸餾 (KD) 在框架層級對齊上會出現師生模型之間意見分歧的問題，最終阻礙了它提升學生模型的效能。為了解決這個問題，本文介紹了一種自我知識蒸餾 (SKD) 方法，在訓練期間引導框架層級對齊。與使用獨立師生模型的傳統方法相反，本研究介紹了一種簡單且有效的方法，共享編碼器層並將子模型應用為學生模型。總的來說，我們的做法有效地提升了資源效率和效能。我們還進行了尖峰時序的實驗分析，以說明所提出的方法透過減少對齊分歧來提升效能。

##### **Ablation Based Counterfactuals**
2406.07908v1 by Zheng Dai, David K Gifford

Diffusion models are a class of generative models that generate high-quality
samples, but at present it is difficult to characterize how they depend upon
their training data. This difficulty raises scientific and regulatory
questions, and is a consequence of the complexity of diffusion models and their
sampling process. To analyze this dependence, we introduce Ablation Based
Counterfactuals (ABC), a method of performing counterfactual analysis that
relies on model ablation rather than model retraining. In our approach, we
train independent components of a model on different but overlapping splits of
a training set. These components are then combined into a single model, from
which the causal influence of any training sample can be removed by ablating a
combination of model components. We demonstrate how we can construct a model
like this using an ensemble of diffusion models. We then use this model to
study the limits of training data attribution by enumerating full
counterfactual landscapes, and show that single source attributability
diminishes with increasing training data size. Finally, we demonstrate the
existence of unattributable samples.

摘要：擴散模型是一類生成模型，可產生高品質的樣本，但目前很難描述它們如何依賴其訓練資料。此困難性引發了科學和法規問題，並且是擴散模型及其取樣過程複雜性的後果。為了分析這種依賴性，我們引入了基於消融的反事實 (ABC)，這是一種執行反事實分析的方法，它依賴於模型消融，而不是模型再訓練。在我們的做法中，我們在訓練集的不同但重疊的分割上訓練模型的獨立組成部分。然後將這些組成部分組合成一個單一模型，從中可以通過消融模型組成部分的組合來移除任何訓練樣本的因果影響。我們展示了如何使用擴散模型的集合構建這樣的模型。然後，我們使用此模型通過列舉完整的反事實場景來研究訓練資料歸因的限制，並表明單一來源可歸因性會隨著訓練資料大小的增加而降低。最後，我們證明了不可歸因樣本的存在。

##### **Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations**
2406.07900v1 by Bulat Khaertdinov, Pedro Jeuris, Annanda Sousa, Enrique Hortal

Recent advancements in Deep and Self-Supervised Learning (SSL) have led to
substantial improvements in Speech Emotion Recognition (SER) performance,
reaching unprecedented levels. However, obtaining sufficient amounts of
accurately labeled data for training or fine-tuning the models remains a costly
and challenging task. In this paper, we propose a multi-view SSL pre-training
technique that can be applied to various representations of speech, including
the ones generated by large speech models, to improve SER performance in
scenarios where annotations are limited. Our experiments, based on wav2vec 2.0,
spectral and paralinguistic features, demonstrate that the proposed framework
boosts the SER performance, by up to 10% in Unweighted Average Recall, in
settings with extremely sparse data annotations.

摘要：深度和自我监督学习 (SSL) 的最新进展带来了
语音情感识别 (SER) 性能的显着提升，
达到前所未有的水平。然而，获取足够数量的
准确标记的数据来训练或微调模型仍然是一项昂贵
且具有挑战性的任务。在本文中，我们提出了一种多视图 SSL 预训练
技术，该技术可应用于语音的各种表示，包括
由大型语音模型生成的表示，以提高 SER 性能
在注释有限的情况下。我们的实验基于 wav2vec 2.0，
频谱和语言外特征表明，所提出的框架
提高了 SER 性能，在极度稀疏的数据注释中，
加权平均召回率提高了 10%。

##### **Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions**
2406.07890v1 by Anfeng Xu, Kevin Huang, Tiantian Feng, Lue Shen, Helen Tager-Flusberg, Shrikanth Narayanan

Speech foundation models, trained on vast datasets, have opened unique
opportunities in addressing challenging low-resource speech understanding, such
as child speech. In this work, we explore the capabilities of speech foundation
models on child-adult speaker diarization. We show that exemplary foundation
models can achieve 39.5% and 62.3% relative reductions in Diarization Error
Rate and Speaker Confusion Rate, respectively, compared to previous speaker
diarization methods. In addition, we benchmark and evaluate the speaker
diarization results of the speech foundation models with varying the input
audio window size, speaker demographics, and training data ratio. Our results
highlight promising pathways for understanding and adopting speech foundation
models to facilitate child speech understanding.

摘要：語音基礎模型在龐大資料集上訓練，為解決具挑戰性的低資源語音理解（例如兒童語音）開啟了獨特的機會。在這項工作中，我們探討了語音基礎模型在兒童成人說話者日記化中的能力。我們展示了範例基礎模型在日記化錯誤率和說話者混淆率方面分別實現了 39.5% 和 62.3% 的相對減少，與先前的說話者日記化方法相比。此外，我們對語音基礎模型的說話者日記化結果進行基準測試和評估，並改變輸入音訊視窗大小、說話者人口統計資料和訓練資料比率。我們的結果突顯了理解和採用語音基礎模型以促進兒童語音理解的有希望途徑。

##### **Classification Modeling with RNN-Based, Random Forest, and XGBoost for Imbalanced Data: A Case of Early Crash Detection in ASEAN-5 Stock Markets**
2406.07888v1 by Deri Siswara, Agus M. Soleh, Aji Hamim Wigena

This research aims to evaluate the performance of several Recurrent Neural
Network (RNN) architectures including Simple RNN, Gated Recurrent Units (GRU),
and Long Short-Term Memory (LSTM), compared to classic algorithms such as
Random Forest and XGBoost in building classification models for early crash
detection in ASEAN-5 stock markets. The study is examined using imbalanced
data, which is common due to the rarity of market crashes. The study analyzes
daily data from 2010 to 2023 across the major stock markets of the ASEAN-5
countries, including Indonesia, Malaysia, Singapore, Thailand, and Philippines.
Market crash is identified as the target variable when the major stock price
indices fall below the Value at Risk (VaR) thresholds of 5%, 2.5% and 1%.
predictors involving technical indicators of major local and global markets as
well as commodity markets. This study includes 213 predictors with their
respective lags (5, 10, 15, 22, 50, 200) and uses a time step of 7, expanding
the total number of predictors to 1491. The challenge of data imbalance is
addressed with SMOTE-ENN. The results show that all RNN-Based architectures
outperform Random Forest and XGBoost. Among the various RNN architectures,
Simple RNN stands out as the most superior, mainly due to the data
characteristics that are not overly complex and focus more on short-term
information. This study enhances and extends the range of phenomena observed in
previous studies by incorporating variables like different geographical zones
and time periods, as well as methodological adjustments.

摘要：本研究旨在評估幾種遞迴神經網路 (RNN) 架構的效能，包括簡單 RNN、門控遞迴單元 (GRU) 和長短期記憶 (LSTM)，並與隨機森林和 XGBoost 等經典演算法進行比較，以建立分類模型，用於東協 5 國股市的早期崩盤偵測。本研究使用不平衡資料進行檢驗，這在市場崩盤的罕見情況下很常見。本研究分析了 2010 年至 2023 年期間東協 5 國（包括印尼、馬來西亞、新加坡、泰國和菲律賓）主要股市中的每日資料。當主要股價指數跌破 5%、2.5% 和 1% 的風險價值 (VaR) 閾值時，市場崩盤被認定為目標變數。預測因子涉及主要本地和全球市場以及商品市場的技術指標。本研究包含 213 個預測因子及其各自的滯後 (5、10、15、22、50、200)，並使用 7 的時間步長，將預測因子的總數擴充至 1491。資料不平衡的挑戰使用 SMOTE-ENN 來解決。結果顯示所有基於 RNN 的架構都優於隨機森林和 XGBoost。在各種 RNN 架構中，簡單 RNN 表現最為出色，這主要是因為資料特性並非過於複雜，且更注重短期資訊。本研究透過納入不同地理區域和時間段等變數，以及方法調整，擴展並延伸了先前研究中觀察到的現象範圍。

##### **An Empirical Study of Mamba-based Language Models**
2406.07887v1 by Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro

Selective state-space models (SSMs) like Mamba overcome some of the
shortcomings of Transformers, such as quadratic computational complexity with
sequence length and large inference-time memory requirements from the key-value
cache. Moreover, recent studies have shown that SSMs can match or exceed the
language modeling capabilities of Transformers, making them an attractive
alternative. In a controlled setting (e.g., same data), however, studies so far
have only presented small scale experiments comparing SSMs to Transformers. To
understand the strengths and weaknesses of these architectures at larger
scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and
Transformer models trained on the same datasets of up to 3.5T tokens. We also
compare these models to a hybrid architecture consisting of 43% Mamba-2, 7%
attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,
we answer the question of whether Mamba models can match Transformers at larger
training budgets. Our results show that while pure SSMs match or exceed
Transformers on many tasks, they lag behind Transformers on tasks which require
strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)
or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid
exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points
on average) and is predicted to be up to 8x faster when generating tokens at
inference time. To validate long-context capabilities, we provide additional
experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended
to support 16K, 32K, and 128K sequences. On an additional 23 long-context
tasks, the hybrid model continues to closely match or exceed the Transformer on
average. To enable further study, we release the checkpoints as well as the
code used to train our models as part of NVIDIA's Megatron-LM project.

摘要：選擇性狀態空間模型 (SSM)，例如 Mamba，克服了 Transformer 的一些缺點，例如序列長度的二次計算複雜度和來自鍵值快取的大量推論時間記憶體需求。此外，最近的研究表明，SSM 可以匹配或超過 Transformer 的語言建模能力，使其成為有吸引力的替代方案。然而，在受控環境（例如，相同資料）中，迄今為止的研究僅提出了小規模實驗，將 SSM 與 Transformer 進行比較。為了了解這些架構在更大規模上的優缺點，我們對在最多 3.5T 個標記的相同資料集上訓練的 8B 參數 Mamba、Mamba-2 和 Transformer 模型進行直接比較。我們還將這些模型與由 43% Mamba-2、7% 注意力和 50% MLP 層（Mamba-2-Hybrid）組成的混合架構進行比較。使用多樣化的任務集，我們回答了 Mamba 模型是否可以在更大的訓練預算下匹配 Transformer 的問題。我們的結果表明，儘管純 SSM 在許多任務上匹配或超過 Transformer，但它們在需要強大的複製或情境學習能力（例如，5 次 MMLU、電話簿）或長情境推理的任務上落後於 Transformer。相比之下，我們發現 8B Mamba-2-Hybrid 在我們評估的所有 12 個標準任務上都超過了 8B Transformer（平均高出 2.65 分），並且預測在推論時間生成標記時速度最高可達 8 倍。為了驗證長情境能力，我們提供了額外的實驗，評估了 Mamba-2-Hybrid 和 Transformer 的變體，以支援 16K、32K 和 128K 序列。在另外 23 個長情境任務中，混合模型繼續與 Transformer 緊密匹配或超過 Transformer。為了進一步研究，我們釋出了檢查點以及用於訓練我們模型的程式碼，作為 NVIDIA 的 Megatron-LM 項目的其中一部分。

##### **Label-aware Hard Negative Sampling Strategies with Momentum Contrastive Learning for Implicit Hate Speech Detection**
2406.07886v1 by Jaehoon Kim, Seungwan Jin, Sohyun Park, Someen Park, Kyungsik Han

Detecting implicit hate speech that is not directly hateful remains a
challenge. Recent research has attempted to detect implicit hate speech by
applying contrastive learning to pre-trained language models such as BERT and
RoBERTa, but the proposed models still do not have a significant advantage over
cross-entropy loss-based learning. We found that contrastive learning based on
randomly sampled batch data does not encourage the model to learn hard negative
samples. In this work, we propose Label-aware Hard Negative sampling strategies
(LAHN) that encourage the model to learn detailed features from hard negative
samples, instead of naive negative samples in random batch, using
momentum-integrated contrastive learning. LAHN outperforms the existing models
for implicit hate speech detection both in- and cross-datasets. The code is
available at https://github.com/Hanyang-HCC-Lab/LAHN

摘要：偵測非直接仇恨的隱含仇恨言論仍然是一項挑戰。最近的研究嘗試透過將對比學習應用於預先訓練的語言模型，例如 BERT 和 RoBERTa，來偵測隱含仇恨言論，但所提出的模型在基於交叉熵損失的學習上仍沒有顯著優勢。我們發現，基於隨機取樣批次資料的對比學習並不會鼓勵模型學習困難的負面樣本。在這項工作中，我們提出標籤感知困難負面取樣策略 (LAHN)，使用動量整合對比學習，鼓勵模型從困難的負面樣本中學習詳細特徵，而不是隨機批次中的單純負面樣本。LAHN 在隱含仇恨言論偵測上優於現有模型，無論是在資料集內還是跨資料集。程式碼可在 https://github.com/Hanyang-HCC-Lab/LAHN 取得

##### **Designing a Dashboard for Transparency and Control of Conversational AI**
2406.07882v1 by Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Viégas

Conversational LLMs function as black box systems, leaving users guessing
about why they see the output they do. This lack of transparency is potentially
problematic, especially given concerns around bias and truthfulness. To address
this issue, we present an end-to-end prototype-connecting interpretability
techniques with user experience design-that seeks to make chatbots more
transparent. We begin by showing evidence that a prominent open-source LLM has
a "user model": examining the internal state of the system, we can extract data
related to a user's age, gender, educational level, and socioeconomic status.
Next, we describe the design of a dashboard that accompanies the chatbot
interface, displaying this user model in real time. The dashboard can also be
used to control the user model and the system's behavior. Finally, we discuss a
study in which users conversed with the instrumented system. Our results
suggest that users appreciate seeing internal states, which helped them expose
biased behavior and increased their sense of control. Participants also made
valuable suggestions that point to future directions for both design and
machine learning research. The project page and video demo of our TalkTuner
system are available at https://bit.ly/talktuner-project-page

摘要：對話式 LLM 的運作方式猶如黑盒子系統，讓使用者猜測他們為何會看到這樣的輸出。這種不透明的狀況潛藏問題，特別是在考量到偏誤和真實性相關疑慮時。為了解決這個問題，我們提出一個端對端的原型，將可解釋性技術與使用者體驗設計結合，試圖讓聊天機器人更透明。我們首先展示證據，證明一個著名的開源 LLM 有一個「使用者模型」：透過檢查系統的內部狀態，我們可以萃取與使用者的年齡、性別、教育程度和社會經濟地位相關的資料。接下來，我們說明一個與聊天機器人介面搭配的儀表板設計，即時顯示這個使用者模型。這個儀表板也可以用來控制使用者模型和系統的行為。最後，我們探討一項研究，其中使用者與裝配儀器的系統對話。我們的結果顯示，使用者樂見看到內部狀態，這有助於他們揭露有偏見的行為，並增加他們的控制感。參與者也提出寶貴的建議，指出設計和機器學習研究的未來方向。我們的 TalkTuner 系統專案頁面和影片示範可於 https://bit.ly/talktuner-project-page 取得

##### **KernelWarehouse: Rethinking the Design of Dynamic Convolution**
2406.07879v1 by Chao Li, Anbang Yao

Dynamic convolution learns a linear mixture of n static kernels weighted with
their input-dependent attentions, demonstrating superior performance than
normal convolution. However, it increases the number of convolutional
parameters by n times, and thus is not parameter efficient. This leads to no
research progress that can allow researchers to explore the setting n>100 (an
order of magnitude larger than the typical setting n<10) for pushing forward
the performance boundary of dynamic convolution while enjoying parameter
efficiency. To fill this gap, in this paper, we propose KernelWarehouse, a more
general form of dynamic convolution, which redefines the basic concepts of
``kernels", ``assembling kernels" and ``attention function" through the lens of
exploiting convolutional parameter dependencies within the same layer and
across neighboring layers of a ConvNet. We testify the effectiveness of
KernelWarehouse on ImageNet and MS-COCO datasets using various ConvNet
architectures. Intriguingly, KernelWarehouse is also applicable to Vision
Transformers, and it can even reduce the model size of a backbone while
improving the model accuracy. For instance, KernelWarehouse (n=4) achieves
5.61%|3.90%|4.38% absolute top-1 accuracy gain on the
ResNet18|MobileNetV2|DeiT-Tiny backbone, and KernelWarehouse (n=1/4) with
65.10% model size reduction still achieves 2.29% gain on the ResNet18 backbone.
The code and models are available at https://github.com/OSVAI/KernelWarehouse.

摘要：動態卷積學習 n 個靜態核的線性混合，其加權方式取決於輸入相關的注意力，展示出比一般卷積更優異的效能。然而，它將卷積參數的數量增加了 n 倍，因此參數效率不佳。這導致沒有研究進展可以讓研究人員探索 n>100（比典型設定 n<10 大一個數量級）的設定，以在享受參數效率的同時，推進動態卷積的效能界線。為了填補這個缺口，我們在本文中提出 KernelWarehouse，這是一種更通用的動態卷積形式，它透過在 ConvNet 的同一層和相鄰層內利用卷積參數相依性，重新定義「核」、「組裝核」和「注意力函數」的基本概念。我們在 ImageNet 和 MS-COCO 資料集上使用各種 ConvNet 架構，證明了 KernelWarehouse 的有效性。有趣的是，KernelWarehouse 也適用於視覺轉換器，它甚至可以在提升模型精確度的同時，縮小主幹模型的大小。例如，KernelWarehouse (n=4) 在 ResNet18|MobileNetV2|DeiT-Tiny 主幹上分別獲得 5.61%|3.90%|4.38% 的絕對頂級 1 準確率提升，而 KernelWarehouse (n=1/4) 在模型大小減少 65.10% 的情況下，仍獲得 ResNet18 主幹上 2.29% 的提升。程式碼和模型可於 https://github.com/OSVAI/KernelWarehouse 取得。

##### **Hierarchical Reinforcement Learning for Swarm Confrontation with High Uncertainty**
2406.07877v1 by Qizhen Wu, Kexin Liu, Lei Chen, Jinhu Lü

In swarm robotics, confrontation including the pursuit-evasion game is a key
scenario. High uncertainty caused by unknown opponents' strategies and dynamic
obstacles complicates the action space into a hybrid decision process. Although
the deep reinforcement learning method is significant for swarm confrontation
since it can handle various sizes, as an end-to-end implementation, it cannot
deal with the hybrid process. Here, we propose a novel hierarchical
reinforcement learning approach consisting of a target allocation layer, a path
planning layer, and the underlying dynamic interaction mechanism between the
two layers, which indicates the quantified uncertainty. It decouples the hybrid
process into discrete allocation and continuous planning layers, with a
probabilistic ensemble model to quantify the uncertainty and regulate the
interaction frequency adaptively. Furthermore, to overcome the unstable
training process introduced by the two layers, we design an integration
training method including pre-training and cross-training, which enhances the
training efficiency and stability. Experiment results in both comparison and
ablation studies validate the effectiveness and generalization performance of
our proposed approach.

摘要：在群體機器人學中，對抗包括追逐迴避遊戲是一個關鍵情境。由未知對手策略和動態障礙物所造成的極大不確定性，使動作空間複雜化為混合決策程序。雖然深度強化學習方法對於群體對抗很重要，因為它可以處理各種規模，但作為端到端實作，它無法處理混合程序。在這裡，我們提出一個新的分層強化學習方法，包括目標分配層、路徑規劃層，以及兩層之間的基礎動態互動機制，這表示量化的不確定性。它將混合程序解耦為離散分配和連續規劃層，並使用機率整體模型來量化不確定性，並適應性地調整互動頻率。此外，為了克服兩層引入的不穩定訓練程序，我們設計了一個整合訓練方法，包括預訓練和交叉訓練，這能增強訓練效率和穩定性。在比較和消融研究中的實驗結果驗證了我們所提出的方法的有效性和泛化效能。

##### **Small Scale Data-Free Knowledge Distillation**
2406.07876v1 by He Liu, Yikai Wang, Huaping Liu, Fuchun Sun, Anbang Yao

Data-free knowledge distillation is able to utilize the knowledge learned by
a large teacher network to augment the training of a smaller student network
without accessing the original training data, avoiding privacy, security, and
proprietary risks in real applications. In this line of research, existing
methods typically follow an inversion-and-distillation paradigm in which a
generative adversarial network on-the-fly trained with the guidance of the
pre-trained teacher network is used to synthesize a large-scale sample set for
knowledge distillation. In this paper, we reexamine this common data-free
knowledge distillation paradigm, showing that there is considerable room to
improve the overall training efficiency through a lens of ``small-scale
inverted data for knowledge distillation". In light of three empirical
observations indicating the importance of how to balance class distributions in
terms of synthetic sample diversity and difficulty during both data inversion
and distillation processes, we propose Small Scale Data-free Knowledge
Distillation SSD-KD. In formulation, SSD-KD introduces a modulating function to
balance synthetic samples and a priority sampling function to select proper
samples, facilitated by a dynamic replay buffer and a reinforcement learning
strategy. As a result, SSD-KD can perform distillation training conditioned on
an extremely small scale of synthetic samples (e.g., 10X less than the original
training data scale), making the overall training efficiency one or two orders
of magnitude faster than many mainstream methods while retaining superior or
competitive model performance, as demonstrated on popular image classification
and semantic segmentation benchmarks. The code is available at
https://github.com/OSVAI/SSD-KD.

摘要：無資料知識萃取能夠利用大型教師網路學習到的知識，在不存取原始訓練資料的情況下擴充較小型學生網路的訓練，避免實際應用中的隱私、安全和專有風險。在這個研究領域中，現有方法通常遵循反演和萃取範例，其中生成式對抗網路在預訓練教師網路的指導下進行即時訓練，用於合成大規模樣本集以進行知識萃取。在本文中，我們重新審視這個常見的無資料知識萃取範例，顯示透過「用於知識萃取的小規模反轉資料」的觀點，有相當大的空間可以改善整體訓練效率。根據三個經驗觀察結果，指出在資料反演和萃取過程中，平衡類別分佈在合成樣本多樣性和難度方面的重要性，我們提出小規模無資料知識萃取 SSD-KD。在公式化中，SSD-KD 導入一個調節函數來平衡合成樣本，並導入一個優先取樣函數來選擇適當的樣本，並透過動態重播緩衝區和強化學習策略來協助。因此，SSD-KD 可以執行萃取訓練，條件是合成樣本的規模極小（例如，比原始訓練資料規模小 10 倍），使整體訓練效率比許多主流方法快一到兩個數量級，同時保有優異或有競爭力的模型效能，如在流行的影像分類和語意分割基準中所展示的。程式碼可在 https://github.com/OSVAI/SSD-KD 取得。

##### **Carbon Market Simulation with Adaptive Mechanism Design**
2406.07875v1 by Han Wang, Wenhao Li, Hongyuan Zha, Baoxiang Wang

A carbon market is a market-based tool that incentivizes economic agents to
align individual profits with the global utility, i.e., reducing carbon
emissions to tackle climate change.
  \textit{Cap and trade} stands as a critical principle based on allocating and
trading carbon allowances (carbon emission credit), enabling economic agents to
follow planned emissions and penalizing excess emissions.
  A central authority is responsible for introducing and allocating those
allowances in cap and trade.
  However, the complexity of carbon market dynamics makes accurate simulation
intractable, which in turn hinders the design of effective allocation
strategies.
  To address this, we propose an adaptive mechanism design framework,
simulating the market using hierarchical, model-free multi-agent reinforcement
learning (MARL).
  Government agents allocate carbon credits, while enterprises engage in
economic activities and carbon trading.
  This framework illustrates agents' behavior comprehensively.
  Numerical results show MARL enables government agents to balance
productivity, equality, and carbon emissions.
  Our project is available at
\url{https://github.com/xwanghan/Carbon-Simulator}.

摘要：碳市場是一種基於市場的工具，用於激勵經濟主體將個人利潤與全球效用保持一致，即減少碳排放以應對氣候變化。
\textit{限額交易}作為一項關鍵原則，基於分配和交易碳配額（碳排放信用額），使經濟主體能夠遵循計劃的排放並對超額排放進行懲罰。
中央機構負責在限額交易中引入和分配這些配額。
然而，碳市場動態的複雜性使得精確的模擬難以實現，這反過來又阻礙了有效分配策略的設計。
為了解決這個問題，我們提出了一個自適應機制設計框架，使用分層的、無模型的多智能體強化學習 (MARL) 來模擬市場。
政府代理分配碳信用額，而企業從事經濟活動和碳交易。
這個框架全面說明了代理的行為。
數值結果表明，MARL 使政府代理能夠平衡生產力、平等和碳排放。
我們的項目可在
\url{https://github.com/xwanghan/Carbon-Simulator} 中找到。

##### **Unveiling the Power of Wavelets: A Wavelet-based Kolmogorov-Arnold Network for Hyperspectral Image Classification**
2406.07869v1 by Seyd Teymoor Seydi

Hyperspectral image classification is a crucial but challenging task due to
the high dimensionality and complex spatial-spectral correlations inherent in
hyperspectral data. This paper employs Wavelet-based Kolmogorov-Arnold Network
(wav-kan) architecture tailored for efficient modeling of these intricate
dependencies. Inspired by the Kolmogorov-Arnold representation theorem, Wav-KAN
incorporates wavelet functions as learnable activation functions, enabling
non-linear mapping of the input spectral signatures. The wavelet-based
activation allows Wav-KAN to effectively capture multi-scale spatial and
spectral patterns through dilations and translations. Experimental evaluation
on three benchmark hyperspectral datasets (Salinas, Pavia, Indian Pines)
demonstrates the superior performance of Wav-KAN compared to traditional
multilayer perceptrons (MLPs) and the recently proposed Spline-based KAN
(Spline-KAN) model. In this work we are: (1) conducting more experiments on
additional hyperspectral datasets (Pavia University, WHU-Hi, and Urban
Hyperspectral Image) to further validate the generalizability of Wav-KAN; (2)
developing a multiresolution Wav-KAN architecture to capture scale-invariant
features; (3) analyzing the effect of dimensional reduction techniques on
classification performance; (4) exploring optimization methods for tuning the
hyperparameters of KAN models; and (5) comparing Wav-KAN with other
state-of-the-art models in hyperspectral image classification.

摘要：高光譜影像分類是一項關鍵但具挑戰性的任務，這是由於高光譜資料中固有的高維度和複雜的空間光譜相關性。本文採用針對這些複雜依賴項的有效建模量身打造的基於小波的柯爾莫哥洛夫-阿諾德網路 (wav-kan) 架構。在柯爾莫哥洛夫-阿諾德表示定理的啟發下，Wav-KAN 將小波函數整合為可學習的活化函數，從而實現輸入光譜特徵的非線性對應。基於小波的活化允許 Wav-KAN 透過膨脹和轉換有效擷取多尺度的空間和光譜模式。在三個基準高光譜資料集 (Salinas、Pavia、Indian Pines) 上的實驗評估證明了 Wav-KAN 比傳統的多層感知器 (MLP) 和最近提出的基於樣條的 KAN (Spline-KAN) 模型具有更優異的效能。在這項工作中，我們：(1) 對其他高光譜資料集 (Pavia University、WHU-Hi 和 Urban Hyperspectral Image) 進行更多實驗，以進一步驗證 Wav-KAN 的概括性；(2) 開發一個多解析度 Wav-KAN 架構來擷取尺度不變的特徵；(3) 分析維度縮減技術對分類效能的影響；(4) 探討用於調整 KAN 模型超參數的最佳化方法；以及 (5) 將 Wav-KAN 與其他高光譜影像分類中的最新模型進行比較。

##### **Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation**
2406.07867v1 by Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, Yong Man Ro

In this paper, we introduce a novel Face-to-Face spoken dialogue model. It
processes audio-visual speech from user input and generates audio-visual speech
as the response, marking the initial step towards creating an avatar chatbot
system without relying on intermediate text. To this end, we newly introduce
MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken
dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded
based on the open domain dialogue dataset, TopicalChat. The MultiDialog
contains parallel audio-visual recordings of conversation partners acting
according to the given script with emotion annotations, which we expect to open
up research opportunities in multimodal synthesis. Our Face-to-Face spoken
dialogue model incorporates a textually pretrained large language model and
adapts it into the audio-visual spoken dialogue domain by incorporating
speech-text joint pretraining. Through extensive experiments, we validate the
effectiveness of our model in facilitating a face-to-face conversation. Demo
and data are available at https://multidialog.github.io and
https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.

摘要：在本文中，我們介紹了一種新穎的面对面口語對話模型。它處理來自使用者輸入的視訊語音，並產生視訊語音作為回應，標誌著在不依賴中間文字的情況下建立頭像聊天機器人系統的初步步驟。為此，我們新引入了 MultiDialog，這是第一個包含約 9,000 個對話、340 小時的對話語料庫，並根據開放域對話資料集 TopicalChat 進行記錄的大規模多模態（即音訊和視覺）口語對話語料庫。MultiDialog 包含對話夥伴根據給定腳本表演的對話音訊視訊並行錄音，以及我們預期將開啟多模態合成研究機會的情緒註解。我們的面對面口語對話模型結合了文字預訓練的大語言模型，並透過結合語音文字聯合預訓練，將其適應到視訊語音對話領域。透過廣泛的實驗，我們驗證了我們的模型在促進面對面對話中的有效性。示範和資料分別可以在 https://multidialog.github.io 和 https://huggingface.co/datasets/IVLLab/MultiDialog 取得。

##### **Self-Distillation Learning Based on Temporal-Spatial Consistency for Spiking Neural Networks**
2406.07862v1 by Lin Zuo, Yongqi Ding, Mengmeng Jing, Kunshan Yang, Yunqian Yu

Spiking neural networks (SNNs) have attracted considerable attention for
their event-driven, low-power characteristics and high biological
interpretability. Inspired by knowledge distillation (KD), recent research has
improved the performance of the SNN model with a pre-trained teacher model.
However, additional teacher models require significant computational resources,
and it is tedious to manually define the appropriate teacher network
architecture. In this paper, we explore cost-effective self-distillation
learning of SNNs to circumvent these concerns. Without an explicit defined
teacher, the SNN generates pseudo-labels and learns consistency during
training. On the one hand, we extend the timestep of the SNN during training to
create an implicit temporal ``teacher" that guides the learning of the original
``student", i.e., the temporal self-distillation. On the other hand, we guide
the output of the weak classifier at the intermediate stage by the final output
of the SNN, i.e., the spatial self-distillation. Our temporal-spatial
self-distillation (TSSD) learning method does not introduce any inference
overhead and has excellent generalization ability. Extensive experiments on the
static image datasets CIFAR10/100 and ImageNet as well as the neuromorphic
datasets CIFAR10-DVS and DVS-Gesture validate the superior performance of the
TSSD method. This paper presents a novel manner of fusing SNNs with KD,
providing insights into high-performance SNN learning methods.

摘要：尖峰神经網路 (SNN) 因其事件驅動、低功耗特性和高生物可解釋性而備受關注。受到知識萃取 (KD) 的啟發，最近的研究已透過預先訓練好的教師模型改善 SNN 模型的效能。然而，額外的教師模型需要大量的運算資源，而且手動定義適當的教師網路架構很繁瑣。在本文中，我們探討 SNN 的具成本效益的自萃取學習，以解決這些問題。在沒有明確定義的教師情況下，SNN 會產生偽標籤並在訓練期間學習一致性。一方面，我們在訓練期間延伸 SNN 的時間步長，以建立一個隱含的時間「教師」，引導原始「學生」的學習，即時間自萃取。另一方面，我們透過 SNN 的最終輸出引導中間階段的弱分類器輸出，即空間自萃取。我們的時間空間自萃取 (TSSD) 學習方法不會引入任何推論負擔，且具備極佳的泛化能力。在靜態影像資料集 CIFAR10/100 和 ImageNet，以及神經形態資料集 CIFAR10-DVS 和 DVS-Gesture 上進行的廣泛實驗驗證了 TSSD 方法的優異效能。本文提出了一種將 SNN 與 KD 融合的新穎方式，提供對高性能 SNN 學習方法的見解。

##### **BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain**
2406.07860v1 by Rahul Kumar, Amar Raja Dibbu, Shrutendra Harsola, Vignesh Subrahmaniam, Ashutosh Modi

Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural
language interfaces to databases have recently been proposed. These datasets
cover a wide breadth of domains but fall short on some essential domains, such
as finance and accounting. Given that accounting databases are used worldwide,
particularly by non-technical people, there is an imminent need to develop
models that could help extract information from accounting databases via
natural language queries. In this resource paper, we aim to fill this gap by
proposing a new large-scale Text-to-SQL dataset for the accounting and
financial domain: BookSQL. The dataset consists of 100k natural language
queries-SQL pairs, and accounting databases of 1 million records. We experiment
with and analyze existing state-of-the-art models (including GPT-4) for the
Text-to-SQL task on BookSQL. We find significant performance gaps, thus
pointing towards developing more focused models for this domain.

摘要：最近提出了幾個用於開發自然語言資料庫介面的大型資料集（例如 WikiSQL、Spider）。這些資料集涵蓋了廣泛的領域，但在某些基本領域（例如財務和會計）上有所不足。鑑於會計資料庫在全球範圍內使用，特別是非技術人員，因此迫切需要開發模型，以通過自然語言查詢從會計資料庫中提取資訊。在本文中，我們旨在通過提出一個新的會計和財務領域的大型 Text-to-SQL 資料集：BookSQL 來填補這一空白。該資料集包含 100k 自然語言查詢-SQL 對，以及 100 萬條記錄的會計資料庫。我們對 Text-to-SQL 任務的 BookSQL 進行了現有最先進模型（包括 GPT-4）的實驗和分析。我們發現了顯著的效能差距，因此指出需要為此領域開發更專注的模型。

##### **VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment**
2406.07855v1 by Bing Han, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Yanming Qian, Yanqing Liu, Sheng Zhao, Jinyu Li, Furu Wei

With the help of discrete neural audio codecs, large language models (LLM)
have increasingly been recognized as a promising methodology for zero-shot
Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies
bring astonishing diversity to generation, but also pose robustness issues such
as typos, omissions and repetition. In addition, the high sampling rate of
audio also brings huge computational overhead to the inference process of
autoregression. To address these issues, we propose VALL-E R, a robust and
efficient zero-shot TTS system, building upon the foundation of VALL-E.
Specifically, we introduce a phoneme monotonic alignment strategy to strengthen
the connection between phonemes and acoustic sequence, ensuring a more precise
alignment by constraining the acoustic tokens to match their associated
phonemes. Furthermore, we employ a codec-merging approach to downsample the
discrete codes in shallow quantization layer, thereby accelerating the decoding
speed while preserving the high quality of speech output. Benefiting from these
strategies, VALL-E R obtains controllablity over phonemes and demonstrates its
strong robustness by approaching the WER of ground truth. In addition, it
requires fewer autoregressive steps, with over 60% time reduction during
inference. This research has the potential to be applied to meaningful
projects, including the creation of speech for those affected by aphasia. Audio
samples will be available at: https://aka.ms/valler.

摘要：<paragraph>在離散神經音訊編解碼器的幫助下，大型語言模型 (LLM) 已逐漸被視為零次學習文字轉語音 (TTS) 合成的有前途的方法。然而，基於取樣的解碼策略為生成帶來了驚人的多樣性，但也帶來了魯棒性問題，例如錯字、遺漏和重複。此外，音訊的高取樣率也為自迴歸的推論過程帶來了巨大的運算負擔。為了解決這些問題，我們提出了 VALL-E R，這是一個穩健且高效的零次學習 TTS 系統，建立在 VALL-E 的基礎上。具體來說，我們引入了一個音素單調對齊策略來加強音素和音訊序列之間的連接，通過約束音訊標記與其關聯的音素相匹配來確保更精確的對齊。此外，我們採用編解碼器合併方法來對淺量化層中的離散碼進行降採樣，從而加速解碼速度，同時保持高品質的語音輸出。受益於這些策略，VALL-E R 獲得了對音素的可控性，並通過接近真實的 WER 來證明其強大的魯棒性。此外，它需要的自迴歸步驟更少，在推論過程中時間減少了 60% 以上。這項研究有可能應用於有意義的專案，包括為失語症患者建立語音。音訊範例將可在 https://aka.ms/valler 取得。</paragraph>

##### **Dynamic Stochastic Decoding Strategy for Open-Domain Dialogue Generation**
2406.07850v1 by Yiwei Li, Fei Mi, Yitong Li, Yasheng Wang, Bin Sun, Shaoxiong Feng, Kan Li

Stochastic sampling strategies such as top-k and top-p have been widely used
in dialogue generation task. However, as an open-domain chatting system, there
will be two different conversation scenarios, i.e. chit-chat and
knowledge-based question answering. In the former situation, responses
diversity is essential due to the one-to-many nature in dialogue. The latter,
on the other hand, requires less randomness given that stochastic decoding
strategy entails the risk of generating incorrect information. As a result, an
adaptive and flexible decoding strategy is needed to cope with these two
scenarios simultaneously. To this end, we propose the dynamic decoding strategy
(DDS), which can adjust the decoding space w.r.t. different contexts. In DDS,
both sequence-level and token-level adaptive search can be achieved to adjust
the decoding process in a unified framework. Besides, our adaptive algorithm
can not only be used during model inference, but it can also be applied during
the model training stage to further enhance the performance. Comprehensive
experiments indicate that the proposed decoding strategy can consistently
improve the performance of pre-trained dialogue models when coupled with four
well-used stochastic decoding algorithms.

摘要：隨機取樣策略，例如 top-k 和 top-p，已廣泛用於對話生成任務中。然而，作為一個開放域聊天系統，將會有兩種不同的對話情境，即閒聊和基於知識的問答。在前一種情況下，回應的多樣性至關重要，因為對話中的一對多性質。後者另一方面，由於隨機解碼策略會帶來產生錯誤資訊的風險，因此需要較少的隨機性。因此，需要一個適應性和靈活性解碼策略來同時應對這兩種情境。為此，我們提出了動態解碼策略 (DDS)，它可以根據不同的上下文調整解碼空間。在 DDS 中，可以在一個統一的框架中實現序列級別和令牌級別的自適應搜索，以調整解碼過程。此外，我們的自適應算法不僅可以在模型推理期間使用，還可以在模型訓練階段應用，以進一步增強性能。綜合實驗表明，所提出的解碼策略在與四種常用的隨機解碼算法結合使用時，可以持續改善預訓練對話模型的性能。

