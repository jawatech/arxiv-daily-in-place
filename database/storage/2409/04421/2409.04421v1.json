{"2409.04421": {"publish_time": "2024-09-06", "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs", "paper_summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.", "paper_summary_zh": "\u7531 LLM \u9a45\u52d5\u7684\u500b\u4eba\u5316\u4ee3\u7406\u7cfb\u7d71\u63a1\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u6839\u64da\u4f7f\u7528\u8005\u7684\u904e\u5f80\u6d3b\u52d5\u9810\u6e2c\u5176\u884c\u70ba\u3002\u7136\u800c\uff0c\u5176\u6709\u6548\u6027\u901a\u5e38\u53d6\u6c7a\u65bc\u6709\u6548\u5229\u7528\u5ee3\u6cdb\u3001\u9577\u671f\u7684\u4f7f\u7528\u8005\u6b77\u53f2\u6578\u64da\u7684\u80fd\u529b\uff0c\u56e0\u70ba\u9019\u4e9b\u6578\u64da\u5177\u6709\u5167\u5728\u7684\u96dc\u8a0a\u548c\u9577\u5ea6\u3002\u73fe\u6709\u7684\u9810\u8a13\u7df4 LLM \u53ef\u80fd\u6703\u7522\u751f\u7c21\u6f54\u4f46\u7f3a\u4e4f\u4e0b\u6e38\u4efb\u52d9\u5fc5\u8981\u80cc\u666f\u7684\u6458\u8981\uff0c\u963b\u7919\u5176\u5728\u500b\u4eba\u5316\u7cfb\u7d71\u4e2d\u7684\u6548\u7528\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u9810\u6e2c\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLPF)\u3002RLPF \u5fae\u8abf LLM \u4ee5\u7522\u751f\u7c21\u6f54\u3001\u4eba\u985e\u53ef\u8b80\u7684\u4f7f\u7528\u8005\u6458\u8981\uff0c\u9019\u4e9b\u6458\u8981\u91dd\u5c0d\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u9032\u884c\u4e86\u6700\u4f73\u5316\u3002\u900f\u904e\u6700\u5927\u5316\u6240\u7522\u751f\u6458\u8981\u7684\u6548\u7528\uff0cRLPF \u6709\u6548\u5730\u63d0\u7149\u4e86\u5ee3\u6cdb\u7684\u4f7f\u7528\u8005\u6b77\u53f2\u6578\u64da\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u4e0b\u6e38\u4efb\u52d9\u7684\u5fc5\u8981\u8cc7\u8a0a\u3002\u6211\u5011\u7684\u7d93\u9a57\u8a55\u4f30\u8b49\u660e\uff0c\u5916\u90e8\u4e0b\u6e38\u4efb\u52d9\u6548\u7528\u548c\u5167\u5728\u6458\u8981\u54c1\u8cea\u90fd\u6709\u986f\u8457\u7684\u6539\u5584\uff0c\u5728 Factuality\u3001Abstractiveness \u548c Readability \u65b9\u9762\uff0c\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u8d85\u8d8a\u57fa\u7dda\u65b9\u6cd5\u9054 22%\uff0c\u7372\u52dd\u7387\u9054 84.59%\u3002RLPF \u9084\u5c07\u80cc\u666f\u9577\u5ea6\u6e1b\u5c11\u4e86 74%\uff0c\u540c\u6642\u6539\u5584\u4e86 19 \u500b\u672a\u898b\u4efb\u52d9\u548c/\u6216\u8cc7\u6599\u96c6\u4e2d\u7684 16 \u500b\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u6027\u3002\u6b64\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u900f\u904e\u6709\u6548\u5730\u5c07\u5197\u9577\u3001\u96dc\u8a0a\u7684\u4f7f\u7528\u8005\u6b77\u53f2\u8f49\u63db\u70ba\u8cc7\u8a0a\u8c50\u5bcc\u4e14\u4eba\u985e\u53ef\u8b80\u7684\u8868\u793a\uff0c\u4f86\u589e\u5f37 LLM \u500b\u4eba\u5316\u3002", "author": "Jiaxing Wu et.al.", "authors": "Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie", "id": "2409.04421v1", "paper_url": "http://arxiv.org/abs/2409.04421v1", "repo": "null"}}