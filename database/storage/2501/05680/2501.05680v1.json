{"2501.05680": {"publish_time": "2025-01-10", "title": "EXION: Exploiting Inter- and Intra-Iteration Output Sparsity for Diffusion Models", "paper_summary": "Over the past few years, diffusion models have emerged as novel AI solutions,\ngenerating diverse multi-modal outputs from text prompts. Despite their\ncapabilities, they face challenges in computing, such as excessive latency and\nenergy consumption due to their iterative architecture. Although prior works\nspecialized in transformer acceleration can be applied, the iterative nature of\ndiffusion models remains unresolved. In this paper, we present EXION, the first\nSW-HW co-designed diffusion accelerator that solves the computation challenges\nby exploiting the unique inter- and intra-iteration output sparsity in\ndiffusion models. To this end, we propose two SW-level optimizations. First, we\nintroduce the FFN-Reuse algorithm that identifies and skips redundant\ncomputations in FFN layers across different iterations (inter-iteration\nsparsity). Second, we use a modified eager prediction method that employs\ntwo-step leading-one detection to accurately predict the attention score,\nskipping unnecessary computations within an iteration (intra-iteration\nsparsity). We also introduce a novel data compaction mechanism named ConMerge,\nwhich can enhance HW utilization by condensing and merging sparse matrices into\ncompact forms. Finally, it has a dedicated HW architecture that supports the\nabove sparsity-inducing algorithms, translating high output sparsity into\nimproved energy efficiency and performance. To verify the feasibility of the\nEXION, we first demonstrate that it has no impact on accuracy in various types\nof multi-modal diffusion models. We then instantiate EXION in both server- and\nedge-level settings and compare its performance against GPUs with similar\nspecifications. Our evaluation shows that EXION achieves dramatic improvements\nin performance and energy efficiency by 3.2-379.3x and 45.1-3067.6x compared to\na server GPU and by 42.6-1090.9x and 196.9-4668.2x compared to an edge GPU.", "paper_summary_zh": "\u5728\u904e\u53bb\u5e7e\u5e74\uff0c\u64f4\u6563\u6a21\u578b\u5df2\u6210\u70ba\u65b0\u7a4e\u7684\u4eba\u5de5\u667a\u6167\u89e3\u6c7a\u65b9\u6848\uff0c\u5f9e\u6587\u5b57\u63d0\u793a\u4e2d\u7522\u751f\u591a\u6a23\u5316\u7684\u591a\u6a21\u614b\u8f38\u51fa\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u529f\u80fd\uff0c\u4f46\u7531\u65bc\u5176\u53cd\u8986\u904b\u7b97\u67b6\u69cb\uff0c\u5b83\u5011\u5728\u904b\u7b97\u4e0a\u4ecd\u9762\u81e8\u6311\u6230\uff0c\u4f8b\u5982\u904e\u9577\u7684\u5ef6\u9072\u548c\u80fd\u6e90\u6d88\u8017\u3002\u5118\u7ba1\u53ef\u4ee5\u61c9\u7528\u5148\u524d\u5c08\u7cbe\u65bcTransformer\u52a0\u901f\u7684\u5de5\u4f5c\uff0c\u4f46\u64f4\u6563\u6a21\u578b\u7684\u53cd\u8986\u904b\u7b97\u6027\u8cea\u4ecd\u672a\u89e3\u6c7a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa EXION\uff0c\u9019\u662f\u7b2c\u4e00\u500b SW-HW \u5171\u540c\u8a2d\u8a08\u7684\u64f4\u6563\u52a0\u901f\u5668\uff0c\u5b83\u900f\u904e\u5229\u7528\u64f4\u6563\u6a21\u578b\u4e2d\u7368\u7279\u7684\u8fed\u4ee3\u9593\u548c\u8fed\u4ee3\u5167\u8f38\u51fa\u7a00\u758f\u6027\u4f86\u89e3\u6c7a\u904b\u7b97\u6311\u6230\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u5169\u7a2e SW \u5c64\u7d1a\u6700\u4f73\u5316\u3002\u9996\u5148\uff0c\u6211\u5011\u5f15\u5165 FFN-Reuse \u6f14\u7b97\u6cd5\uff0c\u5b83\u8b58\u5225\u4e26\u7565\u904e\u4e0d\u540c\u8fed\u4ee3\u4e2d FFN \u5c64\u4e2d\u7684\u91cd\u8907\u904b\u7b97\uff08\u8fed\u4ee3\u9593\u7a00\u758f\u6027\uff09\u3002\u5176\u6b21\uff0c\u6211\u5011\u4f7f\u7528\u4e00\u7a2e\u4fee\u6539\u7684\u71b1\u5207\u9810\u6e2c\u65b9\u6cd5\uff0c\u5b83\u63a1\u7528\u5169\u6b65\u9818\u5148\u4e00\u5075\u6e2c\u4f86\u6e96\u78ba\u9810\u6e2c\u6ce8\u610f\u529b\u5206\u6578\uff0c\u7565\u904e\u8fed\u4ee3\u4e2d\u4e0d\u5fc5\u8981\u7684\u904b\u7b97\uff08\u8fed\u4ee3\u5167\u7a00\u758f\u6027\uff09\u3002\u6211\u5011\u9084\u5f15\u5165\u4e00\u7a2e\u540d\u70ba ConMerge \u7684\u65b0\u7a4e\u8cc7\u6599\u58d3\u7e2e\u6a5f\u5236\uff0c\u5b83\u53ef\u4ee5\u900f\u904e\u5c07\u7a00\u758f\u77e9\u9663\u58d3\u7e2e\u4e26\u5408\u4f75\u6210\u7dca\u6e4a\u5f62\u5f0f\u4f86\u63d0\u9ad8 HW \u5229\u7528\u7387\u3002\u6700\u5f8c\uff0c\u5b83\u6709\u4e00\u500b\u5c08\u7528\u7684 HW \u67b6\u69cb\uff0c\u652f\u63f4\u4e0a\u8ff0\u7a00\u758f\u6027\u8a98\u5c0e\u6f14\u7b97\u6cd5\uff0c\u5c07\u9ad8\u8f38\u51fa\u7a00\u758f\u6027\u8f49\u5316\u70ba\u66f4\u9ad8\u7684\u80fd\u6e90\u6548\u7387\u548c\u6548\u80fd\u3002\u70ba\u4e86\u9a57\u8b49 EXION \u7684\u53ef\u884c\u6027\uff0c\u6211\u5011\u9996\u5148\u8b49\u660e\u5b83\u5c0d\u5404\u7a2e\u985e\u578b\u591a\u6a21\u614b\u64f4\u6563\u6a21\u578b\u7684\u6e96\u78ba\u6027\u6c92\u6709\u5f71\u97ff\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5728\u4f3a\u670d\u5668\u548c\u908a\u7de3\u5c64\u7d1a\u8a2d\u5b9a\u4e2d\u5be6\u4f8b\u5316 EXION\uff0c\u4e26\u5c07\u5176\u6548\u80fd\u8207\u5177\u6709\u985e\u4f3c\u898f\u683c\u7684 GPU \u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u7684\u8a55\u4f30\u986f\u793a\uff0c\u8207\u4f3a\u670d\u5668 GPU \u76f8\u6bd4\uff0cEXION \u5728\u6548\u80fd\u548c\u80fd\u6e90\u6548\u7387\u65b9\u9762\u53d6\u5f97\u986f\u8457\u9032\u6b65\uff0c\u5206\u5225\u63d0\u9ad8\u4e86 3.2-379.3 \u500d\u548c 45.1-3067.6 \u500d\uff1b\u8207\u908a\u7de3 GPU \u76f8\u6bd4\uff0c\u5247\u5206\u5225\u63d0\u9ad8\u4e86 42.6-1090.9 \u500d\u548c 196.9-4668.2 \u500d\u3002", "author": "Jaehoon Heo et.al.", "authors": "Jaehoon Heo, Adiwena Putra, Jieon Yoon, Sungwoong Yune, Hangyeol Lee, Ji-Hoon Kim, Joo-Young Kim", "id": "2501.05680v1", "paper_url": "http://arxiv.org/abs/2501.05680v1", "repo": "null"}}