{"2410.19694": {"publish_time": "2024-10-25", "title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs", "paper_summary": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency.", "paper_summary_zh": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u4e3a\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u8c03\u6574\u81f3\u4e0b\u6e38\u4efb\u52a1\u7684\u5173\u952e\u6280\u672f\u3002\u7136\u800c\uff0cLLM \u7684\u5e9e\u5927\u89c4\u6a21\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u9700\u6c42\u65b9\u9762\u5e26\u6765\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u4f4e\u79e9\u81ea\u9002\u5e94 (LoRA) \u5df2\u6210\u4e3a\u4e00\u9879\u9887\u5177\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u4f4e\u79e9\u81ea\u9002\u5e94\u7684\u5b9e\u9645\u6027\u80fd\u4e0e\u5176\u7406\u8bba\u6700\u4f18\u503c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 XGBLoRA\uff08eXtreme Gradient Boosting LoRA\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u96c6\u6210\u5b66\u4e60\u7684\u529b\u91cf\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002\u53d7\u68af\u5ea6\u63d0\u5347\u7684\u542f\u53d1\uff0cXGBLoRA \u8fed\u4ee3\u5b66\u4e60\u5e76\u5408\u5e76\u4e00\u7cfb\u5217 LoRA \u81ea\u9002\u5e94\u6765\u4f18\u5316\u6a21\u578b\u9884\u6d4b\u3002\u5b83\u6bd4\u6807\u51c6 LoRA \u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4eab\u53d7 1 \u7ea7\u81ea\u9002\u5e94\u7684\u8ba1\u7b97\u6548\u7387\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6765\u5c55\u793a\u6211\u4eec\u65b9\u6cd5\u7684\u6536\u655b\u6027\u548c\u6700\u4f18\u6027\uff0c\u5e76\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cXGBLoRA \u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6 LoRA\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53ef\u8bad\u7ec3\u53c2\u6570\u660e\u663e\u66f4\u5c11\u3002\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86 LLM \u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5e76\u4e3a\u5728\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\u7684\u540c\u65f6\u5c06 LLM \u8c03\u6574\u81f3\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "author": "Yifei Zhang et.al.", "authors": "Yifei Zhang, Hao Zhu, Aiwei Liu, Han Yu, Piotr Koniusz, Irwin King", "id": "2410.19694v1", "paper_url": "http://arxiv.org/abs/2410.19694v1", "repo": "null"}}