{"2406.10079": {"publish_time": "2024-06-14", "title": "Localizing Events in Videos with Multimodal Queries", "paper_summary": "Video understanding is a pivotal task in the digital era, yet the dynamic and\nmultievent nature of videos makes them labor-intensive and computationally\ndemanding to process. Thus, localizing a specific event given a semantic query\nhas gained importance in both user-oriented applications like video search and\nacademic research into video foundation models. A significant limitation in\ncurrent research is that semantic queries are typically in natural language\nthat depicts the semantics of the target event. This setting overlooks the\npotential for multimodal semantic queries composed of images and texts. To\naddress this gap, we introduce a new benchmark, ICQ, for localizing events in\nvideos with multimodal queries, along with a new evaluation dataset\nICQ-Highlight. Our new benchmark aims to evaluate how well models can localize\nan event given a multimodal semantic query that consists of a reference image,\nwhich depicts the event, and a refinement text to adjust the images' semantics.\nTo systematically benchmark model performance, we include 4 styles of reference\nimages and 5 types of refinement texts, allowing us to explore model\nperformance across different domains. We propose 3 adaptation methods that\ntailor existing models to our new setting and evaluate 10 SOTA models, ranging\nfrom specialized to large-scale foundation models. We believe this benchmark is\nan initial step toward investigating multimodal queries in video event\nlocalization.", "paper_summary_zh": "\u5f71\u7247\u7406\u89e3\u662f\u6578\u4f4d\u6642\u4ee3\u7684\u95dc\u9375\u4efb\u52d9\uff0c\u4f46\u5f71\u7247\u7684\u52d5\u614b\u548c\u591a\u4e8b\u4ef6\u7279\u6027\u8b93\u5b83\u5011\u7684\u8655\u7406\u904e\u7a0b\u8017\u6642\u4e14\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u3002\u56e0\u6b64\uff0c\u5728\u8a9e\u610f\u67e5\u8a62\u4e2d\u5b9a\u4f4d\u7279\u5b9a\u4e8b\u4ef6\u5df2\u5728\u4ee5\u4f7f\u7528\u8005\u70ba\u5c0e\u5411\u7684\u61c9\u7528\u7a0b\u5f0f\uff08\u4f8b\u5982\u5f71\u7247\u641c\u5c0b\uff09\u548c\u5f71\u7247\u57fa\u790e\u6a21\u578b\u7684\u5b78\u8853\u7814\u7a76\u4e2d\u8b8a\u5f97\u91cd\u8981\u3002\u76ee\u524d\u7684\u91cd\u5927\u7814\u7a76\u9650\u5236\u5728\u65bc\uff0c\u8a9e\u610f\u67e5\u8a62\u901a\u5e38\u4ee5\u81ea\u7136\u8a9e\u8a00\u5448\u73fe\uff0c\u63cf\u8ff0\u76ee\u6a19\u4e8b\u4ef6\u7684\u8a9e\u610f\u3002\u6b64\u8a2d\u5b9a\u5ffd\u7565\u4e86\u7531\u5f71\u50cf\u548c\u6587\u5b57\u7d44\u6210\u4e4b\u591a\u6a21\u614b\u8a9e\u610f\u67e5\u8a62\u7684\u6f5b\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u91dd\u5c0d\u5f71\u7247\u4e2d\u7684\u4e8b\u4ef6\u5b9a\u4f4d\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u6e96 ICQ\uff0c\u4e26\u4f7f\u7528\u65b0\u7684\u8a55\u4f30\u8cc7\u6599\u96c6 ICQ-Highlight\u3002\u6211\u5011\u7684\u57fa\u6e96\u65e8\u5728\u8a55\u4f30\u6a21\u578b\u5728\u7d66\u5b9a\u591a\u6a21\u614b\u8a9e\u610f\u67e5\u8a62\uff08\u5305\u542b\u63cf\u8ff0\u4e8b\u4ef6\u7684\u53c3\u8003\u5f71\u50cf\u548c\u8abf\u6574\u5f71\u50cf\u8a9e\u610f\u7684\u4fee\u6b63\u6587\u5b57\uff09\u6642\uff0c\u5b9a\u4f4d\u4e8b\u4ef6\u7684\u8868\u73fe\u3002\u70ba\u4e86\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u6a21\u578b\u6548\u80fd\uff0c\u6211\u5011\u5305\u542b 4 \u7a2e\u53c3\u8003\u5f71\u50cf\u98a8\u683c\u548c 5 \u7a2e\u4fee\u6b63\u6587\u5b57\u985e\u578b\uff0c\u8b93\u6211\u5011\u80fd\u5920\u63a2\u7d22\u4e0d\u540c\u9818\u57df\u7684\u6a21\u578b\u6548\u80fd\u3002\u6211\u5011\u63d0\u51fa 3 \u7a2e\u8abf\u6574\u65b9\u6cd5\uff0c\u5c07\u73fe\u6709\u7684\u6a21\u578b\u8abf\u6574\u5230\u6211\u5011\u7684\u8a2d\u5b9a\uff0c\u4e26\u8a55\u4f30 10 \u500b SOTA \u6a21\u578b\uff0c\u5f9e\u5c08\u9580\u6a21\u578b\u5230\u5927\u578b\u57fa\u790e\u6a21\u578b\u3002\u6211\u5011\u76f8\u4fe1\u9019\u500b\u57fa\u6e96\u662f\u91dd\u5c0d\u5f71\u7247\u4e8b\u4ef6\u5b9a\u4f4d\u4e2d\u591a\u6a21\u614b\u67e5\u8a62\u7814\u7a76\u7684\u7b2c\u4e00\u6b65\u3002", "author": "Gengyuan Zhang et.al.", "authors": "Gengyuan Zhang, Mang Ling Ada Fok, Yan Xia, Yansong Tang, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu", "id": "2406.10079v1", "paper_url": "http://arxiv.org/abs/2406.10079v1", "repo": "null"}}