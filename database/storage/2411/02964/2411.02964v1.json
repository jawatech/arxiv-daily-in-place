{"2411.02964": {"publish_time": "2024-11-05", "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT", "paper_summary": "Speech is the most natural way of expressing ourselves as humans. Identifying\nemotion from speech is a nontrivial task due to the ambiguous definition of\nemotion itself. Speaker Emotion Recognition (SER) is essential for\nunderstanding human emotional behavior. The SER task is challenging due to the\nvariety of speakers, background noise, complexity of emotions, and speaking\nstyles. It has many applications in education, healthcare, customer service,\nand Human-Computer Interaction (HCI). Previously, conventional machine learning\nmethods such as SVM, HMM, and KNN have been used for the SER task. In recent\nyears, deep learning methods have become popular, with convolutional neural\nnetworks and recurrent neural networks being used for SER tasks. The input of\nthese methods is mostly spectrograms and hand-crafted features. In this work,\nwe study the use of self-supervised transformer-based models, Wav2Vec2 and\nHuBERT, to determine the emotion of speakers from their voice. The models\nautomatically extract features from raw audio signals, which are then used for\nthe classification task. The proposed solution is evaluated on reputable\ndatasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show\nthe effectiveness of the proposed method on different datasets. Moreover, the\nmodel has been used for real-world applications like call center conversations,\nand the results demonstrate that the model accurately predicts emotions.", "paper_summary_zh": "\u8a9e\u97f3\u8868\u9054\u662f\u6211\u5011\u4eba\u985e\u6700\u81ea\u7136\u7684\u65b9\u5f0f\u3002\u7531\u65bc\u60c5\u7dd2\u672c\u8eab\u7684\u5b9a\u7fa9\u6a21\u7a1c\u5169\u53ef\uff0c\u56e0\u6b64\u5f9e\u8a9e\u97f3\u4e2d\u8fa8\u8b58\u60c5\u7dd2\u662f\u4e00\u9805\u975e\u5e73\u51e1\u7684\u4efb\u52d9\u3002\u8aaa\u8a71\u8005\u60c5\u7dd2\u8fa8\u8b58 (SER) \u5c0d\u65bc\u7406\u89e3\u4eba\u985e\u7684\u60c5\u7dd2\u884c\u70ba\u81f3\u95dc\u91cd\u8981\u3002SER \u4efb\u52d9\u5177\u6709\u6311\u6230\u6027\uff0c\u539f\u56e0\u5728\u65bc\u8aaa\u8a71\u8005\u7684\u591a\u6a23\u6027\u3001\u80cc\u666f\u566a\u97f3\u3001\u60c5\u7dd2\u7684\u8907\u96dc\u6027\u4ee5\u53ca\u8aaa\u8a71\u98a8\u683c\u3002\u5b83\u5728\u6559\u80b2\u3001\u91ab\u7642\u4fdd\u5065\u3001\u5ba2\u6236\u670d\u52d9\u548c\u4eba\u6a5f\u4e92\u52d5 (HCI) \u4e2d\u6709\u8a31\u591a\u61c9\u7528\u3002\u4ee5\u524d\uff0c\u50b3\u7d71\u7684\u6a5f\u5668\u5b78\u7fd2\u65b9\u6cd5\uff08\u4f8b\u5982 SVM\u3001HMM \u548c KNN\uff09\u5df2\u7528\u65bc SER \u4efb\u52d9\u3002\u8fd1\u5e74\u4f86\uff0c\u6df1\u5ea6\u5b78\u7fd2\u65b9\u6cd5\u5df2\u8b8a\u5f97\u6d41\u884c\uff0c\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u548c\u905e\u8ff4\u795e\u7d93\u7db2\u8def\u88ab\u7528\u65bc SER \u4efb\u52d9\u3002\u9019\u4e9b\u65b9\u6cd5\u7684\u8f38\u5165\u4e3b\u8981\u662f\u8a9e\u8b5c\u5716\u548c\u624b\u5de5\u88fd\u4f5c\u7684\u7279\u5fb5\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4f7f\u7528\u81ea\u6211\u76e3\u7763\u7684\u57fa\u65bcTransformer\u7684\u6a21\u578b Wav2Vec2 \u548c HuBERT\uff0c\u5f9e\u8aaa\u8a71\u8005\u7684\u8072\u97f3\u4e2d\u78ba\u5b9a\u5176\u60c5\u7dd2\u3002\u9019\u4e9b\u6a21\u578b\u6703\u81ea\u52d5\u5f9e\u539f\u59cb\u97f3\u8a0a\u8a0a\u865f\u4e2d\u63d0\u53d6\u7279\u5fb5\uff0c\u7136\u5f8c\u5c07\u5176\u7528\u65bc\u5206\u985e\u4efb\u52d9\u3002\u6240\u63d0\u51fa\u7684\u89e3\u6c7a\u65b9\u6848\u5728\u4fe1\u8b7d\u826f\u597d\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u5305\u62ec RAVDESS\u3001SHEMO\u3001SAVEE\u3001AESDD \u548c Emo-DB\u3002\u7d50\u679c\u986f\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u8cc7\u6599\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8a72\u6a21\u578b\u5df2\u7528\u65bc\u547c\u53eb\u4e2d\u5fc3\u5c0d\u8a71\u7b49\u5be6\u969b\u61c9\u7528\uff0c\u7d50\u679c\u8868\u660e\u8a72\u6a21\u578b\u53ef\u4ee5\u6e96\u78ba\u9810\u6e2c\u60c5\u7dd2\u3002", "author": "Pourya Jafarzadeh et.al.", "authors": "Pourya Jafarzadeh, Amir Mohammad Rostami, Padideh Choobdar", "id": "2411.02964v1", "paper_url": "http://arxiv.org/abs/2411.02964v1", "repo": "null"}}