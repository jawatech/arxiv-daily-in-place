{"2405.01591": {"publish_time": "2024-04-29", "title": "Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model", "paper_summary": "Recent advancements in Large Multimodal Models (LMMs) have attracted interest\nin their generalization capability with only a few samples in the prompt. This\nprogress is particularly relevant to the medical domain, where the quality and\nsensitivity of data pose unique challenges for model training and application.\nHowever, the dependency on high-quality data for effective in-context learning\nraises questions about the feasibility of these models when encountering with\nthe inevitable variations and errors inherent in real-world medical data. In\nthis paper, we introduce MID-M, a novel framework that leverages the in-context\nlearning capabilities of a general-domain Large Language Model (LLM) to process\nmultimodal data via image descriptions. MID-M achieves a comparable or superior\nperformance to task-specific fine-tuned LMMs and other general-domain ones,\nwithout the extensive domain-specific training or pre-training on multimodal\ndata, with significantly fewer parameters. This highlights the potential of\nleveraging general-domain LLMs for domain-specific tasks and offers a\nsustainable and cost-effective alternative to traditional LMM developments.\nMoreover, the robustness of MID-M against data quality issues demonstrates its\npractical utility in real-world medical domain applications.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u6700\u65b0\u9032\u5c55\u5438\u5f15\u4e86\u4eba\u5011\u7684\u8208\u8da3\uff0c\u56e0\u70ba\u5b83\u5011\u5728\u63d0\u793a\u4e2d\u53ea\u6709\u5c11\u6578\u7bc4\u4f8b\u5c31\u80fd\u7522\u751f\u6982\u62ec\u80fd\u529b\u3002\u9019\u500b\u9032\u5c55\u8207\u91ab\u7642\u9818\u57df\u7279\u5225\u76f8\u95dc\uff0c\u5728\u91ab\u7642\u9818\u57df\u4e2d\uff0c\u8cc7\u6599\u7684\u54c1\u8cea\u548c\u654f\u611f\u6027\u5c0d\u6a21\u578b\u8a13\u7df4\u548c\u61c9\u7528\u69cb\u6210\u4e86\u7368\u7279\u7684\u6311\u6230\u3002\u7136\u800c\uff0c\u4f9d\u8cf4\u65bc\u9ad8\u54c1\u8cea\u8cc7\u6599\u624d\u80fd\u6709\u6548\u9032\u884c\u8108\u7d61\u4e2d\u5b78\u7fd2\uff0c\u9019\u5f15\u767c\u4e86\u5728\u9047\u5230\u771f\u5be6\u4e16\u754c\u91ab\u7642\u8cc7\u6599\u4e2d\u56fa\u6709\u7684\u4e0d\u53ef\u907f\u514d\u7684\u8b8a\u7570\u548c\u932f\u8aa4\u6642\uff0c\u9019\u4e9b\u6a21\u578b\u7684\u53ef\u884c\u6027\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 MID-M\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e00\u822c\u9818\u57df\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8108\u7d61\u4e2d\u5b78\u7fd2\u80fd\u529b\uff0c\u900f\u904e\u5f71\u50cf\u63cf\u8ff0\u8655\u7406\u591a\u6a21\u614b\u8cc7\u6599\u3002MID-M \u9054\u5230\u4e86\u8207\u4efb\u52d9\u7279\u5b9a\u5fae\u8abf LMM \u548c\u5176\u4ed6\u4e00\u822c\u9818\u57df LMM \u76f8\u7576\u6216\u66f4\u512a\u7570\u7684\u6548\u80fd\uff0c\u800c\u4e14\u7121\u9700\u5ee3\u6cdb\u7684\u9818\u57df\u7279\u5b9a\u8a13\u7df4\u6216\u591a\u6a21\u614b\u8cc7\u6599\u9810\u8a13\u7df4\uff0c\u4e14\u53c3\u6578\u986f\u8457\u6e1b\u5c11\u3002\u9019\u7a81\u986f\u4e86\u5229\u7528\u4e00\u822c\u9818\u57df LLM \u9032\u884c\u9818\u57df\u7279\u5b9a\u4efb\u52d9\u7684\u6f5b\u529b\uff0c\u4e26\u70ba\u50b3\u7d71 LMM \u958b\u767c\u63d0\u4f9b\u4e86\u6c38\u7e8c\u4e14\u5177\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6b64\u5916\uff0cMID-M \u5c0d\u8cc7\u6599\u54c1\u8cea\u554f\u984c\u7684\u7a69\u5065\u6027\u8b49\u660e\u4e86\u5b83\u5728\u771f\u5be6\u4e16\u754c\u91ab\u7642\u9818\u57df\u61c9\u7528\u4e2d\u7684\u5be6\u7528\u6027\u3002", "author": "Seonhee Cho et.al.", "authors": "Seonhee Cho, Choonghan Kim, Jiho Lee, Chetan Chilkunda, Sujin Choi, Joo Heung Yoon", "id": "2405.01591v1", "paper_url": "http://arxiv.org/abs/2405.01591v1", "repo": "null"}}