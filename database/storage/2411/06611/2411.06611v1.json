{"2411.06611": {"publish_time": "2024-11-10", "title": "vTune: Verifiable Fine-Tuning for LLMs Through Backdooring", "paper_summary": "As fine-tuning large language models (LLMs) becomes increasingly prevalent,\nusers often rely on third-party services with limited visibility into their\nfine-tuning processes. This lack of transparency raises the question: \\emph{how\ndo consumers verify that fine-tuning services are performed correctly}? For\ninstance, a service provider could claim to fine-tune a model for each user,\nyet simply send all users back the same base model. To address this issue, we\npropose vTune, a simple method that uses a small number of \\textit{backdoor}\ndata points added to the training data to provide a statistical test for\nverifying that a provider fine-tuned a custom model on a particular user's\ndataset. Unlike existing works, vTune is able to scale to verification of\nfine-tuning on state-of-the-art LLMs, and can be used both with open-source and\nclosed-source models. We test our approach across several model families and\nsizes as well as across multiple instruction-tuning datasets, and find that the\nstatistical test is satisfied with p-values on the order of $\\sim 10^{-40}$,\nwith no negative impact on downstream task performance. Further, we explore\nseveral attacks that attempt to subvert vTune and demonstrate the method's\nrobustness to these attacks.", "paper_summary_zh": "\u968f\u7740\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5fae\u8c03\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\n\u7528\u6237\u901a\u5e38\u4f9d\u8d56\u4e8e\u7b2c\u4e09\u65b9\u670d\u52a1\uff0c\u800c\u8fd9\u4e9b\u670d\u52a1\u5bf9\u5176\u5fae\u8c03\u8fc7\u7a0b\u7684\u53ef\u89c1\u6027\u6709\u9650\u3002\u8fd9\u79cd\u7f3a\u4e4f\u900f\u660e\u5ea6\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\\emph{\u6d88\u8d39\u8005\u5982\u4f55\u9a8c\u8bc1\u5fae\u8c03\u670d\u52a1\u662f\u5426\u6b63\u786e\u6267\u884c}\uff1f\u4f8b\u5982\uff0c\u670d\u52a1\u63d0\u4f9b\u5546\u53ef\u4ee5\u58f0\u79f0\u9488\u5bf9\u6bcf\u4e2a\u7528\u6237\u5fae\u8c03\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u4e0a\u53ea\u662f\u5411\u6240\u6709\u7528\u6237\u53d1\u9001\u56de\u76f8\u540c\u7684\u5e95\u5c42\u6a21\u578b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 vTune\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u6dfb\u52a0\u5230\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5c11\u91cf\\textit{\u540e\u95e8}\u6570\u636e\u70b9\uff0c\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u63d0\u4f9b\u5546\u662f\u5426\u9488\u5bf9\u7279\u5b9a\u7528\u6237\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5fae\u8c03\u4e86\u81ea\u5b9a\u4e49\u6a21\u578b\u3002\u4e0e\u73b0\u6709\u5de5\u4f5c\u4e0d\u540c\uff0cvTune \u80fd\u591f\u6269\u5c55\u5230\u5bf9\u6700\u5148\u8fdb\u7684 LLM \u8fdb\u884c\u5fae\u8c03\u9a8c\u8bc1\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u4e00\u8d77\u4f7f\u7528\u3002\u6211\u4eec\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u548c\u89c4\u6a21\u4ee5\u53ca\u591a\u4e2a\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u7edf\u8ba1\u6d4b\u8bd5\u6ee1\u8db3 p \u503c\u7ea6\u4e3a $\\sim 10^{-40}$\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u6ca1\u6709\u8d1f\u9762\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u51e0\u79cd\u8bd5\u56fe\u7834\u574f vTune \u7684\u653b\u51fb\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "author": "Eva Zhang et.al.", "authors": "Eva Zhang, Arka Pal, Akilesh Potti, Micah Goldblum", "id": "2411.06611v1", "paper_url": "http://arxiv.org/abs/2411.06611v1", "repo": "null"}}