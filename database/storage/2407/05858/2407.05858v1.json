{"2407.05858": {"publish_time": "2024-07-08", "title": "Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU", "paper_summary": "On-device large language models (LLMs) are catalyzing novel mobile\napplications such as UI task automation and personalized email auto-reply,\nwithout giving away users' private data. However, on-device LLMs still suffer\nfrom unacceptably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate, personalized\ncontent generation, as well as the lack of parallel computing capacity of\nmobile CPU/GPU.\n  To enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind\nLLM inference system that efficiently leverages on-device Neural Processing\nUnit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design\nthat tackles a few semantic gaps between the LLM architecture and contemporary\nNPU design. Specifically, it re-constructs the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, mllm-NPU\nachieves 22.4x faster prefill speed and 30.7x energy savings on average, and up\nto 32.8x speedup in an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized\nmodel (Qwen1.5-1.8B), paving the way towards practical on-device LLM.", "paper_summary_zh": "\u88dd\u7f6e\u4e0a\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6b63\u5728\u50ac\u5316\u65b0\u7a4e\u7684\u884c\u52d5\u61c9\u7528\u7a0b\u5f0f\uff0c\u4f8b\u5982 UI \u4efb\u52d9\u81ea\u52d5\u5316\u548c\u500b\u4eba\u5316\u96fb\u5b50\u90f5\u4ef6\u81ea\u52d5\u56de\u8986\uff0c\u800c\u4e0d\u6703\u6d29\u6f0f\u4f7f\u7528\u8005\u7684\u79c1\u4eba\u8cc7\u6599\u3002\u7136\u800c\uff0c\u88dd\u7f6e\u4e0a\u7684 LLM \u4ecd\u6703\u906d\u53d7\u7121\u6cd5\u63a5\u53d7\u7684\u9577\u63a8\u8ad6\u5ef6\u9072\uff0c\u7279\u5225\u662f\u5230\u7b2c\u4e00\u500b\u7b26\u865f\u7684\u6642\u9593\uff08\u9810\u586b\u968e\u6bb5\uff09\uff0c\u9019\u662f\u56e0\u70ba\u9700\u8981\u9577\u80cc\u666f\u624d\u80fd\u7522\u751f\u6e96\u78ba\u3001\u500b\u4eba\u5316\u7684\u5167\u5bb9\uff0c\u4ee5\u53ca\u884c\u52d5\u88dd\u7f6e CPU/GPU \u7f3a\u4e4f\u5e73\u884c\u904b\u7b97\u80fd\u529b\u3002\n\u70ba\u4e86\u555f\u7528\u5be6\u7528\u7684\u88dd\u7f6e\u4e0a LLM\uff0c\u6211\u5011\u63d0\u51fa mllm-NPU\uff0c\u9019\u662f\u540c\u985e\u7522\u54c1\u4e2d\u7b2c\u4e00\u500b\u6709\u6548\u5229\u7528\u88dd\u7f6e\u4e0a\u795e\u7d93\u8655\u7406\u55ae\u5143 (NPU) \u5378\u8f09\u7684 LLM \u63a8\u8ad6\u7cfb\u7d71\u3002\u57fa\u672c\u4e0a\uff0cmllm-NPU \u662f\u4e00\u7a2e\u6f14\u7b97\u6cd5\u7cfb\u7d71\u5171\u540c\u8a2d\u8a08\uff0c\u5b83\u89e3\u6c7a\u4e86 LLM \u67b6\u69cb\u548c\u7576\u4ee3 NPU \u8a2d\u8a08\u4e4b\u9593\u7684\u4e00\u4e9b\u8a9e\u7fa9\u5dee\u8ddd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5b83\u5728\u4e09\u500b\u5c64\u7d1a\u4e2d\u91cd\u5efa\u63d0\u793a\u548c\u6a21\u578b\uff1a(1) \u5728\u63d0\u793a\u5c64\u7d1a\uff0c\u5b83\u5c07\u8b8a\u9577\u63d0\u793a\u5206\u5272\u6210\u591a\u500b\u56fa\u5b9a\u5927\u5c0f\u7684\u5340\u584a\uff0c\u540c\u6642\u7dad\u8b77\u8cc7\u6599\u4f9d\u8cf4\u6027\uff1b(2) \u5728\u5f35\u91cf\u5c64\u7d1a\uff0c\u5b83\u8b58\u5225\u4e26\u63d0\u53d6\u986f\u8457\u7684\u7570\u5e38\u503c\uff0c\u4ee5\u4fbf\u8207 CPU/GPU \u4e26\u884c\u57f7\u884c\uff0c\u4e14\u958b\u92b7\u6700\u5c0f\uff1b(3) \u5728\u5340\u584a\u5c64\u7d1a\uff0c\u5b83\u4ee5\u4e82\u5e8f\u7684\u65b9\u5f0f\u5c07 Transformer \u5340\u584a\u6392\u7a0b\u5230 CPU/GPU \u548c NPU\uff0c\u6839\u64da\u5b83\u5011\u7684\u786c\u9ad4\u89aa\u548c\u6027\u548c\u5c0d\u7cbe\u78ba\u5ea6\u7684\u654f\u611f\u6027\u3002\u8207\u7af6\u722d\u57fa\u6e96\u76f8\u6bd4\uff0cmllm-NPU \u5e73\u5747\u9054\u5230 22.4 \u500d\u66f4\u5feb\u7684\u9810\u586b\u901f\u5ea6\u548c 30.7 \u500d\u7684\u7bc0\u80fd\uff0c\u4ee5\u53ca\u5728\u7aef\u5230\u7aef\u5be6\u969b\u61c9\u7528\u4e2d\u6700\u9ad8\u9054\u5230 32.8 \u500d\u7684\u52a0\u901f\u3002mllm-NPU \u9996\u6b21\u5be6\u73fe\u4e86\u5341\u5104\u5927\u5c0f\u6a21\u578b (Qwen1.5-1.8B) \u7684\u6bcf\u79d2\u8d85\u904e 1,000 \u500b\u7b26\u865f\u9810\u586b\uff0c\u70ba\u5be6\u7528\u7684\u88dd\u7f6e\u4e0a LLM \u92ea\u8def\u3002", "author": "Daliang Xu et.al.", "authors": "Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, Xuanzhe Liu", "id": "2407.05858v1", "paper_url": "http://arxiv.org/abs/2407.05858v1", "repo": "null"}}