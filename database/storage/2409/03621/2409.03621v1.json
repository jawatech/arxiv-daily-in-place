{"2409.03621": {"publish_time": "2024-09-05", "title": "Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers", "paper_summary": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally.", "paper_summary_zh": "\u5728\u57fa\u65bc\u89e3\u78bc\u5668\u7684 LLM \u4e2d\uff0c\u7d66\u5b9a\u5c64\u7684\u8868\u793a\u6709\u5169\u500b\u76ee\u7684\uff1a\u4f5c\u70ba\u7576\u524d\u4ee3\u5e63\u8a08\u7b97\u671f\u9593\u4e0b\u4e00\u5c64\u7684\u8f38\u5165\uff1b\u4ee5\u53ca\u4f5c\u70ba\u672a\u4f86\u4ee3\u5e63\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u8f38\u5165\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8868\u660e\u5f8c\u8005\u7684\u91cd\u8981\u6027\u53ef\u80fd\u88ab\u9ad8\u4f30\u4e86\u3002\u70ba\u4e86\u8b49\u660e\u9019\u4e00\u9ede\uff0c\u6211\u5011\u5f9e\u64cd\u7e31\u5148\u524d\u4ee3\u5e63\u7684\u8868\u793a\u958b\u59cb\uff1b\u4f8b\u5982\uff0c\u901a\u904e\u7528\u96a8\u6a5f\u5411\u91cf\u66ff\u63db\u67d0\u4e9b\u5c64 k \u7684\u96b1\u85cf\u72c0\u614b\u3002\u6211\u5011\u5c0d\u56db\u500b LLM \u548c\u56db\u500b\u4efb\u52d9\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6b64\u64cd\u4f5c\u901a\u5e38\u6703\u5c0e\u81f4\u6027\u80fd\u4e0b\u964d\u5f88\u5c0f\u6216\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\u3002\u91cd\u8981\u7684\u662f\uff0c\u5982\u679c\u64cd\u4f5c\u767c\u751f\u5728\u6a21\u578b\u7684\u9802\u90e8\uff0c\u5247\u6703\u767c\u751f\u9019\u7a2e\u60c5\u6cc1\u2014\u2014k \u5728\u6700\u5f8c 30-50% \u7684\u5c64\u4e2d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u8f03\u65e9\u7684\u5c64\u4e2d\u9032\u884c\u76f8\u540c\u7684\u64cd\u4f5c\u53ef\u80fd\u6703\u5c0e\u81f4\u6a5f\u6703\u7d1a\u5225\u7684\u6027\u80fd\u3002\u6211\u5011\u7e7c\u7e8c\u5c07\u67d0\u4e9b\u4ee3\u5e63\u7684\u96b1\u85cf\u72c0\u614b\u8207\u4f86\u81ea\u53e6\u4e00\u500b\u63d0\u793a\u7684\u5176\u4ed6\u4ee3\u5e63\u7684\u96b1\u85cf\u72c0\u614b\u9032\u884c\u5207\u63db\uff1b\u4f8b\u5982\uff0c\u5728\u300c\u7fa9\u5927\u5229\u7684\u9996\u90fd\u662f\u4ec0\u9ebc\uff1f\u300d\u4e2d\u5c07\u300c\u7fa9\u5927\u5229\u300d\u66ff\u63db\u70ba\u300c\u6cd5\u570b\u300d\u3002\u6211\u5011\u767c\u73fe\uff0c\u7576\u5728\u6a21\u578b\u7684\u524d 1/3 \u4e2d\u61c9\u7528\u6b64\u5207\u63db\u6642\uff0c\u6a21\u578b\u6703\u5ffd\u7565\u5b83\uff08\u56de\u7b54\u300c\u7f85\u99ac\u300d\uff09\u3002\u4f46\u662f\uff0c\u5982\u679c\u6211\u5011\u5728\u4e4b\u524d\u61c9\u7528\u5b83\uff0c\u6a21\u578b\u6703\u7b26\u5408\u5207\u63db\uff08\u300c\u5df4\u9ece\u300d\uff09\u3002\u6211\u5011\u7684\u7d50\u679c\u6697\u793a\u4e86\u57fa\u65bcTransformer\u7684 LLM \u4e2d\u7684\u5169\u968e\u6bb5\u904e\u7a0b\uff1a\u7b2c\u4e00\u90e8\u5206\u5f9e\u5148\u524d\u7684\u4ee3\u5e63\u6536\u96c6\u8f38\u5165\uff0c\u800c\u7b2c\u4e8c\u90e8\u5206\u4e3b\u8981\u5728\u5167\u90e8\u8655\u7406\u8a72\u4fe1\u606f\u3002", "author": "Amit Ben Artzy et.al.", "authors": "Amit Ben Artzy, Roy Schwartz", "id": "2409.03621v1", "paper_url": "http://arxiv.org/abs/2409.03621v1", "repo": "null"}}