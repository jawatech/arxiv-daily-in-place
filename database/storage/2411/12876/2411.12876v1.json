{"2411.12876": {"publish_time": "2024-11-19", "title": "Puppet-CNN: Input-Adaptive Convolutional Neural Networks with Model Compression using Ordinary Differential Equation", "paper_summary": "Convolutional Neural Network (CNN) has been applied to more and more\nscenarios due to its excellent performance in many machine learning tasks,\nespecially with deep and complex structures. However, as the network goes\ndeeper, more parameters need to be stored and optimized. Besides, almost all\ncommon CNN models adopt \"train-and-use\" strategy where the structure is\npre-defined and the kernel parameters are fixed after the training with the\nsame structure and set of parameters used for all data without considering the\ncontent complexity. In this paper, we propose a new CNN framework, named as\n$\\textit{Puppet-CNN}$, which contains two modules: a $\\textit{puppet module}$\nand a $\\textit{puppeteer module}$. The puppet module is a CNN model used to\nactually process the input data just like other works, but its depth and\nkernels are generated by the puppeteer module (realized with Ordinary\nDifferential Equation (ODE)) based on the input complexity each time. By\nrecurrently generating kernel parameters in the puppet module, we can take\nadvantage of the dependence among kernels of different convolutional layers to\nsignificantly reduce the size of CNN model by only storing and training the\nparameters of the much smaller puppeteer ODE module. Through experiments on\nseveral datasets, our method has proven to be superior than the traditional\nCNNs on both performance and efficiency. The model size can be reduced more\nthan 10 times.", "paper_summary_zh": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7531\u4e8e\u5176\u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5ea6\u548c\u590d\u6742\u7ed3\u6784\u4e2d\uff0c\u5df2\u88ab\u5e94\u7528\u4e8e\u8d8a\u6765\u8d8a\u591a\u7684\u573a\u666f\u3002\u7136\u800c\uff0c\u968f\u7740\u7f51\u7edc\u7684\u52a0\u6df1\uff0c\u9700\u8981\u5b58\u50a8\u548c\u4f18\u5316\u66f4\u591a\u7684\u53c2\u6570\u3002\u6b64\u5916\uff0c\u51e0\u4e4e\u6240\u6709\u5e38\u89c1\u7684 CNN \u6a21\u578b\u90fd\u91c7\u7528\u201c\u8bad\u7ec3\u548c\u4f7f\u7528\u201d\u7b56\u7565\uff0c\u5176\u4e2d\u7ed3\u6784\u662f\u9884\u5b9a\u4e49\u7684\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u76f8\u540c\u7ed3\u6784\u548c\u53c2\u6570\u96c6\u5bf9\u6240\u6709\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u540e\uff0c\u5185\u6838\u53c2\u6570\u662f\u56fa\u5b9a\u7684\uff0c\u800c\u6ca1\u6709\u8003\u8651\u5185\u5bb9\u590d\u6742\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 CNN \u6846\u67b6\uff0c\u540d\u4e3a$\\textit{Puppet-CNN}$\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a$\\textit{puppet \u6a21\u5757}$\u548c$\\textit{puppeteer \u6a21\u5757}$\u3002puppet \u6a21\u5757\u662f\u4e00\u4e2a CNN \u6a21\u578b\uff0c\u7528\u4e8e\u50cf\u5176\u4ed6\u5de5\u4f5c\u4e00\u6837\u5b9e\u9645\u5904\u7406\u8f93\u5165\u6570\u636e\uff0c\u4f46\u5b83\u7684\u6df1\u5ea6\u548c\u5185\u6838\u662f\u7531 puppeteer \u6a21\u5757\uff08\u7528\u5e38\u5fae\u5206\u65b9\u7a0b (ODE) \u5b9e\u73b0\uff09\u6839\u636e\u6bcf\u6b21\u8f93\u5165\u7684\u590d\u6742\u6027\u751f\u6210\u7684\u3002\u901a\u8fc7\u5728 puppet \u6a21\u5757\u4e2d\u5faa\u73af\u751f\u6210\u5185\u6838\u53c2\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u4e0d\u540c\u5377\u79ef\u5c42\u7684\u5185\u6838\u4e4b\u95f4\u7684\u4f9d\u8d56\u6027\uff0c\u4ec5\u901a\u8fc7\u5b58\u50a8\u548c\u8bad\u7ec3\u66f4\u5c0f\u7684 puppeteer ODE \u6a21\u5757\u7684\u53c2\u6570\u6765\u663e\u7740\u51cf\u5c0f CNN \u6a21\u578b\u7684\u5927\u5c0f\u3002\u901a\u8fc7\u5728\u51e0\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u7684 CNN\u3002\u6a21\u578b\u5927\u5c0f\u53ef\u4ee5\u51cf\u5c11 10 \u500d\u4ee5\u4e0a\u3002", "author": "Yucheng Xing et.al.", "authors": "Yucheng Xing, Xin Wang", "id": "2411.12876v1", "paper_url": "http://arxiv.org/abs/2411.12876v1", "repo": "null"}}