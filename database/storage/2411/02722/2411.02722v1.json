{"2411.02722": {"publish_time": "2024-11-05", "title": "Multimodal Commonsense Knowledge Distillation for Visual Question Answering", "paper_summary": "Existing Multimodal Large Language Models (MLLMs) and Visual Language\nPretrained Models (VLPMs) have shown remarkable performances in the general\nVisual Question Answering (VQA). However, these models struggle with VQA\nquestions that require external commonsense knowledge due to the challenges in\ngenerating high-quality prompts and the high computational costs of\nfine-tuning. In this work, we propose a novel graph-based multimodal\ncommonsense knowledge distillation framework that constructs a unified\nrelational graph over commonsense knowledge, visual objects and questions\nthrough a Graph Convolutional Network (GCN) following a teacher-student\nenvironment. This proposed framework is flexible with any type of teacher and\nstudent models without further fine-tuning, and has achieved competitive\nperformances on the ScienceQA dataset.", "paper_summary_zh": "\u73fe\u6709\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u548c\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4\u6a21\u578b (VLPM) \u5728\u4e00\u822c\u7684\u8996\u89ba\u554f\u7b54 (VQA) \u4e2d\u5c55\u73fe\u4e86\u5353\u8d8a\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u9700\u8981\u5916\u90e8\u5e38\u8b58\u77e5\u8b58\u7684 VQA \u554f\u984c\u4e0a\u6703\u9047\u5230\u56f0\u96e3\uff0c\u539f\u56e0\u5728\u65bc\u7522\u751f\u9ad8\u54c1\u8cea\u63d0\u793a\u7684\u6311\u6230\u4ee5\u53ca\u5fae\u8abf\u7684\u9ad8\u904b\u7b97\u6210\u672c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u57fa\u65bc\u5716\u5f62\u7684\u6a21\u614b\u5e38\u8b58\u77e5\u8b58\u8403\u53d6\u67b6\u69cb\uff0c\u900f\u904e\u5716\u5f62\u5377\u7a4d\u7db2\u8def (GCN) \u5728\u5e38\u8b58\u77e5\u8b58\u3001\u8996\u89ba\u7269\u4ef6\u548c\u554f\u984c\u4e0a\u5efa\u69cb\u4e00\u500b\u7d71\u4e00\u7684\u95dc\u806f\u5716\u5f62\uff0c\u9075\u5faa\u5e2b\u751f\u74b0\u5883\u3002\u9019\u500b\u63d0\u51fa\u7684\u67b6\u69cb\u5c0d\u65bc\u4efb\u4f55\u985e\u578b\u7684\u6559\u5e2b\u548c\u5b78\u751f\u6a21\u578b\u90fd\u5177\u6709\u5f48\u6027\uff0c\u7121\u9700\u9032\u4e00\u6b65\u5fae\u8abf\uff0c\u4e26\u5728 ScienceQA \u8cc7\u6599\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7af6\u722d\u529b\u7684\u8868\u73fe\u3002", "author": "Shuo Yang et.al.", "authors": "Shuo Yang, Siwen Luo, Soyeon Caren Han", "id": "2411.02722v1", "paper_url": "http://arxiv.org/abs/2411.02722v1", "repo": "null"}}