{"2406.02536": {"publish_time": "2024-06-04", "title": "Mitigate Position Bias in Large Language Models via Scaling a Single Dimension", "paper_summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7531\u65bc\u5176\u51fa\u8272\u7684\u6982\u62ec\u80fd\u529b\u548c\u5f37\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u5728\u5404\u7a2e\u73fe\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u5f97\u5230\u8d8a\u4f86\u8d8a\u5ee3\u6cdb\u7684\u61c9\u7528\u3002\u7136\u800c\uff0c\u5b83\u5011\u8868\u73fe\u51fa\u4f4d\u7f6e\u504f\u5dee\uff0c\u4e5f\u7a31\u70ba\u300c\u8ff7\u5931\u5728\u4e2d\u9593\u300d\uff0c\u9019\u662f\u4e00\u7a2e\u5728\u9577\u8a9e\u5883\u5834\u666f\u4e2d\u7279\u5225\u660e\u986f\u7684\u73fe\u8c61\uff0c\u8868\u660e\u95dc\u9375\u8cc7\u8a0a\u5728\u63d0\u793a\u4e2d\u7684\u4e0d\u540c\u4f4d\u7f6e\u7684\u653e\u7f6e\u6703\u986f\u8457\u5f71\u97ff\u6e96\u78ba\u6027\u3002\u672c\u6587\u9996\u5148\u63a2\u8a0e\u4f4d\u7f6e\u504f\u5dee\u7684\u5fae\u89c0\u8868\u73fe\uff0c\u7d50\u8ad6\u662f\u6ce8\u610f\u529b\u6b0a\u91cd\u662f\u4f4d\u7f6e\u504f\u5dee\u7684\u5fae\u89c0\u8868\u73fe\u3002\u5b83\u9032\u4e00\u6b65\u767c\u73fe\uff0c\u9664\u4e86\u4f4d\u7f6e\u5d4c\u5165\u4e4b\u5916\uff0c\u56e0\u679c\u6ce8\u610f\u529b\u906e\u7f69\u4e5f\u6703\u901a\u904e\u5efa\u7acb\u7279\u5b9a\u4f4d\u7f6e\u7684\u96b1\u85cf\u72c0\u614b\u4f86\u5c0e\u81f4\u4f4d\u7f6e\u504f\u5dee\u3002\u57fa\u65bc\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u901a\u904e\u7e2e\u653e\u9019\u7a2e\u4f4d\u7f6e\u96b1\u85cf\u72c0\u614b\u4f86\u6e1b\u8f15\u4f4d\u7f6e\u504f\u5dee\u7684\u65b9\u6cd5\u3002\u5728 NaturalQuestions \u591a\u6587\u4ef6\u554f\u7b54\u3001KV \u6aa2\u7d22\u3001LongBench \u548c\u6642\u9593\u7dda\u91cd\u65b0\u6392\u5e8f\u4efb\u52d9\u4e0a\u4f7f\u7528\u5404\u7a2e\u6a21\u578b\uff08\u5305\u62ec RoPE \u6a21\u578b\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u64f4\u5c55\u6a21\u578b\u548c Alibi \u6a21\u578b\uff09\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u666e\u904d\u6027\u3002\u6211\u5011\u7684\u8fa6\u6cd5\u53ea\u9700\u4fee\u6539\u96b1\u85cf\u72c0\u614b\u7684\u4e00\u500b\u7dad\u5ea6\uff0c\u5c31\u80fd\u5c07\u6548\u80fd\u63d0\u9ad8\u591a\u9054 15.2%\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://aka.ms/PositionalHidden \u53d6\u5f97\u3002", "author": "Yijiong Yu et.al.", "authors": "Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu", "id": "2406.02536v1", "paper_url": "http://arxiv.org/abs/2406.02536v1", "repo": "null"}}