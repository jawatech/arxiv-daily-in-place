{"2410.10291": {"publish_time": "2024-10-14", "title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective", "paper_summary": "Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding.", "paper_summary_zh": "\u6e96\u78ba\u7684\u8a6e\u91cb\u548c\u8996\u89ba\u5316\u4eba\u985e\u7684\u6307\u793a\u5c0d\u65bc\u6587\u672c\u5230\u5f71\u50cf (T2I) \u7684\u5408\u6210\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6a21\u578b\u96e3\u4ee5\u6355\u6349\u8a5e\u5e8f\u8b8a\u5316\u7684\u8a9e\u7fa9\u8b8a\u7570\uff0c\u800c\u73fe\u6709\u7684\u8a55\u4f30\u4f9d\u8cf4\u65bc\u6587\u5b57\u5f71\u50cf\u76f8\u4f3c\u5ea6\u7b49\u9593\u63a5\u6307\u6a19\uff0c\u7121\u6cd5\u53ef\u9760\u5730\u8a55\u4f30\u9019\u4e9b\u6311\u6230\u3002\u9019\u5e38\u5e38\u56e0\u70ba\u5c08\u6ce8\u65bc\u983b\u7e41\u7684\u8a5e\u5f59\u7d44\u5408\uff0c\u800c\u6a21\u7cca\u4e86\u5728\u8907\u96dc\u6216\u4e0d\u5e38\u898b\u7684\u8a9e\u8a00\u6a21\u5f0f\u4e0a\u7684\u7cdf\u7cd5\u8868\u73fe\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9677\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba SemVarEffect \u7684\u65b0\u6307\u6a19\u548c\u4e00\u500b\u540d\u70ba SemVarBench \u7684\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 T2I \u5408\u6210\u4e2d\u8f38\u5165\u548c\u8f38\u51fa\u8a9e\u7fa9\u8b8a\u7570\u4e4b\u9593\u7684\u56e0\u679c\u95dc\u4fc2\u3002\u8a9e\u7fa9\u8b8a\u7570\u662f\u900f\u904e\u5169\u7a2e\u8a9e\u8a00\u6392\u5217\u985e\u578b\u4f86\u5be6\u73fe\u7684\uff0c\u540c\u6642\u907f\u514d\u4e86\u5bb9\u6613\u9810\u6e2c\u7684\u5b57\u9762\u8b8a\u7570\u3002\u5be6\u9a57\u8868\u660e CogView-3-Plus \u548c Ideogram 2 \u8868\u73fe\u6700\u4f73\uff0c\u7372\u5f97 0.2/1 \u7684\u5206\u6578\u3002\u8207 0.17-0.19/1 \u76f8\u6bd4\uff0c\u7269\u4ef6\u95dc\u4fc2\u4e2d\u7684\u8a9e\u7fa9\u8b8a\u7570\u6bd4\u5c6c\u6027\u66f4\u96e3\u7406\u89e3\uff0c\u5f97\u5206\u70ba 0.07/1\u3002\u6211\u5011\u767c\u73fe UNet \u6216 Transformer \u4e2d\u7684\u8de8\u6a21\u614b\u5c0d\u9f4a\u5728\u8655\u7406\u8a9e\u7fa9\u8b8a\u7570\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u9019\u662f\u4e00\u500b\u4ee5\u524d\u88ab\u5c08\u6ce8\u65bc\u6587\u5b57\u7de8\u78bc\u5668\u6240\u5ffd\u7565\u7684\u56e0\u7d20\u3002\u6211\u5011\u7684\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u500b\u6709\u6548\u7684\u8a55\u4f30\u6846\u67b6\uff0c\u4fc3\u9032\u4e86 T2I \u5408\u6210\u793e\u7fa4\u5c0d\u4eba\u985e\u6307\u4ee4\u7406\u89e3\u7684\u63a2\u7d22\u3002", "author": "Xiangru Zhu et.al.", "authors": "Xiangru Zhu, Penglei Sun, Yaoxian Song, Yanghua Xiao, Zhixu Li, Chengyu Wang, Jun Huang, Bei Yang, Xiaoxiao Xu", "id": "2410.10291v1", "paper_url": "http://arxiv.org/abs/2410.10291v1", "repo": "https://github.com/zhuxiangru/semvarbench"}}