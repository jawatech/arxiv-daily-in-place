{"2408.06854": {"publish_time": "2024-08-13", "title": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models", "paper_summary": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C", "paper_summary_zh": "\u5fae\u8c03\u5177\u6709\u9ad8\u53c2\u6570\u6548\u7387\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ee5\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u5df2\u6210\u4e3a\u4e00\u79cd\u65b0\u8303\u4f8b\u3002\u4f4e\u79e9\u81ea\u9002\u5e94 (LoRA) \u5927\u5e45\u51cf\u5c11\u4e86\u5fae\u8c03\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002\u867d\u7136\u5b83\u5df2\u5c55\u793a\u51fa\u503c\u5f97\u79f0\u9053\u7684\u6027\u80fd\uff0c\u4f46\u5728\u5355\u4e2a\u5c3a\u5ea6\u5185\u66f4\u65b0\u53c2\u6570\u53ef\u80fd\u4e0d\u662f\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u7684\u6700\u4f73\u9009\u62e9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 LoRA \u6269\u5c55\u5230\u591a\u4e2a\u5c3a\u5ea6\uff0c\u79f0\u4e3a LoRA^2\u3002\u6211\u4eec\u9996\u5148\u7ed3\u5408\u6b63\u4ea4\u6295\u5f71\u7406\u8bba\uff0c\u5728\u4e24\u4e2a\u76f8\u4e92\u6b63\u4ea4\u7684\u5e73\u9762\u4e0a\u8bad\u7ec3\u4e00\u7ec4 LoRA\u3002\u7136\u540e\uff0c\u6211\u4eec\u6539\u8fdb\u4e86\u91cd\u8981\u6027\u8bc4\u5206\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u53c2\u6570\u654f\u611f\u6027\u8bc4\u5206\u8ba1\u7b97\u51cf\u5c11\u4e86\u5927\u7ea6 98.5%\u3002\u901a\u8fc7\u4fee\u526a\u5177\u6709\u8f83\u4f4e\u91cd\u8981\u6027\u8bc4\u5206\u7684\u5947\u5f02\u503c\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5bf9\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u5e94\u6027\u3002\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1 LoRA^2 \u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u6bd4\uff0c\u5b83\u5c06\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\u663e\u7740\u51cf\u5c11\u5230\u4ec5 0.72%\uff0c\u540c\u65f6\u4ecd\u7136\u63d0\u4f9b\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u5373\u4f7f\u5c06\u53c2\u6570\u8fdb\u4e00\u6b65\u51cf\u5c11\u5230 0.17M\uff0c\u5b83\u4ecd\u7136\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5177\u6709\u591a 8 \u500d\u53c2\u6570\u7684\u57fa\u7ebf\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728\u6b64\u5904\u83b7\u5f97\uff1ahttps://anonymous.4open.science/r/LoRA-2-5B4C", "author": "Jia-Chen Zhang et.al.", "authors": "Jia-Chen Zhang, Yu-Jie Xiong, He-Xi Qiu, Dong-Hai Zhu, Chun-Ming Xia", "id": "2408.06854v1", "paper_url": "http://arxiv.org/abs/2408.06854v1", "repo": "null"}}