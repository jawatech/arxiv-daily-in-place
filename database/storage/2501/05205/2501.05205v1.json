{"2501.05205": {"publish_time": "2025-01-09", "title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning", "paper_summary": "Infants develop complex visual understanding rapidly, even preceding of the\nacquisition of linguistic inputs. As computer vision seeks to replicate the\nhuman vision system, understanding infant visual development may offer valuable\ninsights. In this paper, we present an interdisciplinary study exploring this\nquestion: can a computational model that imitates the infant learning process\ndevelop broader visual concepts that extend beyond the vocabulary it has heard,\nsimilar to how infants naturally learn? To investigate this, we analyze a\nrecently published model in Science by Vong et al.,which is trained on\nlongitudinal, egocentric images of a single child paired with transcribed\nparental speech. We introduce a training-free framework that can discover\nvisual concept neurons hidden in the model's internal representations. Our\nfindings show that these neurons can classify objects outside its original\nvocabulary. Furthermore, we compare the visual representations in infant-like\nmodels with those in moder computer vision models, such as CLIP or ImageNet\npre-trained model, highlighting key similarities and differences. Ultimately,\nour work bridges cognitive science and computer vision by analyzing the\ninternal representations of a computational model trained on an infant's visual\nand linguistic inputs.", "paper_summary_zh": "\u5b30\u5152\u767c\u5c55\u51fa\u8907\u96dc\u7684\u8996\u89ba\u7406\u89e3\uff0c\u751a\u81f3\u65e9\u65bc\u8a9e\u8a00\u8f38\u5165\u7684\u7372\u5f97\u3002\u7531\u65bc\u96fb\u8166\u8996\u89ba\u8a66\u5716\u8907\u88fd\u4eba\u985e\u8996\u89ba\u7cfb\u7d71\uff0c\u4e86\u89e3\u5b30\u5152\u8996\u89ba\u767c\u5c55\u53ef\u80fd\u6703\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u9805\u63a2\u7d22\u9019\u500b\u554f\u984c\u7684\u8de8\u5b78\u79d1\u7814\u7a76\uff1a\u4e00\u500b\u6a21\u4eff\u5b30\u5152\u5b78\u7fd2\u904e\u7a0b\u7684\u8a08\u7b97\u6a21\u578b\u662f\u5426\u80fd\u767c\u5c55\u51fa\u8d85\u8d8a\u5176\u807d\u5230\u7684\u8a5e\u5f59\u7684\u66f4\u5ee3\u6cdb\u7684\u8996\u89ba\u6982\u5ff5\uff0c\u985e\u4f3c\u65bc\u5b30\u5152\u81ea\u7136\u5b78\u7fd2\u7684\u65b9\u5f0f\uff1f\u70ba\u4e86\u8abf\u67e5\u9019\u4e00\u9ede\uff0c\u6211\u5011\u5206\u6790\u4e86 Vong \u7b49\u4eba\u5728 Science \u4e0a\u6700\u8fd1\u767c\u8868\u7684\u6a21\u578b\uff0c\u8a72\u6a21\u578b\u8a13\u7df4\u65bc\u4e00\u500b\u5b69\u5b50\u7684\u7e31\u5411\u81ea\u6211\u4e2d\u5fc3\u5f71\u50cf\uff0c\u4e26\u914d\u6709\u8f49\u9304\u7684\u7236\u6bcd\u8a9e\u8a00\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u7121\u9700\u8a13\u7df4\u7684\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u5728\u6a21\u578b\u7684\u5167\u90e8\u8868\u793a\u4e2d\u767c\u73fe\u96b1\u85cf\u7684\u8996\u89ba\u6982\u5ff5\u795e\u7d93\u5143\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u9019\u4e9b\u795e\u7d93\u5143\u53ef\u4ee5\u5c0d\u5176\u539f\u59cb\u8a5e\u5f59\u4e4b\u5916\u7684\u7269\u9ad4\u9032\u884c\u5206\u985e\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u985e\u5b30\u5152\u6a21\u578b\u4e2d\u7684\u8996\u89ba\u8868\u793a\u8207\u73fe\u4ee3\u96fb\u8166\u8996\u89ba\u6a21\u578b\uff08\u4f8b\u5982 CLIP \u6216 ImageNet \u9810\u8a13\u7df4\u6a21\u578b\uff09\u4e2d\u7684\u8996\u89ba\u8868\u793a\u9032\u884c\u6bd4\u8f03\uff0c\u7a81\u51fa\u4e86\u95dc\u9375\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u7570\u3002\u6700\u7d42\uff0c\u6211\u5011\u7684\u7814\u7a76\u901a\u904e\u5206\u6790\u5728\u5b30\u5152\u7684\u8996\u89ba\u548c\u8a9e\u8a00\u8f38\u5165\u4e0a\u8a13\u7df4\u7684\u8a08\u7b97\u6a21\u578b\u7684\u5167\u90e8\u8868\u793a\uff0c\u67b6\u8d77\u4e86\u8a8d\u77e5\u79d1\u5b78\u548c\u96fb\u8166\u8996\u89ba\u4e4b\u9593\u7684\u6a4b\u6a11\u3002", "author": "Xueyi Ke et.al.", "authors": "Xueyi Ke, Satoshi Tsutsui, Yayun Zhang, Bihan Wen", "id": "2501.05205v1", "paper_url": "http://arxiv.org/abs/2501.05205v1", "repo": "null"}}