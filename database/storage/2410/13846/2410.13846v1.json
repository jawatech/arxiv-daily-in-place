{"2410.13846": {"publish_time": "2024-10-17", "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction", "paper_summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u5c07\u5176\u529f\u80fd\u64f4\u5c55\u5230\u8655\u7406\u9577\u6587\u8108\u3002\u7136\u800c\uff0c\u589e\u52a0\u6a21\u578b\u5c64\u6578\u548c\u8f38\u5165\u5e8f\u5217\u7684\u9577\u5ea6\u6703\u986f\u8457\u589e\u52a0\u5132\u5b58\u9375\u503c (KV) \u5feb\u53d6\u6240\u9700\u7684\u8a18\u61b6\u9ad4\uff0c\u5c0d\u6709\u6548\u63a8\u8ad6\u69cb\u6210\u6311\u6230\u3002\u70ba\u4e86\u6e1b\u8f15\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SimLayerKV\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u9078\u64c7\u6027\u5730\u5728\u5df2\u8b58\u5225\u7684\u60f0\u6027\u5c64\u4e2d\u6368\u68c4\u5feb\u53d6\uff0c\u4f86\u6e1b\u5c11\u5c64\u9593 KV \u5feb\u53d6\u7684\u5197\u9918\u3002\u6211\u5011\u7684\u505a\u6cd5\u57fa\u65bc\u4ee5\u4e0b\u89c0\u5bdf\uff1a\u9577\u6587\u8108 LLM \u4e2d\u7684\u67d0\u4e9b\u5c64\u8868\u73fe\u51fa\u300c\u60f0\u6027\u300d\u884c\u70ba\uff0c\u8207\u975e\u60f0\u6027\u5c64\u76f8\u6bd4\uff0c\u5c0d\u5efa\u6a21\u9577\u7a0b\u4f9d\u8cf4\u95dc\u4fc2\u7684\u8ca2\u737b\u8f03\u5c11\u3002\u900f\u904e\u5206\u6790\u6ce8\u610f\u529b\u6b0a\u91cd\u6a21\u5f0f\uff0c\u6211\u5011\u767c\u73fe\u9019\u4e9b\u60f0\u6027\u5c64\u7684\u884c\u70ba\u5728\u7d66\u5b9a\u8f38\u5165\u7684\u7522\u751f\u904e\u7a0b\u4e2d\uff0c\u5728\u5404\u500b\u7b26\u865f\u4e4b\u9593\u662f\u4e00\u81f4\u7684\u3002\u6b64\u898b\u89e3\u6fc0\u52f5\u4e86\u6211\u5011\u7684 SimLayerKV\uff0c\u5b83\u53ef\u4ee5\u8b58\u5225\u60f0\u6027\u5c64\u4e26\u76f8\u61c9\u5730\u6e1b\u5c11\u5176 KV \u5feb\u53d6\u3002SimLayerKV \u7121\u9700\u8a13\u7df4\u3001\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e14\u50c5\u9700\u4e03\u884c\u7a0b\u5f0f\u78bc\u5373\u53ef\u5be6\u4f5c\u3002\u6211\u5011\u5c0d\u4e09\u500b\u5177\u4ee3\u8868\u6027\u7684 LLM \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4f8b\u5982 LLaMA2-7B\u3001LLaMA3-8B \u548c Mistral-7B\uff0c\u6db5\u84cb\u4e86 LongBench \u57fa\u6e96\u4e2d\u7684 16 \u9805\u4efb\u52d9\u3002\u7d50\u679c\u8868\u660e\uff0c\u7576\u8207 4 \u4f4d\u5143\u91cf\u5316\u7d50\u5408\u6642\uff0cSimLayerKV \u53ef\u5be6\u73fe 5 \u500d\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u6bd4\uff0c\u6548\u80fd\u4e0b\u964d\u50c5\u70ba 1.2%\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/sail-sg/SimLayerKV \u53d6\u5f97\u3002</paragraph>", "author": "Xuan Zhang et.al.", "authors": "Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin", "id": "2410.13846v1", "paper_url": "http://arxiv.org/abs/2410.13846v1", "repo": "null"}}