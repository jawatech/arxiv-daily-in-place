{"2501.09368": {"publish_time": "2025-01-16", "title": "Aligning Instruction Tuning with Pre-training", "paper_summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose *Aligning Instruction Tuning\nwith Pre-training* (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.", "paper_summary_zh": "\u6307\u4ee4\u5fae\u8abf\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u9075\u5faa\u4eba\u985e\u6307\u4ee4\u57f7\u884c\u5404\u7a2e\u4efb\u52d9\uff0c\u4e26\u4f9d\u8cf4\u65bc\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\u96c6\u4f86\u5f15\u5c0e\u884c\u70ba\u3002\u7136\u800c\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u7121\u8ad6\u662f\u624b\u52d5\u6574\u7406\u6216\u5408\u6210\u7522\u751f\uff0c\u901a\u5e38\u90fd\u904e\u65bc\u72f9\u9698\uff0c\u8207\u9810\u8a13\u7df4\u671f\u9593\u64f7\u53d6\u7684\u5ee3\u6cdb\u5206\u4f48\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86 LLM \u7684\u6982\u5316\u548c\u9810\u8a13\u7df4\u77e5\u8b58\u7684\u6709\u6548\u4f7f\u7528\u3002\u6211\u5011\u63d0\u51fa\u300c\u5c07\u6307\u4ee4\u5fae\u8abf\u8207\u9810\u8a13\u7df4\u5c0d\u9f4a\u300d(AITP)\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u627e\u51fa\u6307\u4ee4\u5fae\u8abf\u8cc7\u6599\u96c6\u4e2d\u7684\u6db5\u84cb\u7bc4\u570d\u4e0d\u8db3\u4e4b\u8655\uff0c\u4e26\u5c07\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u6539\u5beb\u70ba\u9ad8\u54c1\u8cea\u7684\u6307\u4ee4\u56de\u61c9\u914d\u5c0d\uff0c\u4f86\u5f4c\u88dc\u6b64\u5dee\u8ddd\u7684\u65b9\u6cd5\u3002\u9019\u7a2e\u65b9\u6cd5\u8c50\u5bcc\u4e86\u8cc7\u6599\u96c6\u7684\u591a\u6a23\u6027\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u7279\u5b9a\u4efb\u52d9\u7684\u76ee\u6a19\u3002\u5728\u516b\u500b\u57fa\u6e96\u4e0a\u5c0d\u4e09\u500b\u5b8c\u5168\u958b\u653e\u7684 LLM \u9032\u884c\u8a55\u4f30\uff0c\u8b49\u660e AITP \u5177\u6709\u6301\u7e8c\u7684\u6548\u80fd\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u7a81\u51fa\u4e86\u81ea\u9069\u61c9\u8cc7\u6599\u9078\u64c7\u3001\u53d7\u63a7\u6539\u5beb\u548c\u5e73\u8861\u6574\u5408\u7684\u512a\u9ede\uff0c\u5f37\u8abf\u5c07\u6307\u4ee4\u5fae\u8abf\u8207\u9810\u8a13\u7df4\u5206\u4f48\u5c0d\u9f4a\uff0c\u4ee5\u767c\u63ee LLM \u7684\u5168\u90e8\u6f5b\u529b\u3002", "author": "Yiming Liang et.al.", "authors": "Yiming Liang, Tianyu Zheng, Xinrun Du, Ge Zhang, Xingwei Qu, Xiang Yue, Chujie Zheng, Jiaheng Liu, Lei Ma, Wenhu Chen, Guoyin Wang, Zhaoxiang Zhang, Wenhao Huang, Jiajun Zhang", "id": "2501.09368v1", "paper_url": "http://arxiv.org/abs/2501.09368v1", "repo": "null"}}