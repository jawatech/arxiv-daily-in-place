{"2405.14734": {"publish_time": "2024-05-23", "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward", "paper_summary": "Direct Preference Optimization (DPO) is a widely used offline preference\noptimization algorithm that reparameterizes reward functions in reinforcement\nlearning from human feedback (RLHF) to enhance simplicity and training\nstability. In this work, we propose SimPO, a simpler yet more effective\napproach. The effectiveness of SimPO is attributed to a key design: using the\naverage log probability of a sequence as the implicit reward. This reward\nformulation better aligns with model generation and eliminates the need for a\nreference model, making it more compute and memory efficient. Additionally, we\nintroduce a target reward margin to the Bradley-Terry objective to encourage a\nlarger margin between the winning and losing responses, further enhancing the\nalgorithm's performance. We compare SimPO to DPO and its latest variants across\nvarious state-of-the-art training setups, including both base and\ninstruction-tuned models like Mistral and Llama3. We evaluated on extensive\ninstruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the\nrecent challenging Arena-Hard benchmark. Our results demonstrate that SimPO\nconsistently and significantly outperforms existing approaches without\nsubstantially increasing response length. Specifically, SimPO outperforms DPO\nby up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our\ntop-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7\nlength-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the\nleaderboard, and a 33.8 win rate on Arena-Hard -- making it the strongest 8B\nopen-source model.", "paper_summary_zh": "\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u662f\u4e00\u7a2e\u5ee3\u6cdb\u4f7f\u7528\u7684\u96e2\u7dda\u504f\u597d\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\uff0c\u5b83\u6703\u91cd\u65b0\u53c3\u6578\u5316\u4eba\u985e\u56de\u994b (RLHF) \u4e2d\u5f37\u5316\u5b78\u7fd2\u7684\u734e\u52f5\u51fd\u6578\uff0c\u4ee5\u63d0\u5347\u7c21\u6f54\u6027\u548c\u8a13\u7df4\u7a69\u5b9a\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa SimPO\uff0c\u4e00\u7a2e\u66f4\u7c21\u55ae\u4f46\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002SimPO \u7684\u6709\u6548\u6027\u6b78\u529f\u65bc\u4e00\u500b\u95dc\u9375\u8a2d\u8a08\uff1a\u4f7f\u7528\u5e8f\u5217\u7684\u5e73\u5747\u5c0d\u6578\u6a5f\u7387\u4f5c\u70ba\u5167\u96b1\u734e\u52f5\u3002\u6b64\u734e\u52f5\u516c\u5f0f\u66f4\u7b26\u5408\u6a21\u578b\u751f\u6210\uff0c\u4e26\u6d88\u9664\u4e86\u5c0d\u53c3\u8003\u6a21\u578b\u7684\u9700\u6c42\uff0c\u4f7f\u5176\u66f4\u5177\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728 Bradley-Terry \u76ee\u6a19\u4e2d\u5f15\u5165\u4e86\u76ee\u6a19\u734e\u52f5\u908a\u969b\uff0c\u4ee5\u9f13\u52f5\u7372\u52dd\u548c\u5931\u6557\u56de\u61c9\u4e4b\u9593\u6709\u66f4\u5927\u7684\u908a\u969b\uff0c\u9032\u4e00\u6b65\u63d0\u5347\u6f14\u7b97\u6cd5\u7684\u6548\u80fd\u3002\u6211\u5011\u5728\u5404\u7a2e\u6700\u5148\u9032\u7684\u8a13\u7df4\u8a2d\u5b9a\u4e2d\u6bd4\u8f03\u4e86 SimPO \u548c DPO \u53ca\u5176\u6700\u65b0\u8b8a\u9ad4\uff0c\u5305\u62ec\u57fa\u790e\u6a21\u578b\u548c Mistral \u548c Llama3 \u7b49\u7d93\u904e\u6307\u4ee4\u8abf\u6574\u7684\u6a21\u578b\u3002\u6211\u5011\u5728\u5ee3\u6cdb\u7684\u6307\u4ee4\u9075\u5faa\u57fa\u6e96\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u5305\u62ec AlpacaEval 2\u3001MT-Bench \u548c\u6700\u8fd1\u5177\u6709\u6311\u6230\u6027\u7684 Arena-Hard \u57fa\u6e96\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0cSimPO \u6301\u7e8c\u4e14\u986f\u8457\u5730\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u800c\u4e0d\u6703\u5927\u5e45\u589e\u52a0\u56de\u61c9\u9577\u5ea6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSimPO \u5728 AlpacaEval 2 \u4e0a\u6bd4 DPO \u9ad8\u51fa 6.4 \u5206\uff0c\u5728 Arena-Hard \u4e0a\u9ad8\u51fa 7.5 \u5206\u3002\u6211\u5011\u5efa\u7acb\u5728 Llama3-8B-Instruct \u4e0a\u7684\u6548\u80fd\u6700\u4f73\u6a21\u578b\u5728 AlpacaEval 2 \u4e0a\u9054\u5230\u4e86\u9a5a\u4eba\u7684 44.7 \u9577\u5ea6\u63a7\u5236\u7372\u52dd\u7387\uff0c\u5728\u6392\u884c\u699c\u4e0a\u8d85\u8d8a\u4e86 Claude 3 Opus\uff0c\u5728 Arena-Hard \u4e0a\u9054\u5230\u4e86 33.8 \u7684\u7372\u52dd\u7387\uff0c\u4f7f\u5176\u6210\u70ba\u6700\u5f37\u5927\u7684 8B \u958b\u6e90\u6a21\u578b\u3002", "author": "Yu Meng et.al.", "authors": "Yu Meng, Mengzhou Xia, Danqi Chen", "id": "2405.14734v1", "paper_url": "http://arxiv.org/abs/2405.14734v1", "repo": "https://github.com/princeton-nlp/simpo"}}