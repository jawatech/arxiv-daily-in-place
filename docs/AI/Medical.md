
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-Pérez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v1](http://arxiv.org/abs/2409.02681v1)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|null|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|
|**2024-08-31**|**Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**|Mikhail Borisenkov et.al.|[2409.00310v1](http://arxiv.org/abs/2409.00310v1)|null|
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-30**|**Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**|Yuhan Zheng et.al.|[2409.00163v1](http://arxiv.org/abs/2409.00163v1)|null|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377v1](http://arxiv.org/abs/2408.17377v1)|null|
|**2024-08-30**|**Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**|Jutika Borah et.al.|[2408.17011v2](http://arxiv.org/abs/2408.17011v2)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-29**|**Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**|Yifei Chen et.al.|[2408.16343v1](http://arxiv.org/abs/2408.16343v1)|[link](https://github.com/justlfc03/mstnet)|
|**2024-08-29**|**Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**|Xia Jiang et.al.|[2408.16256v1](http://arxiv.org/abs/2408.16256v1)|null|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213v1](http://arxiv.org/abs/2408.16213v1)|null|
|**2024-08-28**|**A Survey on Evaluation of Multimodal Large Language Models**|Jiaxing Huang et.al.|[2408.15769v1](http://arxiv.org/abs/2408.15769v1)|null|
|**2024-08-28**|**Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**|Yijun Zhou et.al.|[2408.15498v1](http://arxiv.org/abs/2408.15498v1)|null|
|**2024-08-27**|**What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**|Jana Schaich Borg et.al.|[2408.15354v1](http://arxiv.org/abs/2408.15354v1)|null|
|**2024-08-27**|**Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**|Weiyi Zhang et.al.|[2408.15217v1](http://arxiv.org/abs/2408.15217v1)|null|
|**2024-08-27**|**Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**|Daniil Filienko et.al.|[2409.00112v1](http://arxiv.org/abs/2409.00112v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-27**|**MiWaves Reinforcement Learning Algorithm**|Susobhan Ghosh et.al.|[2408.15076v1](http://arxiv.org/abs/2408.15076v1)|[link](https://github.com/statisticalreinforcementlearninglab/miwaves_rl_service)|
|**2024-08-27**|**Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**|Yuqi Zhang et.al.|[2408.15032v1](http://arxiv.org/abs/2408.15032v1)|[link](https://github.com/yuqizhang-buaa/mamba2mil)|
|**2024-08-27**|**Sequence-aware Pre-training for Echocardiography Probe Guidance**|Haojun Jiang et.al.|[2408.15026v1](http://arxiv.org/abs/2408.15026v1)|null|
|**2024-08-27**|**Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**|Christos Theodoropoulos et.al.|[2408.15294v2](http://arxiv.org/abs/2408.15294v2)|null|
|**2024-08-27**|**Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**|Qiaoxin Li et.al.|[2408.14754v1](http://arxiv.org/abs/2408.14754v1)|null|
|**2024-08-27**|**Large Language Models for Disease Diagnosis: A Scoping Review**|Shuang Zhou et.al.|[2409.00097v1](http://arxiv.org/abs/2409.00097v1)|null|
|**2024-08-26**|**Improving Clinical Note Generation from Complex Doctor-Patient Conversation**|Yizhan Li et.al.|[2408.14568v1](http://arxiv.org/abs/2408.14568v1)|null|
|**2024-08-26**|**Temporal Ensemble Logic**|Guo-Qiang Zhang et.al.|[2408.14443v2](http://arxiv.org/abs/2408.14443v2)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v2](http://arxiv.org/abs/2408.14418v2)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Foundation Models for Music: A Survey**|Yinghao Ma et.al.|[2408.14340v3](http://arxiv.org/abs/2408.14340v3)|[link](https://github.com/nicolaus625/fm4music)|
|**2024-08-26**|**Uncertainties of Latent Representations in Computer Vision**|Michael Kirchhof et.al.|[2408.14281v1](http://arxiv.org/abs/2408.14281v1)|null|
|**2024-08-26**|**Automatic Medical Report Generation: Methods and Applications**|Li Guo et.al.|[2408.13988v1](http://arxiv.org/abs/2408.13988v1)|null|
|**2024-08-25**|**Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**|Seyed Amir Ahmad Safavi-Naini et.al.|[2409.00084v2](http://arxiv.org/abs/2409.00084v2)|[link](https://github.com/sdamirsa/llm-vlm-in-gastroenterology)|
|**2024-08-25**|**PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**|Zifan Chen et.al.|[2408.13836v1](http://arxiv.org/abs/2408.13836v1)|null|
|**2024-08-24**|**Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**|Andrés Catalino Castillo Jiménez et.al.|[2408.13683v2](http://arxiv.org/abs/2408.13683v2)|null|
|**2024-08-24**|**Towards Case-based Interpretability for Medical Federated Learning**|Laura Latorre et.al.|[2408.13626v1](http://arxiv.org/abs/2408.13626v1)|null|
|**2024-08-23**|**HBIC: A Biclustering Algorithm for Heterogeneous Datasets**|Adán José-García et.al.|[2408.13217v1](http://arxiv.org/abs/2408.13217v1)|[link](https://github.com/clementchauvet/py-hbic)|
|**2024-08-23**|**Causal machine learning for sustainable agroecosystems**|Vasileios Sitokonstantinou et.al.|[2408.13155v1](http://arxiv.org/abs/2408.13155v1)|null|
|**2024-08-23**|**Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey**|Qika Lin et.al.|[2408.12880v1](http://arxiv.org/abs/2408.12880v1)|[link](https://github.com/deepreasoning/aihealth)|
|**2024-08-23**|**COVID-19 Probability Prediction Using Machine Learning: An Infectious Approach**|Mohsen Asghari Ilani et.al.|[2408.12841v1](http://arxiv.org/abs/2408.12841v1)|null|
|**2024-08-23**|**Exploring Machine Learning Models for Lung Cancer Level Classification: A comparative ML Approach**|Mohsen Asghari Ilani et.al.|[2408.12838v1](http://arxiv.org/abs/2408.12838v1)|null|
|**2024-08-23**|**Phrasing for UX: Enhancing Information Engagement through Computational Linguistics and Creative Analytics**|Nimrod Dvir et.al.|[2409.00064v1](http://arxiv.org/abs/2409.00064v1)|null|
|**2024-08-22**|**From Radiologist Report to Image Label: Assessing Latent Dirichlet Allocation in Training Neural Networks for Orthopedic Radiograph Classification**|Jakub Olczak et.al.|[2408.13284v1](http://arxiv.org/abs/2408.13284v1)|null|
|**2024-08-22**|**MultiMed: Massively Multimodal and Multitask Medical Understanding**|Shentong Mo et.al.|[2408.12682v1](http://arxiv.org/abs/2408.12682v1)|null|
|**2024-08-22**|**RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment**|Xiaohan Wang et.al.|[2408.12579v1](http://arxiv.org/abs/2408.12579v1)|null|
|**2024-08-22**|**Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE 2023 Challenge**|Jun Ma et.al.|[2408.12534v1](http://arxiv.org/abs/2408.12534v1)|null|
|**2024-08-22**|**MEDCO: Medical Education Copilots Based on A Multi-Agent Framework**|Hao Wei et.al.|[2408.12496v1](http://arxiv.org/abs/2408.12496v1)|null|
|**2024-08-22**|**AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**|Douwe J. Spaanderman et.al.|[2408.12491v1](http://arxiv.org/abs/2408.12491v1)|null|
|**2024-08-22**|**WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation**|Palak Handa et.al.|[2408.12466v1](http://arxiv.org/abs/2408.12466v1)|[link](https://github.com/misahub2023/benchmarking-codes-of-the-wcebleedgen-dataset)|
|**2024-08-22**|**SAM-SP: Self-Prompting Makes SAM Great Again**|Chunpeng Zhou et.al.|[2408.12364v1](http://arxiv.org/abs/2408.12364v1)|null|
|**2024-08-22**|**Class-balanced Open-set Semi-supervised Object Detection for Medical Images**|Zhanyun Lu et.al.|[2408.12355v1](http://arxiv.org/abs/2408.12355v1)|null|
|**2024-08-22**|**Large Language Models Are Self-Taught Reasoners: Enhancing LLM Applications via Tailored Problem-Solving Demonstrations**|Kai Tzu-iunn Ong et.al.|[2408.12315v1](http://arxiv.org/abs/2408.12315v1)|null|
|**2024-08-22**|**Tipta uzmanlik sinavinda (tus) buyuk dil modelleri insanlardan daha mi basarili?**|Yesim Aygul et.al.|[2408.12305v2](http://arxiv.org/abs/2408.12305v2)|null|
|**2024-08-22**|**Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features**|Shaoxiang Dang et.al.|[2408.12279v1](http://arxiv.org/abs/2408.12279v1)|null|
|**2024-08-22**|**LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction**|Aishik Nagar et.al.|[2408.12249v1](http://arxiv.org/abs/2408.12249v1)|null|
|**2024-08-22**|**MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**|Yanzeng Li et.al.|[2408.12236v1](http://arxiv.org/abs/2408.12236v1)|null|
|**2024-08-22**|**MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents**|Congchi Yin et.al.|[2408.12142v1](http://arxiv.org/abs/2408.12142v1)|[link](https://github.com/lemonsis/mdd-5k)|
|**2024-08-22**|**DRExplainer: Quantifiable Interpretability in Drug Response Prediction with Directed Graph Convolutional Network**|Haoyuan Shi et.al.|[2408.12139v1](http://arxiv.org/abs/2408.12139v1)|[link](https://github.com/vshy-dream/drexplainer)|
|**2024-08-22**|**Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards**|Shresth Verma et.al.|[2408.12112v1](http://arxiv.org/abs/2408.12112v1)|null|
|**2024-08-22**|**uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization**|Aishik Nagar et.al.|[2408.12095v2](http://arxiv.org/abs/2408.12095v2)|null|
|**2024-08-21**|**Federated Diabetes Prediction in Canadian Adults Using Real-world Cross-Province Primary Care Data**|Guojun Tang et.al.|[2408.12029v1](http://arxiv.org/abs/2408.12029v1)|null|
|**2024-08-21**|**Exploring Large Language Models for Feature Selection: A Data-centric Perspective**|Dawei Li et.al.|[2408.12025v1](http://arxiv.org/abs/2408.12025v1)|null|
|**2024-08-21**|**Clinical Insights: A Comprehensive Review of Language Models in Medicine**|Nikita Neveditsin et.al.|[2408.11735v2](http://arxiv.org/abs/2408.11735v2)|null|
|**2024-08-21**|**BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports**|Yuxuan Chen et.al.|[2408.11334v1](http://arxiv.org/abs/2408.11334v1)|null|
|**2024-08-21**|**Probabilistic Medical Predictions of Large Language Models**|Bowen Gu et.al.|[2408.11316v1](http://arxiv.org/abs/2408.11316v1)|null|
|**2024-08-21**|**Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks**|Yining Hua et.al.|[2408.11288v1](http://arxiv.org/abs/2408.11288v1)|null|
|**2024-08-21**|**BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation**|Haotian Peng et.al.|[2408.11281v1](http://arxiv.org/abs/2408.11281v1)|[link](https://github.com/hatton613/bearllm)|
|**2024-08-20**|**From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis**|Guy Lutsker et.al.|[2408.11876v1](http://arxiv.org/abs/2408.11876v1)|null|
|**2024-08-20**|**Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology**|Yihao Hou et.al.|[2408.10715v1](http://arxiv.org/abs/2408.10715v1)|null|

#### Abstracts
##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

摘要：<paragraph>目的：使用不同掃描儀和協定取得的醫學影像，在影像外觀上可能會有顯著差異。這種現象稱為掃描儀領域偏移，可能會導致深度神經網路的效能下降，而這些網路是針對由一種掃描儀取得的資料進行訓練，並在另一種掃描儀上進行測試。這個重要的實際問題已獲得廣泛認可，但目前尚未針對不同形式和診斷任務進行系統性研究。材料和方法：在本文中，我們提出了一項廣泛的實驗研究，評估掃描儀領域偏移對不同自動化診斷任務的卷積神經網路效能的影響。我們在常見的放射學形式中評估這種現象，包括 X 光、電腦斷層掃描和磁振造影。結果：我們發現，來自不同掃描儀的資料在網路上的效能幾乎總是比來自相同掃描儀的資料差，我們量化了不同資料集效能下降的程度。值得注意的是，我們發現這種下降在磁振造影中最为嚴重，在 X 光中為中等，在電腦斷層掃描中相當小，平均而言，我們將其歸因於電腦斷層掃描取得系統的標準化性質，而磁振造影或 X 光中不存在這種性質。我們還研究了將不同數量的目標領域資料注入訓練集，以及向訓練資料加入雜訊，如何有助於泛化。結論：我們的結果提供了廣泛的實驗證據，並量化了深度學習中由掃描儀領域偏移造成的效能下降程度，目標是引導未來針對醫學影像分析的強健深度學習模型的發展。</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

摘要：生存分析是統計學的一個分支，用於建模特定事件發生的時間，並廣泛用於醫學、工程、金融和許多其他領域。在選擇生存模型時，通常在性能和可解釋性之間進行權衡，其中最高性能是由基於深度學習的黑盒模型實現的。這在醫學等領域是一個主要問題，因為從業者不願意盲目信任黑盒模型來做出重要的患者決策。Kolmogorov-阿諾德網絡 (KAN) 最近被提議作為多層感知器 (MLP) 的可解釋且準確的替代方案。我們引入了 CoxKAN，這是一個用於可解釋、高性能生存分析的 Cox 比例風險 Kolmogorov-Arnold 網絡。我們在 4 個合成數據集和 9 個真實醫療數據集上評估了所提出的 CoxKAN。合成實驗表明，CoxKAN 準確地恢復了風險函數的可解釋符號公式，並有效地執行自動特徵選擇。對 9 個真實數據集的評估表明，CoxKAN 始終優於 Cox 比例風險模型，並且達到了優於或與調整後的 MLP 相當的性能。此外，我們發現 CoxKAN 識別了預測變量之間的複雜交互作用，這些交互作用使用現有的生存方法極難識別，並自動找到揭示重要生物標誌物對患者風險的準確影響的符號公式。

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

摘要：多器官疾病由於同時影響多個器官系統，因此會帶來重大的挑戰，需要複雜且具有適應性的治療策略。儘管 AI 驅動的醫療保健決策支援系統最近有進展，但現有解決方案僅限於個別器官系統。它們常常忽略器官系統之間的複雜依賴性，因此無法提供實務上有用的整體治療建議。我們提出一個新穎的分層多智能體強化學習 (HMARL) 架構來解決這些挑戰。此架構為每個器官系統使用專用智能體，並透過明確的智能體間通訊管道建模動態，讓不同器官之間的治療策略能夠協調。此外，我們引入雙層狀態表示技術，在各種層級語境化病患狀況，以提升治療準確性和相關性。透過在敗血症（一種複雜的多器官疾病）管理中進行廣泛的定性和定量評估，我們的做法展示了它學習有效治療政策的能力，可顯著改善病患存活率。此架構標誌著臨床決策支援系統的一大進步，開創了多器官治療建議的全面性方法。

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

摘要：自動醫學影像敘述生成旨在透過直接從放射影像產生精確的臨床描述，減輕放射科醫師的工作負擔。然而，與一般影像標題任務相比，醫學影像中的細微視覺差異和特定領域術語會帶來重大挑戰。現有方法常常忽略正常與異常發現之間的重要區別，導致次佳效能。在這項工作中，我們提出 FODA-PG，這是一個新穎的細粒度器官疾病自適應分割圖形架構，透過領域自適應學習來解決這些限制。FODA-PG 透過將疾病相關屬性依據其臨床重要性和位置分為不同的「特定疾病」和「無疾病」類別，來建構放射學發現的細粒度圖形表示。這種自適應分割使我們的模型能夠捕捉正常與病理狀態之間的細微差異，減輕資料偏差的影響。透過將這種細粒度語義知識整合到強大的基於轉換器的架構中，並提供其有效性的嚴謹數學證明，FODA-PG 能夠生成精確且臨床上連貫的報告，並具備增強的概括能力。在 IU-Xray 和 MIMIC-CXR 基準上的廣泛實驗證明了我們的方法優於最先進的方法，突顯了領域適應在醫學報告生成中的重要性。

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

摘要：<paragraph>準確量化壁面剪應力動態對於基礎和應用研究中的各種應用具有實質性的重要性，涵蓋從人類健康到飛機設計和優化的領域。儘管在實驗測量技術和後處理演算法方面取得了顯著進展，但時間解析壁面剪應力動態仍具有足夠的空間解析度和在合適的空間域中仍然是一個難以捉摸的目標。為了解決這個差距，我們引入了一個深度學習架構，它從湍流壁面約束流的對數層中攝取壁面平行速度場，並輸出相應的 2D 壁面剪應力場，具有相同的空間解析度和域大小。從物理角度來看，我們的框架充當一個代理模型，概括了高能量外層流結構影響控制壁面剪應力動態的各種機制。該網路以監督方式在一個統一的數據集上進行訓練，該數據集包含統計 1D 湍流通道的直接數值模擬和空間發展的湍流邊界層流，摩擦雷諾數範圍從 390 到 1,500。我們展示了對從粒子影像測速測量中獲得的實驗速度場的零次應用，並使用微柱剪應力感測器對雷諾數最高 2,000 的同步壁面剪應力測量驗證了壁面剪應力估計的物理準確性。總之，所提出的框架為從容易獲得的速度測量中提取無法獲得的實驗壁面剪應力資訊奠定了基礎，從而促進了各種實驗應用中的進展。</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

摘要：本文提出了喉镜多模态分析系统 (MASL)，
该系统结合音频和视频数据，自动从喉部视频频闪镜视频中提取关键
片段和指标，用于临床评估。MASL 将声门检测与关键词识别相结合，以分析
患者发声并细化视频重点，以便更好地检查声带运动。该系统包括一个频闪视频提取模块，
该模块通过分析色相、饱和度和值波动来识别帧。
MASL 还为声带麻痹检测提供了有效的指标，
采用两阶段声门分割过程，使用 U-Net，然后进行基于扩散的细化以减少误报。MASL 不使用声门面积波形，而是从声门掩模中估计前声门角波形 (AGAW)，评估左右声带以检测单侧声带麻痹 (UVFP)。通过比较 AGAW 方差，MASL 区分左右麻痹。消融研究和对公共和真实世界数据集的实验验证了 MASL 的分割模块，并证明了其提供可靠的 UVFP 诊断指标的能力。

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

摘要：深度學習等自動化工具在醫學影像分割中使用率提升，減輕了手動輪廓描繪的瓶頸。這已將手動勞動轉移到自動輪廓的品質評估 (QA)，其中包含偵測錯誤並修正它們。半自動化 QA 的潛在解決方案是使用深度貝氏不確定性來建議潛在的錯誤區域，從而減少花費在錯誤偵測上的時間。先前的研究已調查不確定性和錯誤之間的對應關係，然而，尚未對改善貝氏不確定性地圖的「效用」進行研究，以使其僅出現在不準確區域，而不出現在準確區域。我們的研究使用準確度對抗不確定性 (AvU) 損失來訓練 FlipOut 模型，這會促使不確定性僅出現在不準確區域。我們將此方法應用於兩個放射治療部位的資料集，即頭頸部電腦斷層掃描和前列腺核磁共振掃描。使用接收器操作特性 (ROC) 和精確度召回率 (PR) 曲線，針對體素不準確性評估不確定性熱圖（即預測熵）。數值結果顯示，與貝氏基準相比，所提出的方法成功地抑制準確體素的不確定性，對於不準確體素的不確定性存在類似情況。可在 https://github.com/prerakmody/bayesuncertainty-error-correspondence 取得重現實驗的程式碼

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-Pérez, Silvia García-Méndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

摘要：<paragraph>根據官方的估計，全球約有 5000 萬人罹患失智症，且這個數字每年增加 1000 萬名新患者。在沒有治癒方法的情況下，臨床預後和早期介入是延緩其惡化的最有效方法。為此，人工智慧和計算語言學可被用於自然語言分析、個人化評估、監控和治療。然而，傳統方法需要更多語義知識管理和可解釋性能力。此外，儘管這些模型代表了使用智慧系統進行臨床患者溝通的最先進方式，但將大型語言模型 (LLM) 用於認知能力下降診斷仍然很少見。因此，我們利用聊天機器人解決方案中使用最新自然語言處理 (NLP) 技術的 LLM，以提供對認知能力下降的機器學習預測。語言概念特徵被用於適當的自然語言分析。透過可解釋性，我們旨在消除模型的潛在偏差，並提高其在診斷決策中協助臨床工作者的潛力。更詳細地說，所提出的管道包括：(i) 使用基於 NLP 的提示工程進行資料萃取；(ii) 串流式資料處理，包括特徵工程、分析和選擇；(iii) 即時分類；以及 (iv) 可解釋性儀表板，以提供預測結果的可視化和自然語言描述。分類結果在所有評估指標中都超過 80%，心智退化類別的召回率約為 85%。總而言之，我們為這項工作貢獻了一個經濟實惠、靈活、非侵入性、個人化的診斷系統。</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

摘要：全球人口快速老化趨勢導致失智症病例增加，包括阿茲海默症，突顯出早期且準確的診斷方法的迫切需求。傳統的診斷技術，例如認知測驗、神經影像和生物標記分析，在敏感性、可及性和成本方面面臨重大限制，特別是在早期階段。本研究探討機器學習 (ML) 作為一種變革性方法的潛力，通過利用 ML 模型分析和整合複雜的多模式數據集，包括認知評估、神經影像和遺傳信息，來增強早期失智症檢測。對現有文獻進行了全面回顧，以評估各種 ML 模型，包括監督學習、深度學習和先進技術，例如集成學習和Transformer模型，評估其準確性、可解釋性和臨床整合的潛力。研究結果表明，儘管 ML 模型在提高診斷精度和實現早期干預方面顯示出顯著的希望，但其可概化性、可解釋性和道德部署仍然存在挑戰。本研究最後概述了旨在增強 ML 模型在失智症檢測中的臨床效用的未來方向，強調跨學科合作和道德健全的框架，以改善阿茲海默症和其他形式失智症的早期檢測和干預策略。

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

摘要：皮膚癌分割在醫學影像分析中構成一項重大挑戰。現有許多解決方案（主要是基於 CNN）面臨缺乏整體背景理解的問題。或者，一些方法訴諸於大規模 Transformer 模型來彌合整體背景差距，但犧牲了模型大小和計算複雜度。最後，許多基於 Transformer 的方法主要依賴於基於 CNN 的解碼器，而忽視了基於 Transformer 的解碼模型的優點。認識到這些限制，我們通過引入 MobileUNETR 來解決對高效輕量級解決方案的需求，其目標是克服與 CNN 和 Transformer 相關的效能限制，同時最小化模型大小，為高效影像分割邁出有希望的一步。MobileUNETR 有 3 個主要特點。1) MobileUNETR 包含一個輕量級混合 CNN-Transformer 編碼器，以有效的方式幫助平衡局部和整體背景特徵提取；2) 一個新穎的混合解碼器，在解碼階段同時利用不同解析度下的低階和整體特徵，以進行精確的遮罩生成；3) 超越大型而複雜的架構，MobileUNETR 以 300 萬個參數和 1.3 GFLOP 的計算複雜度實現了卓越的效能，分別減少了 10 倍和 23 倍的參數和 FLOP。已經進行了廣泛的實驗，以驗證我們提出的方法在四個公開可用的皮膚病變分割資料集（包括 ISIC 2016、ISIC 2017、ISIC 2018 和 PH2 資料集）上的有效性。程式碼將公開於：https://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

摘要：雷氏複雜圖形測驗 (RCFT) 等繪畫測驗廣泛用於評估視覺空間技能和記憶力等認知功能，使其成為檢測輕度認知障礙 (MCI) 的寶貴工具。儘管它們很有用，但基於這些測驗的現有預測模型通常會受到樣本量小和缺乏外部驗證等限制，這會損害其可靠性。我們開發了一個多串流深度學習框架，它整合了兩個不同的處理串流：一個基於多頭自注意力，使用原始 RCFT 影像的空間串流，以及一個採用先前開發的自動評分系統的評分串流。我們的模型在韓國群組中 1,740 名受試者的資料上進行訓練，並在來自韓國的 222 名受試者的外部醫院資料集上進行驗證。所提出的多串流模型在外部驗證中表現出優於基準模型的效能 (AUC = 0.872，準確率 = 0.781)。空間和評分串流的整合使模型能夠從原始影像擷取複雜的視覺細節，同時也能納入結構化的評分資料，這共同增強了它檢測細微認知障礙的能力。這種雙重方法不僅提高了預測準確性，也增加了模型的穩健性，使其在不同的臨床環境中更可靠。我們的模型對臨床環境有實際的意義，它可以在其中作為早期 MCI 篩檢的具成本效益的工具。

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v1 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of fire spots detected by the AQUA_M-T satellite in
the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network
(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures to predict monthly accumulations of daily detected fire
spots. A summary of the data revealed a consistent seasonality over time, with
annual maximum and minimum fire spot values tending to repeat at the same
periods each year. The primary objective is to verify whether the forecasts
capture this inherent seasonality through rigorous statistical analysis. The
methodology involved careful data preparation, model configuration, and
training using cross-validation with two seeds, ensuring that the data
generalizes well to the test and validation sets, and confirming the
convergence of the model parameters. The results indicate that the mixed LSTM
and GRU model offers improved accuracy in forecasting 12 months ahead,
demonstrating its effectiveness in capturing complex temporal patterns and
modeling the observed time series. This research significantly contributes to
the application of deep learning techniques in environmental monitoring,
specifically in fire spot forecasting. In addition to improving forecast
accuracy, the proposed approach highlights the potential for adaptation to
other time series forecasting challenges, opening new avenues for research and
development in machine learning and natural phenomenon prediction. Keywords:
Time Series Forecasting, Recurrent Neural Networks, Deep Learning.

摘要：本研究提出了一個全面的方法，用於建模和預測巴西亞馬遜地區由 AQUA_M-T 衛星偵測到的歷史火災點時間序列。該方法採用混合遞迴神經網路 (RNN) 模型，結合長短期記憶 (LSTM) 和門控遞迴單元 (GRU) 架構，以預測每日偵測火災點的月累計值。對資料的摘要顯示出隨著時間推移而出現的一致季節性，每年的年度最大和最小火災點值傾向於在同一時期重複出現。主要目標是透過嚴謹的統計分析驗證預測是否捕捉到這種固有的季節性。該方法涉及仔細的資料準備、模型配置，以及使用兩個種子的交叉驗證進行訓練，確保資料能很好地推廣到測試和驗證集，並確認模型參數的收斂性。結果表明，混合 LSTM 和 GRU 模型在預測 12 個月後提供了更高的準確度，證明了其在捕捉複雜時間模式和建模觀測時間序列方面的有效性。本研究顯著地促进了深度學習技術在環境監測中的應用，特別是在火災點預測方面。除了提高預測準確度外，所提出的方法還強調了適應其他時間序列預測挑戰的潛力，為機器學習和自然現象預測的研究和開發開闢了新的途徑。關鍵字：時間序列預測、遞迴神經網路、深度學習。

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

摘要：<paragraph>基於視覺的外科導航由於其非侵入性、成本效益和靈活性優勢而受到越來越多的關注。特別是，基於視覺的導航系統的一個關鍵元素是追蹤手術器械。與 2D 器械追蹤方法相比，3D 器械追蹤在臨床實務中具有更廣泛的價值，但由於紋理弱、遮擋和缺乏用於 3D 配準的電腦輔助設計 (CAD) 模型，因此也更具挑戰性。為了解決這些挑戰，我們提出 SurgTrack，一種適用於無 CAD 和穩健的真實世界應用程式的兩階段 3D 器械追蹤方法。在第一個配準階段，我們整合一個器械簽署距離場 (SDF)，對器械的 3D 表徵進行建模，實現無 CAD 的 3D 配準。因此，我們可以透過將視訊串流與已配準的 SDF 模型進行匹配，取得器械在 3D 空間中的位置和方向。在第二個追蹤階段，我們設計一個姿勢圖最佳化模組，利用姿勢記憶池的歷史追蹤結果來最佳化追蹤結果並改善遮擋的穩健性。此外，我們收集 Instrument3D 資料集，以全面評估手術器械的 3D 追蹤。廣泛的實驗驗證了我們 SurgTrack 的優越性和可擴充性，以顯著的改進優於現有技術。程式碼和資料集可在 https://github.com/wenwucode/SurgTrack 取得。</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

摘要：估計的腎小球過濾率 (eGFR) 是臨床實務中腎臟功能的重要指標。雖然傳統方程式和使用臨床與實驗室資料的機器學習 (ML) 模型可以估計 eGFR，但準確預測未來 eGFR 水平仍然是腎臟科醫師和 ML 研究人員的一大挑戰。最近的研究進展顯示，大型語言模型 (LLM) 和大型多模態模型 (LMM) 可以作為各種應用程式的強健基礎模型。本研究探討 LMM 預測未來 eGFR 水平的潛力，其資料集包含 50 位病患的實驗室和臨床數值。透過整合各種提示技術和 LMM 的合奏，我們的研究結果顯示，這些模型在結合精確提示和 eGFR 軌跡的視覺化表示時，可提供與現有 ML 模型相近的預測效能。這項研究擴展了基礎模型的應用，並為未來研究利用這些模型來應對複雜的醫療預測挑戰提供了途徑。

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

摘要：超音波因其提供非侵入性、無輻射且即時影像的優點，而廣泛用於臨床介入和診斷。
然而，由於操作員需要大量的訓練和專業知識，限制了此靈活程序的可及性。機器人超音波 (RUS) 提供了一個可行的解決方案來解決此限制；
儘管如此，要達到人類等級的熟練度仍然具有挑戰性。學習示範 (LfD) 方法已在 RUS 中進行探討，它從離線示範的資料集學習先驗策略，以編碼專家超音波檢查員的心智模型。然而，迄今尚未探討專家在 RUS 訓練期間的積極參與，即指導。指導已知可以提高人類訓練的效率和績效。本文提出了一個 RUS 指導架構，以提升其績效。此架構結合了 DRL（自我監督實務）與透過指導提供的專家稀疏回饋。DRL 使用離線策略軟性動作-評論 (SAC) 網路，並根據影像品質評分給予獎勵。專家的指導被建模為部分可觀察馬可夫決策過程 (POMDP)，它根據專家的修正來更新策略參數。在模擬人體模型上的驗證研究顯示，指導將學習率提高了 $25\%$，高品質影像擷取數量提高了 $74.5\%$。

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

摘要：注意力缺陷過動症 (ADHD) 會在各種領域造成顯著的損害。提早診斷 ADHD 並接受治療可以大幅改善生活品質和功能。最近，機器學習方法已經提升了 ADHD 診斷程序的準確度和效率。然而，現有方法所需的設備和訓練有素的人員成本通常很高。因此，我們首次將基於影片的幀級動作辨識網路引入 ADHD 診斷。我們也記錄了一個真正的多模式 ADHD 資料集，並從影片模式中萃取出三個動作類別以進行 ADHD 診斷。整個流程的資料已經回報給 CNTW-NHS 基金會，將由醫療顧問/專業人士審查，並將適時公開。

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

摘要：牙科疾病是一種普遍的慢性疾病，與大量的經濟負擔、個人痛苦和增加的全身疾病風險有關。儘管普遍建議每天刷牙兩次，但由於健忘和脫離等因素，對建議的口腔自我保健行為的依從性仍然低於最佳水平。為了解決這個問題，我們開發了 Oralytics，一個 mHealth 介入系統，旨在補充臨床醫生提供的預防保健，以預防有牙科疾病風險的邊緣化個人。Oralytics 結合了一個在線強化學習演算法，以確定提供介入提示的最佳時間，這些提示鼓勵口腔自我保健行為。我們已在註冊的臨床試驗中部署了 Oralytics。該部署需要仔細的設計來管理美國臨床試驗設置中具體的挑戰。在本文中，我們（1）重點介紹了解決這些挑戰的 RL 演算法的關鍵設計決策，以及（2）進行重新抽樣分析以評估演算法設計決策。Oralytics 的第二階段（隨機對照試驗）計劃於 2025 年春季開始。

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

摘要：在医疗保健领域，医学影像分割对于准确的疾病诊断和有效治疗策略的开发至关重要。早期检测可以极大地帮助控制疾病，并可能防止疾病进展。机器学习，尤其是深度卷积神经网络，已成为解决分割挑战的一种有前途的方法。U-Net 等传统方法使用编码块进行局部表示建模和解码块来揭示语义关系。然而，这些模型通常难以处理在纹理和形状上表现出显着变化的多尺度对象，并且它们经常无法捕获输入数据中的远程依赖关系。专为序列到序列预测而设计的 Transformer 已被提出作为替代方案，利用全局自注意力机制。然而，由于粒度细节不足，它们有时可能缺乏精确的定位。为了克服这些限制，我们引入了 TransDAE：一种新颖的方法，它重新构想了自注意力机制，以包含整个特征空间中的空间和通道关联，同时保持计算效率。此外，TransDAE 通过尺度间交互模块增强了跳跃连接路径，促进了特征重用并提高了定位精度。值得注意的是，即使不依赖预训练权重，TransDAE 在 Synaps 多器官数据集上也优于现有的最先进方法。

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain Léguillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte Séroussi, Julien Grosjean, Romain Lelong, Catherine Letord, Stéfan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

摘要：<paragraph>背景：用藥審查是由藥師執行的一種結構化患者訪談，目的在於優化藥物治療。在實務上，用藥審查是一項冗長且認知需求高的任務，需要具備特定知識。雖然已提出臨床實務指引，但其應用很繁瑣。方法：我們根據 STOPP/START v2 指引的實作，並使用表格、圖表和花形符號視覺化呈現彙整的藥物知識，設計了一套用藥審查的臨床決策支援系統 ABiMed。我們在隨機模擬試驗中，讓 39 位社區藥師評估 ABiMed，每位藥師針對兩位虛構患者執行用藥審查，兩次沒有使用 ABiMed，兩次使用 ABiMed。我們記錄了藥師識別出的問題、建議的介入措施、反應時間、感知可用性和評論。將藥師的用藥審查與專家設計的金標準進行比較。結果：使用 ABiMed 後，藥師在用藥審查期間發現了多 1.6 倍相關的藥物相關問題（p=1.1e-12），並提出更好的介入措施（p=9.8e-9），而無需花費更多時間（p=0.56）。系統可用性評分為 82.7，被評為「優良」。在他們的評論中，藥師讚賞 ABiMed 的視覺化面向，以及它比較目前治療與建議治療的能力。多因素分析顯示，ABiMed 提供的支援在藥師的年齡或性別方面沒有差異，就識別出的問題百分比或建議介入措施的品質而言。結論：使用智慧且視覺化的臨床決策支援系統，可以協助藥師執行用藥審查。我們的觀點主要是驗證系統在臨床條件下的效度。</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

摘要：大型語言模型 (LLM) 的成功在很大程度上取決於預訓練階段中學習到的海量預訓練數據。預訓練過程和訓練數據的不透明性導致許多基準測試的結果變得不可靠。如果任何模型已在基準測試集中進行訓練，則可能會嚴重阻礙該領域的發展。為了自動化且有效地測試大型語言模型的能力，許多主流基準採用多選題格式。由於多選題選項內容的互換不影響問題本身的含義，因此我們提出了一種基於此屬性的簡單且有效的数据洩漏檢測方法。具體來說，我們將數據中選項的內容隨機排列以生成對應的派生數據集，然後根據模型在派生數據集上的對數概率分佈檢測數據洩漏。如果對數概率集中存在最大值和異常值，則表示數據已洩漏。我們的方法能夠在不訪問模型訓練數據或權重的黑盒條件下工作，有效地識別模型預訓練數據中基準測試集的數據洩漏，包括選項可能已有意或無意地被打亂的正常場景和複雜場景。通過基於兩個 LLM 和基準設計的實驗，我們證明了我們方法的有效性。此外，我們評估了 31 個主流開源 LLM 在四個基準數據集上的數據洩漏程度，並對每個基準的洩漏 LLM 進行了排名，我們發現 Qwen 家族的 LLM 具有最高的數據洩漏程度。

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

摘要：推導旋轉機器的健康指標對於其維護至關重要。然而，這個過程對普遍採用的智能方法來說具有挑戰性，因為它們可能會採用整個資料分佈，不僅會引入雜訊干擾，而且缺乏可解釋性。為了解決這些問題，我們提出了一種基於擴散的弱監督式方法，用於推導旋轉機器的健康指標，實現早期故障檢測和狀態演變的持續監控。這種方法依賴於使用健康樣本和一些異常值訓練的無分類器擴散模型。這個模型會產生健康樣本。並且通過比較封套譜中原始樣本和生成樣本之間的差異，我們構建了一個異常圖，可以清楚地識別故障。然後推導出健康指標，可以解釋故障類型並減輕雜訊干擾。對兩個案例的比較研究表明，與基準模型相比，所提出的方法提供了卓越的健康監控有效性和魯棒性。

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

摘要：電腦輔助癌症存活風險預測在病患的及時治療中扮演著重要的角色。這是一個困難的弱監督序數回歸任務，與多重臨床因素有關，例如病理圖像、基因組數據等。在本文中，我們提出了一種新的訓練方法，多模態物件層級對比學習，用於癌症存活風險預測。首先，我們根據訓練樣本集中樣本之間的存活風險關係建立對比學習對。接著，我們引入物件層級對比學習方法來訓練存活風險預測器。我們進一步將其延伸至多模態場景，透過應用跨模態對比。考量到病理圖像和基因體數據的異質性，我們分別採用基於注意力的和自標準化的神經網路來建構多模態存活風險預測器。最後，我們提出的方法所訓練的存活風險預測器在兩個公開的多模態癌症資料集上，在存活風險預測方面優於最先進的方法。

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana Hernández, Juan C. Caicedo, Gloria M. Díaz

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

摘要：對比增強模式分析在乳房磁共振影像 (MRI) 中至關重要，可用於區分良性腫瘤和可能是惡性腫瘤。
然而，對比增強影像的擷取非常耗時且昂貴。作為物理擷取的替代方案，本文提出了一個全面的管道，用於從早期對應物生成準確的長期（晚期）對比增強乳房 MRI。所提出的策略著重於在增強區域中保留對比劑模式，同時在整個合成影像中維持視覺屬性。為此，提出了一種新穎的損失函數，利用對比劑 (CA) 在組織中的生物行為（由時間強度 (TI) 增強曲線給出），以最佳化基於像素注意力的生成模型。此外，與傳統的正規化和標準化方法不同，我們開發了一種新的正規化策略，可在多個時間戳的影像序列中維持對比增強模式。這確保了影像前處理後 CA 模式的普遍性，這與傳統方法不同。此外，為了客觀評估合成影像的臨床品質，還引入了兩個指標來測量擷取和合成影像的增強區域的 TI 曲線之間的差異。實驗結果顯示，所提出的策略產生的影像在對比增強區域中的診斷品質明顯優於其他影像，同時維持了整個影像的空間特徵。這些結果表明，在臨床場景中，通過深度學習生成的合成晚期增強影像具有潛在用途。

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea Storås, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, Pål Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

摘要：我們引進 Kvasir-VQA，一個由 HyperKvasir 和 Kvasir-Instrument 資料集衍生的延伸資料集，並加入問題與解答註解，以促進在胃腸 (GI) 診斷中的進階機器學習任務。此資料集包含 6,500 個註解影像，涵蓋各種 GI 道狀況和手術器械，並且支援包括是非題、選擇題、位置和數字計數等多種類型的問題。此資料集適用於影像標題、視覺問答 (VQA)、合成醫學影像的文字生成、物件偵測和分類等應用程式。我們的實驗證明此資料集在訓練三個選定任務的模型中具有成效，展示了在醫學影像分析和診斷中重要的應用。我們也為每個任務提供評估指標，突顯我們資料集的可用性和多功能性。此資料集和支援工件可於 https://datasets.simula.no/kvasir-vqa 取得。

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

摘要：隱式神經表徵 (INR) 近期由於其無需大量訓練資料集就能學習訊號的連續表徵的能力，而推動了深度學習領域的進展。儘管 INR 方法已被研究用於醫學影像超解析度，但其對於醫學影像中局部先驗的適應性尚未被廣泛探討。醫學影像包含豐富的解剖學區分，這些區分可以提供有價值的局部先驗資訊，以增強 INR 的準確性和穩健性。在這項工作中，我們提出了一個新穎的架構，稱為語義條件 INR (SeCo-INR)，它使用醫學影像中的局部先驗來調整 INR，實現準確的模型擬合和插值能力，以實現超解析度。我們的架構學習醫學影像的語意分割特徵的連續表徵，並利用它為影像的每個語意區域推導最佳 INR。我們使用多種醫學影像方式測試我們的架構，並與最先進的方法相比，達到了更高的量化評分和更逼真的超解析度輸出。

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

摘要：<paragraph>在皮肤病学领域，人工诊断皮肤状况的复杂性需要皮肤科医师的专业知识。从癌症到炎症性疾病，对各种皮肤疾病的准确识别至关重要。然而，现有的皮肤病学人工智能 (AI) 模型面临挑战，尤其是在准确诊断不同肤色的疾病时，在较深的肤色上存在明显的性能差距。此外，公开可用的无偏数据集的稀缺性阻碍了包容性 AI 诊断工具的开发。为了应对准确预测不同肤色皮肤状况的挑战，我们采用了一种迁移学习方法，该方法利用了来自各种图像域的丰富可转移知识。我们的方法集成了来自广泛来源的多个预训练模型，包括一般和特定的医学图像，以提高皮肤状况预测的稳健性和包容性。我们使用 Diverse Dermatology Images (DDI) 数据集严格评估了这些模型的有效性，该数据集独特地包含了代表性不足和常见的肤色，使其成为评估我们方法的理想基准。在所有方法中，Med-ViT 由于其从各种图像来源中学到的综合特征表示而成为表现最好的方法。为了进一步提高性能，我们使用 HAM10000 等其他皮肤图像数据集进行了域适应。这种适应显着提高了所有模型的模型性能。</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

摘要：大型語言模型 (LLM) 經常缺乏特定領域的知識，即使經過微調的模型也容易產生幻覺。因此，需要更多可靠的模型來納入外部知識。我們提出了一個流程 4StepFocus，特別是預處理步驟，可以大幅改善 LLM 的答案。這是透過提供受引導的外部知識存取，利用模型自行擷取關聯性脈絡和進行基本推理的能力來實現的。此方法透過在半結構化知識庫中進行基於三元組的搜尋，以直接且可追蹤的方式縮小潛在正確答案的範圍，然後再切換到潛在表徵，根據非結構化資料對這些候選答案進行排名。這與純粹基於潛在表徵的相關方法有所區別。4StepFocus 包含以下步驟：1) 由 LLM 進行三元組產生以擷取關聯資料，2) 在這些三元組中替換變數，以採用知識圖表縮小答案候選範圍，3) 使用涉及關聯非結構化資料的向量相似性搜尋對剩餘候選答案進行排序，4) 由 LLM 重新對最佳候選答案進行排名，並提供背景資料。在醫療、產品推薦和學術論文搜尋測試集中進行的實驗證明，這種方法確實是一種強大的擴充。它不僅增加了來自資訊檢索的相关可追蹤背景資訊，而且與最先進的方法相比，也大幅提升了效能。本文提出了一個新穎且鮮少探索的方向，因此提供了廣泛的未來工作機會。使用的原始碼可在 https://github.com/kramerlab/4StepFocus 取得。

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

摘要：本文提出了一個知識工程和多語言語義推理技術的本体設計，用於建立一個自動化系統，以知識圖譜的形式吸收印度料理的烹飪資訊。重點在於設計智慧方法，以推導本体設計，並全面擷取關於食物、食譜、食材、烹飪特性，以及最重要的營養的知識，並擴大規模。我們在這個研討會論文中介紹了我們正在進行的工作，詳細描述了整理印度料理知識相關的挑戰，並提出了我們的高階本体設計。我們也提出了一種新的工作流程，它使用 AI、LLM 和語言技術，從公共領域的食譜部落格網站中整理資訊，以建立印度料理的知識圖譜。本文提出的知識整理方法是通用的，可以複製到任何領域。設計與應用無關，可用於 AI 驅動的智慧分析、建立個人化數位健康推薦系統，以及使用使用者資訊、食物生物化學、地理資訊、農業資訊等脈絡資訊，來補充印度料理的知識圖譜。

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

摘要：農業生產需要小心管理輸入，例如殺菌劑、殺蟲劑和除草劑，以確保作物成功、高產、有利可圖且具有優良的種子品質。目前最先進的田間作物管理依賴於粗略的作物管理策略，其中整個田地都噴灑了控制病蟲害的化學物質，導致成本增加和土壤和作物管理不佳。為了克服這些挑戰並優化作物生產，我們在虛擬田間環境中利用機器學習工具為農民生成局部管理計畫，以管理生物威脅並同時最大化利潤。具體來說，我們提出了 AgGym，一個模組化、作物和壓力不可知的模擬架構，用於模擬田間生物壓力的擴散，並估算有和沒有化學處理的產量損失。我們使用真實數據進行驗證，顯示 AgGym 可以使用有限的數據進行自訂，以模擬各種生物壓力條件下的產量結果。我們進一步證明，深度強化學習 (RL) 政策可以使用 AgGym 進行訓練，以設計超精確的生物壓力緩解策略，並有可能以更少的化學物質和更低的成本增加產量恢復。我們提出的架構啟用了個人化決策支援，可以將生物壓力管理從基於時間表和被動轉變為機會主義和規範性。我們還將 AgGym 軟體實作作為社區資源釋出，並邀請專家為這個開放原始碼且公開可用的模組化環境架構做出貢獻。可以在以下位置取得原始碼：https://github.com/SCSLabISU/AgGym。

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

摘要：超廣角螢光血管造影（UWF-FA）使用可能具有潛在危害的鈉螢光素，可精確識別眼疾。現有研究已開發出從超廣角掃描雷射眼科鏡（UWF-SLO）產生 UWF-FA 的方法，以減少與注射相關的不良反應。然而，這些方法在產生高品質的後期 UWF-FA 方面效果較差，特別是在病灶區域和精細細節方面。產生高品質後期 UWF-FA 面臨兩項主要挑戰：配對的 UWF-SLO 和早期/後期 UWF-FA 資料集稀少，以及需要在病灶部位和潛在出血區域進行逼真的產生。本研究引進一種改良的潛在擴散模型架構，從有限配對的 UWF 影像產生高品質的後期 UWF-FA。為了應對前面提到的挑戰，我們的方法採用一個模組，利用跨時間區域差異損失，鼓勵模型專注於早期和後期之間的差異。此外，我們在擴散前向過程中引進一種低頻增強雜訊策略，以改善醫學影像的真實性。為了進一步增強變異自動編碼器模組的對應能力，特別是在資料集有限的情況下，我們實作一個閘控卷積編碼器，從條件影像中萃取額外資訊。我們針對超廣角後期螢光血管造影（LPUWF-LDM）的潛在擴散模型有效重建後期 UWF-FA 中的精細細節，並在使用有限資料集時，與其他現有方法相比，達到最先進的結果。我們的原始碼可在以下網址取得：
https://github.com/Tinysqua/****。

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

摘要：心臟聽診是診斷心血管疾病 (CVD) 的一項整合工具，通常依賴於臨床醫師的主觀詮釋，在一致性和準確性方面存在限制。為了解決這個問題，我們引入了 BUET 多重疾病心音 (BMD-HS) 資料集，這是一個全面且經過精心策劃的心音錄音資料集。此資料集包含五種常見心音的 864 個錄音，代表了廣泛的心瓣膜疾病，重點在於診斷困難的病例。BMD-HS 資料集的突出特點是其創新的多標籤註解系統，它涵蓋了各種疾病和獨特的疾病狀態。這個系統顯著增強了資料集在開發自動心音分類和診斷中進階機器學習模型的效用。透過彌合傳統聽診實務與當代資料驅動診斷方法之間的差距，BMD-HS 資料集準備好革新心血管疾病的診斷和管理，為心臟健康研究的進展提供寶貴的資源。此資料集可透過以下連結公開取得：https://github.com/mHealthBuet/BMD-HS-Dataset。

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

摘要：自動診斷技術已演進到能透過使用單一模式眼底影像或光學相干斷層掃描 (OCT) 來辨識年齡相關性黃斑部病變 (AMD)。為了分類眼疾，眼底和 OCT 影像是臨床環境中使用最關鍵的影像模式。大多數基於深度學習的技術建立在單一影像模式上，它在一定程度上考量了眼疾，卻忽略了其他模式，而其他模式包含了不同影像模式之間的詳盡資訊。本文提出了一種模式特定的多尺度色彩空間嵌入整合，並基於用於分類的轉移學習的注意力機制 (MCGAEc)，它能使用不同的色彩空間在不同的尺度上有效提取不同的模式資訊。在這項工作中，我們首先介紹了模式特定的多尺度色彩空間編碼器模型，它透過將不同的特徵色彩空間整合到多尺度中，來納入不同的特徵表徵到一個統一的架構中。從先前的編碼器模組中提取的特徵與注意力機制結合，以提取全域特徵表徵，它與先前提取的特徵整合，並轉移到隨機森林分類器，以進行 AMD 分類。為了分析所提出的 MCGAEc 方法的效能，我們利用了來自 Project Macula for AMD 的公開多模式資料集，並與現有模型進行比較。

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

摘要：調整大型預訓練基礎模型（例如 SAM）以進行醫學影像分割仍是一項重大挑戰。關鍵步驟涉及制定一系列包含特定臨床說明的專門提示。過去的工作在很大程度上依賴於每個例項的單一提示類型，這需要手動輸入理想的正確提示，效率較低。為了解決這個問題，我們建議利用不同粒度的提示，這些提示來自原始影像，以提供更廣泛的臨床見解。然而，由於潛在衝突，結合不同類型的提示可能會構成挑戰。為了解決這個問題，我們設計了一種由粗到細的機制，稱為課程提示，它逐步整合不同類型的提示。透過對各種模式下的三個公共醫學資料集進行廣泛的實驗，我們證明了我們提出的方法的有效性，它不僅自動化提示生成過程，而且與其他基於 SAM 的醫學影像分割方法相比，還產生了更好的效能。程式碼可在以下位置取得：https://github.com/AnnaZzz-zxq/Curriculum-Prompting。

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

摘要：罕見婦科腫瘤 (RGT) 由於其低發生率和異質性，對臨床帶來重大挑戰。缺乏明確的指引導致次佳管理和不良預後。分子腫瘤委員會透過根據生物標記客製化治療，加速取得有效療法，超越癌症類型。需要手動整理的非結構化資料阻礙了生物標記分析在療法配對中的有效使用。本研究探討使用大型語言模型 (LLM) 為 RGT 的精準醫療建構數位雙胞胎。
我們的概念驗證數位雙胞胎系統整合了來自機構和已發表的案例 (n=21) 的臨床和生物標記資料，以及來自文獻的資料 (n=655 篇出版物，n=404,265 名患者)，為轉移性子宮肉瘤癌制定客製化治療計畫，找出傳統單一來源分析可能遺漏的選項。LLM 啟用的數位雙胞胎有效地模擬個別患者的軌跡。從基於器官的腫瘤定義轉變為基於生物學的定義，能實現個人化照護，進而提升 RGT 管理並改善患者預後。

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

摘要：語音型憂鬱檢測對自動化檢測來說是一大挑戰，因為它在不同個體間的表現獨特，且資料稀少。為了應對這些挑戰，我們引入了 DAAMAudioCNNLSTM 和 DAAMAudioTransformer，這兩個參數有效且可解釋的模型，用於音訊特徵萃取和憂鬱檢測。DAAMAudioCNNLSTM 採用創新的 CNN-LSTM 架構，搭配多頭密度自適應注意力機制 (DAAM)，動態關注於有意義的語音區段。DAAMAudioTransformer 利用Transformer編碼器取代 CNN-LSTM 架構，並納入相同的 DAAM 模組，以增強注意力和可解釋性。這些方法不僅增強了檢測的穩健性和可解釋性，還達到了最先進的效能：DAAMAudioCNNLSTM 的 F1 巨觀分數為 0.702，DAAMAudioTransformer 在 DAIC-WOZ 資料集上的 F1 巨觀分數為 0.72，在訓練/驗證期間不依賴於輔助資訊，例如母音位置和說話者資訊，這與先前的做法不同。這兩個模型在利用語音訊號進行憂鬱檢測方面具有顯著的可解釋性和效率，代表著朝向更可靠、臨床上有用的診斷工具邁進了一大步，並預示著語音和心理保健的進步。為了促進這個領域的進一步研究，我們公開了我們的程式碼。

##### **Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**
2409.00310v1 by Mikhail Borisenkov, Andrei Velichko, Maksim Belyaev, Dmitry Korzun, Tatyana Tserne, Larisa Bakutova, Denis Gubin

This study investigates machine learning algorithms to identify objective
features for diagnosing food addiction (FA) and assessing confirmed symptoms
(SC). Data were collected from 81 participants (mean age: 21.5 years, range:
18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food
Addiction Scale (YFAS). Participants provided demographic and anthropometric
data, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch
Eating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist
for a week to record motor activity. Analysis of the actimetric data identified
significant statistical and entropy-based features that accurately predicted FA
and SC using ML. The Matthews correlation coefficient (MCC) was the primary
metric. Activity-related features were more effective for FA prediction
(MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments
yielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51.
Significant correlations were also found between actimetric features related to
FA, emotional, and restrained eating behaviors, supporting the model's
validity. Our results support the concept of a human bionic suite composed of
IoT devices and ML sensors, which implements health digital assistance with
real-time monitoring and analysis of physiological indicators related to FA and
SC.

摘要：本研究調查機器學習演算法，以識別診斷食物成癮 (FA) 和評估已確認症狀 (SC) 的客觀特徵。資料來自 81 位參與者（平均年齡：21.5 歲，範圍：18-61 歲，女性：77.8%），其 FA 和 SC 是使用耶魯食物成癮量表 (YFAS) 測量的。參與者提供了人口統計和人類測量資料，完成了 YFAS、Zung 自我評量憂鬱量表和荷蘭飲食行為問卷，並在非慣用手腕上佩戴活動計一週以記錄運動活動。對活動計資料的分析識別出重要的統計和基於熵的特徵，這些特徵使用機器學習準確預測了 FA 和 SC。馬修斯相關係數 (MCC) 是主要指標。與休息相關的特徵（MCC=0.68）相比，與活動相關的特徵對於 FA 預測更有效（MCC=0.88）。對於 SC，活動區段產生的 MCC=0.47，休息區段 MCC=0.38，組合後 MCC=0.51。還發現與 FA、情緒和受限飲食行為相關的活動計特徵之間存在顯著相關性，這支持了模型的有效性。我們的結果支持由物聯網裝置和機器學習感測器組成的人體仿生套件的概念，該套件實作了健康數位協助，並對與 FA 和 SC 相關的生理指標進行即時監控和分析。

##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

摘要：<paragraph>近年來，已經引進各種方法來解釋「黑箱」AI 模型的輸出。然而，目前並不清楚使用者是否實際理解和信任這些解釋。在本文中，我們專注於評估癌症風險的回歸工具的解釋，並探討解釋的內容和格式對以使用者為中心的理解和信任指標的影響。關於內容，我們實驗了兩種解釋方法：流行的 SHAP，基於博弈論概念，因此對於日常使用者來說可能很複雜，以及基於特徵遮蔽的 occlusion-1，可能更易於理解。關於格式，我們將 SHAP 解釋呈現為圖表 (SC)，這是慣例，而將 occlusion-1 解釋呈現為圖表 (OC) 以及文字 (OT)，其較為簡單的性質也適用於此。這些實驗等同於使用者研究，詢問參與者，具有兩種不同程度的專業知識（一般民眾和具備一些醫學訓練的人），他們對回歸工具輸出解釋的主觀和客觀理解和信任。在兩項研究中，我們發現，在基於內容進行比較時，一般來說，occlusion-1 優於 SHAP 解釋，在主觀理解和信任方面有明顯的偏好。然而，在僅控制格式的情況下直接比較解釋，在大多數情況下只顯示 OT 優於 SC 解釋的證據，這表明 occlusion-1 優於 SHAP 解釋的主導地位可能是由偏好文字而非圖表作為解釋所驅動的。最後，我們沒有發現解釋類型在客觀理解方面的差異證據。因此，總體而言，對解釋的內容和格式的選擇需要仔細注意，因為在某些情況下，格式而非內容，可能在改善使用者體驗方面發揮關鍵作用。</paragraph>

##### **Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**
2409.00163v1 by Yuhan Zheng, Jessie A Elliott, John V Reynolds, Sheraz R Markar, Bartłomiej W. Papież, ENSURE study group

Esophageal cancer is a major cause of cancer-related mortality
internationally, with high recurrence rates and poor survival even among
patients treated with curative-intent surgery. Investigating relevant
prognostic factors and predicting prognosis can enhance post-operative clinical
decision-making and potentially improve patients' outcomes. In this work, we
assessed prognostic factor identification and discriminative performances of
three models for Disease-Free Survival (DFS) and Overall Survival (OS) using a
large multicenter international dataset from ENSURE study. We first employed
Cox Proportional Hazards (CoxPH) model to assess the impact of each feature on
outcomes. Subsequently, we utilised CoxPH and two deep neural network
(DNN)-based models, DeepSurv and DeepHit, to predict DFS and OS. The
significant prognostic factors identified by our models were consistent with
clinical literature, with post-operative pathologic features showing higher
significance than clinical stage features. DeepSurv and DeepHit demonstrated
comparable discriminative accuracy to CoxPH, with DeepSurv slightly
outperforming in both DFS and OS prediction tasks, achieving C-index of 0.735
and 0.74, respectively. While these results suggested the potential of DNNs as
prognostic tools for improving predictive accuracy and providing personalised
guidance with respect to risk stratification, CoxPH still remains an adequately
good prediction model, with the data used in this study.

摘要：食道癌是國際間癌症相關死亡的主要原因，即使接受根治性手術的患者，復發率高且存活率低。探討相關預後因子並預測預後，可以增進術後臨床決策制定，並可能改善患者的結果。在這項工作中，我們評估了三種模型的預後因子識別和判別效能，分別針對無疾病存活期 (DFS) 和整體存活期 (OS)，使用 ENSURE 研究的大型多中心國際數據集。我們首先採用 Cox 比例風險 (CoxPH) 模型來評估每個特徵對結果的影響。隨後，我們利用 CoxPH 和兩個基於深度神經網路 (DNN) 的模型 DeepSurv 和 DeepHit 來預測 DFS 和 OS。我們的模型識別出的顯著預後因子與臨床文獻一致，術後病理特徵顯示出比臨床分期特徵更高的顯著性。DeepSurv 和 DeepHit 展現出與 CoxPH 相當的判別準確度，DeepSurv 在 DFS 和 OS 預測任務中表現略勝一籌，分別達到 C 指數 0.735 和 0.74。雖然這些結果表明 DNN 作為預後工具的潛力，可以提高預測準確度並針對風險分層提供個人化指導，但 CoxPH 仍然是足夠好的預測模型，使用本研究中的數據。

##### **NDP: Next Distribution Prediction as a More Broad Target**
2408.17377v1 by Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li, Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao, Jingbo Zhu

Large language models (LLMs) trained on next-token prediction (NTP) paradigm
have demonstrated powerful capabilities. However, the existing NTP paradigm
contains several limitations, particularly related to planned task
complications and error propagation during inference. In our work, we extend
the critique of NTP, highlighting its limitation also due to training with a
narrow objective: the prediction of a sub-optimal one-hot distribution. To
support this critique, we conducted a pre-experiment treating the output
distribution from powerful LLMs as efficient world data compression. By
evaluating the similarity between the $n$-gram distribution and the one-hot
distribution with LLMs, we observed that the $n$-gram distributions align more
closely with the output distribution of LLMs. Based on this insight, we
introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions
to replace the one-hot targets, enhancing learning without extra online
training time. We conducted experiments across translation, general task,
language transfer, and medical domain adaptation. Compared to NTP, NDP can
achieve up to +2.97 COMET improvement in translation tasks, +0.61 average
improvement in general tasks, and incredible +10.75 average improvement in the
medical domain. This demonstrates the concrete benefits of addressing the
target narrowing problem, pointing to a new direction for future work on
improving NTP.

摘要：大型語言模型 (LLM) 根據下一個符號預測 (NTP) 範例進行訓練，已展現強大的功能。然而，現有的 NTP 範例包含了幾個限制，特別是與計畫任務複雜性和推論期間的錯誤傳播有關。在我們的研究中，我們擴展了 NTP 的批判，強調其限制也由於使用狹隘的目標進行訓練：預測次佳的一熱分佈。為了支持這項批判，我們進行了一個前置實驗，將強大 LLM 的輸出分佈視為有效的世界資料壓縮。透過評估 $n$-gram 分佈與 LLM 的一熱分佈之間的相似性，我們觀察到 $n$-gram 分佈與 LLM 的輸出分佈更為接近。基於這個見解，我們引入了下一個分佈預測 (NDP)，它使用 $n$-gram 分佈來取代一熱目標，在沒有額外線上訓練時間的情況下，加強學習。我們進行了翻譯、一般任務、語言轉移和醫學領域適應的實驗。與 NTP 相比，NDP 在翻譯任務中可以達到 +2.97 COMET 改進，在一般任務中平均改進 +0.61，在醫學領域中令人難以置信的平均改進 +10.75。這證明了解決目標收窄問題的具體好處，指出了改進 NTP 未來工作的全新方向。

##### **Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**
2408.17011v2 by Jutika Borah, Kumaresh Sarmah, Hidam Kumarjit Singh

Imaging techniques such as Chest X-rays, whole slide images, and optical
coherence tomography serve as the initial screening and detection for a wide
variety of medical pulmonary and ophthalmic conditions respectively. This paper
investigates the intricacies of using pretrained deep convolutional neural
networks with transfer learning across diverse medical imaging datasets with
varying modalities for binary and multiclass classification. We conducted a
comprehensive performance analysis with ten network architectures and model
families each with pretraining and random initialization. Our finding showed
that the use of pretrained models as fixed feature extractors yields poor
performance irrespective of the datasets. Contrary, histopathology microscopy
whole slide images have better performance. It is also found that deeper and
more complex architectures did not necessarily result in the best performance.
This observation implies that the improvements in ImageNet are not parallel to
the medical imaging tasks. Within a medical domain, the performance of the
network architectures varies within model families with shifts in datasets.
This indicates that the performance of models within a specific modality may
not be conclusive for another modality within the same domain. This study
provides a deeper understanding of the applications of deep learning techniques
in medical imaging and highlights the impact of pretrained networks across
different medical imaging datasets under five different experimental settings.

摘要：影像技術，例如胸部 X 光、全切片影像和光學相干斷層掃描，分別作為各種醫學肺部和眼科疾病的初步篩檢和偵測。本文探討了使用預訓練深度卷積神經網路搭配遷移學習，橫跨不同醫療影像資料集，以進行二元和多類別分類的複雜性。我們對十種網路架構和模型系列進行了全面的效能分析，每個架構和系列都經過預訓練和隨機初始化。我們的發現顯示，將預訓練模型用作固定特徵萃取器會產生不佳的效能，與資料集無關。相反地，組織病理學顯微鏡全切片影像有較好的效能。我們也發現，較深且複雜的架構並非一定會產生最佳效能。此觀察結果意味著 ImageNet 的改良並未與醫療影像任務平行。在醫療領域內，網路架構的效能會隨著資料集的轉換而改變模型系列。這表示在特定模式中模型的效能可能無法決定在同一個領域中另一種模式的效能。本研究提供了對深度學習技術在醫療影像中的應用更深入的理解，並強調了預訓練網路在五種不同實驗設定下跨不同醫療影像資料集的影響。

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro Liò, Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

摘要：大型語言模型 (LLM) 的最新突破提供了前所未有的自然語言理解和生成能力。然而，現有關於生物醫學中 LLM 的調查通常專注於特定應用或模型架構，缺乏整合各種生物醫學領域最新進展的全面分析。本綜述基於對來自 PubMed、Web of Science 和 arXiv 等數據庫的 484 篇出版物的分析，深入探討了生物醫學中 LLM 的當前現況、應用、挑戰和前景，其特點是關注這些模型在現實世界生物醫學背景中的實際應用。首先，我們探討了 LLM 在廣泛的生物醫學任務中的零次學習能力，包括診斷輔助、藥物發現和個性化醫療等，並從 137 項關鍵研究中汲取見解。然後，我們討論了 LLM 的適應策略，包括單模態和多模態 LLM 的微調方法，以增強它們在零次學習無法實現的專業生物醫學背景中的性能，例如醫療問題解答和生物醫學文獻的有效處理。最後，我們討論了 LLM 在生物醫學領域面臨的挑戰，包括數據隱私問題、模型可解釋性有限、數據集質量問題以及由於生物醫學數據的敏感性、對高度可靠模型輸出的需求以及在醫療保健中部署 AI 的倫理影響而產生的倫理問題。為了應對這些挑戰，我們還確定了生物醫學中 LLM 未來的研究方向，包括用於保護數據隱私的聯合學習方法以及整合可解釋 AI 方法以增強 LLM 的透明度。

##### **Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**
2408.16343v1 by Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang

Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.

摘要：阿茲海默症 (AD) 是一種複雜的神經退化性疾病，特徵是記憶力喪失、執行功能障礙和人格改變。由於症狀微妙且表現形式多樣，早期診斷具有挑戰性，通常由於傳統單模態診斷方法的範圍有限而導致誤診。本研究引入了一個先進的多模態分類模型，它整合了臨床、認知、神經影像和腦電圖數據，以提高診斷準確性。該模型結合了一個具有表格數據編碼架構的特徵標籤器，並利用 TimesBlock 模組來捕捉腦電圖 (EEG) 數據中的複雜時間模式。通過採用跨模態注意力聚合模組，該模型有效地融合了磁共振成像 (MRI) 空間資訊和腦電圖時間數據，顯著改善了 AD、輕度認知障礙和正常認知之間的區別。同時，我們構建了第一個 AD 分類數據集，其中包含三種模態：腦電圖、磁共振成像和表格數據。我們的創新方法旨在促進早期診斷和干預，潛在地減緩 AD 的進展。原始碼和我們的私人 ADMC 數據集可在 https://github.com/JustlfC03/MSTNet 獲得。

##### **Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**
2408.16256v1 by Xia Jiang, Yijun Zhou, Alan Wells, Adam Brufsky

Breast cancer is one of the two cancers responsible for the most deaths in
women, with about 42,000 deaths each year in the US. That there are over
300,000 breast cancers newly diagnosed each year suggests that only a fraction
of the cancers result in mortality. Thus, most of the women undergo seemingly
curative treatment for localized cancers, but a significant later succumb to
metastatic disease for which current treatments are only temporizing for the
vast majority. The current prognostic metrics are of little actionable value
for 4 of the 5 women seemingly cured after local treatment, and many women are
exposed to morbid and even mortal adjuvant therapies unnecessarily, with these
adjuvant therapies reducing metastatic recurrence by only a third. Thus, there
is a need for better prognostics to target aggressive treatment at those who
are likely to relapse and spare those who were actually cured. While there is a
plethora of molecular and tumor-marker assays in use and under-development to
detect recurrence early, these are time consuming, expensive and still often
un-validated as to actionable prognostic utility. A different approach would
use large data techniques to determine clinical and histopathological
parameters that would provide accurate prognostics using existing data. Herein,
we report on machine learning, together with grid search and Bayesian Networks
to develop algorithms that present a AUC of up to 0.9 in ROC analyses, using
only extant data. Such algorithms could be rapidly translated to clinical
management as they do not require testing beyond routine tumor evaluations.

摘要：乳癌是造成女性死亡人數最多的兩種癌症之一，每年約有 42,000 名女性死於乳癌。每年有超過 300,000 例乳癌新確診，這表示只有少部分癌症會導致死亡。因此，大多數女性接受局部癌症的根治性治療，但許多人後來仍會死於轉移性疾病，而目前的治療方法對絕大多數患者來說只是暫時的。目前的預後指標對於 5 名接受局部治療後看似治癒的女性中，有 4 名幾乎沒有實際價值，許多女性不必要地接受病態甚至致命的輔助療法，而這些輔助療法僅能將轉移性復發率降低三分之一。因此，需要更好的預後指標，才能針對那些可能復發的人進行積極治療，並避免那些實際上已經治癒的人接受治療。雖然有許多分子和腫瘤標記檢測方法正在使用和開發中，可以及早發現復發，但這些方法耗時、昂貴，而且作為可操作的預後工具，其效用仍然常常未經驗證。另一種方法會使用大量的資料技術，來確定臨床和組織病理學參數，並使用現有資料提供準確的預後指標。在此，我們報告了機器學習，以及網格搜尋和貝氏網路，用來開發演算法，在 ROC 分析中提供高達 0.9 的 AUC，僅使用現有資料。此類演算法可以快速轉換為臨床管理，因為它們不需要進行常規腫瘤評估以外的測試。

##### **M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**
2408.16213v1 by Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi

The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.

摘要：人工智慧的快速發展，特別是在大型語言模型 (LLM) 中，已對包括醫療保健在內的各個領域產生重大影響。在胸部 X 光 (CXR) 分析中，先前的研究已採用 LLM，但有其限制：不是未能充分利用 LLM 的多任務處理能力，就是缺乏臨床準確性。本文提出 M4CXR，一種多模態 LLM，旨在增強 CXR 解釋。該模型訓練於視覺指令遵循資料集，其中以對話格式整合各種特定任務資料集。因此，該模型支援多項任務，例如醫療報告產生 (MRG)、視覺基礎和視覺問題回答 (VQA)。M4CXR 透過採用思考鏈提示策略，在 MRG 中達成最先進的臨床準確性，其中它會識別 CXR 影像中的發現，並隨後產生對應的報告。該模型可根據可用輸入（例如單一影像、多重影像和多重研究脈絡）適應各種 MRG 情境。除了 MRG 之外，M4CXR 以與專門模型相當的層級執行視覺基礎，並在 VQA 中展現出色的效能。定量和定性評估均顯示出 M4CXR 在 MRG、視覺基礎和 VQA 中的多功能性，同時持續維持臨床準確性。

##### **A Survey on Evaluation of Multimodal Large Language Models**
2408.15769v1 by Jiaxing Huang, Jingyi Zhang

Multimodal Large Language Models (MLLMs) mimic human perception and reasoning
system by integrating powerful Large Language Models (LLMs) with various
modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and
various modality encoders as sensory organs. This framework endows MLLMs with
human-like capabilities, and suggests a potential pathway towards achieving
artificial general intelligence (AGI). With the emergence of all-round MLLMs
like GPT-4V and Gemini, a multitude of evaluation methods have been developed
to assess their capabilities across different dimensions. This paper presents a
systematic and comprehensive review of MLLM evaluation methods, covering the
following key aspects: (1) the background of MLLMs and their evaluation; (2)
"what to evaluate" that reviews and categorizes existing MLLM evaluation tasks
based on the capabilities assessed, including general multimodal recognition,
perception, reasoning and trustworthiness, and domain-specific applications
such as socioeconomic, natural sciences and engineering, medical usage, AI
agent, remote sensing, video and audio processing, 3D point cloud analysis, and
others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into
general and specific benchmarks; (4) "how to evaluate" that reviews and
illustrates MLLM evaluation steps and metrics; Our overarching goal is to
provide valuable insights for researchers in the field of MLLM evaluation,
thereby facilitating the development of more capable and reliable MLLMs. We
emphasize that evaluation should be regarded as a critical discipline,
essential for advancing the field of MLLMs.

摘要：多模態大型語言模型 (MLLM) 透過整合強大的大型語言模型 (LLM) 與各種模態編碼器（例如視覺、音訊），模擬人類的感知和推理系統，將 LLM 定位為「大腦」，而將各種模態編碼器定位為感官器官。此架構賦予 MLLM 類似人類的能力，並提出實現人工通用智慧 (AGI) 的潛在途徑。隨著 GPT-4V 和 Gemini 等全方位 MLLM 的出現，已經開發出多種評估方法來評估它們在不同維度上的能力。本文對 MLLM 評估方法進行了系統且全面的回顧，涵蓋以下幾個關鍵面向：(1) MLLM 及其評估的背景；(2)「要評估什麼」根據評估的能力，回顧並分類現有的 MLLM 評估任務，包括一般多模態辨識、感知、推理和可信度，以及特定領域的應用，例如社會經濟、自然科學和工程、醫療用途、AI 代理、遙測、影片和音訊處理、3D 點雲分析等；(3)「在哪裡評估」將 MLLM 評估基準總結為一般基準和特定基準；(4)「如何評估」回顧並說明 MLLM 評估步驟和指標。我們的首要目標是為 MLLM 評估領域的研究人員提供有價值的見解，從而促進更強大且可靠的 MLLM 的開發。我們強調評估應被視為一項關鍵的學科，對於推進 MLLM 領域至關重要。

##### **Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**
2408.15498v1 by Yijun Zhou, Om Arora-Jain, Xia Jiang

While machine learning has advanced in medicine, its widespread use in
clinical applications, especially in predicting breast cancer metastasis, is
still limited. We have been dedicated to constructing a DFNN model to predict
breast cancer metastasis n years in advance. However, the challenge lies in
efficiently identifying optimal hyperparameter values through grid search,
given the constraints of time and resources. Issues such as the infinite
possibilities for continuous hyperparameters like l1 and l2, as well as the
time-consuming and costly process, further complicate the task. To address
these challenges, we developed Single Hyperparameter Grid Search (SHGS)
strategy, serving as a preselection method before grid search. Our experiments
with SHGS applied to DFNN models for breast cancer metastasis prediction focus
on analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,
learning rate, decay, and momentum. We created three figures, each depicting
the experiment results obtained from three LSM-I-10-Plus-year datasets. These
figures illustrate the relationship between model performance and the target
hyperparameter values. For each hyperparameter, we analyzed whether changes in
this hyperparameter would affect model performance, examined if there were
specific patterns, and explored how to choose values for the particular
hyperparameter. Our experimental findings reveal that the optimal value of a
hyperparameter is not only dependent on the dataset but is also significantly
influenced by the settings of other hyperparameters. Additionally, our
experiments suggested some reduced range of values for a target hyperparameter,
which may be helpful for low-budget grid search. This approach serves as a
prior experience and foundation for subsequent use of grid search to enhance
model performance.

摘要：儘管機器學習在醫學領域已有所進展，但其在臨床應用中的廣泛使用，特別是在預測乳癌轉移方面，仍有其限制。我們致力於建構 DFNN 模型，以預測乳癌轉移 n 年。然而，挑戰在於透過網格搜尋有效率地找出最佳超參數值，這受到時間和資源的限制。諸如 l1 和 l2 等連續超參數的可能性無窮，以及耗時且昂貴的過程等問題，更讓這項任務變得複雜。為了應對這些挑戰，我們開發了單一超參數網格搜尋 (SHGS) 策略，作為網格搜尋前的預選方法。我們針對乳癌轉移預測應用的 DFNN 模型進行 SHGS 實驗，重點分析八個目標超參數：epoch 次數、批次大小、中斷、L1、L2、學習率、衰減和動量。我們製作了三幅圖，每幅圖都描繪了從三個 LSM-I-10-Plus-year 資料集獲得的實驗結果。這些圖表說明了模型效能與目標超參數值之間的關係。對於每個超參數，我們分析了超參數的變動是否會影響模型效能，並檢視是否有特定模式，以及如何針對特定超參數選擇值。我們的實驗結果顯示，超參數的最佳值不僅取決於資料集，也受到其他超參數設定的顯著影響。此外，我們的實驗建議縮小目標超參數值的範圍，這可能有助於低預算的網格搜尋。此方法可作為後續使用網格搜尋以增強模型效能的先前經驗和基礎。

##### **What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**
2408.15354v1 by Jana Schaich Borg, Hannah Read

Interest is growing in artificial empathy, but so is confusion about what
artificial empathy is or needs to be. This confusion makes it challenging to
navigate the technical and ethical issues that accompany empathic AI
development. Here, we outline a framework for thinking about empathic AI based
on the premise that different constellations of capabilities associated with
empathy are important for different empathic AI applications. We describe
distinctions of capabilities that we argue belong under the empathy umbrella,
and show how three medical empathic AI use cases require different sets of
these capabilities. We conclude by discussing why appreciation of the diverse
capabilities under the empathy umbrella is important for both AI creators and
users.

摘要：對人工同理心越來越感興趣，但對於人工同理心是什麼或需要什麼也越來越困惑。這種混淆使得難以解決伴隨同理心 AI 開發而來的技術和倫理問題。在此，我們概述了一個思考同理心 AI 的架構，其基於這樣一個前提：與同理心相關的不同能力組合對於不同的同理心 AI 應用很重要。我們描述了我們認為屬於同理心範疇的能力區別，並展示了三個醫療同理心 AI 使用案例需要這些能力的不同組合。我們最後討論了為什麼欣賞同理心範疇下的各種能力對於 AI 創造者和使用者都很重要的原因。

##### **Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**
2408.15217v1 by Weiyi Zhang, Siyu Huang, Jiancheng Yang, Ruoyu Chen, Zongyuan Ge, Yingfeng Zheng, Danli Shi, Mingguang He

Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal
vascular dynamics and aiding in the diagnosis of eye diseases. However, its
invasive nature and less accessibility compared to Color Fundus (CF) images
pose significant challenges. Current CF to FFA translation methods are limited
to static generation. In this work, we pioneer dynamic FFA video generation
from static CF images. We introduce an autoregressive GAN for smooth,
memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic
lesion changes in FFA regions, we design a knowledge mask based on clinical
experience. Leveraging this mask, our approach integrates innovative knowledge
mask-guided techniques, including knowledge-boosted attention, knowledge-aware
discriminators, and mask-enhanced patchNCE loss, aimed at refining generation
in critical areas and addressing the pixel misalignment challenge. Our method
achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common
video generation approaches. Human assessment by an ophthalmologist confirms
its high generation quality. Notably, our knowledge mask surpasses supervised
lesion segmentation masks, offering a promising non-invasive alternative to
traditional FFA for research and clinical applications. The code is available
at https://github.com/Michi-3000/Fundus2Video.

摘要：眼底螢光血管攝影 (FFA) 是評估視網膜血管動力學和協助診斷眼疾的重要工具。然而，與彩色眼底 (CF) 影像相比，其侵入性較高且取得不易，因此造成重大挑戰。目前 CF 轉換成 FFA 的翻譯方法僅限於靜態產生。在這項工作中，我們率先從靜態 CF 影像產生動態 FFA 影片。我們引入一個自迴歸 GAN，以進行流暢且節省記憶體的逐幀 FFA 合成。為了加強對 FFA 區域中動態病灶變化的關注，我們根據臨床經驗設計了一個知識遮罩。透過利用這個遮罩，我們的做法整合了創新的知識遮罩引導技術，包括知識增強的注意力、知識感知的辨別器以及遮罩增強的 patchNCE 損失，旨在改善關鍵區域的生成並解決像素未對齊的挑戰。與其他常見的影片生成方法相比，我們的做法達到了最佳的 FVD 1503.21 和 PSNR 11.81。眼科醫師的人為評估證實了其生成品質很高。值得注意的是，我們的知識遮罩超越了有監督的病灶分割遮罩，為傳統 FFA 提供了一個有前途的非侵入性替代方案，可用於研究和臨床應用。程式碼可在 https://github.com/Michi-3000/Fundus2Video 取得。

##### **Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**
2409.00112v1 by Daniil Filienko, Yinzhou Wang, Caroline El Jazmi, Serena Xie, Trevor Cohen, Martine De Cock, Weichao Yuwen

While Large Language Models (LLMs) are being quickly adapted to many domains,
including healthcare, their strengths and pitfalls remain under-explored. In
our study, we examine the effects of prompt engineering to guide Large Language
Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session
via text, particularly during the symptom identification and assessment phase
for personalized goal setting. We present evaluation results of the models'
performances by automatic metrics and experienced medical professionals. We
demonstrate that the models' capability to deliver protocolized therapy can be
improved with the proper use of prompt engineering methods, albeit with
limitations. To our knowledge, this study is among the first to assess the
effects of various prompting techniques in enhancing a generalist model's
ability to deliver psychotherapy, focusing on overall quality, consistency, and
empathy. Exploring LLMs' potential in delivering psychotherapy holds promise
with the current shortage of mental health professionals amid significant
needs, enhancing the potential utility of AI-based and AI-enhanced care
services.

摘要：隨著大型語言模型（LLM）快速適應於許多領域，包括醫療保健，它們的優勢和缺陷仍未得到充分探索。在我們的研究中，我們探討了提示工程在引導大型語言模型（LLM）通過文字提供問題解決療法（PST）環節的部分內容的效果，特別是在症狀識別和評估階段，用於個性化目標設定。我們通過自動化指標和經驗豐富的醫療專業人員展示了模型性能的評估結果。我們證明了模型提供程式化治療的能力可以使用提示工程方法適當使用來改進，儘管有局限性。據我們所知，這項研究是第一批評估各種提示技術對增強通才模型提供心理治療能力的影響的研究之一，重點關注整體品質、一致性和同理心。在心理健康專業人員嚴重短缺且需求巨大的情況下，探索 LLM 在提供心理治療方面的潛力很有前景，增強了基於 AI 和 AI 增強的護理服務的潛在效用。

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

摘要：人工智慧（AI）在醫療和保健應用中投入了大量的投資和開發，進而導致醫療技術中的先進控制系統。然而，AI 系統的不透明性引發了對此類敏感應用中所需基本特性的擔憂，例如透明度和可信度。我們的研究透過調查一個程序來解決這些問題，用於選擇最充分的可解釋 AI（XAI）方法，以符合歐盟法規在醫療器材的智慧型生物電子學中的說明要求。採用的方法從透過其控制機制（開迴路、閉迴路和半閉迴路系統）對智慧型裝置進行分類，並深入探討其技術開始。然後，我們分析這些法規以定義其對各種裝置和相關目標的可解釋性要求。同時，我們透過其說明目標對 XAI 方法進行分類。這允許將法律可解釋性要求與 XAI 說明目標相匹配，並確定適當的 XAI 演算法來達成它們。我們的研究結果提供了對哪些 XAI 演算法更符合歐盟法規以適用於不同類型的醫療器材的細緻理解。我們透過不同神經植入物的實際案例研究來證明這一點，從慢性疾病管理到先進的義肢。這項研究填補了將生物電子學中的 XAI 應用與歐盟法規的嚴格規定相符的重要空白。它為開發人員和研究人員提供了一個實用的架構，確保其 AI 創新能促進醫療技術並遵守法律和道德標準。

##### **MiWaves Reinforcement Learning Algorithm**
2408.15076v1 by Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy

The escalating prevalence of cannabis use poses a significant public health
challenge globally. In the U.S., cannabis use is more prevalent among emerging
adults (EAs) (ages 18-25) than any other age group, with legalization in the
multiple states contributing to a public perception that cannabis is less risky
than in prior decades. To address this growing concern, we developed MiWaves, a
reinforcement learning (RL) algorithm designed to optimize the delivery of
personalized intervention prompts to reduce cannabis use among EAs. MiWaves
leverages domain expertise and prior data to tailor the likelihood of delivery
of intervention messages. This paper presents a comprehensive overview of the
algorithm's design, including key decisions and experimental outcomes. The
finalized MiWaves RL algorithm was deployed in a clinical trial from March to
May 2024.

摘要：大麻使用率不斷上升，對全球公共衛生構成重大挑戰。在美國，大麻使用率在年輕成年人（EA）（18-25 歲）中比任何其他年齡組都要普遍，多個州的合法化導致公眾認為大麻比過去幾十年風險較低。為了解決這個日益嚴重的問題，我們開發了 MiWaves，這是一種增強學習 (RL) 演算法，旨在優化個性化干預提示的傳遞，以減少 EA 中的大麻使用。MiWaves 利用領域專業知識和先前的數據來調整干預訊息傳遞的可能性。本文全面概述了演算法的設計，包括關鍵決策和實驗結果。最終的 MiWaves RL 演算法已於 2024 年 3 月至 5 月在臨床試驗中部署。

##### **Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**
2408.15032v1 by Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng, Chao Tong

Computational pathology (CPath) has significantly advanced the clinical
practice of pathology. Despite the progress made, Multiple Instance Learning
(MIL), a promising paradigm within CPath, continues to face challenges,
particularly related to incomplete information utilization. Existing
frameworks, such as those based on Convolutional Neural Networks (CNNs),
attention, and selective scan space state sequential model (SSM), lack
sufficient flexibility and scalability in fusing diverse features, and cannot
effectively fuse diverse features. Additionally, current approaches do not
adequately exploit order-related and order-independent features, resulting in
suboptimal utilization of sequence information. To address these limitations,
we propose a novel MIL framework called Mamba2MIL. Our framework utilizes the
state space duality model (SSD) to model long sequences of patches of whole
slide images (WSIs), which, combined with weighted feature selection, supports
the fusion processing of more branching features and can be extended according
to specific application needs. Moreover, we introduce a sequence transformation
method tailored to varying WSI sizes, which enhances sequence-independent
features while preserving local sequence information, thereby improving
sequence information utilization. Extensive experiments demonstrate that
Mamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive
experiments across multiple datasets, achieving improvements in nearly all
performance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a
binary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the
BRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an
accuracy of 0.4981. The code is available at
https://github.com/YuqiZhang-Buaa/Mamba2MIL.

摘要：<paragraph>計算病理學 (CPath) 已顯著提升病理學的臨床實務。儘管已有進展，作為 CPath 中一個有前途的範例，多重實例學習 (MIL) 持續面臨挑戰，特別是與不完整資訊使用有關。現有的架構，例如基於卷積神經網路 (CNN)、注意力和選擇性掃描空間狀態序列模型 (SSM) 的架構，在融合各種特徵時缺乏足夠的彈性和可擴充性，且無法有效融合各種特徵。此外，目前的作法並未充分利用與順序相關和與順序無關的特徵，導致序列資訊使用率不佳。為了解決這些限制，我們提出一個名為 Mamba2MIL 的新 MIL 架構。我們的架構利用狀態空間對偶模型 (SSD) 來建模全幻燈片影像 (WSI) 的長序列貼片，這與加權特徵選取結合使用，支援更多分支特徵的融合處理，且可根據特定應用需求進行延伸。此外，我們引入一種針對不同 WSI 大小量身打造的序列轉換方法，這增強了與序列無關的特徵，同時保留了局部序列資訊，進而改善序列資訊使用率。廣泛的實驗證明 Mamba2MIL 超越了最先進的 MIL 方法。我們在多個資料集上進行廣泛的實驗，在幾乎所有效能指標上均獲得改善。特別是在 NSCLC 資料集上，Mamba2MIL 達到 0.9533 的二元腫瘤分類 AUC 和 0.8794 的準確度。在 BRACS 資料集上，它達到 0.7986 的多類別分類 AUC 和 0.4981 的準確度。程式碼可在 https://github.com/YuqiZhang-Buaa/Mamba2MIL 取得。</paragraph>

##### **Sequence-aware Pre-training for Echocardiography Probe Guidance**
2408.15026v1 by Haojun Jiang, Zhenguo Sun, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang

Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe
pose to obtain high-quality sectional images. Cardiac ultrasound faces two
major challenges: (1) the inherently complex structure of the heart, and (2)
significant individual variations. Previous works have only learned the
population-averaged 2D and 3D structures of the heart rather than personalized
cardiac structural features, leading to a performance bottleneck. Clinically,
we observed that sonographers adjust their understanding of a patient's cardiac
structure based on prior scanning sequences, thereby modifying their scanning
strategies. Inspired by this, we propose a sequence-aware self-supervised
pre-training method. Specifically, our approach learns personalized 2D and 3D
cardiac structural features by predicting the masked-out images and actions in
a scanning sequence. We hypothesize that if the model can predict the missing
content it has acquired a good understanding of the personalized cardiac
structure. In the downstream probe guidance task, we also introduced a sequence
modeling approach that models individual cardiac structural information based
on the images and actions from historical scan data, enabling more accurate
navigation decisions. Experiments on a large-scale dataset with 1.36 million
samples demonstrated that our proposed sequence-aware paradigm can
significantly reduce navigation errors, with translation errors decreasing by
15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared
to state-of-the-art methods.

摘要：<paragraph>心臟超音波探頭引導旨在幫助新手調整 6-DOF 探頭姿勢，以取得高品質的斷面影像。心臟超音波面臨兩項主要挑戰：(1) 心臟結構複雜且固有，以及 (2) 個體差異顯著。先前的研究僅學習了整體平均的 2D 和 3D 心臟結構，而非個人化的解剖特徵，導致效能瓶頸。臨床上，我們觀察到超音波技師會根據先前的掃描序列調整他們對患者心臟結構的理解，進而修改他們的掃描策略。受到此啟發，我們提出一個具序列感知的自監督預訓練方法。具體來說，我們的做法透過預測掃描序列中遮罩的影像和動作，來學習個人化的 2D 和 3D 心臟解剖特徵。我們假設，如果模型可以預測遺漏的內容，它便對個人化的解剖結構有了良好的理解。在下游的探頭引導任務中，我們也導入一個序列建模方法，該方法根據歷史掃描資料中的影像和動作，模擬個別心臟解剖資訊，進而做出更精確的導航決策。在有 136 萬個樣本的大規模資料集上進行的實驗證明，我們提出的具序列感知的典範可以大幅減少導航錯誤，其中平移錯誤減少了 15.90% 至 36.87%，旋轉錯誤減少了 11.13% 至 20.77%，與最先進的方法相比。</paragraph>

##### **Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**
2408.15294v2 by Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva

Developing novel predictive models with complex biomedical information is
challenging due to various idiosyncrasies related to heterogeneity,
standardization or sparseness of the data. We previously introduced a
person-centric ontology to organize information about individual patients, and
a representation learning framework to extract person-centric knowledge graphs
(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a
systematic approach to examine the results of GNN models trained with both
structured and unstructured information from the MIMIC-III dataset. Through
ablation studies on different clinical, demographic, and social data, we show
the robustness of this approach in identifying predictive features in PKGs for
the task of readmission prediction.

摘要：開發具有複雜生物醫學資訊的新穎預測模型，由於資料的異質性、標準化或稀疏性，因此具有挑戰性。我們先前介紹了一個以人為中心的本體，用於組織有關個別患者的資訊，以及一個表示學習架構，用於提取以人為中心的知識圖譜 (PKG) 和訓練圖形神經網路 (GNN)。在本文中，我們提出了一種系統性的方法來檢驗使用 MIMIC-III 資料集中的結構化和非結構化資訊訓練的 GNN 模型的結果。透過對不同的臨床、人口統計和社會資料進行消融研究，我們展示了這種方法在識別 PKG 中的預測特徵以進行再入院預測任務時的穩健性。

##### **Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**
2408.14754v1 by Qiaoxin Li, Ruifeng Chen, Peng Wang, Guotao Quan, Yanfeng Du, Dong Liang, Yinsheng Li

Dual-energy computed tomography (DECT) has been widely used to obtain
quantitative elemental composition of imaged subjects for personalized and
precise medical diagnosis. Compared with DECT leveraging advanced X-ray source
and/or detector technologies, the use of the sequential-scanning data
acquisition scheme to implement DECT may make a broader impact on clinical
practice because this scheme requires no specialized hardware designs and can
be directly implemented into conventional CT systems. However, since the
concentration of iodinated contrast agent in the imaged subject varies over
time, sequentially scanned data sets acquired at two tube potentials are
temporally inconsistent. As existing material basis image reconstruction
approaches assume that the data sets acquired at two tube potentials are
temporally consistent, the violation of this assumption results in inaccurate
quantification of material concentration. In this work, we developed
sequential-scanning DECT imaging using high temporal resolution image
reconstruction and error-compensated material basis image generation,
ACCELERATION in short, to address the technical challenge induced by temporal
inconsistency of sequentially scanned data sets and improve quantification
accuracy of material concentration in sequential-scanning DECT. ACCELERATION
has been validated and evaluated using numerical simulation data sets generated
from clinical human subject exams and experimental human subject studies.
Results demonstrated the improvement of quantification accuracy and image
quality using ACCELERATION.

摘要：雙能量電腦斷層掃描 (DECT) 已廣泛用於取得影像化受試者的定量元素組成，以進行個人化且精確的醫療診斷。與使用進階 X 光源和/或偵測器技術的 DECT 相比，使用連續掃描資料擷取方案來實作 DECT 可能對臨床實務產生更廣泛的影響，因為此方案不需要專門的硬體設計，且可直接實作到傳統 CT 系統中。然而，由於影像化受試者中碘化對比劑的濃度會隨時間而變化，因此在兩個管電位下擷取的連續掃描資料集在時間上並不一致。由於現有的材料基礎影像重建方法假設在兩個管電位下擷取的資料集在時間上是一致的，因此違反此假設會導致材料濃度的量化不準確。在這項工作中，我們開發了使用高時間解析度影像重建和誤差補償材料基礎影像產生的連續掃描 DECT 影像，簡稱 ACCELERATION，以解決連續掃描資料集時間不一致所引發的技術挑戰，並改善連續掃描 DECT 中材料濃度的量化準確度。ACCELERATION 已使用從臨床人體受試者檢查和實驗人體受試者研究產生的數值模擬資料集進行驗證和評估。結果證明使用 ACCELERATION 可改善量化準確度和影像品質。

##### **Large Language Models for Disease Diagnosis: A Scoping Review**
2409.00097v1 by Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Yi Fang, Liqiao Xia, Jeremy Yeung, Daochen Zha, Mingquan Lin, Rui Zhang

Automatic disease diagnosis has become increasingly valuable in clinical
practice. The advent of large language models (LLMs) has catalyzed a paradigm
shift in artificial intelligence, with growing evidence supporting the efficacy
of LLMs in diagnostic tasks. Despite the growing attention in this field, many
critical research questions remain under-explored. For instance, what diseases
and LLM techniques have been investigated for diagnostic tasks? How can
suitable LLM techniques and evaluation methods be selected for clinical
decision-making? To answer these questions, we performed a comprehensive
analysis of LLM-based methods for disease diagnosis. This scoping review
examined the types of diseases, associated organ systems, relevant clinical
data, LLM techniques, and evaluation methods reported in existing studies.
Furthermore, we offered guidelines for data preprocessing and the selection of
appropriate LLM techniques and evaluation strategies for diagnostic tasks. We
also assessed the limitations of current research and delineated the challenges
and future directions in this research field. In summary, our review outlined a
blueprint for LLM-based disease diagnosis, helping to streamline and guide
future research endeavors.

摘要：自動疾病診斷在臨床實務中變得越來越有價值。大語言模型 (LLM) 的出現催化了人工智能的典範轉移，越來越多證據支持 LLM 在診斷任務中的效能。儘管這個領域受到越來越多的關注，但許多關鍵的研究問題仍未得到充分探討。例如，哪些疾病和 LLM 技術已被調查用於診斷任務？如何為臨床決策制定選擇合適的 LLM 技術和評估方法？為了回答這些問題，我們對基於 LLM 的疾病診斷方法進行了全面的分析。這項範圍探討回顧了現有研究中報告的疾病類型、相關器官系統、相關臨床資料、LLM 技術和評估方法。此外，我們提供了資料前處理和選擇適當的 LLM 技術和評估策略以進行診斷任務的指南。我們還評估了當前研究的限制，並描述了這個研究領域的挑戰和未來方向。總之，我們的回顧概述了基於 LLM 的疾病診斷藍圖，有助於簡化和指導未來的研究工作。

##### **Improving Clinical Note Generation from Complex Doctor-Patient Conversation**
2408.14568v1 by Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu

Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.

摘要：撰寫臨床筆記和記錄醫療檢查是醫療保健專業人員的一項重要任務，是患者照護文件中的重要組成部分。然而，手動撰寫這些筆記很耗時，並且會影響臨床醫生花在直接患者互動和其他任務上的時間。因此，自動化臨床筆記生成系統的開發已成為 AI 在健康領域中具有臨床意義的研究領域。在本文中，我們提出了使用大型語言模型 (LLM) 進行臨床筆記生成的領域的 3 項關鍵貢獻。首先，我們介紹了 CliniKnote，這是一個綜合性數據集，包含 1,200 個複雜的醫患對話及其完整的臨床筆記。此數據集由醫學專家在現代神經網路的幫助下創建和策劃，為臨床筆記生成任務中的模型訓練和評估提供了寶貴的資源。其次，我們提出了 K-SOAP（關鍵字、主觀、客觀、評估和計畫）筆記格式，它通過在頂部添加一個關鍵字部分來增強傳統的 SOAP~\cite{podder2023soap}（主觀、客觀、評估和計畫）筆記，以便快速識別基本資訊。第三，我們開發了一個自動化管道，從醫患對話中生成 K-SOAP 筆記，並使用各種指標對各種現代 LLM 進行基準測試。我們的結果表明，與標準 LLM 微調方法相比，效率和性能有了顯著的提升。

##### **Temporal Ensemble Logic**
2408.14443v2 by Guo-Qiang Zhang

We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal
logic for linear-time temporal reasoning. TEL includes primitive temporal
constructs such as ``always up to $t$ time later'' ($\Box_t$), ``sometimes
before $t$ time in the future'' ($\Diamond_t$), and ``$t$-time later''
$\varphi_t$. TEL has been motivated from the requirement for rigor and
reproducibility for cohort specification and discovery in clinical and
population health research, to fill a gap in formalizing temporal reasoning in
biomedicine. Existing logical frameworks such as linear temporal logic are too
restrictive to express temporal and sequential properties in biomedicine, or
too permissive in semantic constructs, such as in Halpern-Shoham logic, to
serve this purpose. In this paper, we first introduce TEL in a general set up,
with discrete and dense time as special cases. We then focus on the theoretical
development of discrete TEL on the temporal domain of positive integers
$\mathbb{N}^+$, denoted as ${\rm TEL}_{\mathbb{N}^+}$. ${\rm
TEL}_{\mathbb{N}^+}$ is strictly more expressive than the standard monadic
second order logic, characterized by B\"{u}chi automata. We present its formal
semantics, a proof system, and provide a proof for the undecidability of the
satisfiability of ${\rm TEL}_{\mathbb{N}^+}$. We also include initial results
on expressiveness and decidability fragments for ${\rm TEL}_{\mathbb{N}^+}$,
followed by application outlook and discussions.

摘要：<paragraph>我們介紹時間集合邏輯 (TEL)，一種用於線性時間時態推理的一階單子模態邏輯。TEL 包含原始時態結構，例如「始終在 $t$ 時間後」($\Box_t$)、「有時在未來 $t$ 時間前」($\Diamond_t$) 和「$t$ 時間後」$\varphi_t$。TEL 的動機是為了臨床和人口健康研究中群組規範和發現的嚴謹性和可再現性，以填補生物醫學中時態推理形式化的空白。現有的邏輯框架（例如線性時態邏輯）對於表達生物醫學中的時態和順序屬性過於嚴格，或者在語義結構上過於寬鬆（例如在 Halpern-Shoham 邏輯中），無法達到此目的。在本文中，我們首先在一般設置中介紹 TEL，其中離散時間和稠密時間為特殊情況。然後，我們專注於正整數時間域 $\mathbb{N}^+$ 上離散 TEL 的理論發展，表示為 ${\rm TEL}_{\mathbb{N}^+}$. ${\rm TEL}_{\mathbb{N}^+}$ 比標準單子二階邏輯更具表達力，其特徵在於 B\"{u}chi 自動機。我們展示其形式語義、證明系統，並提供 ${\rm TEL}_{\mathbb{N}^+}$ 可滿足性的不可判定性的證明。我們還包括 ${\rm TEL}_{\mathbb{N}^+}$ 的表達力和可判定片段的初步結果，然後是應用前景和討論。</paragraph>

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v2 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

摘要：自動語音辨識 (ASR) 系統在將語音轉錄成文字方面至關重要，但它們所產生的錯誤可能會大幅降低摘要等下游任務的效能。這個問題在臨床對話摘要中特別明顯，這是一個低資源的領域，其中用於微調的監督資料很稀少，因此必須使用 ASR 模型作為黑盒解決方案。由於缺乏足夠的醫療對話音訊錄音和對應的 ASR 轉錄，採用傳統資料擴充來增強摘要模型的抗噪性也不可行。為了應對這個挑戰，我們提出了 MEDSAGE，這是一種使用大型語言模型 (LLM) 產生合成樣本進行資料擴充的方法。具體來說，我們利用 LLM 的情境學習能力，並指示它們根據少數帶有音訊錄音的可用醫療對話範例產生類似的 ASR 錯誤。實驗結果顯示，LLM 可以有效地模擬 ASR 雜訊，而將這些雜訊資料納入訓練過程中可以顯著提高醫療對話摘要系統的穩健性和準確性。這種方法應對了關鍵應用中 ASR 輸出雜訊的問題，提供了一個穩健的解決方案來增強臨床對話摘要的可靠性。

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, Julián N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

摘要：近期人工智能的進展顯著改善了放射報告的自動生成。然而，現有的評估方法無法揭示模型對放射影像的理解，以及它們在描述中達到人類層級精細度的能力。為了彌補這個差距，我們引進一個名為 ReXKG 的系統，它從處理過的報告中萃取出結構化的資訊，以建構一個全面的放射知識圖譜。接著，我們提出三個指標來評估各種知識圖譜中節點的相似性 (ReXKG-NSC)、邊緣的分布 (ReXKG-AMS) 和子圖的涵蓋範圍 (ReXKG-SCS)。我們對 AI 生成的和人類撰寫的放射報告進行深入的比較分析，評估專家和通才模型的效能。我們的研究提供對目前 AI 模型在放射報告生成中的能力和限制更深入的理解，並提供有價值的見解來改善模型效能和臨床應用。

##### **Foundation Models for Music: A Survey**
2408.14340v3 by Yinghao Ma, Anders Øland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, Fabio Morreale, Ge Zhang, György Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang

In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.

摘要：近年來，基礎模型 (FM)，例如大型語言模型 (LLM) 和潛在擴散模型 (LDM)，已對包括音樂在內的不同產業產生深遠影響。這篇全面性的評論探討了音樂領域中最先進 (SOTA) 的預訓練模型和基礎模型，涵蓋了表徵學習、生成式學習和多模態學習。我們首先將音樂在各產業的重要性脈絡化，並追溯 AI 在音樂中的演進。透過描繪基礎模型所針對的模態，我們發現許多音樂表徵在 FM 開發中尚未被充分探索。接著，我們強調先前方法在不同音樂應用中缺乏多樣性，以及 FM 在音樂理解、生成和醫療應用中的潛力。透過全面探討模型預訓練典範、架構選擇、標記化、微調方法和可控性的細節，我們強調了應深入探討的重要主題，例如指令微調和情境學習、規模定律和新興能力，以及長序列建模等。專門的章節提供了對音樂代理的見解，並附有對預訓練和下游任務至關重要的資料集和評估的深入分析。最後，透過強調倫理考量的至關重要性，我們主張後續關於音樂 FM 的研究應更專注於可解釋性、透明度、人類責任和版權問題等議題。本文提供了對音樂 FM 未來挑戰和趨勢的見解，旨在形塑人類與 AI 在音樂領域中合作的軌跡。

##### **Uncertainties of Latent Representations in Computer Vision**
2408.14281v1 by Michael Kirchhof

Uncertainty quantification is a key pillar of trustworthy machine learning.
It enables safe reactions under unsafe inputs, like predicting only when the
machine learning model detects sufficient evidence, discarding anomalous data,
or emitting warnings when an error is likely to be inbound. This is
particularly crucial in safety-critical areas like medical image classification
or self-driving cars. Despite the plethora of proposed uncertainty
quantification methods achieving increasingly higher scores on performance
benchmarks, uncertainty estimates are often shied away from in practice. Many
machine learning projects start from pretrained latent representations that
come without uncertainty estimates. Uncertainties would need to be trained by
practitioners on their own, which is notoriously difficult and
resource-intense.
  This thesis makes uncertainty estimates easily accessible by adding them to
the latent representation vectors of pretrained computer vision models. Besides
proposing approaches rooted in probability and decision theory, such as
Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both
theoretical and empirical questions. We show that these unobservable
uncertainties about unobservable latent representations are indeed provably
correct. We also provide an uncertainty-aware representation learning (URL)
benchmark to compare these unobservables against observable ground-truths.
Finally, we compile our findings to pretrain lightweight representation
uncertainties on large-scale computer vision models that transfer to unseen
datasets in a zero-shot manner.
  Our findings do not only advance the current theoretical understanding of
uncertainties over latent variables, but also facilitate the access to
uncertainty quantification for future researchers inside and outside the field,
enabling straightforward but trustworthy machine learning.

摘要：不確定量化是值得信賴機器學習的一大支柱。
它能讓機器學習模型在不安全的輸入下做出安全的反應，例如只在機器學習模型偵測到足夠證據時才進行預測、捨棄異常資料，或是在可能發生錯誤時發出警告。這在醫療影像分類或自駕車等安全關鍵領域中特別重要。儘管有許多已提出的不確定量化方法在效能基準上取得越來越高的分數，但在實務上卻常常迴避不確定性估計。許多機器學習專案從預訓練的潛在表徵開始，而這些表徵沒有不確定性估計。實務工作者需要自行訓練不確定性，這出了名的困難且耗費資源。
本論文透過將不確定性估計新增到預訓練電腦視覺模型的潛在表徵向量中，讓不確定性估計易於取得。除了提出植基於機率和決策理論的方法，例如蒙地卡羅資訊對比估計 (MCInfoNCE) 和損失預測之外，我們還深入探討理論和實證問題。我們證明這些關於不可觀察潛在表徵的不可觀察不確定性確實可以證明是正確的。我們還提供一個不確定性感知表徵學習 (URL) 基準，用來比較這些不可觀察的不確定性與可觀察的真實值。最後，我們將我們的發現彙整起來，在大型電腦視覺模型上預訓練輕量級表徵不確定性，並以零次學習的方式轉移到未見過的資料集。
我們的發現不僅提升了當前對潛在變數不確定性的理論理解，還促進了未來研究人員在該領域內外取得不確定量化，進而實現直接但值得信賴的機器學習。

##### **Automatic Medical Report Generation: Methods and Applications**
2408.13988v1 by Li Guo, Anas M. Tahir, Dong Zhang, Z. Jane Wang, Rabab K. Ward

The increasing demand for medical imaging has surpassed the capacity of
available radiologists, leading to diagnostic delays and potential
misdiagnoses. Artificial intelligence (AI) techniques, particularly in
automatic medical report generation (AMRG), offer a promising solution to this
dilemma. This review comprehensively examines AMRG methods from 2021 to 2024.
It (i) presents solutions to primary challenges in this field, (ii) explores
AMRG applications across various imaging modalities, (iii) introduces publicly
available datasets, (iv) outlines evaluation metrics, (v) identifies techniques
that significantly enhance model performance, and (vi) discusses unresolved
issues and potential future research directions. This paper aims to provide a
comprehensive understanding of the existing literature and inspire valuable
future research.

摘要：由於對醫學影像的需求日益增長，已經超過了現有放射科醫師的能力，導致診斷延誤和潛在的誤診。人工智慧 (AI) 技術，特別是在自動醫療報告生成 (AMRG) 方面，為此困境提供了有希望的解決方案。本篇評論全面探討了 2021 年至 2024 年的 AMRG 方法。它 (i) 提出解決此領域中主要挑戰的方案，(ii) 探討 AMRG 在各種影像模式中的應用，(iii) 介紹公開可用的資料集，(iv) 概述評估指標，(v) 找出顯著提升模型效能的技術，以及 (vi) 討論尚未解決的問題和潛在的未來研究方向。本文旨在提供對現有文獻的全面了解，並激發有價值的未來研究。

##### **Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**
2409.00084v2 by Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

Background and Aims: This study evaluates the medical reasoning performance
of large language models (LLMs) and vision language models (VLMs) in
gastroenterology.
  Methods: We used 300 gastroenterology board exam-style multiple-choice
questions, 138 of which contain images to systematically assess the impact of
model configurations and parameters and prompt engineering strategies utilizing
GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs
(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),
Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces
(web and API), computing environments (cloud and local), and model precisions
(with and without quantization). Finally, we assessed accuracy using a
semiautomated pipeline.
  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet
(74.0%) achieved the highest accuracy, outperforming the top open-source
models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).
Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)
performed best. The scores of the quantized models were comparable to those of
the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM
performance on image-containing questions did not improve when the images were
provided and worsened when LLM-generated captions were provided. In contrast, a
10% increase in accuracy was observed when images were accompanied by
human-crafted image descriptions.
  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in
medical reasoning, the integration of visual data remains a challenge for VLMs.
Effective deployment involves carefully determining optimal model
configurations, encouraging users to consider either the high performance of
proprietary models or the flexible adaptability of open-source models.

摘要：<paragraph>背景與目標：本研究評估大型語言模型 (LLM) 和視覺語言模型 (VLM) 在腸胃病學中的醫療推理表現。
方法：我們使用 300 個腸胃病學專科考試風格的多選題，其中 138 個包含影像，以系統性地評估模型配置和參數以及利用 GPT-3.5 的提示工程策略的影響。接下來，我們評估專有和開源 LLM（版本）的表現，包括 GPT（3.5、4、4o、4omini）、Claude（3、3.5）、Gemini（1.0）、Mistral、Llama（2、3、3.1）、Mixtral 和 Phi（3），跨不同介面（網路和 API）、運算環境（雲端和本地）和模型精確度（有和沒有量化）。最後，我們使用半自動化管道評估準確度。
結果：在專有模型中，GPT-4o（73.7%）和 Claude3.5-Sonnet（74.0%）達到最高準確度，優於頂尖的開源模型：Llama3.1-405b（64%）、Llama3.1-70b（58.3%）和 Mixtral-8x7b（54.3%）。在量化的開源模型中，6 位元量化的 Phi3-14b（48.7%）表現最佳。量化模型的分數與全精度模型 Llama2-7b、Llama2--13b 和 Gemma2-9b 相當。值得注意的是，當提供影像時，VLM 在包含影像的問題上的表現並未改善，而在提供 LLM 生成的標題時表現惡化。相反地，當影像附有人工製作的影像描述時，準確度觀察到增加了 10%。
結論：結論而言，雖然 LLM 在醫療推理中表現出強健的零次學習表現，但視覺資料的整合仍然是 VLM 的一項挑戰。有效的部署涉及仔細確定最佳模型配置，鼓勵使用者考慮專有模型的高效能或開源模型的靈活適應性。</paragraph>

##### **PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**
2408.13836v1 by Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin, Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang

Volumetric segmentation is crucial for medical imaging but is often
constrained by labor-intensive manual annotations and the need for
scenario-specific model training. Furthermore, existing general segmentation
models are inefficient due to their design and inferential approaches.
Addressing this clinical demand, we introduce PropSAM, a propagation-based
segmentation model that optimizes the use of 3D medical structure information.
PropSAM integrates a CNN-based UNet for intra-slice processing with a
Transformer-based module for inter-slice propagation, focusing on structural
and semantic continuities to enhance segmentation across various modalities.
Distinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box
or sketch mask, unlike conventional models that require two-view prompts. It
has demonstrated superior performance, significantly improving the Dice
Similarity Coefficient (DSC) across 44 medical datasets and various imaging
modalities, outperforming models like MedSAM and SegVol with an average DSC
improvement of 18.1%. PropSAM also maintains stable predictions despite prompt
deviations and varying propagation configurations, confirmed by one-way ANOVA
tests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient
architecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)
and reduces user interaction time by 37.8% compared to two-view prompt models.
Its ability to handle irregular and complex objects with robust performance
further demonstrates its potential in clinical settings, facilitating more
automated and reliable medical imaging analyses with minimal retraining.

摘要：體積分割對於醫學影像至關重要，但通常受到耗費大量人力的標註和特定場景模型訓練需求的限制。此外，現有的通用分割模型由於其設計和推論方法而效率低下。為了滿足這項臨床需求，我們引入了 PropSAM，這是一種基於傳播的分割模型，優化了 3D 醫學結構資訊的使用。PropSAM 整合了一個基於 CNN 的 UNet，用於切片內處理，以及一個基於 Transformer 的模組，用於切片間傳播，重點關注結構和語義連續性，以增強各種模式下的分割。與需要兩視提示的傳統模型不同，PropSAM 獨特地運作於單視提示上，例如 2D 邊界框或草圖遮罩。它已證明具有優異的效能，顯著改善了 44 個醫學資料集和各種影像模式下的骰子相似係數 (DSC)，優於 MedSAM 和 SegVol 等模型，平均 DSC 提升了 18.1%。儘管提示偏差和傳播配置不同，PropSAM 仍能維持穩定的預測，這已通過單向 ANOVA 測試得到證實，分別為 P>0.5985 和 P>0.6131。此外，PropSAM 的高效架構能實現更快的推論速度（Wilcoxon 等級和總和檢定，P<0.001），並將使用者互動時間減少了 37.8%，優於兩視提示模型。它在處理不規則和複雜物件時能展現出穩健的效能，進一步證明了其在臨床環境中的潛力，有助於以最少的重新訓練進行更自動化和可靠的醫學影像分析。

##### **Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**
2408.13683v2 by Andrés Catalino Castillo Jiménez, Ege C. Kaya, Lintao Ye, Abolfazl Hashemi

In a conventional Federated Learning framework, client selection for training
typically involves the random sampling of a subset of clients in each
iteration. However, this random selection often leads to disparate performance
among clients, raising concerns regarding fairness, particularly in
applications where equitable outcomes are crucial, such as in medical or
financial machine learning tasks. This disparity typically becomes more
pronounced with the advent of performance-centric client sampling techniques.
This paper introduces two novel methods, namely SUBTRUNC and UNIONFL, designed
to address the limitations of random client selection. Both approaches utilize
submodular function maximization to achieve more balanced models. By modifying
the facility location problem, they aim to mitigate the fairness concerns
associated with random selection. SUBTRUNC leverages client loss information to
diversify solutions, while UNIONFL relies on historical client selection data
to ensure a more equitable performance of the final model. Moreover, these
algorithms are accompanied by robust theoretical guarantees regarding
convergence under reasonable assumptions. The efficacy of these methods is
demonstrated through extensive evaluations across heterogeneous scenarios,
revealing significant improvements in fairness as measured by a client
dissimilarity metric.

摘要：在傳統的聯邦學習框架中，訓練的用戶端選擇通常涉及在每次迭代中隨機抽取用戶端子集。然而，這種隨機選擇通常會導致用戶端之間的表現差異，引發了公平性的擔憂，特別是在公平結果至關重要的應用中，例如醫療或金融機器學習任務。這種差異通常會隨著以效能為中心的用戶端抽樣技術的出現而變得更加明顯。本文介紹了兩種新方法，即 SUBTRUNC 和 UNIONFL，旨在解決隨機用戶端選擇的限制。這兩種方法都利用次模函數最大化來實現更平衡的模型。通過修改設施位置問題，它們旨在緩解與隨機選擇相關的公平性問題。SUBTRUNC 利用用戶端損失資訊來分散解決方案，而 UNIONFL 依賴於歷史用戶端選擇資料，以確保最終模型的效能更公平。此外，這些演算法附帶了關於合理假設下收斂性的強大理論保證。這些方法的效力通過各種異質場景的廣泛評估得到證明，顯示出公平性有顯著改善，這是通過用戶端差異度量指標測量的。

##### **Towards Case-based Interpretability for Medical Federated Learning**
2408.13626v1 by Laura Latorre, Liliana Petrychenko, Regina Beets-Tan, Taisiya Kopytova, Wilson Silva

We explore deep generative models to generate case-based explanations in a
medical federated learning setting. Explaining AI model decisions through
case-based interpretability is paramount to increasing trust and allowing
widespread adoption of AI in clinical practice. However, medical AI training
paradigms are shifting towards federated learning settings in order to comply
with data protection regulations. In a federated scenario, past data is
inaccessible to the current user. Thus, we use a deep generative model to
generate synthetic examples that protect privacy and explain decisions. Our
proof-of-concept focuses on pleural effusion diagnosis and uses publicly
available Chest X-ray data.

摘要：我們探索深度生成模型，在醫療聯邦學習設置中生成基於案例的說明。透過基於案例的可解釋性來解釋 AI 模型決策，對於增加信任並允許 AI 在臨床實務中廣泛採用至關重要。然而，醫療 AI 訓練範例正轉向聯邦學習設置，以符合資料保護法規。在聯邦情境中，過去的資料對目前的使用者而言是無法取得的。因此，我們使用深度生成模型來產生保護隱私和解釋決策的合成範例。我們的概念驗證著重於胸腔積液診斷，並使用公開可取得的胸部 X 光資料。

##### **HBIC: A Biclustering Algorithm for Heterogeneous Datasets**
2408.13217v1 by Adán José-García, Julie Jacques, Clément Chauvet, Vincent Sobanski, Clarisse Dhaenens

Biclustering is an unsupervised machine-learning approach aiming to cluster
rows and columns simultaneously in a data matrix. Several biclustering
algorithms have been proposed for handling numeric datasets. However,
real-world data mining problems often involve heterogeneous datasets with mixed
attributes. To address this challenge, we introduce a biclustering approach
called HBIC, capable of discovering meaningful biclusters in complex
heterogeneous data, including numeric, binary, and categorical data. The
approach comprises two stages: bicluster generation and bicluster model
selection. In the initial stage, several candidate biclusters are generated
iteratively by adding and removing rows and columns based on the frequency of
values in the original matrix. In the second stage, we introduce two approaches
for selecting the most suitable biclusters by considering their size and
homogeneity. Through a series of experiments, we investigated the suitability
of our approach on a synthetic benchmark and in a biomedical application
involving clinical data of systemic sclerosis patients. The evaluation
comparing our method to existing approaches demonstrates its ability to
discover high-quality biclusters from heterogeneous data. Our biclustering
approach is a starting point for heterogeneous bicluster discovery, leading to
a better understanding of complex underlying data structures.

摘要：雙聚類是一種非監督機器學習方法，旨在同時對資料矩陣中的列和行進行聚類。已提出多種雙聚類演算法來處理數值資料集。然而，現實世界的資料探勘問題通常涉及具有混合屬性的異質資料集。為了應對這一挑戰，我們引入了一種名為 HBIC 的雙聚類方法，它能夠在複雜的異質資料（包括數值、二進制和類別資料）中發現有意義的雙聚類。該方法包括兩個階段：雙聚類生成和雙聚類模型選擇。在初始階段，通過根據原始矩陣中的值頻率新增和移除列和行，反覆生成多個候選雙聚類。在第二階段，我們引入了兩種方法，通過考慮雙聚類的大小和同質性來選擇最合適的雙聚類。通過一系列實驗，我們研究了我們的方法在合成基準和涉及全身性硬化症患者臨床資料的生物醫學應用中的適用性。將我們的演算法與現有方法進行比較，評估結果證明了其從異質資料中發現高品質雙聚類的能力。我們的雙聚類方法是異質雙聚類發現的起點，有助於更好地理解複雜的底層資料結構。

##### **Causal machine learning for sustainable agroecosystems**
2408.13155v1 by Vasileios Sitokonstantinou, Emiliano Díaz Salas Porras, Jordi Cerdà Bautista, Maria Piles, Ioannis Athanasiadis, Hannah Kerner, Giulia Martini, Lily-belle Sweet, Ilias Tsoumas, Jakob Zscheischler, Gustau Camps-Valls

In a changing climate, sustainable agriculture is essential for food security
and environmental health. However, it is challenging to understand the complex
interactions among its biophysical, social, and economic components. Predictive
machine learning (ML), with its capacity to learn from data, is leveraged in
sustainable agriculture for applications like yield prediction and weather
forecasting. Nevertheless, it cannot explain causal mechanisms and remains
descriptive rather than prescriptive. To address this gap, we propose causal
ML, which merges ML's data processing with causality's ability to reason about
change. This facilitates quantifying intervention impacts for evidence-based
decision-making and enhances predictive model robustness. We showcase causal ML
through eight diverse applications that benefit stakeholders across the
agri-food chain, including farmers, policymakers, and researchers.

摘要：在氣候變遷的影響下，永續農業對於糧食安全和環境健康至關重要。然而，要了解其生物物理、社會和經濟成分之間的複雜互動卻是一項挑戰。預測機器學習 (ML) 具備從資料中學習的能力，因此被運用於永續農業中，例如產量預測和天氣預報等應用。儘管如此，它無法解釋因果機制，且仍然是描述性的而非規範性的。為了解決這個差距，我們提出了因果 ML，它將 ML 的資料處理與因果關係推理變化的能力結合在一起。這有助於量化干預影響，以利於基於證據的決策制定，並增強預測模型的穩健性。我們透過八個不同的應用展示因果 ML，這些應用惠及農業食品鏈中的利害關係人，包括農民、政策制定者和研究人員。

##### **Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey**
2408.12880v1 by Qika Lin, Yifan Zhu, Xin Mei, Ling Huang, Jingying Ma, Kai He, Zhen Peng, Erik Cambria, Mengling Feng

The rapid development of artificial intelligence has constantly reshaped the
field of intelligent healthcare and medicine. As a vital technology, multimodal
learning has increasingly garnered interest due to data complementarity,
comprehensive modeling form, and great application potential. Currently,
numerous researchers are dedicating their attention to this field, conducting
extensive studies and constructing abundant intelligent systems. Naturally, an
open question arises that has multimodal learning delivered universal
intelligence in healthcare? To answer the question, we adopt three unique
viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey
of the current progress of medical multimodal learning from the perspectives of
datasets, task-oriented methods, and universal foundation models. Based on
them, we further discuss the proposed question from five issues to explore the
real impacts of advanced techniques in healthcare, from data and technologies
to performance and ethics. The answer is that current technologies have NOT
achieved universal intelligence and there remains a significant journey to
undertake. Finally, in light of the above reviews and discussions, we point out
ten potential directions for exploration towards the goal of universal
intelligence in healthcare.

摘要：人工智能的快速发展持续重塑着智能医疗和医学领域。作为一项至关重要的技术，多模态学习由于数据互补性、综合建模形式和巨大的应用潜力而日益受到关注。目前，众多研究者将注意力投向这一领域，开展了广泛的研究并构建了丰富的智能系统。自然而然地，一个开放的问题出现了，即多模态学习是否在医疗保健中提供了通用智能？为了回答这个问题，我们采用三个独特的视角进行整体分析。首先，我们从数据集、面向任务的方法和通用基础模型的角度对医学多模态学习的当前进展进行了全面的调查。在此基础上，我们进一步从五个问题讨论了提出的问题，以探讨先进技术在医疗保健中的实际影响，从数据和技术到性能和伦理。答案是，当前技术尚未实现通用智能，并且仍有很长的路要走。最后，根据上述回顾和讨论，我们指出了实现医疗保健通用智能目标的十个潜在探索方向。

##### **COVID-19 Probability Prediction Using Machine Learning: An Infectious Approach**
2408.12841v1 by Mohsen Asghari Ilani, Saba Moftakhar Tehran, Ashkan Kavei, Arian Radmehr

The ongoing COVID-19 pandemic continues to pose significant challenges to
global public health, despite the widespread availability of vaccines. Early
detection of the disease remains paramount in curbing its transmission and
mitigating its impact on public health systems. In response, this study delves
into the application of advanced machine learning (ML) techniques for
predicting COVID-19 infection probability. We conducted a rigorous
investigation into the efficacy of various ML models, including XGBoost, LGBM,
AdaBoost, Logistic Regression, Decision Tree, RandomForest, CatBoost, KNN, and
Deep Neural Networks (DNN). Leveraging a dataset comprising 4000 samples, with
3200 allocated for training and 800 for testing, our experiment offers
comprehensive insights into the performance of these models in COVID-19
prediction. Our findings reveal that Deep Neural Networks (DNN) emerge as the
top-performing model, exhibiting superior accuracy and recall metrics. With an
impressive accuracy rate of 89%, DNN demonstrates remarkable potential in early
COVID-19 detection. This underscores the efficacy of deep learning approaches
in leveraging complex data patterns to identify COVID-19 infections accurately.
This study underscores the critical role of machine learning, particularly deep
learning methodologies, in augmenting early detection efforts amidst the
ongoing pandemic. The success of DNN in accurately predicting COVID-19
infection probability highlights the importance of continued research and
development in leveraging advanced technologies to combat infectious diseases.

摘要：持續進行的 COVID-19 大流行病持續對全球公共衛生構成重大挑戰，儘管疫苗已廣泛提供。早期發現疾病仍然是遏制其傳播和減輕其對公共衛生系統影響的首要任務。為此，本研究深入探討先進機器學習 (ML) 技術在預測 COVID-19 感染機率方面的應用。我們對各種 ML 模型的效能進行了嚴謹的調查，包括 XGBoost、LGBM、AdaBoost、邏輯迴歸、決策樹、隨機森林、CatBoost、KNN 和深度神經網路 (DNN)。利用包含 4000 個樣本的資料集，其中 3200 個分配給訓練，800 個分配給測試，我們的實驗對這些模型在 COVID-19 預測中的效能提供了全面的見解。我們的研究結果顯示，深度神經網路 (DNN) 成為表現最佳的模型，展現出優異的準確度和召回率指標。DNN 以 89% 的驚人準確度，證明了在早期 COVID-19 檢測中的傑出潛力。這突顯了深度學習方法在利用複雜資料模式準確識別 COVID-19 感染方面的效能。本研究強調了機器學習，特別是深度學習方法，在持續的大流行病中擴增早期檢測工作中的關鍵作用。DNN 在準確預測 COVID-19 感染機率方面的成功，突顯了持續研究和開發利用先進技術來對抗傳染病的重要性。

##### **Exploring Machine Learning Models for Lung Cancer Level Classification: A comparative ML Approach**
2408.12838v1 by Mohsen Asghari Ilani, Saba Moftakhar Tehran, Ashkan Kavei, Hamed Alizadegan

This paper explores machine learning (ML) models for classifying lung cancer
levels to improve diagnostic accuracy and prognosis. Through parameter tuning
and rigorous evaluation, we assess various ML algorithms. Techniques like
minimum child weight and learning rate monitoring were used to reduce
overfitting and optimize performance. Our findings highlight the robust
performance of Deep Neural Network (DNN) models across all phases. Ensemble
methods, including voting and bagging, also showed promise in enhancing
predictive accuracy and robustness. However, Support Vector Machine (SVM)
models with the Sigmoid kernel faced challenges, indicating a need for further
refinement. Overall, our study provides insights into ML-based lung cancer
classification, emphasizing the importance of parameter tuning to optimize
model performance and improve diagnostic accuracy in oncological care.

摘要：本論文探討機器學習 (ML) 模型，用於分類肺癌等級以提升診斷準確度和預後。透過參數調整和嚴謹評估，我們評估各種 ML 演算法。使用最小子權重和學習率監控等技術來減少過度擬合並最佳化效能。我們的研究結果強調深度神經網路 (DNN) 模型在所有階段的強健效能。包括投票和 bagging 在內的整體方法，也在提升預測準確度和強健性方面展現優勢。然而，使用 Sigmoid 核心的支援向量機 (SVM) 模型面臨挑戰，顯示需要進一步改良。整體而言，我們的研究提供機器學習為基礎的肺癌分類見解，強調參數調整對於最佳化模型效能和提升腫瘤照護診斷準確度的重要性。

##### **Phrasing for UX: Enhancing Information Engagement through Computational Linguistics and Creative Analytics**
2409.00064v1 by Nimrod Dvir

This study explores the relationship between textual features and Information
Engagement (IE) on digital platforms. It highlights the impact of computational
linguistics and analytics on user interaction. The READ model is introduced to
quantify key predictors like representativeness, ease of use, affect, and
distribution, which forecast engagement levels. The model's effectiveness is
validated through AB testing and randomized trials, showing strong predictive
performance in participation (accuracy: 0.94), perception (accuracy: 0.85),
perseverance (accuracy: 0.81), and overall IE (accuracy: 0.97).
  While participation metrics are strong, perception and perseverance show
slightly lower recall and F1-scores, indicating some challenges. The study
demonstrates that modifying text based on the READ model's insights leads to
significant improvements. For example, increasing representativeness and
positive affect boosts selection rates by 11 percent, raises evaluation
averages from 3.98 to 4.46, and improves retention rates by 11 percent. These
findings highlight the importance of linguistic factors in IE, providing a
framework for enhancing digital text engagement. The research offers practical
strategies applicable to fields like education, health, and media.

摘要：本研究探討了數位平台上文本特徵與資訊參與度 (IE) 之間的關係。它強調了計算語言學和分析對使用者互動的影響。READ 模型被引入用於量化關鍵預測因子，例如代表性、易用性、影響力，以及預測參與程度的分布。該模型的有效性已通過 AB 測試和隨機試驗得到驗證，在參與度（準確度：0.94）、感知（準確度：0.85）、堅持度（準確度：0.81）和整體 IE（準確度：0.97）中顯示出強大的預測效能。雖然參與度指標很強，但感知和堅持度顯示出稍低的召回率和 F1 分數，表明存在一些挑戰。研究表明，根據 READ 模型的見解修改文字會帶來顯著的改進。例如，提高代表性和正面影響力可將選擇率提高 11%，將評分平均值從 3.98 提高到 4.46，並將保留率提高 11%。這些發現強調了語言因素在 IE 中的重要性，提供了一個增強數位文字參與度的框架。該研究提供了適用於教育、健康和媒體等領域的實用策略。

##### **From Radiologist Report to Image Label: Assessing Latent Dirichlet Allocation in Training Neural Networks for Orthopedic Radiograph Classification**
2408.13284v1 by Jakub Olczak, Max Gordon

Background: Radiography (X-rays) is the dominant modality in orthopedics, and
improving the interpretation of radiographs is clinically relevant. Machine
learning (ML) has revolutionized data analysis and has been applied to
medicine, with some success, in the form of natural language processing (NLP)
and artificial neural networks (ANN). Latent Dirichlet allocation (LDA) is an
NLP method that automatically categorizes documents into topics. Successfully
applying ML to orthopedic radiography could enable the creation of
computer-aided decision systems for use in the clinic. We studied how an
automated ML pipeline could classify orthopedic trauma radiographs from
radiologist reports. Methods: Wrist and ankle radiographs from Danderyd
Hospital in Sweden taken between 2002 and 2015, with radiologist reports. LDA
was used to create image labels for radiographs from the radiologist reports.
Radiographs and labels were used to train an image recognition ANN. The ANN
outcomes were manually reviewed to get an accurate estimate of the method's
utility and accuracy. Results: Image Labels generated via LDA could
successfully train the ANN. The ANN reached an accuracy between 91% and 60%
compared to a gold standard, depending on the label. Conclusions: We found that
LDA was unsuited to label orthopedic radiographs from reports with high
accuracy. However, despite this, the ANN could learn to detect some features in
radiographs with high accuracy. The study also illustrates how ML and ANN can
be applied to medical research.

摘要：<paragraph>背景：放射科中，放射線照相（X 光）是主要的模式，而改善放射線照相的解讀在臨床上具有相關性。機器學習（ML）徹底改變了數據分析，並以自然語言處理（NLP）和人工神經網路（ANN）的形式應用於醫學，並取得了一些成功。潛在狄利克雷配置（LDA）是一種 NLP 方法，可自動將文件分類為主題。成功將 ML 應用於骨科放射線照相可以創建電腦輔助決策系統，供診所使用。我們研究了自動化 ML 管線如何從放射科醫師的報告中對骨科創傷放射線照相進行分類。方法：使用放射科醫師的報告，於 2002 年至 2015 年間在瑞典 Danderyd 醫院拍攝的手腕和腳踝放射線照相。LDA 用於根據放射科醫師的報告為放射線照相建立影像標籤。放射線照相和標籤用於訓練影像辨識 ANN。手動檢閱 ANN 結果以準確估計方法的效用和準確性。結果：透過 LDA 生成的影像標籤可以成功訓練 ANN。與黃金標準相比，ANN 達到了 91% 到 60% 的準確度，具體取決於標籤。結論：我們發現 LDA 不適合從報告中標記骨科放射線照相，準確度高。然而，儘管如此，ANN 仍可以學習以高準確度偵測放射線照相中的一些特徵。該研究還說明了 ML 和 ANN 如何應用於醫學研究。</paragraph>

##### **MultiMed: Massively Multimodal and Multitask Medical Understanding**
2408.12682v1 by Shentong Mo, Paul Pu Liang

Biomedical data is inherently multimodal, consisting of electronic health
records, medical imaging, digital pathology, genome sequencing, wearable
sensors, and more. The application of artificial intelligence tools to these
multifaceted sensing technologies has the potential to revolutionize the
prognosis, diagnosis, and management of human health and disease. However,
current approaches to biomedical AI typically only train and evaluate with one
or a small set of medical modalities and tasks. This limitation hampers the
development of comprehensive tools that can leverage the rich interconnected
information across many heterogeneous biomedical sensors. To address this
challenge, we present MultiMed, a benchmark designed to evaluate and enable
large-scale learning across a wide spectrum of medical modalities and tasks.
MultiMed consists of 2.56 million samples across ten medical modalities such as
medical reports, pathology, genomics, and protein data, and is structured into
eleven challenging tasks, including disease prognosis, protein structure
prediction, and medical question answering. Using MultiMed, we conduct
comprehensive experiments benchmarking state-of-the-art unimodal, multimodal,
and multitask models. Our analysis highlights the advantages of training
large-scale medical models across many related modalities and tasks. Moreover,
MultiMed enables studies of generalization across related medical concepts,
robustness to real-world noisy data and distribution shifts, and novel modality
combinations to improve prediction performance. MultiMed will be publicly
available and regularly updated and welcomes inputs from the community.

摘要：生物医学数据本质上是多模态的，由电子健康记录、医学影像、数字病理学、基因组测序、可穿戴传感器等组成。将人工智能工具应用于这些多方面的传感技术有可能彻底改变人类健康和疾病的预后、诊断和管理。然而，当前对生物医学人工智能的方法通常只针对一种或一小组医学方式和任务进行训练和评估。这种限制阻碍了能够利用许多异构生物医学传感器之间的丰富互联信息来开发综合工具。为了应对这一挑战，我们提出了 MultiMed，这是一个旨在评估和支持跨广泛医学方式和任务进行大规模学习的基准。MultiMed 包含了十种医学方式（例如医学报告、病理学、基因组学和蛋白质数据）中的 256 万个样本，并被构建成十一个具有挑战性的任务，包括疾病预后、蛋白质结构预测和医学问题解答。使用 MultiMed，我们进行了全面的实验，对最先进的单模态、多模态和多任务模型进行了基准测试。我们的分析突出了跨许多相关方式和任务训练大规模医学模型的优势。此外，MultiMed 支持对相关医学概念的泛化、对真实世界噪声数据和分布变化的鲁棒性以及新的方式组合以提高预测性能的研究。MultiMed 将公开提供并定期更新，并欢迎社区的意见。

##### **RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment**
2408.12579v1 by Xiaohan Wang, Xiaoyan Yang, Yuqi Zhu, Yue Shen, Jian Wang, Peng Wei, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang

Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve
performance competitively with human experts across various medical benchmarks.
However, they still face challenges in making professional diagnoses akin to
physicians, particularly in efficiently gathering patient information and
reasoning the final diagnosis. To this end, we introduce the RuleAlign
framework, designed to align LLMs with specific diagnostic rules. We develop a
medical dialogue dataset comprising rule-based communications between patients
and physicians and design an alignment learning approach through preference
learning. Experimental results demonstrate the effectiveness of the proposed
approach. We hope that our work can serve as an inspiration for exploring the
potential of LLMs as AI physicians.

摘要：大型語言模型（LLM），例如 GPT-4、MedPaLM-2 和 Med-Gemini，在各種醫療基準上達到了與人類專家競爭的表現。
然而，他們在做出類似於醫師的專業診斷方面仍面臨挑戰，特別是在有效收集患者資訊和推論最終診斷方面。為此，我們引入了 RuleAlign 框架，旨在將 LLM 與特定診斷規則保持一致。我們開發了一個醫療對話資料集，其中包含患者與醫師之間基於規則的溝通，並透過偏好學習設計了一種比對學習方法。實驗結果證明了所提方法的有效性。我們希望我們的工作能激勵探索 LLM 作為 AI 醫師的潛力。

##### **Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE 2023 Challenge**
2408.12534v1 by Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Ershuai Wang, Qin Zhou, Ziyan Huang, Pengju Lyu, Jian He, Bo Wang

Organ and cancer segmentation in abdomen Computed Tomography (CT) scans is
the prerequisite for precise cancer diagnosis and treatment. Most existing
benchmarks and algorithms are tailored to specific cancer types, limiting their
ability to provide comprehensive cancer analysis. This work presents the first
international competition on abdominal organ and pan-cancer segmentation by
providing a large-scale and diverse dataset, including 4650 CT scans with
various cancer types from over 40 medical centers. The winning team established
a new state-of-the-art with a deep learning-based cascaded framework, achieving
average Dice Similarity Coefficient scores of 92.3% for organs and 64.9% for
lesions on the hidden multi-national testing set. The dataset and code of top
teams are publicly available, offering a benchmark platform to drive further
innovations https://codalab.lisn.upsaclay.fr/competitions/12239.

摘要：器官和癌症分割在腹部電腦斷層掃描 (CT) 中是精確癌症診斷和治療的先決條件。大多數現有的基準和演算法都是針對特定癌症類型量身打造，這限制了它們提供全面癌症分析的能力。這項工作提供了第一個關於腹部器官和泛癌分割的國際競賽，方法是提供一個大型且多樣化的資料集，其中包括來自 40 多個醫療中心的 4650 個 CT 掃描，其中包含各種癌症類型。獲勝團隊建立了一個新的最先進的深度學習級聯框架，在隱藏的多國測試集中實現了器官的平均 Dice 相似性系數分數 92.3%，病灶的平均 Dice 相似性系數分數 64.9%。頂尖團隊的資料集和程式碼公開提供，提供了一個基準平台，以推動進一步的創新 https://codalab.lisn.upsaclay.fr/competitions/12239。

##### **MEDCO: Medical Education Copilots Based on A Multi-Agent Framework**
2408.12496v1 by Hao Wei, Jianing Qiu, Haibao Yu, Wu Yuan

Large language models (LLMs) have had a significant impact on diverse
research domains, including medicine and healthcare. However, the potential of
LLMs as copilots in medical education remains underexplored. Current
AI-assisted educational tools are limited by their solitary learning approach
and inability to simulate the multi-disciplinary and interactive nature of
actual medical training. To address these limitations, we propose MEDCO
(Medical EDucation COpilots), a novel multi-agent-based copilot system
specially developed to emulate real-world medical training environments. MEDCO
incorporates three primary agents: an agentic patient, an expert doctor, and a
radiologist, facilitating a multi-modal and interactive learning environment.
Our framework emphasizes the learning of proficient question-asking skills,
multi-disciplinary collaboration, and peer discussions between students. Our
experiments show that simulated virtual students who underwent training with
MEDCO not only achieved substantial performance enhancements comparable to
those of advanced models, but also demonstrated human-like learning behaviors
and improvements, coupled with an increase in the number of learning samples.
This work contributes to medical education by introducing a copilot that
implements an interactive and collaborative learning approach. It also provides
valuable insights into the effectiveness of AI-integrated training paradigms.

摘要：大型語言模型 (LLM) 對不同研究領域產生重大影響，包括醫學和保健。然而，LLM 作為醫學教育副手的潛力仍未被充分探討。目前的 AI 輔助教育工具受到其單獨學習方法的限制，且無法模擬實際醫學訓練的多學科和互動性質。為了解決這些限制，我們提出 MEDCO（醫學教育副手），一種新穎的多代理人協作系統，專門開發用於模擬真實世界的醫學訓練環境。MEDCO 結合了三個主要代理人：一個代理人患者、一個專家醫生和一個放射科醫生，促進多模式和互動學習環境。我們的架構強調學習熟練的提問技巧、跨學科協作和學生之間的同儕討論。我們的實驗表明，接受 MEDCO 訓練的模擬虛擬學生不僅獲得與進階模型相當的顯著效能提升，還表現出類似人類的學習行為和進步，並伴隨著學習樣本數量的增加。這項工作透過引入一個實施互動和協作學習方法的副手，對醫學教育有所貢獻。它也提供了對 AI 整合訓練模式有效性的寶貴見解。

##### **AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**
2408.12491v1 by Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. Gruühagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans

Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging
lesions with variable clinical behaviours and treatment approaches. This
systematic review provides an overview of Artificial Intelligence (AI) methods
using radiological imaging for diagnosis and prognosis of these tumours,
highlighting challenges in clinical translation, and evaluating study alignment
with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI
international consensus guidelines for trustworthy and deployable AI to promote
the clinical translation of AI methods. The review covered literature from
several bibliographic databases, including papers published before 17/07/2024.
Original research in peer-reviewed journals focused on radiology-based AI for
diagnosing or prognosing primary STBT was included. Exclusion criteria were
animal, cadaveric, or laboratory studies, and non-English papers. Abstracts
were screened by two of three independent reviewers for eligibility. Eligible
papers were assessed against guidelines by one of three independent reviewers.
The search identified 15,015 abstracts, from which 325 articles were included
for evaluation. Most studies performed moderately on CLAIM, averaging a score
of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out
of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,
indicating significant room for improvement. Future efforts by AI developers
should focus on design (e.g. define unmet clinical need, intended clinical
setting and how AI would be integrated in clinical workflow), development (e.g.
build on previous work, explainability), evaluation (e.g. evaluating and
addressing biases, evaluating AI against best practices), and data
reproducibility and availability (making documented code and data publicly
available). Following these recommendations could improve clinical translation
of AI methods.

摘要：軟組織和骨骼腫瘤（STBT）是罕見、診斷具有挑戰性的病灶，其臨床行為和治療方法各不相同。這篇系統性回顧提供了使用放射影像進行診斷和預後的人工智慧 (AI) 方法的概觀，重點說明了臨床轉譯的挑戰，並評估研究與醫療影像 AI 核查表 (CLAIM) 和 FUTURE-AI 可信賴且可部署 AI 的國際共識準則的一致性，以促進 AI 方法的臨床轉譯。這篇回顧涵蓋了幾個書目資料庫中的文獻，包括在 2024 年 7 月 17 日之前發表的論文。納入了以放射為基礎的 AI 診斷或預後原發性 STBT 的同行評審期刊中的原始研究。排除標準是動物、屍體或實驗室研究，以及非英文論文。摘要由三位獨立審查員中的兩位篩選資格。合格的論文由三位獨立審查員中的一位根據準則進行評估。搜索識別出 15,015 篇摘要，其中 325 篇文章被納入評估。大多數研究在 CLAIM 中表現中等，平均得分為 53 分中的 28.9±7.5 分，但在 FUTURE-AI 中表現不佳，平均得分為 30 分中的 5.1±2.1 分。STBT 的影像 AI 工具仍處於概念驗證階段，表明有顯著的改進空間。AI 開發人員未來的努力應集中在設計（例如定義未滿足的臨床需求、預期的臨床環境以及 AI 如何整合到臨床工作流程中）、開發（例如建立在先前的工作、可解釋性）、評估（例如評估和解決偏差、評估 AI 與最佳實務）、以及數據可複製性和可用性（公開提供文件化的代碼和數據）。遵循這些建議可以改善 AI 方法的臨床轉譯。

##### **WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation**
2408.12466v1 by Palak Handa, Manas Dhir, Amirreza Mahbod, Florian Schwarzhans, Ramona Woitek, Nidhi Goel, Deepak Gunjan

Computer-based analysis of Wireless Capsule Endoscopy (WCE) is crucial.
However, a medically annotated WCE dataset for training and evaluation of
automatic classification, detection, and segmentation of bleeding and
non-bleeding frames is currently lacking. The present work focused on
development of a medically annotated WCE dataset called WCEbleedGen for
automatic classification, detection, and segmentation of bleeding and
non-bleeding frames. It comprises 2,618 WCE bleeding and non-bleeding frames
which were collected from various internet resources and existing WCE datasets.
A comprehensive benchmarking and evaluation of the developed dataset was done
using nine classification-based, three detection-based, and three
segmentation-based deep learning models. The dataset is of high-quality, is
class-balanced and contains single and multiple bleeding sites. Overall, our
standard benchmark results show that Visual Geometric Group (VGG) 19, You Only
Look Once version 8 nano (YOLOv8n), and Link network (Linknet) performed best
in automatic classification, detection, and segmentation-based evaluations,
respectively. Automatic bleeding diagnosis is crucial for WCE video
interpretations. This diverse dataset will aid in developing of real-time,
multi-task learning-based innovative solutions for automatic bleeding diagnosis
in WCE. The dataset and code are publicly available at
https://zenodo.org/records/10156571 and
https://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.

摘要：<paragraph>無線膠囊內視鏡 (WCE) 的電腦分析至關重要。
然而，目前缺乏一個醫學標註的 WCE 資料集，用於訓練和評估出血和非出血幀的自動分類、檢測和分割。本研究專注於開發一個名為 WCEbleedGen 的醫學標註 WCE 資料集，用於出血和非出血幀的自動分類、檢測和分割。它包含 2,618 個 WCE 出血和非出血幀，這些幀是從各種網路資源和現有的 WCE 資料集中收集的。使用九個基於分類、三個基於檢測和三個基於分割的深度學習模型對開發的資料集進行了全面的基準測試和評估。該資料集質量高、類別平衡且包含單個和多個出血部位。總體而言，我們的標準基準測試結果表明，Visual Geometric Group (VGG) 19、You Only Look Once 版本 8 nano (YOLOv8n) 和 Link 網路 (Linknet) 在自動分類、檢測和基於分割的評估中表現最佳，分別。自動出血診斷對於 WCE 視訊解讀至關重要。這個多樣化的資料集將有助於開發用於 WCE 中自動出血診斷的基於實時、多任務學習的創新解決方案。該資料集和程式碼可在 https://zenodo.org/records/10156571 和 https://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset 公開獲得。</paragraph>

##### **SAM-SP: Self-Prompting Makes SAM Great Again**
2408.12364v1 by Chunpeng Zhou, Kangjie Ning, Qianqian Shen, Sheng Zhou, Zhi Yu, Haishuai Wang

The recently introduced Segment Anything Model (SAM), a Visual Foundation
Model (VFM), has demonstrated impressive capabilities in zero-shot segmentation
tasks across diverse natural image datasets. Despite its success, SAM
encounters noticeably performance degradation when applied to specific domains,
such as medical images. Current efforts to address this issue have involved
fine-tuning strategies, intended to bolster the generalizability of the vanilla
SAM. However, these approaches still predominantly necessitate the utilization
of domain specific expert-level prompts during the evaluation phase, which
severely constrains the model's practicality.
  To overcome this limitation, we introduce a novel self-prompting based
fine-tuning approach, called SAM-SP, tailored for extending the vanilla SAM
model. Specifically, SAM-SP leverages the output from the previous iteration of
the model itself as prompts to guide subsequent iteration of the model. This
self-prompting module endeavors to learn how to generate useful prompts
autonomously and alleviates the dependence on expert prompts during the
evaluation phase, significantly broadening SAM's applicability. Additionally,
we integrate a self-distillation module to enhance the self-prompting process
further. Extensive experiments across various domain specific datasets validate
the effectiveness of the proposed SAM-SP. Our SAM-SP not only alleviates the
reliance on expert prompts but also exhibits superior segmentation performance
comparing to the state-of-the-art task-specific segmentation approaches, the
vanilla SAM, and SAM-based approaches.

摘要：最近推出的 Segment Anything Model (SAM)，一種視覺基礎模型 (VFM)，在各種自然影像資料集的零次分段任務中展現出令人印象深刻的能力。儘管 SAM 成功，但應用於特定領域（例如醫學影像）時，效能明顯下降。目前解決此問題的方法包括微調策略，旨在加強香草 SAM 的概括性。然而，這些方法在評估階段仍然主要需要使用特定領域的專家級提示，這嚴重限制了模型的實用性。
為了克服此限制，我們引入一種基於自我提示的新型微調方法，稱為 SAM-SP，專門用於擴充香草 SAM 模型。具體來說，SAM-SP 利用模型本身先前反覆運算的輸出作為提示，引導模型後續反覆運算。此自我提示模組努力學習如何自主產生有用的提示，並減輕評估階段對專家提示的依賴性，顯著擴展 SAM 的適用性。此外，我們整合一個自我蒸餾模組，進一步增強自我提示的過程。在各種特定領域資料集中的大量實驗驗證了所提出的 SAM-SP 的有效性。我們的 SAM-SP 不僅減輕了對專家提示的依賴性，而且與最先進的特定任務分割方法、香草 SAM 和基於 SAM 的方法相比，還展現出優異的分割效能。

##### **Class-balanced Open-set Semi-supervised Object Detection for Medical Images**
2408.12355v1 by Zhanyun Lu, Renshu Gu, Huimin Cheng, Siyu Pang, Mingyu Xu, Peifang Xu, Yaqi Wang, Yuichiro Kinoshita, Juan Ye, Gangyong Jia, Qing Wu

Medical image datasets in the real world are often unlabeled and imbalanced,
and Semi-Supervised Object Detection (SSOD) can utilize unlabeled data to
improve an object detector. However, existing approaches predominantly assumed
that the unlabeled data and test data do not contain out-of-distribution (OOD)
classes. The few open-set semi-supervised object detection methods have two
weaknesses: first, the class imbalance is not considered; second, the OOD
instances are distinguished and simply discarded during pseudo-labeling. In
this paper, we consider the open-set semi-supervised object detection problem
which leverages unlabeled data that contain OOD classes to improve object
detection for medical images. Our study incorporates two key innovations:
Category Control Embed (CCE) and out-of-distribution Detection Fusion
Classifier (OODFC). CCE is designed to tackle dataset imbalance by constructing
a Foreground information Library, while OODFC tackles open-set challenges by
integrating the ``unknown'' information into basic pseudo-labels. Our method
outperforms the state-of-the-art SSOD performance, achieving a 4.25 mAP
improvement on the public Parasite dataset.

摘要：真實世界的醫學影像資料集通常未標籤且不平衡，而半監督物件偵測 (SSOD) 可以利用未標籤資料來改善物件偵測器。然而，現有方法主要假設未標籤資料和測試資料不包含分佈外 (OOD) 類別。少數開放式半監督物件偵測方法有兩個缺點：首先，類別不平衡未被考慮；其次，OOD 實例在偽標籤期間被區分並簡單地捨棄。在本文中，我們考慮了開放式半監督物件偵測問題，它利用包含 OOD 類別的未標籤資料來改善醫學影像的物件偵測。我們的研究包含兩項關鍵創新：類別控制嵌入 (CCE) 和分佈外偵測融合分類器 (OODFC)。CCE 旨在透過建構前景資訊庫來解決資料集不平衡，而 OODFC 透過將「未知」資訊整合到基本偽標籤中來解決開放式挑戰。我們的模型優於最先進的 SSOD 效能，在公開寄生蟲資料集上達到 4.25 mAP 的提升。

##### **Large Language Models Are Self-Taught Reasoners: Enhancing LLM Applications via Tailored Problem-Solving Demonstrations**
2408.12315v1 by Kai Tzu-iunn Ong, Taeyoon Kwon, Jinyoung Yeo

Guiding large language models with a selected set of human-authored
demonstrations is a common practice for improving LLM applications. However,
human effort can be costly, especially in specialized domains (e.g., clinical
diagnosis), and does not guarantee optimal performance due to the potential
discrepancy of target skills between selected demonstrations and real test
instances. Motivated by these, this paper explores the automatic creation of
customized demonstrations, whose target skills align with the given target
instance. We present SELF-TAUGHT, a problem-solving framework, which
facilitates demonstrations that are "tailored" to the target problem and
"filtered" for better quality (i.e., correctness) in a zero-shot manner. In 15
tasks of multiple-choice questions of diverse domains and the diagnosis of
Alzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves
superior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,
Auto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its
generalizability to existing prompting methods and different LLMs, the quality
of its intermediate generation, and more.

摘要：使用一組由人類撰寫的示範來指導大型語言模型是一種改善 LLM 應用程式的常見做法。然而，人力成本可能很高，特別是在專業領域（例如臨床診斷）中，而且由於選定的示範與實際測試實例之間目標技能的潛在差異，並不能保證最佳效能。基於這些動機，本文探討了自動建立自訂示範，其目標技能與給定的目標實例一致。我們提出了一個問題解決架構 SELF-TAUGHT，它可以促進「針對」目標問題「篩選」出更高品質（即正確性）的示範，且採用零次學習的方式。在多個領域的多選題任務和對真實世界患者進行阿茲海默症 (AD) 診斷的 15 項任務中，SELF-TAUGHT 達到了優於強大基準（例如 Few-shot CoT、Plan-and-Solve、Auto-CoT）的效能。我們對 SELF-TAUGHT 進行了全面的分析，包括其對現有提示方法和不同 LLM 的概括性、其中間產生的品質等等。

##### **Tipta uzmanlik sinavinda (tus) buyuk dil modelleri insanlardan daha mi basarili?**
2408.12305v2 by Yesim Aygul, Muge Olucoglu, Adil Alpkocak

The potential of artificial intelligence in medical education and assessment
has been made evident by recent developments in natural language processing and
artificial intelligence. Medical questions can now be successfully answered by
artificial intelligence algorithms. It can help medical practitioners. This
study evaluates the performance of three different artificial intelligence
models in answering Turkish medical questions in the 2021 1st Term Medical
Specialization Examination (MSE). MSE consists of a total of 240 questions
across clinical (CMST) and basic (BMST) medical sciences. According to the
results in CMST, it was concluded that Gemini correctly answered 82 questions,
ChatGPT-4 answered 105 questions and ChatGPT-4o answered 117 questions. In
BMST, Gemini and ChatGPT-4 answered 93 questions and ChatGPT-4o answered 107
questions correctly according to the answer key. ChatGPT-4o outperformed the
candidate with the highest scores of 113 and 106 according to CMST and BMST
respectively. This study highlights the importance of the potential of
artificial intelligence in medical education and assessment. It demonstrates
that advanced models can achieve high accuracy and contextual understanding,
demonstrating their potential role in medical education and evaluation.

摘要：人工智能在医学教育和評估中的潛力
最近自然語言處理和人工智能的發展，證明了人工智能在醫學教育和評估中的潛力。人工智能演算法現在可以成功回答醫學問題。它可以幫助醫療從業人員。這項研究評估了三種不同的人工智能模型在回答 2021 年第 1 學期醫學專科考試 (MSE) 中的土耳其醫學問題時的表現。MSE 總共包含 240 題，涵蓋臨床 (CMST) 和基礎 (BMST) 醫學科學。根據 CMST 的結果，結論是 Gemini 正確回答了 82 題，ChatGPT-4 回答了 105 題，ChatGPT-4o 回答了 117 題。在 BMST 中，根據答案關鍵，Gemini 和 ChatGPT-4 回答了 93 題，ChatGPT-4o 回答了 107 題。根據 CMST 和 BMST，ChatGPT-4o 的表現優於分別獲得 113 分和 106 分的最高分考生。這項研究突顯了人工智能在醫學教育和評估中的潛力。它證明了進階模型可以達到高準確度和情境理解，展現了它們在醫學教育和評估中的潛在作用。

##### **Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features**
2408.12279v1 by Shaoxiang Dang, Tetsuya Matsumoto, Yoshinori Takeuchi, Takashi Tsuboi, Yasuhiro Tanaka, Daisuke Nakatsubo, Satoshi Maesawa, Ryuta Saito, Masahisa Katsuno, Hiroaki Kudo

The potential of deep learning in clinical speech processing is immense, yet
the hurdles of limited and imbalanced clinical data samples loom large. This
article addresses these challenges by showcasing the utilization of automatic
speech recognition and self-supervised learning representations, pre-trained on
extensive datasets of normal speech. This innovative approach aims to estimate
voice quality of patients with impaired vocal systems. Experiments involve
checks on PVQD dataset, covering various causes of vocal system damage in
English, and a Japanese dataset focusing on patients with Parkinson's disease
before and after undergoing subthalamic nucleus deep brain stimulation
(STN-DBS) surgery. The results on PVQD reveal a notable correlation (>0.8 on
PCC) and an extraordinary accuracy (<0.5 on MSE) in predicting Grade, Breathy,
and Asthenic indicators. Meanwhile, progress has been achieved in predicting
the voice quality of patients in the context of STN-DBS.

摘要：深度學習在臨床語音處理的潛力巨大，但
受限且不平衡的臨床資料樣本的障礙卻很大。這
篇文章透過展示自動語音辨識和自我監督學習表徵的應用，來解決這些挑戰，這些表徵是預先在正常的語音的廣泛資料集上訓練過的。這種創新的方法旨在評估發聲系統受損的病患的聲音品質。實驗包含在 PVQD 資料集上的檢查，涵蓋英文中各種造成發聲系統損傷的原因，以及一個日文資料集，專注於在接受丘腦下核深部腦部刺激 (STN-DBS) 手術前後的帕金森氏症病患。PVQD 的結果顯示出顯著相關性（PCC 上 >0.8），以及預測等級、氣音和無力指標的非凡準確性（MSE 上 <0.5）。同時，在預測 STN-DBS 背景下病患的聲音品質方面也取得了進展。

##### **LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction**
2408.12249v1 by Aishik Nagar, Viktor Schlegel, Thanh-Tung Nguyen, Hao Li, Yuping Wu, Kuluhan Binici, Stefan Winkler

Large Language Models (LLMs) are increasingly adopted for applications in
healthcare, reaching the performance of domain experts on tasks such as
question answering and document summarisation. Despite their success on these
tasks, it is unclear how well LLMs perform on tasks that are traditionally
pursued in the biomedical domain, such as structured information extration. To
breach this gap, in this paper, we systematically benchmark LLM performance in
Medical Classification and Named Entity Recognition (NER) tasks. We aim to
disentangle the contribution of different factors to the performance,
particularly the impact of LLMs' task knowledge and reasoning capabilities,
their (parametric) domain knowledge, and addition of external knowledge. To
this end we evaluate various open LLMs -- including BioMistral and Llama-2
models -- on a diverse set of biomedical datasets, using standard prompting,
Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as
Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.
Counter-intuitively, our results reveal that standard prompting consistently
outperforms more complex techniques across both tasks, laying bare the
limitations in the current application of CoT, self-consistency and RAG in the
biomedical domain. Our findings suggest that advanced prompting methods
developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are
not easily portable to biomedical tasks where precise structured outputs are
required. This highlights the need for more effective integration of external
knowledge and reasoning mechanisms in LLMs to enhance their performance in
real-world biomedical applications.

摘要：大型語言模型 (LLM) 愈來愈多用於醫療保健應用，在回答問題和文件摘要等任務上達到領域專家的表現。儘管這些任務獲得成功，但尚不清楚 LLM 在生物醫學領域傳統上執行的任務，例如結構化資訊萃取，表現如何。為了彌補這個差距，我們在這篇論文中系統性地評量 LLM 在醫學分類和命名實體辨識 (NER) 任務中的表現。我們的目標是釐清不同因素對表現的貢獻，特別是 LLM 的任務知識和推理能力、它們的（參數）領域知識，以及外部知識的加入。為此，我們評估各種開放的 LLM（包括 BioMistral 和 Llama-2 模型），使用標準提示、基於思考鏈 (CoT) 和自洽性的推理以及使用 PubMed 和維基百科語料庫的檢索增強生成 (RAG) 在多樣化的生物醫學資料集上。與直覺相反，我們的結果顯示標準提示在兩項任務中始終優於更複雜的技術，揭露了在生物醫學領域中 CoT、自洽性和 RAG 的當前應用中的限制。我們的發現表明，為知識或推理密集型任務（例如 CoT 或 RAG）開發的高階提示方法不容易移植到需要精確結構化輸出的生物醫學任務。這突顯出需要更有效地整合外部知識和推理機制到 LLM 中，以增強它們在實際生物醫學應用中的表現。

##### **MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**
2408.12236v1 by Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou

Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.

摘要：醫學教育高度依賴模擬病人 (SP) 提供一個安全的環境，讓學生練習臨床技能，包括醫學影像分析。然而，招募合格 SP 的高成本和缺乏多樣的醫學影像資料集已造成顯著的挑戰。為了解決這些問題，本文介紹 MedDiT，一個新穎的知識控制對話架構，它可以動態產生符合模擬病人症狀的合理醫學影像，實現多樣的診斷技能訓練。具體來說，MedDiT 整合了各種病人知識圖譜 (KG)，描述病人的屬性和症狀，以動態提示大型語言模型 (LLM) 的行為，並控制病人特徵，減輕醫學對話中的幻覺。此外，還納入一個經過微調的擴散Transformer (DiT) 模型，根據 KG 中指定的病人屬性產生醫學影像。在本文中，我們透過實際示範展示 MedDiT 的功能，展示它在不同模擬病人案例中作用並產生相應醫學影像的能力。這可以為學生提供豐富且互動的學習體驗，透過提供身歷其境的模擬平台，提升醫學教育，造福未來的醫療保健專業人員。這項工作闡明了在教育應用中整合 LLM、KG 和 DiT 等先進技術的可行性，突顯它們在解決模擬病人為基礎的醫學教育所面臨挑戰的潛力。

##### **MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents**
2408.12142v1 by Congchi Yin, Feng Li, Shu Zhang, Zike Wang, Jun Shao, Piji Li, Jianhua Chen, Xun Jiang

The clinical diagnosis of most mental disorders primarily relies on the
conversations between psychiatrist and patient. The creation of such diagnostic
conversation datasets is promising to boost the AI mental healthcare community.
However, directly collecting the conversations in real diagnosis scenarios is
near impossible due to stringent privacy and ethical considerations. To address
this issue, we seek to synthesize diagnostic conversation by exploiting
anonymous patient cases that are easier to access. Specifically, we design a
neuro-symbolic multi-agent framework for synthesizing the diagnostic
conversation of mental disorders with large language models. It takes patient
case as input and is capable of generating multiple diverse conversations with
one single patient case. The framework basically involves the interaction
between a doctor agent and a patient agent, and achieves text generation under
symbolic control via a dynamic diagnosis tree from a tool agent. By applying
the proposed framework, we develop the largest Chinese mental disorders
diagnosis dataset MDD-5k, which is built upon 1000 cleaned real patient cases
by cooperating with a pioneering psychiatric hospital, and contains 5000
high-quality long conversations with diagnosis results as labels. To the best
of our knowledge, it's also the first labelled Chinese mental disorders
diagnosis dataset. Human evaluation demonstrates the proposed MDD-5k dataset
successfully simulates human-like diagnostic process of mental disorders. The
dataset and code will become publicly accessible in
https://github.com/lemonsis/MDD-5k.

摘要：大多數精神疾病的臨床診斷主要依賴於精神科醫師與病患之間的對話。建立此類診斷對話資料集有望提升 AI 心理保健社群。然而，由於嚴格的隱私和倫理考量，直接在真實診斷情境中收集對話幾乎是不可能的。為了解決這個問題，我們試圖利用較容易取得的匿名病患案例來合成診斷對話。具體來說，我們設計了一個神經符號多主體架構，用於合成具有大型語言模型的精神疾病診斷對話。它以病患案例作為輸入，並且能夠針對單一病患案例產生多個不同的對話。此架構基本上涉及醫師主體和病患主體之間的互動，並透過來自工具主體的動態診斷樹來實現符號控制下的文字產生。透過應用建議的架構，我們開發了最大的中文精神疾病診斷資料集 MDD-5k，它是建立在 1000 個透過與一家先驅精神病院合作而整理過的真實病患案例上，並包含 5000 個帶有診斷結果標籤的高品質長對話。據我們所知，它也是第一個標記的中文精神疾病診斷資料集。人類評估證明，建議的 MDD-5k 資料集成功模擬了類人的精神疾病診斷流程。資料集和程式碼將在 https://github.com/lemonsis/MDD-5k 公開。

##### **DRExplainer: Quantifiable Interpretability in Drug Response Prediction with Directed Graph Convolutional Network**
2408.12139v1 by Haoyuan Shi, Tao Xu, Xiaodi Li, Qian Gao, Junfeng Xia, Zhenyu Yue

Predicting the response of a cancer cell line to a therapeutic drug is
pivotal for personalized medicine. Despite numerous deep learning methods that
have been developed for drug response prediction, integrating diverse
information about biological entities and predicting the directional response
remain major challenges. Here, we propose a novel interpretable predictive
model, DRExplainer, which leverages a directed graph convolutional network to
enhance the prediction in a directed bipartite network framework. DRExplainer
constructs a directed bipartite network integrating multi-omics profiles of
cell lines, the chemical structure of drugs and known drug response to achieve
directed prediction. Then, DRExplainer identifies the most relevant subgraph to
each prediction in this directed bipartite network by learning a mask,
facilitating critical medical decision-making. Additionally, we introduce a
quantifiable method for model interpretability that leverages a ground truth
benchmark dataset curated from biological features. In computational
experiments, DRExplainer outperforms state-of-the-art predictive methods and
another graph-based explanation method under the same experimental setting.
Finally, the case studies further validate the interpretability and the
effectiveness of DRExplainer in predictive novel drug response. Our code is
available at: https://github.com/vshy-dream/DRExplainer.

摘要：預測癌症細胞株對治療藥物的反應對於個人化醫療至關重要。儘管已經開發出許多用於藥物反應預測的深度學習方法，但整合生物實體的多樣信息和預測方向反應仍然是主要的挑戰。在這裡，我們提出了一個新穎的可解釋預測模型 DRExplainer，它利用有向圖卷積網路在有向二部網路框架中增強預測。DRExplainer 構建了一個有向二部網路，整合了細胞系的組學概況、藥物的化學結構和已知的藥物反應，以實現有向預測。然後，DRExplainer 通過學習遮罩識別此有向二部網路中與每個預測最相關的子圖，促進關鍵的醫療決策制定。此外，我們引入了一種量化模型可解釋性的方法，利用從生物特徵中策劃的地面實況基準資料集。在計算實驗中，DRExplainer 在相同的實驗設置下優於最先進的預測方法和另一種基於圖表的解釋方法。最後，案例研究進一步驗證了 DRExplainer 在預測新藥物反應中的可解釋性和有效性。我們的程式碼可在 https://github.com/vshy-dream/DRExplainer 取得。

##### **Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards**
2408.12112v1 by Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe

LLMs are increasingly used to design reward functions based on human
preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards
for Restless Multi-Armed Bandits, a framework for allocating limited resources
among agents. In applications such as public health, this approach empowers
grassroots health workers to tailor automated allocation decisions to community
needs. In the presence of multiple agents, altering the reward function based
on human preferences can impact subpopulations very differently, leading to
complex tradeoffs and a multi-objective resource allocation problem. We are the
first to present a principled method termed Social Choice Language Model for
dealing with these tradeoffs for LLM-designed rewards for multiagent planners
in general and restless bandits in particular. The novel part of our model is a
transparent and configurable selection component, called an adjudicator,
external to the LLM that controls complex tradeoffs via a user-selected social
welfare function. Our experiments demonstrate that our model reliably selects
more effective, aligned, and balanced reward functions compared to purely
LLM-based approaches.

摘要：LLM  zunehmend verwendet werden, um Belohnungsfunktionen basierend auf menschlichen Präferenzen in Reinforcement Learning (RL) zu entwerfen. Wir konzentrieren uns auf LLM-entworfene Belohnungen für Restless Multi-Armed Bandits, ein Framework zur Zuweisung begrenzter Ressourcen unter Agenten. In Anwendungen wie dem öffentlichen Gesundheitswesen ermöglicht dieser Ansatz Basisgesundheitsfachkräften, automatisierte Zuweisungsentscheidungen auf die Bedürfnisse der Gemeinschaft zuzuschneiden. In Gegenwart mehrerer Agenten kann die Änderung der Belohnungsfunktion basierend auf menschlichen Präferenzen Untergruppen sehr unterschiedlich beeinflussen, was zu komplexen Kompromissen und einem mehrzielgerichteten Ressourcenzuweisungsproblem führt. Wir sind die Ersten, die eine prinzipielle Methode namens Social Choice Language Model für den Umgang mit diesen Kompromissen für LLM-entworfene Belohnungen für Multiagentenplaner im Allgemeinen und unruhige Banditen im Besonderen vorstellen. Der neuartige Teil unseres Modells ist eine transparente und konfigurierbare Auswahlkomponente, die als Schiedsrichter bezeichnet wird und sich außerhalb des LLM befindet und komplexe Kompromisse über eine vom Benutzer ausgewählte soziale Wohlfahrtsfunktion steuert. Unsere Experimente zeigen, dass unser Modell im Vergleich zu rein LLM-basierten Ansätzen zuverlässig effektivere, ausgerichtete und ausgewogene Belohnungsfunktionen auswählt.

##### **uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization**
2408.12095v2 by Aishik Nagar, Yutong Liu, Andy T. Liu, Viktor Schlegel, Vijay Prakash Dwivedi, Arun-Kumar Kaliya-Perumal, Guna Pratheep Kalanchiam, Yili Tang, Robby T. Tan

Medical abstractive summarization faces the challenge of balancing
faithfulness and informativeness. Current methods often sacrifice key
information for faithfulness or introduce confabulations when prioritizing
informativeness. While recent advancements in techniques like in-context
learning (ICL) and fine-tuning have improved medical summarization, they often
overlook crucial aspects such as faithfulness and informativeness without
considering advanced methods like model reasoning and self-improvement.
Moreover, the field lacks a unified benchmark, hindering systematic evaluation
due to varied metrics and datasets. This paper addresses these gaps by
presenting a comprehensive benchmark of six advanced abstractive summarization
methods across three diverse datasets using five standardized metrics. Building
on these findings, we propose uMedSum, a modular hybrid summarization framework
that introduces novel approaches for sequential confabulation removal followed
by key missing information addition, ensuring both faithfulness and
informativeness. Our work improves upon previous GPT-4-based state-of-the-art
(SOTA) medical summarization methods, significantly outperforming them in both
quantitative metrics and qualitative domain expert evaluations. Notably, we
achieve an average relative performance improvement of 11.8% in reference-free
metrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more
than previous SOTA in difficult cases where there are chances of confabulations
or missing information. These results highlight uMedSum's effectiveness and
generalizability across various datasets and metrics, marking a significant
advancement in medical summarization.

摘要：<paragraph>醫學摘要摘要面臨平衡忠實度和資訊性的挑戰。目前的許多方法經常犧牲關鍵資訊以換取忠實度，或在優先考量資訊性時引入虛構。雖然情境內學習 (ICL) 和微調等技術的最新進展已改善了醫學摘要，但它們經常忽略關鍵層面，例如忠實度和資訊性，而未考慮模型推理和自我改善等進階方法。此外，該領域缺乏統一的基準，由於不同的指標和資料集而阻礙了系統性評估。本文透過提出六種進階抽象摘要方法的綜合基準，使用五種標準化指標跨越三個不同的資料集，來解決這些差距。根據這些發現，我們提出了 uMedSum，這是一個模組化的混合摘要架構，它引入了用於連續虛構移除的創新方法，接著是關鍵遺漏資訊的加入，確保忠實度和資訊性。我們的研究改進了先前的基於 GPT-4 的最先進 (SOTA) 醫學摘要方法，在量化指標和定性領域專家評估中都顯著優於它們。值得注意的是，我們在無參考指標中實現了相對於先前 SOTA 11.8% 的平均相對效能提升。在可能出現虛構或遺漏資訊的困難案例中，醫生比先前的 SOTA 更喜歡 uMedSum 的摘要 6 倍。這些結果突顯了 uMedSum 在各種資料集和指標中的有效性和概括性，標誌著醫學摘要的重大進展。</paragraph>

##### **Federated Diabetes Prediction in Canadian Adults Using Real-world Cross-Province Primary Care Data**
2408.12029v1 by Guojun Tang, Jason E. Black, Tyler S. Williamson, Steve H. Drew

Integrating Electronic Health Records (EHR) and the application of machine
learning present opportunities for enhancing the accuracy and accessibility of
data-driven diabetes prediction. In particular, developing data-driven machine
learning models can provide early identification of patients with high risk for
diabetes, potentially leading to more effective therapeutic strategies and
reduced healthcare costs. However, regulation restrictions create barriers to
developing centralized predictive models. This paper addresses the challenges
by introducing a federated learning approach, which amalgamates predictive
models without centralized data storage and processing, thus avoiding privacy
issues. This marks the first application of federated learning to predict
diabetes using real clinical datasets in Canada extracted from the Canadian
Primary Care Sentinel Surveillance Network (CPCSSN) without crossprovince
patient data sharing. We address class-imbalance issues through downsampling
techniques and compare federated learning performance against province-based
and centralized models. Experimental results show that the federated MLP model
presents a similar or higher performance compared to the model trained with the
centralized approach. However, the federated logistic regression model showed
inferior performance compared to its centralized peer.

摘要：整合電子健康記錄 (EHR) 和機器學習的應用為增強資料驅動糖尿病預測的準確性和可及性提供了機會。特別是，開發資料驅動的機器學習模型可以及早找出糖尿病高風險患者，進而可能導致更有效的治療策略和降低醫療保健成本。然而，法規限制會為開發集中式預測模型製造障礙。本文透過介紹聯邦學習方法來解決挑戰，這種方法結合預測模型，而無需集中式資料儲存和處理，從而避免隱私問題。這標誌著首次應用聯邦學習來預測糖尿病，方法是使用從加拿大初級照護哨兵監控網絡 (CPCSSN) 萃取的加拿大真實臨床資料集，而無需跨省份分享患者資料。我們透過降採樣技術來解決類別不平衡問題，並比較聯邦學習效能與基於省份和集中式的模型。實驗結果顯示，與使用集中式方法訓練的模型相比，聯邦 MLP 模型表現出相似或更高的效能。然而，與其集中式的同儕相比，聯邦邏輯迴歸模型表現出較差的效能。

##### **Exploring Large Language Models for Feature Selection: A Data-centric Perspective**
2408.12025v1 by Dawei Li, Zhen Tan, Huan Liu

The rapid advancement of Large Language Models (LLMs) has significantly
influenced various domains, leveraging their exceptional few-shot and zero-shot
learning capabilities. In this work, we aim to explore and understand the
LLMs-based feature selection methods from a data-centric perspective. We begin
by categorizing existing feature selection methods with LLMs into two groups:
data-driven feature selection which requires samples values to do statistical
inference and text-based feature selection which utilizes prior knowledge of
LLMs to do semantical associations using descriptive context. We conduct
extensive experiments in both classification and regression tasks with LLMs in
various sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the
effectiveness and robustness of text-based feature selection methods and
showcase their potentials using a real-world medical application. We also
discuss the challenges and future opportunities in employing LLMs for feature
selection, offering insights for further research and development in this
emerging field.

摘要：大型語言模型 (LLM) 的快速進步顯著地影響了各種領域，利用它們卓越的少樣本和零樣本學習能力。在這項工作中，我們旨在從以數據為中心的觀點探索和理解基於 LLM 的特徵選擇方法。我們首先將現有的 LLM 特徵選擇方法分類為兩組：需要樣本值來進行統計推論的數據驅動特徵選擇，以及利用 LLM 的先驗知識使用描述性上下文進行語義關聯的基於文本的特徵選擇。我們在各種規模的 LLM（例如 GPT-4、ChatGPT 和 LLaMA-2）中對分類和迴歸任務進行了廣泛的實驗。我們的研究結果強調了基於文本的特徵選擇方法的有效性和穩健性，並展示了它們在現實世界醫療應用中的潛力。我們還討論了在特徵選擇中採用 LLM 的挑戰和未來機會，為這個新興領域的進一步研究和開發提供了見解。

##### **Clinical Insights: A Comprehensive Review of Language Models in Medicine**
2408.11735v2 by Nikita Neveditsin, Pawan Lingras, Vijay Mago

This paper provides a detailed examination of the advancements and
applications of large language models in the healthcare sector, with a
particular emphasis on clinical applications. The study traces the evolution of
LLMs from their foundational technologies to the latest developments in
domain-specific models and multimodal integration. It explores the technical
progression from encoder-based models requiring fine-tuning to sophisticated
approaches that integrate textual, visual, and auditory data, thereby
facilitating comprehensive AI solutions in healthcare. The paper discusses both
the opportunities these technologies present for enhancing clinical efficiency
and the challenges they pose in terms of ethics, data privacy, and
implementation. Additionally, it critically evaluates the deployment strategies
of LLMs, emphasizing the necessity of open-source models to ensure data privacy
and adaptability within healthcare environments. Future research directions are
proposed, focusing on empirical studies to evaluate the real-world efficacy of
LLMs in healthcare and the development of open datasets for further research.
This review aims to provide a comprehensive resource for both newcomers and
multidisciplinary researchers interested in the intersection of AI and
healthcare.

摘要：本文詳細探討了大型語言模型在醫療保健領域的進展與應用，特別強調臨床應用。這項研究追溯了大型語言模型從基礎技術演變到特定領域模型和多模態整合的最新發展。它探討了技術進展，從需要微調的編碼器模型到整合文本、視覺和聽覺資料的複雜方法，從而促進醫療保健中的全面人工智慧解決方案。本文討論了這些技術在提升臨床效率方面帶來的機遇，以及它們在倫理、資料隱私和實施方面帶來的挑戰。此外，它批判性地評估了大型語言模型的部署策略，強調了開源模型對於確保醫療保健環境中的資料隱私和適應性的必要性。提出了未來的研究方向，重點是實證研究，以評估大型語言模型在醫療保健中的實際功效，以及開發開放式資料集以進行進一步的研究。本篇評論旨在為對人工智慧和醫療保健交叉領域感興趣的新手和跨領域研究人員提供全面的資源。

##### **BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports**
2408.11334v1 by Yuxuan Chen, Haoyan Yang, Hengkai Pan, Fardeen Siddiqui, Antonio Verdone, Qingyang Zhang, Sumit Chopra, Chen Zhao, Yiqiu Shen

Breast ultrasound is essential for detecting and diagnosing abnormalities,
with radiology reports summarizing key findings like lesion characteristics and
malignancy assessments. Extracting this critical information is challenging due
to the unstructured nature of these reports, with varied linguistic styles and
inconsistent formatting. While proprietary LLMs like GPT-4 are effective, they
are costly and raise privacy concerns when handling protected health
information. This study presents a pipeline for developing an in-house LLM to
extract clinical information from radiology reports. We first use GPT-4 to
create a small labeled dataset, then fine-tune a Llama3-8B model on it.
Evaluated on clinician-annotated reports, our model achieves an average F1
score of 84.6%, which is on par with GPT-4. Our findings demonstrate the
feasibility of developing an in-house LLM that not only matches GPT-4's
performance but also offers cost reductions and enhanced data privacy.

摘要：乳房超音波對於偵測和診斷異常至關重要，
放射科報告會總結關鍵發現，例如病灶特徵和惡性評估。由於這些報告的非結構化性質、語言風格多變且格式不一致，因此提取這些關鍵資訊具有挑戰性。雖然像 GPT-4 這樣的專有 LLM 很有效，但它們在處理受保護的健康資訊時成本高昂且會引起隱私問題。這項研究提出了一個開發內部 LLM 的管道，以從放射科報告中提取臨床資訊。我們首先使用 GPT-4 建立一個小型標籤資料集，然後對 Llama3-8B 模型進行微調。根據臨床醫師註解的報告進行評估，我們的模型達到平均 F1 分數為 84.6%，這與 GPT-4 相當。我們的研究結果證明了開發內部 LLM 的可行性，它不僅能與 GPT-4 的效能相匹配，還能降低成本並增強資料隱私。

##### **Probabilistic Medical Predictions of Large Language Models**
2408.11316v1 by Bowen Gu, Rishi J. Desai, Kueiyu Joshua Lin, Jie Yang

Large Language Models (LLMs) have demonstrated significant potential in
clinical applications through prompt engineering, which enables the generation
of flexible and diverse clinical predictions. However, they pose challenges in
producing prediction probabilities, which are essential for transparency and
allowing clinicians to apply flexible probability thresholds in
decision-making. While explicit prompt instructions can lead LLMs to provide
prediction probability numbers through text generation, LLMs' limitations in
numerical reasoning raise concerns about the reliability of these
text-generated probabilities. To assess this reliability, we compared explicit
probabilities derived from text generation to implicit probabilities calculated
based on the likelihood of predicting the correct label token. Experimenting
with six advanced open-source LLMs across five medical datasets, we found that
the performance of explicit probabilities was consistently lower than implicit
probabilities with respect to discrimination, precision, and recall. Moreover,
these differences were enlarged on small LLMs and imbalanced datasets,
emphasizing the need for cautious interpretation and applications, as well as
further research into robust probability estimation methods for LLMs in
clinical contexts.

摘要：大型語言模型 (LLM) 透過提示工程展示了在臨床應用上的顯著潛力，這使得產生靈活多樣的臨床預測成為可能。然而，它們在產生預測機率上遇到了挑戰，而這對於透明度和允許臨床醫師在決策中套用靈活的機率閾值至關重要。儘管明確的提示說明可以引導 LLM 透過文字產生提供預測機率數字，但 LLM 在數字推理上的限制引發了對於這些文字產生的機率可靠性的疑慮。為了評估這種可靠性，我們將從文字產生中衍生的明確機率與根據預測正確標記符號的可能性計算的隱含機率進行比較。我們使用六種先進的開源 LLM 針對五個醫療資料集進行實驗，發現明確機率的表現始終低於隱含機率，無論是在判別、精準度和召回率方面皆是如此。此外，這些差異在小型 LLM 和不平衡資料集上被放大了，這強調了謹慎解讀和應用以及進一步研究 LLM 在臨床情境中穩健機率估計方法的必要性。

##### **Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks**
2408.11288v1 by Yining Hua, Hongbin Na, Zehan Li, Fenglin Liu, Xiao Fang, David Clifton, John Torous

Large language models (LLMs) are emerging as promising tools for mental
health care, offering scalable support through their ability to generate
human-like responses. However, the effectiveness of these models in clinical
settings remains unclear. This scoping review aimed to assess the current
generative applications of LLMs in mental health care, focusing on studies
where these models were tested with human participants in real-world scenarios.
A systematic search across APA PsycNet, Scopus, PubMed, and Web of Science
identified 726 unique articles, of which 17 met the inclusion criteria. These
studies encompassed applications such as clinical assistance, counseling,
therapy, and emotional support. However, the evaluation methods were often
non-standardized, with most studies relying on ad hoc scales that limit
comparability and robustness. Privacy, safety, and fairness were also
frequently underexplored. Moreover, reliance on proprietary models, such as
OpenAI's GPT series, raises concerns about transparency and reproducibility.
While LLMs show potential in expanding mental health care access, especially in
underserved areas, the current evidence does not fully support their use as
standalone interventions. More rigorous, standardized evaluations and ethical
oversight are needed to ensure these tools can be safely and effectively
integrated into clinical practice.

摘要：大型語言模型 (LLM) 正作為心理保健的工具浮現，透過產生類人的回應提供可擴充的支持。然而，這些模型在臨床環境中的有效性仍不明確。本範圍探討旨在評估 LLM 在心理保健中的現有生成應用，重點在於在真實世界情境中以人類參與者測試這些模型的研究。系統性搜尋 APA PsycNet、Scopus、PubMed 和 Web of Science 找出 726 篇獨特文章，其中 17 篇符合納入標準。這些研究涵蓋臨床協助、諮詢、治療和情緒支持等應用。然而，評估方法通常未標準化，大多數研究依賴於限制可比較性和穩健性的臨時量表。隱私、安全和公平性也經常未充分探討。此外，依賴於專有模型（例如 OpenAI 的 GPT 系列）會引發對透明度和可複製性的疑慮。雖然 LLM 在擴展心理保健服務方面展現潛力，特別是在服務不足的地區，但目前的證據並不完全支持將其用作獨立干預措施。需要更嚴謹、標準化的評估和倫理監督，以確保這些工具能安全且有效地整合到臨床實務中。

##### **BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation**
2408.11281v1 by Haotian Peng, Jiawei Liu, Jinsong Du, Jie Gao, Wei Wang

We propose a bearing health management framework leveraging large language
models (BearLLM), a novel multimodal model that unifies multiple
bearing-related tasks by processing user prompts and vibration signals.
Specifically, we introduce a prior knowledge-enhanced unified vibration signal
representation to handle various working conditions across multiple datasets.
This involves adaptively sampling the vibration signals based on the sampling
rate of the sensor, incorporating the frequency domain to unify input
dimensions, and using a fault-free reference signal as an auxiliary input. To
extract features from vibration signals, we first train a fault classification
network, then convert and align the extracted features into word embedding, and
finally concatenate these with text embedding as input to an LLM. To evaluate
the performance of the proposed method, we constructed the first large-scale
multimodal bearing health management (MBHM) dataset, including paired vibration
signals and textual descriptions. With our unified vibration signal
representation, BearLLM using one set of pre-trained weights achieves
state-of-the-art performance on nine publicly available fault diagnosis
benchmarks, outperforming specific methods designed for individual datasets. We
provide a dataset, our model, and code to inspire future research on building
more capable industrial multimodal models
(https://github.com/hatton613/BearLLM).

摘要：我們提出一個軸承健康管理架構，利用大型語言模型 (BearLLM)，這是一個新穎的多模態模型，它透過處理使用者提示和振動訊號，統一多個與軸承相關的任務。具體來說，我們引入一個先驗知識增強的統一振動訊號表示，以處理多個資料集中的各種工作條件。這包含根據感測器的取樣率自適應取樣振動訊號、結合頻率域以統一輸入維度，以及使用無故障參考訊號作為輔助輸入。為了從振動訊號中提取特徵，我們首先訓練一個故障分類網路，然後將提取的特徵轉換並對齊到字詞嵌入中，最後將這些特徵與文字嵌入串接作為 LLM 的輸入。為了評估所提出方法的效能，我們建構了第一個大型多模態軸承健康管理 (MBHM) 資料集，其中包括配對的振動訊號和文字描述。透過我們的統一振動訊號表示，使用一組預訓練權重的 BearLLM 在九個公開可用的故障診斷基準測試中達成最先進的效能，優於專門為個別資料集設計的特定方法。我們提供一個資料集、我們的模型和程式碼，以激勵未來在建構更強大的產業多模態模型方面的研究 (https://github.com/hatton613/BearLLM)。

##### **From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis**
2408.11876v1 by Guy Lutsker, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R Greenfield, Dorit Samocha-Bonet, Shie Mannor, Eli Meirom, Gal Chechik, Hagai Rossman, Eran Segal

Recent advances in self-supervised learning enabled novel medical AI models,
known as foundation models (FMs) that offer great potential for characterizing
health from diverse biomedical data. Continuous glucose monitoring (CGM)
provides rich, temporal data on glycemic patterns, but its full potential for
predicting broader health outcomes remains underutilized. Here, we present
GluFormer, a generative foundation model on biomedical temporal data based on a
transformer architecture, and trained on over 10 million CGM measurements from
10,812 non-diabetic individuals. We tokenized the CGM training data and trained
GluFormer using next token prediction in a generative, autoregressive manner.
We demonstrate that GluFormer generalizes effectively to 15 different external
datasets, including 4936 individuals across 5 different geographical regions, 6
different CGM devices, and several metabolic disorders, including
normoglycemic, prediabetic, and diabetic populations, as well as those with
gestational diabetes and obesity. GluFormer produces embeddings which
outperform traditional CGM analysis tools, and achieves high Pearson
correlations in predicting clinical parameters such as HbA1c, liver-related
parameters, blood lipids, and sleep-related indices. Notably, GluFormer can
also predict onset of future health outcomes even 4 years in advance. We also
show that CGM embeddings from pre-intervention periods in Randomized Clinical
Trials (RCTs) outperform other methods in predicting primary and secondary
outcomes. When integrating dietary data into GluFormer, we show that the
enhanced model can accurately generate CGM data based only on dietary intake
data, simulate outcomes of dietary interventions, and predict individual
responses to specific foods. Overall, we show that GluFormer accurately
predicts health outcomes which generalize across different populations
metabolic conditions.

摘要：<paragraph>自我監督式學習的最新進展促成了新型醫療AI模型，
稱為基礎模型（FM），它提供了從多樣化的生物醫學數據中表徵
健康的巨大潛力。連續血糖監測（CGM）
提供豐富的時間數據，了解血糖模式，但其在
預測更廣泛的健康結果方面的全部潛力仍未得到充分利用。在此，我們提出
GluFormer，一個基於Transformer架構的生物醫學時間數據的生成基礎模型，並在超過 1000 萬個 CGM 測量值上進行訓練，來自
10,812 名非糖尿病患者。我們對 CGM 訓練數據進行了標記化，並使用生成式、自迴歸方式訓練了 GluFormer，以進行下一個標記預測。
我們證明 GluFormer 有效地概括為 15 個不同的外部
數據集，包括來自 5 個不同地理區域的 4936 人、6
不同的 CGM 設備，以及幾種代謝疾病，包括
常血糖、糖尿病前期和糖尿病人群，以及
患有妊娠糖尿病和肥胖症的人。GluFormer 產生的嵌入
優於傳統的 CGM 分析工具，並且在預測 HbA1c、肝臟相關
參數、血脂和睡眠相關指標等臨床參數時實現了很高的 Pearson
相關性。值得注意的是，GluFormer 甚至可以
預測未來健康結果的發作，即使提前 4 年。我們還
表明，在隨機臨床試驗 (RCT) 中干預前期的 CGM 嵌入
在預測主要和次要方面優於其他方法
結果。當將飲食數據整合到 GluFormer 中時，我們表明增強的模型可以僅根據飲食攝入準確生成 CGM 數據
數據，模擬飲食干預的結果，並預測個體
對特定食物的反應。總的來說，我們表明 GluFormer 準確
預測健康結果，這些結果概括了不同的人群
代謝狀況。</paragraph>

##### **Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology**
2408.10715v1 by Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz

Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.

摘要：<paragraph>在日常臨床實務中，生成醫師信函是一項耗時的任務。本研究探討大型語言模型 (LLM) 的局部微調，特別是 LLaMA 模型，在放射腫瘤學領域中以隱私保護的方式生成醫師信函。我們的研究結果表明，基礎 LLaMA 模型在沒有微調的情況下，不足以有效生成醫師信函。QLoRA 演算法提供了一種有效的方法，可以在有限的運算資源（即醫院內單一 48 GB GPU 工作站）下，進行 LLM 的局部院內微調。微調後的 LLM 成功學習了放射腫瘤學的特定資訊，並以特定於機構的風格生成醫師信函。生成的摘要報告的 ROUGE 分數突顯了 8B LLaMA-3 模型優於 13B LLaMA-2 模型。進一步的多維醫師評估顯示，儘管微調後的 LLaMA-3 模型生成超出提供輸入資料的內容的能力有限，但它成功地生成了問候語、診斷和治療病史、進一步治療建議和計畫行程。整體而言，臨床專家對臨床效益的評分很高（在 4 分制中平均得分為 3.44）。透過仔細的醫師審查和更正，基於 LLM 的自動化醫師信函生成具有顯著的實用價值。</paragraph>

