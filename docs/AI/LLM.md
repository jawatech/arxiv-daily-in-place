
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-02**|**MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention**|Huiqiang Jiang et.al.|[2407.02490v1](http://arxiv.org/abs/2407.02490v1)|[link](https://github.com/microsoft/MInference)|
|**2024-07-02**|**Magic Insert: Style-Aware Drag-and-Drop**|Nataniel Ruiz et.al.|[2407.02489v1](http://arxiv.org/abs/2407.02489v1)|null|
|**2024-07-02**|**Neurocache: Efficient Vector Retrieval for Long-range Language Modeling**|Ali Safaya et.al.|[2407.02486v1](http://arxiv.org/abs/2407.02486v1)|[link](https://github.com/alisafaya/neurocache)|
|**2024-07-02**|**RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs**|Yue Yu et.al.|[2407.02485v1](http://arxiv.org/abs/2407.02485v1)|null|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483v1](http://arxiv.org/abs/2407.02483v1)|null|
|**2024-07-02**|**Understanding Alignment in Multimodal LLMs: A Comprehensive Study**|Elmira Amirloo et.al.|[2407.02477v1](http://arxiv.org/abs/2407.02477v1)|null|
|**2024-07-02**|**Free Energy in a Circumplex Model of Emotion**|Candice Pattisapu et.al.|[2407.02474v1](http://arxiv.org/abs/2407.02474v1)|null|
|**2024-07-02**|**ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions**|Chan Young Park et.al.|[2407.02472v1](http://arxiv.org/abs/2407.02472v1)|[link](https://github.com/stellali7/valueScope)|
|**2024-07-02**|**PWM: Policy Learning with Large World Models**|Ignat Georgiev et.al.|[2407.02466v1](http://arxiv.org/abs/2407.02466v1)|null|
|**2024-07-02**|**Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets**|Kheir Eddine Daouadi et.al.|[2407.02448v1](http://arxiv.org/abs/2407.02448v1)|null|
|**2024-07-02**|**Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling**|Margaret Li et.al.|[2407.02446v1](http://arxiv.org/abs/2407.02446v1)|null|
|**2024-07-02**|**Evaluating the Robustness of Adverse Drug Event Classification Models Using Templates**|Dorothea MacPhail et.al.|[2407.02432v1](http://arxiv.org/abs/2407.02432v1)|null|
|**2024-07-02**|**Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects**|Raphael Bensadoun et.al.|[2407.02430v1](http://arxiv.org/abs/2407.02430v1)|null|
|**2024-07-02**|**CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models**|Song Wang et.al.|[2407.02408v1](http://arxiv.org/abs/2407.02408v1)|null|
|**2024-07-02**|**Assessing the Code Clone Detection Capability of Large Language Models**|Zixian Zhang et.al.|[2407.02402v1](http://arxiv.org/abs/2407.02402v1)|null|
|**2024-07-02**|**Learning to Refine with Fine-Grained Natural Language Feedback**|Manya Wadhwa et.al.|[2407.02397v1](http://arxiv.org/abs/2407.02397v1)|[link](https://github.com/manyawadhwa/dcr)|
|**2024-07-02**|**Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval**|Jiexin Wang et.al.|[2407.02395v1](http://arxiv.org/abs/2407.02395v1)|null|
|**2024-07-02**|**SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation**|Sayan Nag et.al.|[2407.02389v1](http://arxiv.org/abs/2407.02389v1)|null|
|**2024-07-02**|**Talking to Machines: do you read me?**|Lina M. Rojas-Barahona et.al.|[2407.02354v1](http://arxiv.org/abs/2407.02354v1)|null|
|**2024-07-02**|**Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification**|Pritish Sahu et.al.|[2407.02352v1](http://arxiv.org/abs/2407.02352v1)|null|
|**2024-07-02**|**Generative Large Language Models in Automated Fact-Checking: A Survey**|Ivan Vykopal et.al.|[2407.02351v1](http://arxiv.org/abs/2407.02351v1)|null|
|**2024-07-02**|**MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space**|Yihong Tang et.al.|[2407.02345v1](http://arxiv.org/abs/2407.02345v1)|null|
|**2024-07-02**|**RVISA: Reasoning and Verification for Implicit Sentiment Analysis**|Wenna Lai et.al.|[2407.02340v1](http://arxiv.org/abs/2407.02340v1)|null|
|**2024-07-02**|**Open foundation models for Azerbaijani language**|Jafar Isbarov et.al.|[2407.02337v1](http://arxiv.org/abs/2407.02337v1)|null|
|**2024-07-02**|**CALICO: Confident Active Learning with Integrated Calibration**|Lorenzo S. Querol et.al.|[2407.02335v1](http://arxiv.org/abs/2407.02335v1)|null|
|**2024-07-02**|**Why do LLaVA Vision-Language Models Reply to Images in English?**|Musashi Hinck et.al.|[2407.02333v1](http://arxiv.org/abs/2407.02333v1)|null|
|**2024-07-02**|**Efficient Sparse Attention needs Adaptive Token Release**|Chaoran Zhang et.al.|[2407.02328v1](http://arxiv.org/abs/2407.02328v1)|null|
|**2024-07-02**|**Exploring the Role of Transliteration in In-Context Learning for Low-resource Languages Written in Non-Latin Scripts**|Chunlan Ma et.al.|[2407.02320v1](http://arxiv.org/abs/2407.02320v1)|null|
|**2024-07-02**|**Soft Language Prompts for Language Transfer**|Ivan Vykopal et.al.|[2407.02317v1](http://arxiv.org/abs/2407.02317v1)|null|
|**2024-07-02**|**VFIMamba: Video Frame Interpolation with State Space Models**|Guozhen Zhang et.al.|[2407.02315v1](http://arxiv.org/abs/2407.02315v1)|null|
|**2024-07-02**|**Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining Tasks**|Adrian Rebmann et.al.|[2407.02310v1](http://arxiv.org/abs/2407.02310v1)|null|
|**2024-07-02**|**Semantically Guided Representation Learning For Action Anticipation**|Anxhelo Diko et.al.|[2407.02309v1](http://arxiv.org/abs/2407.02309v1)|null|
|**2024-07-02**|**Towards Human Understanding of Paraphrase Types in ChatGPT**|Dominik Meier et.al.|[2407.02302v1](http://arxiv.org/abs/2407.02302v1)|null|
|**2024-07-02**|**CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models**|Ying Nie et.al.|[2407.02301v1](http://arxiv.org/abs/2407.02301v1)|null|
|**2024-07-02**|**Strategic Demand-Planning in Wireless Networks: Can Generative-AI Save Spectrum and Energy?**|Berk Çiloğlu et.al.|[2407.02292v1](http://arxiv.org/abs/2407.02292v1)|null|
|**2024-07-02**|**Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in Adverse Weather**|Junsung Park et.al.|[2407.02286v1](http://arxiv.org/abs/2407.02286v1)|null|
|**2024-07-02**|**Renard: A Modular Pipeline for Extracting Character Networks from Narrative Texts**|Arthur Amalvy et.al.|[2407.02284v1](http://arxiv.org/abs/2407.02284v1)|null|
|**2024-07-02**|**FedIA: Federated Medical Image Segmentation with Heterogeneous Annotation Completeness**|Yangyang Xiang et.al.|[2407.02280v1](http://arxiv.org/abs/2407.02280v1)|[link](https://github.com/hustxyy/fedia)|
|**2024-07-02**|**Learning Paradigms and Modelling Methodologies for Digital Twins in Process Industry**|Michael Mayr et.al.|[2407.02275v1](http://arxiv.org/abs/2407.02275v1)|null|
|**2024-07-02**|**Multilingual Trolley Problems for Language Models**|Zhijing Jin et.al.|[2407.02273v1](http://arxiv.org/abs/2407.02273v1)|null|
|**2024-07-02**|**Footprints of Data in a Classifier Model: The Privacy Issues and Their Mitigation through Data Obfuscation**|Payel Sadhukhan et.al.|[2407.02268v1](http://arxiv.org/abs/2407.02268v1)|null|
|**2024-07-02**|**Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization**|Yuchen Hu et.al.|[2407.02243v1](http://arxiv.org/abs/2407.02243v1)|null|
|**2024-07-02**|**Indian Stock Market Prediction using Augmented Financial Intelligence ML**|Anishka Chauhan et.al.|[2407.02236v1](http://arxiv.org/abs/2407.02236v1)|null|
|**2024-07-02**|**Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional Brain CT Report Generation**|Cheng-Yi Li et.al.|[2407.02235v1](http://arxiv.org/abs/2407.02235v1)|[link](https://github.com/charlierabea/FORTE)|
|**2024-07-02**|**Synthetic Multimodal Question Generation**|Ian Wu et.al.|[2407.02233v1](http://arxiv.org/abs/2407.02233v1)|null|
|**2024-07-02**|**MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders**|Baijiong Lin et.al.|[2407.02228v1](http://arxiv.org/abs/2407.02228v1)|[link](https://github.com/envision-research/mtmamba)|
|**2024-07-02**|**Embodied AI in Mobile Robots: Coverage Path Planning with Large Language Models**|Xiangrui Kong et.al.|[2407.02220v1](http://arxiv.org/abs/2407.02220v1)|null|
|**2024-07-02**|**Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style Reinforcement Learning**|Zakariae El Asri et.al.|[2407.02217v1](http://arxiv.org/abs/2407.02217v1)|null|
|**2024-07-02**|**PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning**|Jiaru Zou et.al.|[2407.02211v1](http://arxiv.org/abs/2407.02211v1)|null|
|**2024-07-02**|**Generative Monoculture in Large Language Models**|Fan Wu et.al.|[2407.02209v1](http://arxiv.org/abs/2407.02209v1)|null|
|**2024-07-02**|**How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise on Machine Translation**|Yan Meng et.al.|[2407.02208v1](http://arxiv.org/abs/2407.02208v1)|null|
|**2024-07-02**|**Automatic Adaptation Rule Optimization via Large Language Models**|Yusei Ishimizu et.al.|[2407.02203v1](http://arxiv.org/abs/2407.02203v1)|null|
|**2024-07-02**|**Research on Reliable and Safe Occupancy Grid Prediction in Underground Parking Lots**|JiaQi Luo et.al.|[2407.02197v1](http://arxiv.org/abs/2407.02197v1)|null|
|**2024-07-02**|**Attack-Aware Noise Calibration for Differential Privacy**|Bogdan Kulynych et.al.|[2407.02191v1](http://arxiv.org/abs/2407.02191v1)|[link](https://github.com/bogdan-kulynych/riskcal)|
|**2024-07-02**|**LlamAr & GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning**|Hasna Chouikhi et.al.|[2407.02147v1](http://arxiv.org/abs/2407.02147v1)|null|
|**2024-07-02**|**Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks**|Wataru Hashimoto et.al.|[2407.02138v1](http://arxiv.org/abs/2407.02138v1)|null|
|**2024-07-02**|**Black Big Boxes: Do Language Models Hide a Theory of Adjective Order?**|Jaap Jumelet et.al.|[2407.02136v1](http://arxiv.org/abs/2407.02136v1)|null|
|**2024-07-02**|**Fake News Detection: It's All in the Data!**|Soveatin Kuntur et.al.|[2407.02122v1](http://arxiv.org/abs/2407.02122v1)|null|
|**2024-07-02**|**Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning**|Yifang Chen et.al.|[2407.02119v1](http://arxiv.org/abs/2407.02119v1)|null|
|**2024-07-02**|**Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale**|Wenzhen Zheng et.al.|[2407.02118v1](http://arxiv.org/abs/2407.02118v1)|null|
|**2024-07-02**|**A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data**|Andrej Tschalzev et.al.|[2407.02112v1](http://arxiv.org/abs/2407.02112v1)|null|
|**2024-07-02**|**HRSAM: Efficiently Segment Anything in High-Resolution Images**|You Huang et.al.|[2407.02109v1](http://arxiv.org/abs/2407.02109v1)|null|
|**2024-07-02**|**Automated Knowledge Graph Learning in Industrial Processes**|Lolitta Ammann et.al.|[2407.02106v1](http://arxiv.org/abs/2407.02106v1)|null|
|**2024-07-02**|**Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior**|Pedro Henrique Luz de Araujo et.al.|[2407.02099v1](http://arxiv.org/abs/2407.02099v1)|null|
|**2024-07-02**|**Latent Diffusion Model for Generating Ensembles of Climate Simulations**|Johannes Meuer et.al.|[2407.02070v1](http://arxiv.org/abs/2407.02070v1)|null|
|**2024-07-02**|**Crossroads of Continents: Automated Artifact Extraction for Cultural Adaptation with Large Multimodal Models**|Anjishnu Mukherjee et.al.|[2407.02067v1](http://arxiv.org/abs/2407.02067v1)|[link](https://github.com/iamshnoo/crossroads)|
|**2024-07-02**|**BiasDora: Exploring Hidden Biased Associations in Vision-Language Models**|Chahat Raj et.al.|[2407.02066v1](http://arxiv.org/abs/2407.02066v1)|[link](https://github.com/chahatraj/BiasDora)|
|**2024-07-02**|**Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?**|Wataru Hashimoto et.al.|[2407.02062v1](http://arxiv.org/abs/2407.02062v1)|null|
|**2024-07-02**|**Terminating Differentiable Tree Experts**|Jonathan Thomm et.al.|[2407.02060v1](http://arxiv.org/abs/2407.02060v1)|null|
|**2024-07-02**|**Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation**|Xinglin Wang et.al.|[2407.02056v1](http://arxiv.org/abs/2407.02056v1)|[link](https://github.com/WangXinglin/FSC)|
|**2024-07-02**|**Abstract Dialectical Frameworks are Boolean Networks (full version)**|Jesse Heyninck et.al.|[2407.02055v1](http://arxiv.org/abs/2407.02055v1)|null|
|**2024-07-02**|**Accompanied Singing Voice Synthesis with Fully Text-controlled Melody**|Ruiqi Li et.al.|[2407.02049v1](http://arxiv.org/abs/2407.02049v1)|null|
|**2024-07-02**|**Concise and Precise Context Compression for Tool-Using Language Models**|Yang Xu et.al.|[2407.02043v1](http://arxiv.org/abs/2407.02043v1)|null|
|**2024-07-02**|**Fake News Detection and Manipulation Reasoning via Large Vision-Language Models**|Ruihan Jin et.al.|[2407.02042v1](http://arxiv.org/abs/2407.02042v1)|null|
|**2024-07-02**|**ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation**|Zhiyuan Ma et.al.|[2407.02040v1](http://arxiv.org/abs/2407.02040v1)|[link](https://github.com/theericma/scaledreamer)|
|**2024-07-02**|**Prompt Stability Scoring for Text Annotation with Large Language Models**|Christopher Barrie et.al.|[2407.02039v1](http://arxiv.org/abs/2407.02039v1)|null|
|**2024-07-02**|**SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules**|Suyi Li et.al.|[2407.02031v1](http://arxiv.org/abs/2407.02031v1)|null|
|**2024-07-02**|**Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis**|Chahat Raj et.al.|[2407.02030v1](http://arxiv.org/abs/2407.02030v1)|[link](https://github.com/chahatraj/breakingbias)|
|**2024-07-02**|**Why does in-context learning fail sometimes? Evaluating in-context learning on open and closed questions**|Xiang Li et.al.|[2407.02028v1](http://arxiv.org/abs/2407.02028v1)|null|
|**2024-07-02**|**On the Expressive Power of Sparse Geometric MPNNs**|Yonatan Sverdlov et.al.|[2407.02025v1](http://arxiv.org/abs/2407.02025v1)|null|
|**2024-07-02**|**An End-to-End Speech Summarization Using Large Language Model**|Hengchao Shang et.al.|[2407.02005v1](http://arxiv.org/abs/2407.02005v1)|null|
|**2024-07-02**|**SAVE: Segment Audio-Visual Easy way using Segment Anything Model**|Khanh-Binh Nguyen et.al.|[2407.02004v1](http://arxiv.org/abs/2407.02004v1)|null|
|**2024-07-02**|**Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion**|Ananjan Nandi et.al.|[2407.01994v1](http://arxiv.org/abs/2407.01994v1)|null|
|**2024-07-02**|**Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?**|Nishant Balepur et.al.|[2407.01992v1](http://arxiv.org/abs/2407.01992v1)|null|
|**2024-07-02**|**A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding**|Jinghui Lu et.al.|[2407.01976v1](http://arxiv.org/abs/2407.01976v1)|null|
|**2024-07-02**|**MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation**|Zijie J. Wang et.al.|[2407.01972v1](http://arxiv.org/abs/2407.01972v1)|[link](https://github.com/poloclub/mememo)|
|**2024-07-02**|**AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment**|Yilong Lai et.al.|[2407.01965v1](http://arxiv.org/abs/2407.01965v1)|[link](https://github.com/init0xyz/AdaCQR)|
|**2024-07-02**|**Enabling Discriminative Reasoning in Large Language Models for Legal Judgment Prediction**|Chenlong Deng et.al.|[2407.01964v1](http://arxiv.org/abs/2407.01964v1)|null|
|**2024-07-02**|**S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models**|Parsa Kavehzadeh et.al.|[2407.01955v1](http://arxiv.org/abs/2407.01955v1)|null|
|**2024-07-02**|**CatMemo at the FinLLM Challenge Task: Fine-Tuning Large Language Models using Data Fusion in Financial Applications**|Yupeng Cao et.al.|[2407.01953v1](http://arxiv.org/abs/2407.01953v1)|null|
|**2024-07-02**|**LDP: A Local Diffusion Planner for Efficient Robot Navigation and Collision Avoidance**|Wenhao Yu et.al.|[2407.01950v1](http://arxiv.org/abs/2407.01950v1)|null|
|**2024-07-02**|**Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation**|Pablo Messina et.al.|[2407.01948v1](http://arxiv.org/abs/2407.01948v1)|null|
|**2024-07-02**|**Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness**|Khyathi Raghavi Chandu et.al.|[2407.01942v1](http://arxiv.org/abs/2407.01942v1)|null|
|**2024-07-02**|**Efficient-Empathy: Towards Efficient and Effective Selection of Empathy Data**|Linzhuang Sun et.al.|[2407.01937v1](http://arxiv.org/abs/2407.01937v1)|null|
|**2024-07-02**|**What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models**|Shengqi Zhu et.al.|[2407.01929v1](http://arxiv.org/abs/2407.01929v1)|null|
|**2024-07-02**|**To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models**|Bozhong Tian et.al.|[2407.01920v1](http://arxiv.org/abs/2407.01920v1)|[link](https://github.com/zjunlp/knowundo)|
|**2024-07-02**|**A Method to Facilitate Membership Inference Attacks in Deep Learning Models**|Zitao Chen et.al.|[2407.01919v1](http://arxiv.org/abs/2407.01919v1)|null|
|**2024-07-02**|**Sequential Manipulation Against Rank Aggregation: Theory and Algorithm**|Ke Ma et.al.|[2407.01916v1](http://arxiv.org/abs/2407.01916v1)|null|
|**2024-07-02**|**Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model**|Yu-Kuan Fu et.al.|[2407.01911v1](http://arxiv.org/abs/2407.01911v1)|null|
|**2024-07-02**|**MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation**|Yongan Zhang et.al.|[2407.01910v1](http://arxiv.org/abs/2407.01910v1)|null|

#### Abstracts
##### **MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention**
2407.02490v1 by Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu

The computational challenges of Large Language Model (LLM) inference remain a
significant barrier to their widespread deployment, especially as prompt
lengths continue to increase. Due to the quadratic complexity of the attention
computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens
(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for
speeding up prefilling often fail to maintain acceptable accuracy or efficiency
when applied to long-context LLMs. To address this gap, we introduce MInference
(Milliontokens Inference), a sparse calculation method designed to accelerate
pre-filling of long-sequence processing. Specifically, we identify three unique
patterns in long-context attention matrices-the A-shape, Vertical-Slash, and
Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We
determine the optimal pattern for each attention head offline and dynamically
build sparse indices based on the assigned pattern during inference. With the
pattern and sparse indices, we perform efficient sparse attention calculations
via our optimized GPU kernels to significantly reduce the latency in the
pre-filling stage of long-context LLMs. Our proposed technique can be directly
applied to existing LLMs without any modifications to the pre-training setup or
additional fine-tuning. By evaluating on a wide range of downstream tasks,
including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models
including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we
demonstrate that MInference effectively reduces inference latency by up to 10x
for pre-filling on an A100, while maintaining accuracy. Our code is available
at https://aka.ms/MInference.

摘要：大型語言模型 (LLM) 推論的運算挑戰仍然是其廣泛部署的重大障礙，特別是因為提示長度持續增加。由於注意力運算的二次複雜度，8B LLM 處理 1M 個 token 的提示（即預填入階段）需要在單個 A100 GPU 上花費 30 分鐘。現有的加速預填入方法在應用於長語境 LLM 時，通常無法維持可接受的準確度或效率。為了解決這個差距，我們引入了 MInference（百萬 token 推論），這是一種稀疏計算方法，旨在加速長序列處理的預填入。具體來說，我們在長語境注意力矩陣中識別出三種獨特的模式——A 形、垂直斜線和區塊稀疏，這些模式可以利用 GPU 上的有效稀疏計算。我們離線確定每個注意力頭的最佳模式，並在推論期間根據指定的模式動態構建稀疏索引。有了模式和稀疏索引，我們通過優化的 GPU 核心執行有效的稀疏注意力計算，以顯著減少長語境 LLM 預填入階段的延遲。我們提出的技術可以直接應用於現有的 LLM，而無需對預訓練設定或額外微調進行任何修改。通過在廣泛的下游任務上進行評估，包括 InfiniteBench、RULER、PG-19 和 Needle In A Haystack，以及 LLaMA-3-1M、GLM4-1M、Yi-200K、Phi-3-128K 和 Qwen2-128K 等模型，我們證明了 MInference 有效地將預填入的推論延遲降低了 10 倍，同時保持準確度。我們的程式碼可在 https://aka.ms/MInference 取得。

##### **Magic Insert: Style-Aware Drag-and-Drop**
2407.02489v1 by Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David E. Jacobs, Shlomi Fruchter

We present Magic Insert, a method for dragging-and-dropping subjects from a
user-provided image into a target image of a different style in a physically
plausible manner while matching the style of the target image. This work
formalizes the problem of style-aware drag-and-drop and presents a method for
tackling it by addressing two sub-problems: style-aware personalization and
realistic object insertion in stylized images. For style-aware personalization,
our method first fine-tunes a pretrained text-to-image diffusion model using
LoRA and learned text tokens on the subject image, and then infuses it with a
CLIP representation of the target style. For object insertion, we use
Bootstrapped Domain Adaption to adapt a domain-specific photorealistic object
insertion model to the domain of diverse artistic styles. Overall, the method
significantly outperforms traditional approaches such as inpainting. Finally,
we present a dataset, SubjectPlop, to facilitate evaluation and future progress
in this area. Project page: https://magicinsert.github.io/

摘要：<paragraph>我們提出 Magic Insert，一種以物理合理的方式將主體從使用者提供的影像拖放至不同風格的目標影像，同時匹配目標影像的風格。這項工作將風格感知拖放的問題形式化，並提出一個方法來解決它，方法是解決兩個子問題：風格感知個性化和樣式化影像中的真實物件插入。對於風格感知個性化，我們的做法首先使用 LoRA 和學習到的主體影像文字代號微調預訓練的文字轉影像擴散模型，然後注入目標風格的 CLIP 表徵。對於物件插入，我們使用引導域適應將特定於領域的照片寫實物件插入模型適應到各種藝術風格的領域。總體而言，此方法顯著優於傳統方法，例如修復。最後，我們提出一個資料集 SubjectPlop，以利於評估和未來進展。專案頁面：https://magicinsert.github.io/</paragraph>

##### **Neurocache: Efficient Vector Retrieval for Long-range Language Modeling**
2407.02486v1 by Ali Safaya, Deniz Yuret

This paper introduces Neurocache, an approach to extend the effective context
size of large language models (LLMs) using an external vector cache to store
its past states. Like recent vector retrieval approaches, Neurocache uses an
efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states
and incorporate them into the attention process. Neurocache improves upon
previous methods by (1) storing compressed states, which reduces cache size;
(2) performing a single retrieval operation per token which increases inference
speed; and (3) extending the retrieval window to neighboring states, which
improves both language modeling and downstream task accuracy. Our experiments
show the effectiveness of Neurocache both for models trained from scratch and
for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the
cache mechanism. We also compare Neurocache with text retrieval methods and
show improvements in single-document question-answering and few-shot learning
tasks. We made the source code available under:
https://github.com/alisafaya/neurocache

摘要：本文介绍了 Neurocache，这是一种使用外部向量缓存来存储其过去状态以扩展大型语言模型 (LLM) 的有效上下文大小的方法。与最近的向量检索方法类似，Neurocache 使用高效的 k 近邻 (kNN) 算法来检索相关的过去状态并将其纳入注意力过程中。Neurocache 通过以下方式改进以前的方法：(1) 存储压缩状态，这可以减小缓存大小；(2) 对每个标记执行单个检索操作，这可以提高推理速度；以及 (3) 将检索窗口扩展到相邻状态，这可以提高语言建模和下游任务的准确性。我们的实验表明，Neurocache 对从头开始训练的模型和使用缓存机制增强的预训练模型（例如 Llama2-7B 和 Mistral-7B）都非常有效。我们还将 Neurocache 与文本检索方法进行了比较，并展示了在单文档问答和少样本学习任务中的改进。我们提供了以下源代码：https://github.com/alisafaya/neurocache

##### **RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs**
2407.02485v1 by Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, Bryan Catanzaro

Large language models (LLMs) typically utilize the top-k contexts from a
retriever in retrieval-augmented generation (RAG). In this work, we propose a
novel instruction fine-tuning framework RankRAG, which instruction-tunes a
single LLM for the dual purpose of context ranking and answer generation in
RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding
a small fraction of ranking data into the training blend, and outperform
existing expert ranking models, including the same LLM exclusively fine-tuned
on a large amount of ranking data. For generation, we compare our model with
many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and
ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG
benchmarks. Specifically, our Llama3-RankRAG significantly outperforms
Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In
addition, it also performs comparably to GPT-4 on five RAG benchmarks in the
biomedical domain without instruction fine-tuning on biomedical data,
demonstrating its superb capability for generalization to new domains.

摘要：大型語言模型（LLM）通常在檢索增強生成（RAG）中利用檢索器的 top-k 背景。在這項工作中，我們提出了一個新穎的指令微調框架 RankRAG，它為單一 LLM 進行指令微調，以達到 RAG 中背景排名和答案生成的雙重目的。特別是，指令微調的 LLM 透過將一小部分排名資料新增到訓練混合中而產生驚人的效果，並優於現有的專家排名模型，包括在大量排名資料上進行獨家微調的相同 LLM。對於生成，我們將我們的模型與許多強大的基線進行比較，包括 GPT-4-0613、GPT-4-turbo-2024-0409 和 ChatQA-1.5，這是一個在 RAG 基準上具有最先進效能的開源模型。具體來說，我們的 Llama3-RankRAG 在九個知識密集型基準上顯著優於 Llama3-ChatQA-1.5 和 GPT-4 模型。此外，它在生物醫學領域的五個 RAG 基準上也表現出與 GPT-4 相當的效能，而無需對生物醫學資料進行指令微調，證明了它對新領域的出色泛化能力。

##### **MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**
2407.02483v1 by Binxu Li, Tiankai Yan, Yuanting Pan, Zhe Xu, Jie Luo, Ruiyang Ji, Shilong Liu, Haoyu Dong, Zihao Lin, Yixin Wang

Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit
limited generality and often fall short when compared to specialized models.
Recently, LLM-based agents have been developed to address these challenges by
selecting appropriate specialized models as tools based on user inputs.
However, such advancements have not been extensively explored within the
medical domain. To bridge this gap, this paper introduces the first agent
explicitly designed for the medical field, named \textbf{M}ulti-modal
\textbf{Med}ical \textbf{Agent} (MMedAgent). We curate an instruction-tuning
dataset comprising six medical tools solving seven tasks, enabling the agent to
choose the most suitable tools for a given task. Comprehensive experiments
demonstrate that MMedAgent achieves superior performance across a variety of
medical tasks compared to state-of-the-art open-source methods and even the
closed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in
updating and integrating new medical tools.

摘要：儘管多模態大型語言模型 (MLLM) 成功，但其普遍性有限，與專用模型相比時常有所不足。
最近，基於 LLM 的代理已被開發出來，以透過根據使用者輸入選擇適當的專用模型作為工具來解決這些挑戰。
然而，此類進展尚未在醫療領域中廣泛探討。為了彌補此差距，本文介紹了第一個專門為醫療領域設計的代理，名為**M**ulti-modal **Med**ical **Agent** (MMedAgent)。我們整理了一個由六種解決七項任務的醫療工具組成的指令調整資料集，讓代理能夠為特定任務選擇最合適的工具。全面的實驗證明，與最先進的開源方法，甚至閉源模型 GPT-4o 相比，MMedAgent 在各種醫療任務中都取得了優異的表現。此外，MMedAgent 在更新和整合新的醫療工具方面展現出效率。

##### **Understanding Alignment in Multimodal LLMs: A Comprehensive Study**
2407.02477v1 by Elmira Amirloo, Jean-Philippe Fauconnier, Christoph Roesmann, Christian Kerl, Rinu Boney, Yusu Qian, Zirui Wang, Afshin Dehghan, Yinfei Yang, Zhe Gan, Peter Grasch

Preference alignment has become a crucial component in enhancing the
performance of Large Language Models (LLMs), yet its impact in Multimodal Large
Language Models (MLLMs) remains comparatively underexplored. Similar to
language models, MLLMs for image understanding tasks encounter challenges like
hallucination. In MLLMs, hallucination can occur not only by stating incorrect
facts but also by producing responses that are inconsistent with the image
content. A primary objective of alignment for MLLMs is to encourage these
models to align responses more closely with image information. Recently,
multiple works have introduced preference datasets for MLLMs and examined
different alignment methods, including Direct Preference Optimization (DPO) and
Proximal Policy Optimization (PPO). However, due to variations in datasets,
base model types, and alignment methods, it remains unclear which specific
elements contribute most significantly to the reported improvements in these
works. In this paper, we independently analyze each aspect of preference
alignment in MLLMs. We start by categorizing the alignment algorithms into two
groups, offline (such as DPO), and online (such as online-DPO), and show that
combining offline and online methods can improve the performance of the model
in certain scenarios. We review a variety of published multimodal preference
datasets and discuss how the details of their construction impact model
performance. Based on these insights, we introduce a novel way of creating
multimodal preference data called Bias-Driven Hallucination Sampling (BDHS)
that needs neither additional annotation nor external models, and show that it
can achieve competitive performance to previously published alignment work for
multimodal models across a range of benchmarks.

摘要：偏好對齊已成為增強大型語言模型 (LLM) 效能的關鍵組成部分，但其在多模態大型語言模型 (MLLM) 中的影響力仍相對未被充分探討。與語言模型類似，用於影像理解任務的 MLLM 會遇到幻覺等挑戰。在 MLLM 中，幻覺不僅可能透過陳述不正確的事實產生，還可能透過產生與影像內容不一致的回應產生。MLLM 對齊的主要目標是鼓勵這些模型將回應與影像資訊更緊密地對齊。最近，多項研究引入了 MLLM 的偏好資料集，並檢視了不同的對齊方法，包括直接偏好最佳化 (DPO) 和近端策略最佳化 (PPO)。然而，由於資料集、基礎模型類型和對齊方法的差異，目前仍不清楚哪些特定元素對這些研究中報告的改進貢獻最大。在本文中，我們獨立分析了 MLLM 中偏好對齊的每個面向。我們首先將對齊演算法分類為兩組，離線（例如 DPO）和線上（例如線上 DPO），並展示結合離線和線上方法可以在某些情況下改善模型的效能。我們檢視了各種已發表的模態偏好資料集，並討論其建構的詳細資訊如何影響模型效能。基於這些見解，我們提出了一種創新的多模態偏好資料建立方式，稱為偏差驅動幻覺取樣 (BDHS)，它不需要額外的註解或外部模型，並展示它可以在各種基準上為多模態模型達成與先前發表的對齊研究競爭的效能。

##### **Free Energy in a Circumplex Model of Emotion**
2407.02474v1 by Candice Pattisapu, Tim Verbelen, Riddhi J. Pitliya, Alex B. Kiefer, Mahault Albarracin

Previous active inference accounts of emotion translate fluctuations in free
energy to a sense of emotion, mainly focusing on valence. However, in affective
science, emotions are often represented as multi-dimensional. In this paper, we
propose to adopt a Circumplex Model of emotion by mapping emotions into a
two-dimensional spectrum of valence and arousal. We show how one can derive a
valence and arousal signal from an agent's expected free energy, relating
arousal to the entropy of posterior beliefs and valence to utility less
expected utility. Under this formulation, we simulate artificial agents engaged
in a search task. We show that the manipulation of priors and object presence
results in commonsense variability in emotional states.

摘要：先前的主动推論情緒帳戶將自由能的波動轉化為情緒感，主要關注於效價。然而，在情感科學中，情緒通常被表示為多維的。在本文中，我們提議採用情緒環狀模型，將情緒映射到效價和喚醒的二維光譜中。我們展示了如何從代理預期的自由能中推導出效價和喚醒信號，將喚醒與後驗信念的熵聯繫起來，將效價與效用聯繫起來，而效用則較低。在這個公式下，我們模擬了從事搜尋任務的人工代理。我們表明，先驗和物件存在的操作會導致情緒狀態的常識變異性。

##### **ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions**
2407.02472v1 by Chan Young Park, Shuyue Stella Li, Hayoung Jung, Svitlana Volkova, Tanushree Mitra, David Jurgens, Yulia Tsvetkov

This study introduces ValueScope, a framework leveraging language models to
quantify social norms and values within online communities, grounded in social
science perspectives on normative structures. We employ ValueScope to dissect
and analyze linguistic and stylistic expressions across 13 Reddit communities
categorized under gender, politics, science, and finance. Our analysis provides
a quantitative foundation showing that even closely related communities exhibit
remarkably diverse norms. This diversity supports existing theories and adds a
new dimension--community preference--to understanding community interactions.
ValueScope not only delineates differing social norms among communities but
also effectively traces their evolution and the influence of significant
external events like the U.S. presidential elections and the emergence of new
sub-communities. The framework thus highlights the pivotal role of social norms
in shaping online interactions, presenting a substantial advance in both the
theory and application of social norm studies in digital spaces.

摘要：本研究介紹 ValueScope，一個利用語言模型來量化網路社群中的社會規範與價值觀的架構，其基礎在於規範結構的社會科學觀點。我們使用 ValueScope 來解剖並分析 13 個 Reddit 社群中的語言和風格表達，這些社群分類在性別、政治、科學和金融之下。我們的分析提供了一個量化的基礎，顯示即使是密切相關的社群也展現出非常多樣的規範。這種多樣性支持了現有的理論，並為了解社群互動增加了新的面向——社群偏好。ValueScope 不僅描繪了社群間不同的社會規範，也有效地追蹤它們的演變和重大外部事件的影響，例如美國總統選舉和新次社群的出現。因此，這個架構突顯了社會規範在形塑網路互動中的關鍵角色，在數位空間中社會規範研究的理論和應用上都呈現出實質的進展。

##### **PWM: Policy Learning with Large World Models**
2407.02466v1 by Ignat Georgiev, Varun Giridhar, Nicklas Hansen, Animesh Garg

Reinforcement Learning (RL) has achieved impressive results on complex tasks
but struggles in multi-task settings with different embodiments. World models
offer scalability by learning a simulation of the environment, yet they often
rely on inefficient gradient-free optimization methods. We introduce Policy
learning with large World Models (PWM), a novel model-based RL algorithm that
learns continuous control policies from large multi-task world models. By
pre-training the world model on offline data and using it for first-order
gradient policy learning, PWM effectively solves tasks with up to 152 action
dimensions and outperforms methods using ground-truth dynamics. Additionally,
PWM scales to an 80-task setting, achieving up to 27% higher rewards than
existing baselines without the need for expensive online planning.
Visualizations and code available at https://policy-world-model.github.io

摘要：強化學習 (RL) 在複雜任務上已取得令人印象深刻的成果，但對於具有不同具體實例的多任務設定則有困難。世界模型透過學習環境模擬來提供可擴充性，但它們通常依賴於低效率的無梯度最佳化方法。我們引入了大型世界模型 (PWM) 的策略學習，一種新穎的基於模型的 RL 演算法，它從大型多任務世界模型中學習連續控制策略。透過在離線資料上預先訓練世界模型，並將其用於一階梯度策略學習，PWM 有效地解決了具有高達 152 個動作維度的任務，並優於使用真實動態的方法。此外，PWM 可擴充到 80 個任務設定，比現有的基線高出 27% 的獎勵，而無需昂貴的線上規劃。視覺化和程式碼可於 https://policy-world-model.github.io 取得

##### **Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets**
2407.02448v1 by Kheir Eddine Daouadi, Yaakoub Boualleg, Kheir Eddine Haouaouchi

Today, hate speech classification from Arabic tweets has drawn the attention
of several researchers. Many systems and techniques have been developed to
resolve this classification task. Nevertheless, two of the major challenges
faced in this context are the limited performance and the problem of imbalanced
data. In this study, we propose a novel approach that leverages ensemble
learning and semi-supervised learning based on previously manually labeled. We
conducted experiments on a benchmark dataset by classifying Arabic tweets into
5 distinct classes: non-hate, general hate, racial, religious, or sexism.
Experimental results show that: (1) ensemble learning based on pre-trained
language models outperforms existing related works; (2) Our proposed data
augmentation improves the accuracy results of hate speech detection from Arabic
tweets and outperforms existing related works. Our main contribution is the
achievement of encouraging results in Arabic hate speech detection.

摘要：今日，仇恨言论从阿拉伯语推文中分类已引起多位研究者的注意。许多系统和技术已被开发来解决此分类任务。尽管如此，在这种情况下面临的两个主要挑战是有限的性能和不平衡数据的问题。在本研究中，我们提出了一种新颖的方法，该方法利用基于先前手动标记的集成学习和半监督学习。我们对基准数据集进行了实验，将阿拉伯语推文分类为 5 个不同的类别：非仇恨、一般仇恨、种族、宗教或性别歧视。实验结果表明：(1) 基于预训练语言模型的集成学习优于现有相关工作；(2) 我们提出的数据增强提高了从阿拉伯语推文中检测仇恨言论的准确度，并且优于现有相关工作。我们的主要贡献是在阿拉伯语仇恨言论检测中取得了令人鼓舞的结果。

##### **Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling**
2407.02446v1 by Margaret Li, Weijia Shi, Artidoro Pagnoni, Peter West, Ari Holtzman

RLHF-aligned LMs have shown unprecedented ability on both benchmarks and
long-form text generation, yet they struggle with one foundational task:
next-token prediction. As RLHF models become agent models aimed at interacting
with humans, they seem to lose their world modeling -- the ability to predict
what comes next in arbitrary documents, which is the foundational training
objective of the Base LMs that RLHF adapts.
  Besides empirically demonstrating this trade-off, we propose a potential
explanation: to perform coherent long-form generation, RLHF models restrict
randomness via implicit blueprints. In particular, RLHF models concentrate
probability on sets of anchor spans that co-occur across multiple generations
for the same prompt, serving as textual scaffolding but also limiting a model's
ability to generate documents that do not include these spans. We study this
trade-off on the most effective current agent models, those aligned with RLHF,
while exploring why this may remain a fundamental trade-off between models that
act and those that predict, even as alignment techniques improve.

摘要：RLHF 對齊的 LM 在基準和長篇文字生成方面都展現出前所未有的能力，但它們在一個基礎任務上仍有困難：下一個代幣預測。由於 RLHF 模型成為旨在與人類互動的代理模型，它們似乎失去了世界建模能力，也就是預測任意文件接下來會出現什麼的能力，而這正是 RLHF 所適應的基礎 LM 的基礎訓練目標。
除了經驗性地證明這種權衡之外，我們提出了一個潛在的解釋：為了執行連貫的長篇生成，RLHF 模型會透過隱含藍圖來限制隨機性。特別是，RLHF 模型將機率集中在多個世代中針對相同提示而共同出現的錨點區間組上，作為文字架構，但也限制了模型生成不包含這些區間的文件的能力。我們在最有效的現行代理模型（那些與 RLHF 對齊的模型）上研究這種權衡，同時探討為什麼這可能仍然是行動模型和預測模型之間的一項基本權衡，即使對齊技術有所改進。

##### **Evaluating the Robustness of Adverse Drug Event Classification Models Using Templates**
2407.02432v1 by Dorothea MacPhail, David Harbecke, Lisa Raithel, Sebastian Möller

An adverse drug effect (ADE) is any harmful event resulting from medical drug
treatment. Despite their importance, ADEs are often under-reported in official
channels. Some research has therefore turned to detecting discussions of ADEs
in social media. Impressive results have been achieved in various attempts to
detect ADEs. In a high-stakes domain such as medicine, however, an in-depth
evaluation of a model's abilities is crucial. We address the issue of thorough
performance evaluation in English-language ADE detection with hand-crafted
templates for four capabilities: Temporal order, negation, sentiment, and
beneficial effect. We find that models with similar performance on held-out
test sets have varying results on these capabilities.

摘要：藥物不良反應 (ADE) 是任何因藥物治療造成的有害事件。儘管它們很重要，但 ADE 經常在官方管道中被低報。因此，一些研究轉向偵測社群媒體中對 ADE 的討論。在偵測 ADE 的各種嘗試中，已取得令人印象深刻的成果。然而，在醫學等高風險領域中，深入評估模型的能力至關重要。我們以手工製作的範本，針對英文 ADE 偵測中的四項能力進行徹底的效能評估：時間順序、否定、情緒和有益效果。我們發現，在留存測試集中具有類似效能的模型，在這些能力上的結果卻不同。

##### **Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects**
2407.02430v1 by Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, Oran Gafni

The recent availability and adaptability of text-to-image models has sparked
a new era in many related domains that benefit from the learned text priors as
well as high-quality and fast generation capabilities, one of which is texture
generation for 3D objects. Although recent texture generation methods achieve
impressive results by using text-to-image networks, the combination of global
consistency, quality, and speed, which is crucial for advancing texture
generation to real-world applications, remains elusive. To that end, we
introduce Meta 3D TextureGen: a new feedforward method comprised of two
sequential networks aimed at generating high-quality and globally consistent
textures for arbitrary geometries of any complexity degree in less than 20
seconds. Our method achieves state-of-the-art results in quality and speed by
conditioning a text-to-image model on 3D semantics in 2D space and fusing them
into a complete and high-resolution UV texture map, as demonstrated by
extensive qualitative and quantitative evaluations. In addition, we introduce a
texture enhancement network that is capable of up-scaling any texture by an
arbitrary ratio, producing 4k pixel resolution textures.

摘要：最近文本转图像模型的可用性和适应性激发了在许多相关领域的全新时代，这些领域受益于学习的文本先验以及高质量和快速的生成能力，其中之一是 3D 物体的纹理生成。尽管最近的纹理生成方法通过使用文本到图像网络取得了令人印象深刻的结果，但全局一致性、质量和速度的结合对于推进纹理生成到实际应用至关重要，这一点仍然难以捉摸。为此，我们引入了 Meta 3D TextureGen：一种新的前馈方法，由两个顺序网络组成，旨在为任意复杂程度的几何形状生成高质量且全局一致的纹理，时间少于 20 秒。我们的方法通过在 2D 空间中对文本到图像模型进行 3D 语义调节，并将它们融合到完整且高分辨率的 UV 纹理贴图中，从而在质量和速度方面取得了最先进的结果，这一点已通过广泛的定性和定量评估得到证明。此外，我们引入了一个纹理增强网络，该网络能够以任意比例放大任何纹理，生成 4k 像素分辨率的纹理。

##### **CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models**
2407.02408v1 by Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li

As Large Language Models (LLMs) are increasingly deployed to handle various
natural language processing (NLP) tasks, concerns regarding the potential
negative societal impacts of LLM-generated content have also arisen. To
evaluate the biases exhibited by LLMs, researchers have recently proposed a
variety of datasets. However, existing bias evaluation efforts often focus on
only a particular type of bias and employ inconsistent evaluation metrics,
leading to difficulties in comparison across different datasets and LLMs. To
address these limitations, we collect a variety of datasets designed for the
bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation
Benchmark that covers different types of bias across different social groups
and tasks. The curation of CEB is based on our newly proposed compositional
taxonomy, which characterizes each dataset from three dimensions: bias types,
social groups, and tasks. By combining the three dimensions, we develop a
comprehensive evaluation strategy for the bias in LLMs. Our experiments
demonstrate that the levels of bias vary across these dimensions, thereby
providing guidance for the development of specific bias mitigation methods.

摘要：由於大型語言模型 (LLM)  zunehmend zur Bewältigung verschiedener Aufgaben der Verarbeitung natürlicher Sprache (NLP) eingesetzt werden, sind auch Bedenken hinsichtlich der potenziellen negativen gesellschaftlichen Auswirkungen von LLM-generierten Inhalten aufgekommen. Um die von LLMs aufgewiesenen Verzerrungen zu bewerten, haben Forscher in jüngster Zeit eine Vielzahl von Datensätzen vorgeschlagen. Allerdings konzentrieren sich bestehende Bemühungen zur Verzerrungsbewertung oft nur auf eine bestimmte Art von Verzerrung und verwenden inkonsistente Bewertungsmetriken, was zu Schwierigkeiten beim Vergleich verschiedener Datensätze und LLMs führt. Um diese Einschränkungen zu beheben, sammeln wir eine Vielzahl von Datensätzen, die für die Verzerrungsbewertung von LLMs konzipiert wurden, und schlagen außerdem CEB vor, einen kompositorischen Bewertungsmaßstab, der verschiedene Arten von Verzerrungen in verschiedenen sozialen Gruppen und Aufgaben abdeckt. Die Zusammenstellung von CEB basiert auf unserer neu vorgeschlagenen kompositorischen Taxonomie, die jeden Datensatz anhand von drei Dimensionen charakterisiert: Verzerrungstypen, soziale Gruppen und Aufgaben. Durch die Kombination der drei Dimensionen entwickeln wir eine umfassende Bewertungsstrategie für die Verzerrung in LLMs. Unsere Experimente zeigen, dass die Verzerrungsstufen in diesen Dimensionen variieren und bieten so eine Anleitung für die Entwicklung spezifischer Methoden zur Reduzierung von Verzerrungen.

##### **Assessing the Code Clone Detection Capability of Large Language Models**
2407.02402v1 by Zixian Zhang, Takfarinas Saber

This study aims to assess the performance of two advanced Large Language
Models (LLMs), GPT-3.5 and GPT-4, in the task of code clone detection. The
evaluation involves testing the models on a variety of code pairs of different
clone types and levels of similarity, sourced from two datasets: BigCloneBench
(human-made) and GPTCloneBench (LLM-generated). Findings from the study
indicate that GPT-4 consistently surpasses GPT-3.5 across all clone types. A
correlation was observed between the GPTs' accuracy at identifying code clones
and code similarity, with both GPT models exhibiting low effectiveness in
detecting the most complex Type-4 code clones. Additionally, GPT models
demonstrate a higher performance identifying code clones in LLM-generated code
compared to humans-generated code. However, they do not reach impressive
accuracy. These results emphasize the imperative for ongoing enhancements in
LLM capabilities, particularly in the recognition of code clones and in
mitigating their predisposition towards self-generated code clones--which is
likely to become an issue as software engineers are more numerous to leverage
LLM-enabled code generation and code refactoring tools.

摘要：本研究旨在評估兩個進階大型語言模型 (LLM) GPT-3.5 和 GPT-4 在程式碼複製偵測任務中的表現。評估包括在不同複製類型和相似度層級的各種程式碼配對上測試模型，這些配對來自兩個資料集：BigCloneBench（人工製作）和 GPTCloneBench（LLM 生成）。研究結果顯示，GPT-4 在所有複製類型中始終優於 GPT-3.5。觀察到 GPT 在識別程式碼複製和程式碼相似度之間存在相關性，兩個 GPT 模型在偵測最複雜的 Type-4 程式碼複製時都表現出低效率。此外，與人類生成的程式碼相比，GPT 模型在識別 LLM 生成的程式碼中的程式碼複製時表現出更高的效能。然而，它們並未達到令人印象深刻的準確度。這些結果強調了持續強化 LLM 能力的必要性，特別是在程式碼複製識別方面，以及減輕它們對自生程式碼複製的傾向——隨著軟體工程師越來越頻繁地利用 LLM 啟用的程式碼生成和程式碼重構工具，這可能會成為一個問題。

##### **Learning to Refine with Fine-Grained Natural Language Feedback**
2407.02397v1 by Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, Greg Durrett

Recent work has explored the capability of large language models (LLMs) to
identify and correct errors in LLM-generated responses. These refinement
approaches frequently evaluate what sizes of models are able to do refinement
for what problems, but less attention is paid to what effective feedback for
refinement looks like. In this work, we propose looking at refinement with
feedback as a composition of three distinct LLM competencies: (1)
identification of bad generations; (2) fine-grained natural language feedback
generation; (3) refining with fine-grained feedback. The first step can be
implemented with a high-performing discriminative model and steps 2 and 3 can
be implemented either via prompted or fine-tuned LLMs. A key property of this
approach is that the step 2 critique model can give fine-grained feedback about
errors, made possible by offloading the discrimination to a separate model in
step 1. We show that models of different capabilities benefit from refining
with this approach on the task of improving factual consistency of document
grounded summaries. Overall, our proposed method consistently outperforms
existing end-to-end refinement approaches and current trained models not
fine-tuned for factuality critiquing.

摘要：最近的研究探讨了大型语言模型 (LLM) 识别和更正 LLM 生成的响应中错误的能力。这些优化方法经常评估什么规模的模型能够对什么问题进行优化，但较少关注有效的优化反馈是什么样的。在这项工作中，我们建议将优化与反馈视为三个不同的 LLM 能力的组合：(1) 识别不良生成；(2) 细粒度的自然语言反馈生成；(3) 使用细粒度反馈进行优化。第一步可以通过高性能判别模型来实现，步骤 2 和 3 可以通过提示或微调 LLM 来实现。这种方法的一个关键特性是第 2 步的批判模型可以提供关于错误的细粒度反馈，这可以通过在第 1 步将判别卸载到一个单独的模型来实现。我们表明，不同能力的模型受益于使用这种方法来优化改进文档基础摘要的事实一致性的任务。总体而言，我们提出的方法始终优于现有的端到端优化方法和当前未针对事实性批判进行微调的训练模型。

##### **Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval**
2407.02395v1 by Jiexin Wang, Xitong Luo, Liuwen Cao, Hongkui He, Hailin Huang, Jiayuan Xie, Adam Jatowt, Yi Cai

Large language models (LLMs) have brought significant advancements to code
generation and code repair, benefiting both novice and experienced developers.
However, their training using unsanitized data from open-source repositories,
like GitHub, raises the risk of inadvertently propagating security
vulnerabilities. Despite numerous studies investigating the safety of code
LLMs, there remains a gap in comprehensively addressing their security
features. In this work, we aim to present a comprehensive study aimed at
precisely evaluating and enhancing the security aspects of code LLMs. To
support our research, we introduce CodeSecEval, a meticulously curated dataset
designed to address 44 critical vulnerability types with 180 distinct samples.
CodeSecEval serves as the foundation for the automatic evaluation of code
models in two crucial tasks: code generation and code repair, with a strong
emphasis on security. Our experimental results reveal that current models
frequently overlook security issues during both code generation and repair
processes, resulting in the creation of vulnerable code. In response, we
propose different strategies that leverage vulnerability-aware information and
insecure code explanations to mitigate these security vulnerabilities.
Furthermore, our findings highlight that certain vulnerability types
particularly challenge model performance, influencing their effectiveness in
real-world applications. Based on these findings, we believe our study will
have a positive impact on the software engineering community, inspiring the
development of improved methods for training and utilizing LLMs, thereby
leading to safer and more trustworthy model deployment.

摘要：大型語言模型 (LLM) 為程式碼產生和修復帶來了顯著的進步，造福了新手和經驗豐富的開發人員。然而，使用來自開放原始碼儲存庫（如 GitHub）中未經清理的資料進行訓練，會增加無意間傳播安全漏洞的風險。儘管有許多研究探討程式碼 LLM 的安全性，但仍缺乏全面解決其安全功能的辦法。在這項工作中，我們旨在提出一個全面的研究，以精確評估和增強程式碼 LLM 的安全層面。為了支持我們的研究，我們引入了 CodeSecEval，這是一個經過精心策劃的資料集，旨在解決 44 種關鍵漏洞類型，並有 180 個不同的範例。CodeSecEval 作為自動評估程式碼模型在兩個關鍵任務中的基礎：程式碼產生和程式碼修復，並特別強調安全性。我們的實驗結果顯示，目前的模型在程式碼產生和修復過程中經常忽略安全問題，導致產生有漏洞的程式碼。針對此問題，我們提出了不同的策略，利用漏洞感知資訊和不安全的程式碼說明來減輕這些安全漏洞。此外，我們的研究結果強調，某些類型的漏洞特別會影響模型效能，影響其在實際應用中的有效性。根據這些發現，我們相信我們的研究將對軟體工程社群產生正面的影響，激勵開發改進的 LLM 訓練和使用方式，從而導致更安全和更值得信賴的模型部署。

##### **SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation**
2407.02389v1 by Sayan Nag, Koustava Goswami, Srikrishna Karanam

Referring Expression Segmentation (RES) aims to provide a segmentation mask
of the target object in an image referred to by the text (i.e., referring
expression). Existing methods require large-scale mask annotations. Moreover,
such approaches do not generalize well to unseen/zero-shot scenarios. To
address the aforementioned issues, we propose a weakly-supervised bootstrapping
architecture for RES with several new algorithmic innovations. To the best of
our knowledge, ours is the first approach that considers only a fraction of
both mask and box annotations (shown in Figure 1 and Table 1) for training. To
enable principled training of models in such low-annotation settings, improve
image-text region-level alignment, and further enhance spatial localization of
the target object in the image, we propose Cross-modal Fusion with Attention
Consistency module. For automatic pseudo-labeling of unlabeled samples, we
introduce a novel Mask Validity Filtering routine based on a spatially aware
zero-shot proposal scoring approach. Extensive experiments show that with just
30% annotations, our model SafaRi achieves 59.31 and 48.26 mIoUs as compared to
58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTR
respectively on RefCOCO+@testA and RefCOCO+testB datasets. SafaRi also
outperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in a
fully-supervised setting and demonstrates strong generalization capabilities in
unseen/zero-shot tasks.

摘要：指涉表達分割 (RES) 旨在提供由文字所指稱的影像中目標物件的分割遮罩（即指涉表達）。現有的方法需要大規模的遮罩註解。此外，此類方法無法很好地推廣到未見過的/零次學習場景。為了解決上述問題，我們提出了一種 RES 的弱監督引導架構，並具備多項新的演算法創新。據我們所知，我們的做法是第一個只考慮一小部分遮罩和方框註解（如圖 1 和表 1 所示）進行訓練的方法。為了在這種低註解設定中啟用模型的原則性訓練，改善影像文字區域等級對齊，並進一步增強影像中目標物件的空間定位，我們提出了帶有注意力一致性模組的跨模態融合。對於未標記樣本的自動偽標籤，我們基於空間感知的零次學習提議評分方法，引進了一種新的遮罩有效性過濾常式。廣泛的實驗顯示，我們的 SafaRi 模型僅使用 30% 的註解，就能在 RefCOCO+@testA 和 RefCOCO+testB 資料集上分別達到 59.31 和 48.26 mIoU，而全監督的 SOTA 方法 SeqTR 則分別獲得 58.93 和 48.19 mIoU。在全監督設定中，SafaRi 也比 SeqTR 高出 11.7%（在 RefCOCO+testA 上）和 19.6%（在 RefCOCO+testB 上），並在未見過的/零次學習任務中展現出強大的泛化能力。

##### **Talking to Machines: do you read me?**
2407.02354v1 by Lina M. Rojas-Barahona

In this dissertation I would like to guide the reader to the research on
dialogue but more precisely the research I have conducted during my career
since my PhD thesis. Starting from modular architectures with machine
learning/deep learning and reinforcement learning to end-to-end deep neural
networks. Besides my work as research associate, I also present the work I have
supervised in the last years.
  I review briefly the state of the art and highlight the open research
problems on conversational agents. Afterwards, I present my contribution to
Task-Oriented Dialogues (TOD), both as research associate and as the industrial
supervisor of CIFRE theses. I discuss conversational QA. Particularly, I
present the work of two PhD candidates Thibault Cordier and Sebastien Montella;
as well as the work of the young researcher Quentin Brabant. Finally, I present
the scientific project, where I discuss about Large Language Models (LLMs) for
Task-Oriented Dialogue and Multimodal Task-Oriented Dialogue.

摘要：在本文中，我想引導讀者了解對話研究，更準確地說，是我自博士論文以來，在職業生涯中進行的研究。從具備機器學習/深度學習和強化學習的模組化架構，到端到端的深度神經網路開始。除了我的研究助理工作外，我也會介紹我在過去幾年監督的工作。
我簡要回顧了對話代理的現狀，並重點說明了開放的研究問題。之後，我將介紹我在任務導向對話 (TOD) 中的貢獻，既作為研究助理，也作為 CIFRE 論文的產業指導。我討論了對話式問答。特別是，我介紹了兩位博士候選人 Thibault Cordier 和 Sebastien Montella 的工作；以及青年研究員 Quentin Brabant 的工作。最後，我介紹了科學計畫，其中我討論了任務導向對話和多模態任務導向對話的大型語言模型 (LLM)。

##### **Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification**
2407.02352v1 by Pritish Sahu, Karan Sikka, Ajay Divakaran

Large Visual Language Models (LVLMs) struggle with hallucinations in visual
instruction following task(s), limiting their trustworthiness and real-world
applicability. We propose Pelican -- a novel framework designed to detect and
mitigate hallucinations through claim verification. Pelican first decomposes
the visual claim into a chain of sub-claims based on first-order predicates.
These sub-claims consist of (predicate, question) pairs and can be
conceptualized as nodes of a computational graph. We then use
Program-of-Thought prompting to generate Python code for answering these
questions through flexible composition of external tools. Pelican improves over
prior work by introducing (1) intermediate variables for precise grounding of
object instances, and (2) shared computation for answering the sub-question to
enable adaptive corrections and inconsistency identification. We finally use
reasoning abilities of LLM to verify the correctness of the the claim by
considering the consistency and confidence of the (question, answer) pairs from
each sub-claim. Our experiments reveal a drop in hallucination rate by
$\sim$8%-32% across various baseline LVLMs and a 27% drop compared to
approaches proposed for hallucination mitigation on MMHal-Bench. Results on two
other benchmarks further corroborate our results.

摘要：大型视觉语言模型 (LVLMs) 在视觉指令遵循任务中会产生幻觉，这限制了它们的可靠性和现实世界的适用性。我们提出了 Pelican——一种旨在通过声明验证来检测和减轻幻觉的新型框架。Pelican 首先根据一阶谓词将视觉声明分解成一个子声明链。这些子声明由 (谓词、问题) 对组成，可以被概念化为计算图的节点。然后，我们使用思想计划提示来生成 Python 代码，通过外部工具的灵活组合来回答这些问题。Pelican 通过引入 (1) 用于对象实例精确接地的中间变量，以及 (2) 用于回答子问题以实现自适应校正和不一致性识别的共享计算，改进了之前的工作。我们最终使用 LLM 的推理能力，通过考虑每个子声明的 (问题、答案) 对的一致性和置信度来验证声明的正确性。我们的实验表明，在各种基线 LVLMs 中，幻觉率下降了约 8%-32%，与 MMHal-Bench 上提出的幻觉缓解方法相比，下降了 27%。在另外两个基准上的结果进一步证实了我们的结果。

##### **Generative Large Language Models in Automated Fact-Checking: A Survey**
2407.02351v1 by Ivan Vykopal, Matúš Pikuliak, Simon Ostermann, Marián Šimko

The dissemination of false information across online platforms poses a
serious societal challenge, necessitating robust measures for information
verification. While manual fact-checking efforts are still instrumental, the
growing volume of false information requires automated methods. Large language
models (LLMs) offer promising opportunities to assist fact-checkers, leveraging
LLM's extensive knowledge and robust reasoning capabilities. In this survey
paper, we investigate the utilization of generative LLMs in the realm of
fact-checking, illustrating various approaches that have been employed and
techniques for prompting or fine-tuning LLMs. By providing an overview of
existing approaches, this survey aims to improve the understanding of utilizing
LLMs in fact-checking and to facilitate further progress in LLMs' involvement
in this process.

摘要：在線上平台上散布虛假訊息會造成嚴重的社會問題，因此需要採取強而有力的措施來驗證資訊。雖然手動查核事實的工作仍至關重要，但虛假訊息的數量與日俱增，因此需要自動化的方法。大型語言模型 (LLM) 提供了協助查核人員的絕佳機會，它能運用 LLM 廣泛的知識和強大的推理能力。在這篇調查報告中，我們研究了在查核事實領域中使用生成式 LLM 的情況，說明了已採用的各種方法，以及提示或微調 LLM 的技術。透過提供現有方法的概觀，這項調查旨在增進對在查核事實中使用 LLM 的理解，並促進 LLM 在此過程中進一步參與。

##### **MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space**
2407.02345v1 by Yihong Tang, Bo Wang, Dongming Zhao, Xiaojia Jin, Jijun Zhang, Ruifang He, Yuexian Hou

Personalized Dialogue Generation (PDG) aims to create coherent responses
according to roles or personas. Traditional PDG relies on external role data,
which can be scarce and raise privacy concerns. Approaches address these issues
by extracting role information from dialogue history, which often fail to
generically model roles in continuous space. To overcome these limitations, we
introduce a novel framework \textbf{MO}dels \textbf{R}oles from
\textbf{P}ersonalized Dialogue \textbf{H}istory by \textbf{E}xploring and
\textbf{U}tilizing Latent \textbf{S}pace (MORPHEUS) through a three-stage
training process. Specifically, we create a persona codebook to represent roles
in latent space compactly, and this codebook is used to construct a posterior
distribution of role information. This method enables the model to generalize
across roles, allowing the generation of personalized dialogues even for unseen
roles. Experiments on both Chinese and English datasets demonstrate that
MORPHEUS enhances the extraction of role information, and improves response
generation without external role data. Additionally, MORPHEUS can be considered
an efficient fine-tuning for large language models.

摘要：個性化對話生成 (PDG) 旨在根據角色或身分建立連貫的回應。傳統 PDG 依賴於外部角色資料，這類資料可能稀少且引起隱私問題。方法透過從對話記錄中萃取角色資訊來解決這些問題，但通常無法在連續空間中普遍建立角色模型。為了克服這些限制，我們引進一個新架構，透過探索和利用潛在空間 (MORPHEUS) 中的個性化對話歷史，建立角色模型 (MOR)。特別是，我們建立一個角色碼本，以簡潔的方式在潛在空間中表示角色，而這個碼本用於建構角色資訊的事後分配。這個方法讓模型能夠在角色間概化，即使對於未見過的角色，也能產生個性化的對話。在中文和英文資料集上的實驗證明，MORPHEUS 增強了角色資訊的萃取，並在沒有外部角色資料的情況下改善了回應生成。此外，MORPHEUS 可被視為大型語言模型的有效微調。

##### **RVISA: Reasoning and Verification for Implicit Sentiment Analysis**
2407.02340v1 by Wenna Lai, Haoran Xie, Guandong Xu, Qing Li

With an increasing social demand for fine-grained sentiment analysis (SA),
implicit sentiment analysis (ISA) poses a significant challenge with the
absence of salient cue words in expressions. It necessitates reliable reasoning
to understand how the sentiment is aroused and thus determine implicit
sentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)
LLMs have gained popularity to serve as backbone models for SA applications,
considering impressive text comprehension and reasoning ability among diverse
tasks. On the other hand, Decoder-only (DO) LLMs exhibit superior natural
language generation and in-context learning capabilities. However, their
responses may contain misleading or inaccurate information. To identify
implicit sentiment with reliable reasoning, this study proposes RVISA, a
two-stage reasoning framework that harnesses the generation ability of DO LLMs
and the reasoning ability of ED LLMs to train an enhanced reasoner.
Specifically, we adopt three-hop reasoning prompting to explicitly furnish
sentiment elements as cues. The generated rationales are utilized to fine-tune
an ED LLM into a skilled reasoner. Additionally, we develop a straightforward
yet effective verification mechanism to ensure the reliability of the reasoning
learning. We evaluated the proposed method on two benchmark datasets and
achieved state-of-the-art results in ISA performance.

摘要：<paragraph>隨著對細粒度情感分析 (SA) 的社會需求不斷增加，
隱式情感分析 (ISA) 在表達式中缺乏顯著提示詞的情況下，構成了重大挑戰。它需要可靠的推理來理解情感是如何被激發的，從而確定隱含的情感。在大語言模型 (LLM) 時代，編碼器-解碼器 (ED) LLM 已廣受歡迎，可用作 SA 應用程式的骨幹模型，考慮到在各種任務中令人印象深刻的文本理解和推理能力。另一方面，僅解碼器 (DO) LLM 表現出優越的自然語言生成和情境學習能力。然而，它們的回應可能包含誤導或不準確的資訊。為了以可靠的推理識別隱含的情感，本研究提出了 RVISA，一個兩階段推理框架，利用 DO LLM 的生成能力和 ED LLM 的推理能力來訓練一個增強的推理器。具體來說，我們採用三跳推理提示來明確提供情感元素作為線索。生成的依據用於微調 ED LLM，使其成為一個熟練的推理器。此外，我們開發了一種簡單但有效的驗證機制，以確保推理學習的可靠性。我們在兩個基準資料集上評估了所提出的方法，並在 ISA 效能方面取得了最先進的成果。</paragraph>

##### **Open foundation models for Azerbaijani language**
2407.02337v1 by Jafar Isbarov, Kavsar Huseynova, Elvin Mammadov, Mammad Hajili

The emergence of multilingual large language models has enabled the
development of language understanding and generation systems in Azerbaijani.
However, most of the production-grade systems rely on cloud solutions, such as
GPT-4. While there have been several attempts to develop open foundation models
for Azerbaijani, these works have not found their way into common use due to a
lack of systemic benchmarking. This paper encompasses several lines of work
that promote open-source foundation models for Azerbaijani. We introduce (1) a
large text corpus for Azerbaijani, (2) a family of encoder-only language models
trained on this dataset, (3) labeled datasets for evaluating these models, and
(4) extensive evaluation that covers all major open-source models with
Azerbaijani support.

摘要：多語言大型語言模型的出現，使得我們得以開發亞塞拜然語的語言理解與生成系統。然而，大多數的生產級系統依賴於雲端解決方案，例如 GPT-4。儘管已經有幾次嘗試開發亞塞拜然語的開放基礎模型，但由於缺乏系統性的基準測試，這些作品並未普及。本文包含了幾項推廣亞塞拜然語的開放原始碼基礎模型的工作。我們介紹了 (1) 亞塞拜然語的大型語料庫、(2) 在此資料集上訓練的一系列編碼器專用語言模型、(3) 用於評估這些模型的標籤資料集，以及 (4) 涵蓋所有支援亞塞拜然語的主要開放原始碼模型的廣泛評估。

##### **CALICO: Confident Active Learning with Integrated Calibration**
2407.02335v1 by Lorenzo S. Querol, Hajime Nagahara, Hideaki Hayashi

The growing use of deep learning in safety-critical applications, such as
medical imaging, has raised concerns about limited labeled data, where this
demand is amplified as model complexity increases, posing hurdles for domain
experts to annotate data. In response to this, active learning (AL) is used to
efficiently train models with limited annotation costs. In the context of deep
neural networks (DNNs), AL often uses confidence or probability outputs as a
score for selecting the most informative samples. However, modern DNNs exhibit
unreliable confidence outputs, making calibration essential. We propose an AL
framework that self-calibrates the confidence used for sample selection during
the training process, referred to as Confident Active Learning with Integrated
CalibratiOn (CALICO). CALICO incorporates the joint training of a classifier
and an energy-based model, instead of the standard softmax-based classifier.
This approach allows for simultaneous estimation of the input data distribution
and the class probabilities during training, improving calibration without
needing an additional labeled dataset. Experimental results showcase improved
classification performance compared to a softmax-based classifier with fewer
labeled samples. Furthermore, the calibration stability of the model is
observed to depend on the prior class distribution of the data.

摘要：深度學習在安全關鍵應用中使用日益廣泛，例如醫學影像，這引發了對標籤數據有限的擔憂，隨著模型複雜性的增加，這種需求會被放大，這對領域專家註解數據構成了障礙。為了應對這一問題，主動學習 (AL) 被用於以有限的註解成本有效地訓練模型。在深度神經網路 (DNN) 的背景下，AL 經常使用置信度或機率輸出作為選擇最有資訊性的樣本的分數。然而，現代 DNN 表現出不可靠的置信度輸出，這使得校準至關重要。我們提出了一個 AL 框架，它會在訓練過程中自行校準用於樣本選擇的置信度，稱為具有整合校準的自信主動學習 (CALICO)。CALICO 結合了分類器和基於能量的模型的聯合訓練，而不是標準的基於 softmax 的分類器。這種方法允許在訓練期間同時估計輸入數據分佈和類別機率，從而改進校準，而無需額外的標籤數據集。實驗結果表明，與基於 softmax 的分類器相比，在標籤樣本較少的情況下，分類性能得到了改善。此外，觀察到模型的校準穩定性取決於數據的先驗類別分佈。

##### **Why do LLaVA Vision-Language Models Reply to Images in English?**
2407.02333v1 by Musashi Hinck, Carolin Holtermann, Matthew Lyle Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shaoyen Tseng, Vasudev Lal

We uncover a surprising multilingual bias occurring in a popular class of
multimodal vision-language models (VLMs). Including an image in the query to a
LLaVA-style VLM significantly increases the likelihood of the model returning
an English response, regardless of the language of the query. This paper
investigates the causes of this loss with a two-pronged approach that combines
extensive ablation of the design space with a mechanistic analysis of the
models' internal representations of image and text inputs. Both approaches
indicate that the issue stems in the language modelling component of the LLaVA
model. Statistically, we find that switching the language backbone for a
bilingual language model has the strongest effect on reducing this error.
Mechanistically, we provide compelling evidence that visual inputs are not
mapped to a similar space as text ones, and that intervening on intermediary
attention layers can reduce this bias. Our findings provide important insights
to researchers and engineers seeking to understand the crossover between
multimodal and multilingual spaces, and contribute to the goal of developing
capable and inclusive VLMs for non-English contexts.

摘要：我們發現一類流行的多模態視覺語言模型 (VLM) 中出現令人驚訝的多語言偏差。在 LLaVA 風格的 VLM 中，在查詢中包含影像會大幅增加模型傳回英文回應的可能性，不論查詢的語言為何。本文透過雙管齊下的方法探討此損失的原因，結合設計空間的廣泛消融與模型對影像和文字輸入的內部表徵的機制分析。兩種方法皆顯示問題源自 LLaVA 模型的語言建模元件。在統計上，我們發現將語言主幹切換為雙語語言模型對減少此錯誤的影響最大。在機制上，我們提供了令人信服的證據，證明視覺輸入未對應到與文字輸入類似的空間，且介入中間注意力層可以減少此偏差。我們的研究結果為尋求了解多模態與多語言空間之間交叉的研究人員和工程師提供了重要見解，並有助於達成開發適用於非英語語境的強大且包容的 VLM 的目標。

##### **Efficient Sparse Attention needs Adaptive Token Release**
2407.02328v1 by Chaoran Zhang, Lixin Zou, Dan Luo, Min Tang, Xiangyang Luo, Zihao Li, Chenliang Li

In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities across a wide array of text-centric tasks. However, their `large'
scale introduces significant computational and storage challenges, particularly
in managing the key-value states of the transformer, which limits their wider
applicability. Therefore, we propose to adaptively release resources from
caches and rebuild the necessary key-value states. Particularly, we accomplish
this by a lightweight controller module to approximate an ideal top-$K$ sparse
attention. This module retains the tokens with the highest top-$K$ attention
weights and simultaneously rebuilds the discarded but necessary tokens, which
may become essential for future decoding. Comprehensive experiments in natural
language generation and modeling reveal that our method is not only competitive
with full attention in terms of performance but also achieves a significant
throughput improvement of up to 221.8%. The code for replication is available
on the https://github.com/WHUIR/ADORE.

摘要：近年來，大型語言模型 (LLM) 已在各種以文字為中心的任務中展現出非凡的能力。然而，它們的「大」規模引入了顯著的計算和儲存挑戰，特別是在管理Transformer的鍵值狀態時，這限制了它們更廣泛的適用性。因此，我們建議自適應地從快取釋放資源，並重建必要的鍵值狀態。特別是，我們透過一個輕量級控制器模組來近似一個理想的頂部-$K$ 稀疏注意力來完成這項任務。此模組保留具有最高頂部-$K$ 注意力權重的代碼，並同時重建被捨棄但必要的代碼，這些代碼可能對未來的解碼至關重要。在自然語言生成和建模中的全面實驗揭示了我們的模型不僅在效能方面與完全注意力的方式相匹敵，還達到了高達 221.8% 的顯著吞吐量提升。複製的程式碼可在 https://github.com/WHUIR/ADORE 上取得。

##### **Exploring the Role of Transliteration in In-Context Learning for Low-resource Languages Written in Non-Latin Scripts**
2407.02320v1 by Chunlan Ma, Yihong Liu, Haotian Ye, Hinrich Schütze

Decoder-only large language models (LLMs) excel in high-resource languages
across various tasks through few-shot or even zero-shot in-context learning
(ICL). However, their performance often does not transfer well to low-resource
languages, especially those written in non-Latin scripts. Inspired by recent
work that leverages transliteration in encoder-only models, we investigate
whether transliteration is also effective in improving LLMs' performance for
low-resource languages written in non-Latin scripts. To this end, we propose
three prompt templates, where the target-language text is represented in (1)
its original script, (2) Latin script, or (3) both. We apply these methods to
several representative LLMs of different sizes on various tasks including text
classification and sequential labeling. Our findings show that the
effectiveness of transliteration varies by task type and model size. For
instance, all models benefit from transliterations for sequential labeling
(with increases of up to 25%).

摘要：僅解碼器的大型語言模型 (LLM) 在各種任務中表現出色，例如少次嘗試或甚至零次嘗試的語境學習 (ICL)。然而，它們的效能通常無法順利轉移到低資源語言，尤其是以非拉丁字母撰寫的語言。受到最近利用編碼器專用模型中的音譯工作的啟發，我們研究音譯是否也能有效提升 LLM 對以非拉丁字母撰寫的低資源語言的效能。為此，我們提出三個提示範本，其中目標語言文字以 (1) 其原始文字、(2) 拉丁文字或 (3) 兩者表示。我們將這些方法應用於各種不同規模的代表性 LLM，執行各種任務，包括文字分類和順序標記。我們的研究結果顯示，音譯的有效性會因任務類型和模型大小而異。例如，所有模型都能從音譯中受益，進行順序標記（增加幅度最高達 25%）。

##### **Soft Language Prompts for Language Transfer**
2407.02317v1 by Ivan Vykopal, Simon Ostermann, Marián Šimko

Cross-lingual knowledge transfer, especially between high- and low-resource
languages, remains a challenge in natural language processing (NLP). This study
offers insights for improving cross-lingual NLP applications through the
combination of parameter-efficient fine-tuning methods. We systematically
explore strategies for enhancing this cross-lingual transfer through the
incorporation of language-specific and task-specific adapters and soft prompts.
We present a detailed investigation of various combinations of these methods,
exploring their efficiency across six languages, focusing on three low-resource
languages, including the to our knowledge first use of soft language prompts.
Our findings demonstrate that in contrast to claims of previous work, a
combination of language and task adapters does not always work best; instead,
combining a soft language prompt with a task adapter outperforms other
configurations in many cases.

摘要：跨語言知識轉移，特別是在高資源和低資源語言之間，仍然是自然語言處理 (NLP) 中的挑戰。本研究透過結合參數有效微調方法，提供改進跨語言 NLP 應用程式的見解。我們系統性地探索策略，透過整合特定語言和特定任務的適配器和軟提示，來增強這種跨語言轉移。我們對這些方法的各種組合進行詳細調查，探討它們在六種語言中的效率，重點關注三種低資源語言，包括我們所知首次使用軟語言提示。我們的研究結果表明，與先前工作的說法相反，語言和任務適配器的組合並不總是效果最好；相反，在許多情況下，將軟語言提示與任務適配器結合使用會優於其他配置。

##### **VFIMamba: Video Frame Interpolation with State Space Models**
2407.02315v1 by Guozhen Zhang, Chunxu Liu, Yutao Cui, Xiaotong Zhao, Kai Ma, Limin Wang

Inter-frame modeling is pivotal in generating intermediate frames for video
frame interpolation (VFI). Current approaches predominantly rely on convolution
or attention-based models, which often either lack sufficient receptive fields
or entail significant computational overheads. Recently, Selective State Space
Models (S6) have emerged, tailored specifically for long sequence modeling,
offering both linear complexity and data-dependent modeling capabilities. In
this paper, we propose VFIMamba, a novel frame interpolation method for
efficient and dynamic inter-frame modeling by harnessing the S6 model. Our
approach introduces the Mixed-SSM Block (MSB), which initially rearranges
tokens from adjacent frames in an interleaved fashion and subsequently applies
multi-directional S6 modeling. This design facilitates the efficient
transmission of information across frames while upholding linear complexity.
Furthermore, we introduce a novel curriculum learning strategy that
progressively cultivates proficiency in modeling inter-frame dynamics across
varying motion magnitudes, fully unleashing the potential of the S6 model.
Experimental findings showcase that our method attains state-of-the-art
performance across diverse benchmarks, particularly excelling in
high-resolution scenarios. In particular, on the X-TEST dataset, VFIMamba
demonstrates a noteworthy improvement of 0.80 dB for 4K frames and 0.96 dB for
2K frames.

摘要：<paragraph>影格間建模對於生成影片補幀 (VFI) 的中間影格至關重要。目前的作法主要依賴於卷積或基於注意力的模型，這些模型通常缺乏足夠的感受野，或需要大量的運算開銷。最近出現了選擇性狀態空間模型 (S6)，專門針對長序列建模而設計，同時具備線性複雜度和依資料而定的建模能力。在本文中，我們提出 VFIMamba，這是一種新穎的影格補幀方法，透過利用 S6 模型進行有效率且動態的影格間建模。我們的作法引入了混合 SSM 區塊 (MSB)，它最初會以交錯的方式重新排列相鄰影格中的標記，然後套用多向 S6 建模。此設計促進了影格間的有效資訊傳遞，同時維持線性複雜度。此外，我們引進了一種新穎的課程學習策略，它會逐步培養建模不同運動幅度影格間動態的能力，充分發揮 S6 模型的潛力。實驗結果顯示，我們的模型在各種基準測試中都達到了最先進的效能，特別是在高解析度場景中表現出色。特別是在 X-TEST 資料集上，VFIMamba 對於 4K 影格展示了顯著的 0.80 dB 提升，對於 2K 影格則提升了 0.96 dB。</paragraph>

##### **Evaluating the Ability of LLMs to Solve Semantics-Aware Process Mining Tasks**
2407.02310v1 by Adrian Rebmann, Fabian David Schmidt, Goran Glavaš, Han van der Aa

The process mining community has recently recognized the potential of large
language models (LLMs) for tackling various process mining tasks. Initial
studies report the capability of LLMs to support process analysis and even, to
some extent, that they are able to reason about how processes work. This latter
property suggests that LLMs could also be used to tackle process mining tasks
that benefit from an understanding of process behavior. Examples of such tasks
include (semantic) anomaly detection and next activity prediction, which both
involve considerations of the meaning of activities and their inter-relations.
In this paper, we investigate the capabilities of LLMs to tackle such
semantics-aware process mining tasks. Furthermore, whereas most works on the
intersection of LLMs and process mining only focus on testing these models out
of the box, we provide a more principled investigation of the utility of LLMs
for process mining, including their ability to obtain process mining knowledge
post-hoc by means of in-context learning and supervised fine-tuning.
Concretely, we define three process mining tasks that benefit from an
understanding of process semantics and provide extensive benchmarking datasets
for each of them. Our evaluation experiments reveal that (1) LLMs fail to solve
challenging process mining tasks out of the box and when provided only a
handful of in-context examples, (2) but they yield strong performance when
fine-tuned for these tasks, consistently surpassing smaller, encoder-based
language models.

摘要：<paragraph>流程挖掘社群最近已意识到大型语言模型 (LLM) 在处理各种流程挖掘任务的潜力。初期研究报告指出，LLM 具有支持流程分析的能力，甚至在一定程度上能够推理流程的工作方式。后一项特性表明，LLM 也可用于处理受益于了解流程行为的流程挖掘任务。此类任务的范例包括（语义）异常检测和下一个活动预测，这两个任务都涉及考虑活动及其相互关系的含义。在本文中，我们调查了 LLM 处理此类语义感知流程挖掘任务的能力。此外，虽然大多数关于 LLM 和流程挖掘交叉点的工作仅专注于开箱即用地测试这些模型，但我们提供了对 LLM 在流程挖掘中的效用的更具原则性的调查，包括它们通过情境学习和监督微调事后获取流程挖掘知识的能力。具体来说，我们定义了三个受益于了解流程语义的流程挖掘任务，并为每个任务提供了广泛的基准数据集。我们的评估实验表明，（1）LLM 无法开箱即用地解决具有挑战性的流程挖掘任务，并且仅在提供少数情境示例时无法解决，（2）但当针对这些任务进行微调时，它们会产生强大的性能，始终优于较小的基于编码器的语言模型。</paragraph>

##### **Semantically Guided Representation Learning For Action Anticipation**
2407.02309v1 by Anxhelo Diko, Danilo Avola, Bardh Prenkaj, Federico Fontana, Luigi Cinque

Action anticipation is the task of forecasting future activity from a
partially observed sequence of events. However, this task is exposed to
intrinsic future uncertainty and the difficulty of reasoning upon
interconnected actions. Unlike previous works that focus on extrapolating
better visual and temporal information, we concentrate on learning action
representations that are aware of their semantic interconnectivity based on
prototypical action patterns and contextual co-occurrences. To this end, we
propose the novel Semantically Guided Representation Learning (S-GEAR)
framework. S-GEAR learns visual action prototypes and leverages language models
to structure their relationship, inducing semanticity. To gather insights on
S-GEAR's effectiveness, we test it on four action anticipation benchmarks,
obtaining improved results compared to previous works: +3.5, +2.7, and +3.5
absolute points on Top-1 Accuracy on Epic-Kitchen 55, EGTEA Gaze+ and 50
Salads, respectively, and +0.8 on Top-5 Recall on Epic-Kitchens 100. We further
observe that S-GEAR effectively transfers the geometric associations between
actions from language to visual prototypes. Finally, S-GEAR opens new research
frontiers in anticipation tasks by demonstrating the intricate impact of action
semantic interconnectivity.

摘要：動作預測是根據部分觀察到的事件序列預測未來活動的任務。然而，此任務會受到內在未來的不確定性以及推理相互關聯動作的難度的影響。與專注於推斷出更佳視覺和時間資訊的先前研究不同，我們專注於學習動作表示，這些表示會根據原型動作模式和情境共現而了解它們的語義互連性。為此，我們提出了新穎的語義引導表示學習 (S-GEAR) 架構。S-GEAR 學習視覺動作原型並利用語言模型來建構它們的關係，誘導語義性。為了收集關於 S-GEAR 有效性的見解，我們在四個動作預測基準上對其進行測試，與先前的研究相比獲得了更好的結果：在 Epic-Kitchen 55、EGTEA Gaze+ 和 50 Salads 上的 Top-1 準確率分別提高了 3.5、2.7 和 3.5 個絕對點，而 Epic-Kitchens 100 上的 Top-5 召回率提高了 0.8。我們進一步觀察到，S-GEAR 有效地將動作之間的幾何關聯從語言傳遞到視覺原型。最後，S-GEAR 透過展示動作語義互連性的複雜影響，為預測任務開啟了新的研究領域。

##### **Towards Human Understanding of Paraphrase Types in ChatGPT**
2407.02302v1 by Dominik Meier, Jan Philip Wahle, Terry Ruas, Bela Gipp

Paraphrases represent a human's intuitive ability to understand expressions
presented in various different ways. Current paraphrase evaluations of language
models primarily use binary approaches, offering limited interpretability of
specific text changes. Atomic paraphrase types (APT) decompose paraphrases into
different linguistic changes and offer a granular view of the flexibility in
linguistic expression (e.g., a shift in syntax or vocabulary used). In this
study, we assess the human preferences towards ChatGPT in generating English
paraphrases with ten APTs and five prompting techniques. We introduce APTY
(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level
annotations by 15 annotators. The dataset also provides a human preference
ranking of paraphrases with different types that can be used to fine-tune
models with RLHF and DPO methods. Our results reveal that ChatGPT can generate
simple APTs, such as additions and deletions, but struggle with complex
structures (e.g., subordination changes). This study contributes to
understanding which aspects of paraphrasing language models have already
succeeded at understanding and what remains elusive. In addition, our curated
datasets can be used to develop language models with specific linguistic
capabilities.

摘要：同義改寫代表人類直覺理解各種不同表達方式的能力。目前的語言模型同義改寫評量主要使用二元方法，提供有限的特定文字變更的可解釋性。原子同義改寫類型 (APT) 將同義改寫分解成不同的語言變化，並提供語言表達靈活性（例如，用法的句法或詞彙轉換）的細部觀點。在本研究中，我們評估人類偏好 ChatGPT 使用十種 APT 和五種提示技巧來產生英文同義改寫。我們介紹 APTY（原子同義改寫類型），一個由 15 位註解者註解的 500 個句子層級和字詞層級的資料集。此資料集也提供人類偏好排名，包含不同類型的同義改寫，可用於微調具有 RLHF 和 DPO 方法的模型。我們的結果顯示 ChatGPT 可以產生簡單的 APT，例如增刪，但難以處理複雜的結構（例如，從屬變更）。本研究有助於了解語言模型中的哪些同義改寫面向已經成功理解，哪些仍然難以捉摸。此外，我們整理的資料集可用於開發具有特定語言能力的語言模型。

##### **CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models**
2407.02301v1 by Ying Nie, Binwei Yan, Tianyu Guo, Hao Liu, Haoyu Wang, Wei He, Binfan Zheng, Weihao Wang, Qiang Li, Weijian Sun, Yunhe Wang, Dacheng Tao

Large language models (LLMs) have achieved remarkable performance on various
NLP tasks, yet their potential in more challenging and domain-specific task,
such as finance, has not been fully explored. In this paper, we present
CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark
to date, for assessing the financial knowledge of LLMs under Chinese context.
In practice, to better align with the career trajectory of Chinese financial
practitioners, we build a systematic evaluation from 4 first-level categories:
(1) Financial Subject: whether LLMs can memorize the necessary basic knowledge
of financial subjects, such as economics, statistics and auditing. (2)
Financial Qualification: whether LLMs can obtain the needed financial qualified
certifications, such as certified public accountant, securities qualification
and banking qualification. (3) Financial Practice: whether LLMs can fulfill the
practical financial jobs, such as tax consultant, junior accountant and
securities analyst. (4) Financial Law: whether LLMs can meet the requirement of
financial laws and regulations, such as tax law, insurance law and economic
law. CFinBench comprises 99,100 questions spanning 43 second-level categories
with 3 question types: single-choice, multiple-choice and judgment. We conduct
extensive experiments of 50 representative LLMs with various model size on
CFinBench. The results show that GPT4 and some Chinese-oriented models lead the
benchmark, with the highest average accuracy being 60.16%, highlighting the
challenge presented by CFinBench. The dataset and evaluation code are available
at https://cfinbench.github.io/.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務上取得了顯著的表現，但它們在更具挑戰性和特定領域的任務中的潛力，例如金融，尚未得到充分探索。在本文中，我們提出 CFinBench：一個精心製作的、迄今為止最全面的評估基準，用於評估 LLM 在中文背景下的金融知識。在實務中，為了更好地與中國金融從業人員的職業發展軌跡保持一致，我們從 4 個一級類別建立了一個系統性的評估：(1) 金融科目：LLM 是否能記住必要的金融科目基礎知識，例如經濟學、統計學和審計。(2) 金融資格：LLM 是否能取得所需的金融資格認證，例如註冊會計師、證券資格和銀行資格。(3) 金融實務：LLM 是否能履行實務的金融工作，例如稅務顧問、初級會計師和證券分析師。(4) 金融法規：LLM 是否能符合金融法規的要求，例如稅法、保險法和經濟法。CFinBench 包含 99,100 個問題，涵蓋 43 個二級類別，有 3 種類型的問題：單選題、多選題和判斷題。我們對 50 個具有各種模型大小的代表性 LLM 在 CFinBench 上進行了廣泛的實驗。結果表明，GPT4 和一些以中文為導向的模型領先於基準，平均準確率最高為 60.16%，突顯了 CFinBench 所帶來的挑戰。資料集和評估程式碼可在 https://cfinbench.github.io/ 取得。

##### **Strategic Demand-Planning in Wireless Networks: Can Generative-AI Save Spectrum and Energy?**
2407.02292v1 by Berk Çiloğlu, Görkem Berkay Koç, Afsoon Alidadi Shamsabadi, Metin Ozturk, Halim Yanikomeroglu

Wireless communications advance hand-in-hand with artificial intelligence
(AI), indicating an interconnected advancement where each facilitates and
benefits from the other. This synergy is particularly evident in the
development of the sixth-generation technology standard for mobile networks
(6G), envisioned to be AI-native. Generative-AI (GenAI), a novel technology
capable of producing various types of outputs, including text, images, and
videos, offers significant potential for wireless communications, with its
distinctive features. Traditionally, conventional AI techniques have been
employed for predictions, classifications, and optimization, while GenAI has
more to offer. This article introduces the concept of strategic demand-planning
through demand-labeling, demand-shaping, and demand-rescheduling. Accordingly,
GenAI is proposed as a powerful tool to facilitate demand-shaping in wireless
networks. More specifically, GenAI is used to compress and convert the content
of various kind (e.g., from a higher bandwidth mode to a lower one, such as
from a video to text), which subsequently enhances performance of wireless
networks in various usage scenarios such as cell-switching, user association
and load balancing, interference management, and disaster scenarios management.
Therefore, GenAI can serve a function in saving energy and spectrum in wireless
networks. With recent advancements in AI, including sophisticated algorithms
like large-language-models and the development of more powerful hardware built
exclusively for AI tasks, such as AI accelerators, the concept of
demand-planning, particularly demand-shaping through GenAI, becomes
increasingly relevant. Furthermore, recent efforts to make GenAI accessible on
devices, such as user terminals, make the implementation of this concept even
more straightforward and feasible.

摘要：無線通訊與人工智慧（AI）攜手並進，顯示出一個相互促進和受益的互聯進步。這種協同效應在構想為 AI 原生的第六代行動網路（6G）技術標準的開發中尤其明顯。生成式 AI（GenAI）是一種新穎的技術，能夠產生各種類型的輸出，包括文字、影像和影片，並具備其獨特的功能，為無線通訊提供了顯著的潛力。傳統上，傳統的 AI 技術已被用於預測、分類和最佳化，而 GenAI 則提供了更多功能。本文介紹了透過需求標籤、需求塑造和需求重新排程來進行策略性需求規劃的概念。因此，GenAI 被提議作為一個強大的工具來促進無線網路中的需求塑造。更具體地說，GenAI 被用於壓縮和轉換各種類型的內容（例如，從較高的頻寬模式轉換到較低的頻寬模式，例如從影片轉換成文字），這進而增強了無線網路在各種使用情境中的效能，例如小区切換、使用者關聯和負載平衡、干擾管理和災害情境管理。因此，GenAI 可以發揮在無線網路中節省能源和頻譜的功能。隨著 AI 的最新進展，包括大型語言模型等複雜演算法，以及專門為 AI 任務打造的更強大硬體（例如 AI 加速器）的開發，需求規劃的概念，特別是透過 GenAI 進行的需求塑造，變得越來越相關。此外，最近讓 GenAI 在裝置上（例如使用者終端）可用的努力，讓這個概念的實作變得更加直接和可行。

##### **Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in Adverse Weather**
2407.02286v1 by Junsung Park, Kyungmin Kim, Hyunjung Shim

Existing LiDAR semantic segmentation methods often struggle with performance
declines in adverse weather conditions. Previous research has addressed this
issue by simulating adverse weather or employing universal data augmentation
during training. However, these methods lack a detailed analysis and
understanding of how adverse weather negatively affects LiDAR semantic
segmentation performance. Motivated by this issue, we identified key factors of
adverse weather and conducted a toy experiment to pinpoint the main causes of
performance degradation: (1) Geometric perturbation due to refraction caused by
fog or droplets in the air and (2) Point drop due to energy absorption and
occlusions. Based on these findings, we propose new strategic data augmentation
techniques. First, we introduced a Selective Jittering (SJ) that jitters points
in the random range of depth (or angle) to mimic geometric perturbation.
Additionally, we developed a Learnable Point Drop (LPD) to learn vulnerable
erase patterns with Deep Q-Learning Network to approximate the point drop
phenomenon from adverse weather conditions. Without precise weather simulation,
these techniques strengthen the LiDAR semantic segmentation model by exposing
it to vulnerable conditions identified by our data-centric analysis.
Experimental results confirmed the suitability of the proposed data
augmentation methods for enhancing robustness against adverse weather
conditions. Our method attains a remarkable 39.5 mIoU on the
SemanticKITTI-to-SemanticSTF benchmark, surpassing the previous
state-of-the-art by over 5.4%p, tripling the improvement over the baseline
compared to previous methods achieved.

摘要：<paragraph>現有的 LiDAR 語意分割方法在惡劣的天氣條件下常常會遇到效能下降的問題。先前的研究透過模擬惡劣的天氣或在訓練期間採用通用資料擴充來解決這個問題。然而，這些方法缺乏對惡劣天氣如何對 LiDAR 語意分割效能造成負面影響的詳細分析和理解。受此問題的啟發，我們找出惡劣天氣的主要因素並進行了一個玩具實驗，以找出效能下降的主要原因：(1) 由於空氣中的霧或水滴造成的折射導致的幾何擾動，以及 (2) 由於能量吸收和遮擋造成的點下降。根據這些發現，我們提出了新的策略性資料擴充技術。首先，我們引入了選擇性抖動 (SJ)，它會在深度（或角度）的隨機範圍內抖動點，以模擬幾何擾動。此外，我們開發了一個可學習點下降 (LPD)，以利用深度 Q 學習網路學習容易受影響的擦除模式，以近似惡劣天氣條件下的點下降現象。這些技術在沒有精確的天氣模擬下，透過將 LiDAR 語意分割模型暴露於我們以資料為中心的分析所識別的容易受影響的條件，來強化該模型。實驗結果證實了所提出的資料擴充方法在增強對抗惡劣天氣條件的穩健性方面的適用性。我們的模型在 SemanticKITTI-to-SemanticSTF 基準上達到了 39.5 mIoU 的顯著成績，比先前的技術水準提高了 5.4%p，與先前方法相比，對基準的改進提高了三倍。</paragraph>

##### **Renard: A Modular Pipeline for Extracting Character Networks from Narrative Texts**
2407.02284v1 by Arthur Amalvy, Vincent Labatut, Richard Dufour

Renard (Relationships Extraction from NARrative Documents) is a Python
library that allows users to define custom natural language processing (NLP)
pipelines to extract character networks from narrative texts. Contrary to the
few existing tools, Renard can extract dynamic networks, as well as the more
common static networks. Renard pipelines are modular: users can choose the
implementation of each NLP subtask needed to extract a character network. This
allows users to specialize pipelines to particular types of texts and to study
the impact of each subtask on the extracted network.

摘要：Renard（從敘事文件中萃取關係）是一個 Python 函式庫，讓使用者定義自訂的自然語言處理 (NLP) 管線，從敘事文本中萃取角色網絡。與現有的少數工具相反，Renard 可以萃取動態網絡，以及更常見的靜態網絡。Renard 管線具有模組化：使用者可以選擇每項 NLP 子任務的實作，以萃取角色網絡。這讓使用者可以針對特定類型的文本專門化管線，並研究每個子任務對萃取網絡的影響。

##### **FedIA: Federated Medical Image Segmentation with Heterogeneous Annotation Completeness**
2407.02280v1 by Yangyang Xiang, Nannan Wu, Li Yu, Xin Yang, Kwang-Ting Cheng, Zengqiang Yan

Federated learning has emerged as a compelling paradigm for medical image
segmentation, particularly in light of increasing privacy concerns. However,
most of the existing research relies on relatively stringent assumptions
regarding the uniformity and completeness of annotations across clients.
Contrary to this, this paper highlights a prevalent challenge in medical
practice: incomplete annotations. Such annotations can introduce incorrectly
labeled pixels, potentially undermining the performance of neural networks in
supervised learning. To tackle this issue, we introduce a novel solution, named
FedIA. Our insight is to conceptualize incomplete annotations as noisy data
(\textit{i.e.}, low-quality data), with a focus on mitigating their adverse
effects. We begin by evaluating the completeness of annotations at the client
level using a designed indicator. Subsequently, we enhance the influence of
clients with more comprehensive annotations and implement corrections for
incomplete ones, thereby ensuring that models are trained on accurate data. Our
method's effectiveness is validated through its superior performance on two
extensively used medical image segmentation datasets, outperforming existing
solutions. The code is available at https://github.com/HUSTxyy/FedIA.

摘要：联邦学习已成为医学影像分割的一个引人注目的范例，尤其是在隐私问题日益严重的背景下。然而，现有的大部分研究都依赖于相对严格的假设，即跨客户端注释的一致性和完整性。与此相反，本文重点介绍了医学实践中普遍存在的挑战：不完整注释。此类注释可能会引入错误标记的像素，从而可能损害神经网络在监督学习中的性能。为了解决这个问题，我们引入了一种名为 FedIA 的新颖解决方案。我们的见解是将不完整注释概念化为噪声数据（即低质量数据），重点在于减轻其不利影响。我们首先使用设计的指示符评估客户端级别的注释的完整性。随后，我们增强了具有更全面注释的客户端的影响力，并对不完整的注释实施了更正，从而确保模型在准确的数据上进行训练。我们的方法的有效性通过其在两个广泛使用的医学影像分割数据集上的卓越性能得到验证，优于现有的解决方案。代码可在 https://github.com/HUSTxyy/FedIA 获得。

##### **Learning Paradigms and Modelling Methodologies for Digital Twins in Process Industry**
2407.02275v1 by Michael Mayr, Georgios C. Chasparis, Josef Küng

Central to the digital transformation of the process industry are Digital
Twins (DTs), virtual replicas of physical manufacturing systems that combine
sensor data with sophisticated data-based or physics-based models, or a
combination thereof, to tackle a variety of industrial-relevant tasks like
process monitoring, predictive control or decision support. The backbone of a
DT, i.e. the concrete modelling methodologies and architectural frameworks
supporting these models, are complex, diverse and evolve fast, necessitating a
thorough understanding of the latest state-of-the-art methods and trends to
stay on top of a highly competitive market. From a research perspective,
despite the high research interest in reviewing various aspects of DTs,
structured literature reports specifically focusing on unravelling the utilized
learning paradigms (e.g. self-supervised learning) for DT-creation in the
process industry are a novel contribution in this field. This study aims to
address these gaps by (1) systematically analyzing the modelling methodologies
(e.g. Convolutional Neural Network, Encoder-Decoder, Hidden Markov Model) and
paradigms (e.g. data-driven, physics-based, hybrid) used for DT-creation; (2)
assessing the utilized learning strategies (e.g. supervised, unsupervised,
self-supervised); (3) analyzing the type of modelling task (e.g. regression,
classification, clustering); and (4) identifying the challenges and research
gaps, as well as, discuss potential resolutions provided.

摘要：數位轉型製程產業的核心是數位雙胞胎 (DT)，這是實體製造系統的虛擬複製品，結合感測器資料與精密的資料為基礎或物理為基礎的模型，或兩者的結合，以處理各種與產業相關的任務，例如製程監控、預測控制或決策支援。DT 的主幹，也就是支援這些模型的具體建模方法與架構框架，複雜且多元，而且演進快速，因此需要徹底了解最新的最先進方法與趨勢，才能在高度競爭的市場中保持領先。從研究的角度來看，儘管對檢視 DT 的各個面向有高度的研究興趣，但專注於解開在製程產業中用於建立 DT 的學習範例（例如自我監督學習）的結構化文獻報告，是這個領域的新貢獻。本研究旨在透過 (1) 系統性分析用於建立 DT 的建模方法（例如卷積神經網路、編碼器-解碼器、隱藏馬可夫模型）與範例（例如資料驅動、基於物理、混合）；(2) 評估所使用的學習策略（例如監督式、非監督式、自我監督）；(3) 分析建模任務的類型（例如迴歸、分類、分群）；以及 (4) 找出挑戰與研究差距，並討論提供的潛在解決方案來解決這些差距。

##### **Multilingual Trolley Problems for Language Models**
2407.02273v1 by Zhijing Jin, Sydney Levine, Max Kleiman-Weiner, Giorgio Piatti, Jiarui Liu, Fernando Gonzalez Adauto, Francesco Ortu, András Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Schölkopf

As large language models (LLMs) are deployed in more and more real-world
situations, it is crucial to understand their decision-making when faced with
moral dilemmas. Inspired by a large-scale cross-cultural study of human moral
preferences, "The Moral Machine Experiment", we set up the same set of moral
choices for LLMs. We translate 1K vignettes of moral dilemmas, parametrically
varied across key axes, into 100+ languages, and reveal the preferences of LLMs
in each of these languages. We then compare the responses of LLMs to that of
human speakers of those languages, harnessing a dataset of 40 million human
moral judgments. We discover that LLMs are more aligned with human preferences
in languages such as English, Korean, Hungarian, and Chinese, but less aligned
in languages such as Hindi and Somali (in Africa). Moreover, we characterize
the explanations LLMs give for their moral choices and find that fairness is
the most dominant supporting reason behind GPT-4's decisions and utilitarianism
by GPT-3. We also discover "language inequality" (which we define as the
model's different development levels in different languages) in a series of
meta-properties of moral decision making.

摘要：隨著大型語言模型 (LLM) 在越來越多的現實世界情況中部署，在面對道德困境時了解它們的決策至關重要。受到人類道德偏好大規模跨文化研究「道德機器實驗」的啟發，我們為 LLM 設定了相同的道德選擇。我們將 1K 個道德困境的小插曲翻譯成 100 多種語言，並根據關鍵軸線進行參數化變化，並揭示 LLM 在每種語言中的偏好。然後，我們將 LLM 的回答與這些語言的人類說話者的回答進行比較，利用包含 4000 萬個人類道德判斷的數據集。我們發現，LLM 與英語、韓語、匈牙利語和中文等語言中的人類偏好更一致，但在印地語和索馬里語（在非洲）等語言中則較不一致。此外，我們描述了 LLM 對其道德選擇的解釋，並發現公平是 GPT-4 決策背後最主要的支持理由，而 GPT-3 則以功利主義為基礎。我們還在道德決策制定的一系列元屬性中發現了「語言不平等」（我們將其定義為模型在不同語言中的不同發展水平）。

##### **Footprints of Data in a Classifier Model: The Privacy Issues and Their Mitigation through Data Obfuscation**
2407.02268v1 by Payel Sadhukhan, Tanujit Chakraborty

The avalanche of AI deployment and its security-privacy concerns are two
sides of the same coin. Article 17 of GDPR calls for the Right to Erasure; data
has to be obliterated from a system to prevent its compromise. Extant research
in this aspect focuses on effacing sensitive data attributes. However, several
passive modes of data compromise are yet to be recognized and redressed. The
embedding of footprints of training data in a prediction model is one such
facet; the difference in performance quality in test and training data causes
passive identification of data that have trained the model. This research
focuses on addressing the vulnerability arising from the data footprints. The
three main aspects are -- i] exploring the vulnerabilities of different
classifiers (to segregate the vulnerable and the non-vulnerable ones), ii]
reducing the vulnerability of vulnerable classifiers (through data obfuscation)
to preserve model and data privacy, and iii] exploring the privacy-performance
tradeoff to study the usability of the data obfuscation techniques. An
empirical study is conducted on three datasets and eight classifiers to explore
the above objectives. The results of the initial research identify the
vulnerability in classifiers and segregate the vulnerable and non-vulnerable
classifiers. The additional experiments on data obfuscation techniques reveal
their utility to render data and model privacy and also their capability to
chalk out a privacy-performance tradeoff in most scenarios. The results can aid
the practitioners with their choice of classifiers in different scenarios and
contexts.

摘要：人工智慧部署的雪崩效應及其對安全和隱私的疑慮，是同一枚硬幣的兩面。GDPR 第 17 條呼籲建立刪除權；資料必須從系統中徹底刪除，以防止其受到危害。現有研究在這方面的重點在於消除敏感資料屬性。然而，多種資料危害的被動模式尚未被發現並加以解決。訓練資料的足跡嵌入預測模型中，就是這樣的一個面向；測試資料和訓練資料在效能品質上的差異，會導致被動識別訓練模型的資料。本研究的重點在於解決由資料足跡引發的漏洞。三個主要面向為：一、探索不同分類器的漏洞（區分有漏洞和無漏洞的分類器），二、降低有漏洞分類器的漏洞（透過資料混淆），以維護模型和資料的隱私，三、探索隱私效能權衡，以研究資料混淆技術的可用性。針對三個資料集和八個分類器進行實證研究，以探討上述目標。初步研究的結果找出分類器中的漏洞，並區分有漏洞和無漏洞的分類器。針對資料混淆技術的額外實驗揭露其在呈現資料和模型隱私方面的效用，以及它們在多數情況下勾勒出隱私效能權衡的能力。這些結果有助於實務工作者在不同的情況和脈絡中選擇分類器。

##### **Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization**
2407.02243v1 by Yuchen Hu, Chen Chen, Siyin Wang, Eng Siong Chng, Chao Zhang

In this paper, we propose reverse inference optimization (RIO), a simple and
effective method designed to enhance the robustness of
autoregressive-model-based zero-shot text-to-speech (TTS) systems using
reinforcement learning from human feedback (RLHF). To assess the quality of
speech produced by the TTS system without human annotations, RIO introduces a
novel concept termed as reverse inference based on the Bayesian principle,
which suggests that a high-quality generated speech should be able to be used
as a prompt for subsequent generation using the same TTS model. By leveraging
reverse inference as the standard to select exemplars used in RLHF from the
speech samples generated by the TTS system itself, RIO steers the subsequent
optimization towards a direction of enhancing the TTS robustness. The RIO
framework, comprising sampling, automatic annotating, and learning, obviates
the need for a reward model or pairwise preference data, and significantly
improves the stability of zero-shot TTS performance by reducing the
discrepancies between training and inference conditions. Our experimental
results verify that RIO can effectively improve both subjective and objective
metrics, including mean opinion scores, word error rates, and speaker
similarity. Remarkably, RIO can also diminish the incidence of bad outputs to
nearly zero percent, rivalling the robustness when using ground-truth speech as
the prompt.

摘要：在本文中，我們提出反向推論最佳化 (RIO)，這是一個簡單且有效的方法，旨在透過人類回饋 (RLHF) 的強化學習，來增強基於自迴歸模型的零次學習文字轉語音 (TTS) 系統的穩健性。為了評估 TTS 系統產生的語音品質，RIO 引入一個稱為反向推論的新概念，它基於貝氏原理，表明一個高品質的生成語音應該可以用作後續使用相同 TTS 模型生成的提示。透過利用反向推論作為標準，從 TTS 系統本身生成的語音樣本中選擇 RLHF 中使用的範例，RIO 將後續最佳化引導至增強 TTS 穩健性的方向。RIO 架構包含取樣、自動註解和學習，消除了對獎勵模型或成對偏好資料的需求，並透過減少訓練和推論條件之間的差異，顯著改善了零次學習 TTS 的穩定性。我們的實驗結果驗證了 RIO 可以有效改善主觀和客觀指標，包括平均意見分數、字元錯誤率和說話者相似性。值得注意的是，RIO 也可以將不良輸出的發生率減少到接近 0%，與使用真實語音作為提示時的穩健性相媲美。

##### **Indian Stock Market Prediction using Augmented Financial Intelligence ML**
2407.02236v1 by Anishka Chauhan, Pratham Mayur, Yeshwanth Sai Gokarakonda, Pooriya Jamie, Naman Mehrotra

This paper presents price prediction models using Machine Learning algorithms
augmented with Superforecasters predictions, aimed at enhancing investment
decisions. Five Machine Learning models are built, including Bidirectional
LSTM, ARIMA, a combination of CNN and LSTM, GRU, and a model built using LSTM
and GRU algorithms. The models are evaluated using the Mean Absolute Error to
determine their predictive accuracy. Additionally, the paper suggests
incorporating human intelligence by identifying Superforecasters and tracking
their predictions to anticipate unpredictable shifts or changes in stock prices
. The predictions made by these users can further enhance the accuracy of stock
price predictions when combined with Machine Learning and Natural Language
Processing techniques. Predicting the price of any commodity can be a
significant task but predicting the price of a stock in the stock market deals
with much more uncertainty. Recognising the limited knowledge and exposure to
stocks among certain investors, this paper proposes price prediction models
using Machine Learning algorithms. In this work, five Machine learning models
are built using Bidirectional LSTM, ARIMA, a combination of CNN and LSTM, GRU
and the last one is built using LSTM and GRU algorithms. Later these models are
assessed using MAE scores to find which model is predicting with the highest
accuracy. In addition to this, this paper also suggests the use of human
intelligence to closely predict the shift in price patterns in the stock market
The main goal is to identify Superforecasters and track their predictions to
anticipate unpredictable shifts or changes in stock prices. By leveraging the
combined power of Machine Learning and the Human Intelligence, predictive
accuracy can be significantly increased.

摘要：本文提出了使用機器學習演算法的價格預測模型，並結合超級預測者的預測，旨在增強投資決策。構建了五個機器學習模型，包括雙向 LSTM、ARIMA、CNN 和 LSTM 的組合、GRU，以及使用 LSTM 和 GRU 演算法構建的模型。使用平均絕對誤差評估模型，以確定其預測準確度。此外，本文建議透過識別超級預測者並追蹤其預測，來納入人類智慧，以預測股票價格中不可預測的變化或變動。這些使用者做出的預測，在與機器學習和自然語言處理技術結合使用時，可以進一步提高股票價格預測的準確度。預測任何商品的價格可能是一項重要的任務，但在股票市場中預測股票的價格會遇到更多不確定性。本文認識到某些投資者對股票的知識和接觸有限，因此提出了使用機器學習演算法的價格預測模型。在這項工作中，使用雙向 LSTM、ARIMA、CNN 和 LSTM 的組合、GRU 以及最後一個使用 LSTM 和 GRU 演算法構建的模型，構建了五個機器學習模型。稍後使用 MAE 分數評估這些模型，以找出預測準確度最高的模型。除此之外，本文還建議使用人類智慧來密切預測股票市場中價格模式的變化。主要目標是識別超級預測者並追蹤其預測，以預測股票價格中不可預測的變化或變動。透過結合機器學習和人類智慧的力量，可以顯著提高預測準確度。

##### **Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional Brain CT Report Generation**
2407.02235v1 by Cheng-Yi Li, Kao-Jung Chang, Cheng-Fu Yang, Hsin-Yu Wu, Wenting Chen, Hritik Bansal, Ling Chen, Yi-Ping Yang, Yu-Chun Chen, Shih-Pin Chen, Jiing-Feng Lirng, Kai-Wei Chang, Shih-Hwa Chiou

Multi-modal large language models (MLLMs) have been given free rein to
explore exciting medical applications with a primary focus on radiology report
generation. Nevertheless, the preliminary success in 2D radiology captioning is
incompetent to reflect the real-world diagnostic challenge in the volumetric 3D
anatomy. To mitigate three crucial limitation aspects in the existing
literature, including (1) data complexity, (2) model capacity, and (3)
evaluation metric fidelity, we collected an 18,885 text-scan pairs 3D-BrainCT
dataset and applied clinical visual instruction tuning (CVIT) to train BrainGPT
models to generate radiology-adherent 3D brain CT reports. Statistically, our
BrainGPT scored BLEU-1 = 44.35, BLEU-4 = 20.38, METEOR = 30.13, ROUGE-L = 47.6,
and CIDEr-R = 211.77 during internal testing and demonstrated an accuracy of
0.91 in captioning midline shifts on the external validation CQ500 dataset. By
further inspecting the captioned report, we reported that the traditional
metrics appeared to measure only the surface text similarity and failed to
gauge the information density of the diagnostic purpose. To close this gap, we
proposed a novel Feature-Oriented Radiology Task Evaluation (FORTE) to estimate
the report's clinical relevance (lesion feature and landmarks). Notably, the
BrainGPT model scored an average FORTE F1-score of 0.71 (degree=0.661;
landmark=0.706; feature=0.693; impression=0.779). To demonstrate that BrainGPT
models possess objective readiness to generate human-like radiology reports, we
conducted a Turing test that enrolled 11 physician evaluators, and around 74%
of the BrainGPT-generated captions were indistinguishable from those written by
humans. Our work embodies a holistic framework that showcased the first-hand
experience of curating a 3D brain CT dataset, fine-tuning anatomy-sensible
language models, and proposing robust radiology evaluation metrics.

摘要：<paragraph>多模态大型语言模型 (MLLM) 已获得自由探索令人兴奋的医学应用，重点放在放射学报告生成上。然而，2D 放射学标题的初步成功不足以反映体积 3D 解剖中的真实世界诊断挑战。为了减轻现有文献中的三个关键限制方面，包括 (1) 数据复杂性、(2) 模型容量和 (3) 评估指标保真度，我们收集了一个包含 18,885 个文本扫描对的 3D-BrainCT 数据集，并应用临床视觉指令调优 (CVIT) 来训练 BrainGPT 模型以生成符合放射学的 3D 脑 CT 报告。从统计学上讲，我们的 BrainGPT 在内部测试中获得了 BLEU-1 = 44.35、BLEU-4 = 20.38、METEOR = 30.13、ROUGE-L = 47.6 和 CIDEr-R = 211.77 的分数，并在外部验证 CQ500 数据集上对中线偏移的标题显示出 0.91 的准确度。通过进一步检查标题报告，我们报告说传统指标似乎只测量了表面文本相似性，而未能衡量诊断目的的信息密度。为了弥补这一差距，我们提出了一种新颖的面向特征的放射学任务评估 (FORTE) 来估计报告的临床相关性（病变特征和地标）。值得注意的是，BrainGPT 模型的平均 FORTE F1 分数为 0.71（程度=0.661；地标=0.706；特征=0.693；印象=0.779）。为了证明 BrainGPT 模型具备生成类似人类的放射学报告的客观准备，我们进行了一项图灵测试，招募了 11 位医生评估员，大约 74% 的 BrainGPT 生成的标题与人类写的标题无法区分。我们的工作体现了一个整体框架，展示了策划 3D 脑 CT 数据集、微调对解剖学敏感的语言模型以及提出稳健的放射学评估指标的直接经验。</paragraph>

##### **Synthetic Multimodal Question Generation**
2407.02233v1 by Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig

Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to
question-answering over multimodal documents. A key challenge with evaluating
MMRAG is the paucity of high-quality datasets matching the question styles and
modalities of interest. In light of this, we propose SMMQG, a synthetic data
generation framework. SMMQG leverages interplay between a retriever, large
language model (LLM) and large multimodal model (LMM) to generate question and
answer pairs directly from multimodal documents, with the questions conforming
to specified styles and modalities. We use SMMQG to generate an MMRAG dataset
of 1024 questions over Wikipedia documents and evaluate state-of-the-art models
using it, revealing insights into model performance that are attainable only
through style- and modality-specific evaluation data. Next, we measure the
quality of data produced by SMMQG via a human study. We find that the quality
of our synthetic data is on par with the quality of the crowdsourced benchmark
MMQA and that downstream evaluation results using both datasets strongly
concur.

摘要：多模态检索增强生成（MMRAG）是一种针对多模态文档进行问答的强大方法。MMRAG 评估的一个主要挑战是缺乏与问题样式和感兴趣模式相匹配的高质量数据集。有鉴于此，我们提出了 SMMQG，一种合成数据生成框架。SMMQG 利用检索器、大型语言模型 (LLM) 和大型多模态模型 (LMM) 之间的相互作用，直接从多模态文档中生成问题和答案对，其中问题符合指定的样式和模式。我们使用 SMMQG 生成一个包含 1024 个问题的 MMRAG 数据集，这些问题来自维基百科文档，并使用该数据集评估最先进的模型，揭示了只有通过特定于样式和模式的评估数据才能获得的模型性能见解。接下来，我们通过一项人为研究来衡量 SMMQG 生成的数据质量。我们发现我们合成数据的质量与众包基准 MMQA 的质量相当，并且使用两个数据集的下游评估结果强烈一致。

##### **MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders**
2407.02228v1 by Baijiong Lin, Weisen Jiang, Pengguang Chen, Yu Zhang, Shu Liu, Ying-Cong Chen

Multi-task dense scene understanding, which learns a model for multiple dense
prediction tasks, has a wide range of application scenarios. Modeling
long-range dependency and enhancing cross-task interactions are crucial to
multi-task dense prediction. In this paper, we propose MTMamba, a novel
Mamba-based architecture for multi-task scene understanding. It contains two
types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM)
block. STM handles long-range dependency by leveraging Mamba, while CTM
explicitly models task interactions to facilitate information exchange across
tasks. Experiments on NYUDv2 and PASCAL-Context datasets demonstrate the
superior performance of MTMamba over Transformer-based and CNN-based methods.
Notably, on the PASCAL-Context dataset, MTMamba achieves improvements of +2.08,
+5.01, and +4.90 over the previous best method in the tasks of semantic
segmentation, human parsing, and object boundary detection, respectively. The
code is available at \url{https://github.com/EnVision-Research/MTMamba}.

摘要：多任务密集场景理解，学习一个针对多个密集预测任务的模型，具有广泛的应用场景。对多任务密集预测而言，建模远程依赖性和增强跨任务交互至关重要。在本文中，我们提出了 MTMamba，一种用于多任务场景理解的新型基于 Mamba 的架构。它包含两种类型的核心模块：自任务 Mamba (STM) 模块和跨任务 Mamba (CTM) 模块。STM 通过利用 Mamba 处理远程依赖性，而 CTM 明确建模任务交互以促进跨任务的信息交换。在 NYUDv2 和 PASCAL-Context 数据集上的实验表明，MTMamba 优于基于 Transformer 和基于 CNN 的方法。值得注意的是，在 PASCAL-Context 数据集中，MTMamba 在语义分割、人体解析和对象边界检测任务中分别比之前最好的方法提高了 +2.08、+5.01 和 +4.90。代码可在 \url{https://github.com/EnVision-Research/MTMamba} 获得。

##### **Embodied AI in Mobile Robots: Coverage Path Planning with Large Language Models**
2407.02220v1 by Xiangrui Kong, Wenxiao Zhang, Jin Hong, Thomas Braunl

In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities in understanding and solving mathematical problems, leading to
advancements in various fields. We propose an LLM-embodied path planning
framework for mobile agents, focusing on solving high-level coverage path
planning issues and low-level control. Our proposed multi-layer architecture
uses prompted LLMs in the path planning phase and integrates them with the
mobile agents' low-level actuators. To evaluate the performance of various
LLMs, we propose a coverage-weighted path planning metric to assess the
performance of the embodied models. Our experiments show that the proposed
framework improves LLMs' spatial inference abilities. We demonstrate that the
proposed multi-layer framework significantly enhances the efficiency and
accuracy of these tasks by leveraging the natural language understanding and
generative capabilities of LLMs. Our experiments show that this framework can
improve LLMs' 2D plane reasoning abilities and complete coverage path planning
tasks. We also tested three LLM kernels: gpt-4o, gemini-1.5-flash, and
claude-3.5-sonnet. The experimental results show that claude-3.5 can complete
the coverage planning task in different scenarios, and its indicators are
better than those of the other models.

摘要：近年来，大型语言模型 (LLM) 在理解和解决数学问题方面表现出非凡的能力，促进了各个领域的发展。我们提出了一种面向移动代理的 LLM 具体化路径规划框架，重点解决高级覆盖路径规划问题和低级控制。我们提出的多层架构在路径规划阶段使用提示式 LLM，并将它们与移动代理的低级执行器集成在一起。为了评估各种 LLM 的性能，我们提出了一种覆盖加权路径规划指标来评估具体化模型的性能。我们的实验表明，所提出的框架改进了 LLM 的空间推理能力。我们证明，所提出的多层框架通过利用 LLM 的自然语言理解和生成能力，显著提高了这些任务的效率和准确性。我们的实验表明，该框架可以提高 LLM 的二维平面推理能力并完成覆盖路径规划任务。我们还测试了三个 LLM 内核：gpt-4o、gemini-1.5-flash 和 claude-3.5-sonnet。实验结果表明，claude-3.5 可以在不同的场景中完成覆盖规划任务，其指标优于其他模型。

##### **Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style Reinforcement Learning**
2407.02217v1 by Zakariae El Asri, Olivier Sigaud, Nicolas Thome

Applying reinforcement learning (RL) to real-world applications requires
addressing a trade-off between asymptotic performance, sample efficiency, and
inference time. In this work, we demonstrate how to address this triple
challenge by leveraging partial physical knowledge about the system dynamics.
Our approach involves learning a physics-informed model to boost sample
efficiency and generating imaginary trajectories from this model to learn a
model-free policy and Q-function. Furthermore, we propose a hybrid planning
strategy, combining the learned policy and Q-function with the learned model to
enhance time efficiency in planning. Through practical demonstrations, we
illustrate that our method improves the compromise between sample efficiency,
time efficiency, and performance over state-of-the-art methods.

摘要：將強化學習 (RL) 應用於實際應用中需要
解決漸近效能、樣本效率和推論時間之間的權衡。在這項工作中，我們展示如何透過利用系統動態的部分物理知識來解決這個三重挑戰。
我們的做法包括學習一個物理資訊模型來提升樣本效率，並從這個模型產生假想軌跡來學習一個無模型策略和 Q 函數。此外，我們提出一個混合規劃策略，將學習到的策略和 Q 函數與學習到的模型結合，以提升規劃中的時間效率。透過實際示範，我們說明我們的方法改善了樣本效率、時間效率和效能之間的折衷，優於最先進的方法。

##### **PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning**
2407.02211v1 by Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, Dongmei Zhang

Large language models (LLMs) have played a fundamental role in various
natural language processing tasks with powerful prompt techniques. However, in
real-world applications, there are often similar prompt components for repeated
queries, which causes significant computational burdens during inference.
Existing prompt compression and direct fine-tuning methods aim to tackle these
challenges, yet they frequently struggle to strike an optimal balance between
cost-efficiency and performance effectiveness, especially in complex tasks such
as NL2Code. In this paper, we propose a novel method namely PromptIntern to
internalize the prompt knowledge into model parameters via progressive
fine-tuning. Our method enables LLMs to emulate the human learning process for
a new task, where detailed templates and examples in a prompt are gradually
internalized and phased out progressively as the model grows accustomed to the
task. Extensive experiments demonstrate that our method reduces inference
tokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary
cost.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務中扮演著基本的角色，並使用強大的提示技術。然而，在實際應用中，重複的查詢通常有類似的提示組成，這會在推理過程中造成大量的運算負擔。現有的提示壓縮和直接微調方法旨在解決這些挑戰，但它們經常難以在成本效益和效能之間取得最佳平衡，尤其是在 NL2Code 等複雜任務中。在本文中，我們提出了一種新的方法，稱為 PromptIntern，通過漸進式微調將提示知識內化到模型參數中。我們的模型讓 LLM 能模擬人類學習新任務的過程，其中提示中的詳細範本和範例會逐漸內化，並隨著模型逐漸適應任務而逐步淘汰。廣泛的實驗證明，我們的模型將推理代幣減少了 90% 以上，將推理速度提高了 4.2 倍，並節省了 88.3% 的金錢成本。

##### **Generative Monoculture in Large Language Models**
2407.02209v1 by Fan Wu, Emily Black, Varun Chandrasekaran

We introduce {\em generative monoculture}, a behavior observed in large
language models (LLMs) characterized by a significant narrowing of model output
diversity relative to available training data for a given task: for example,
generating only positive book reviews for books with a mixed reception. While
in some cases, generative monoculture enhances performance (e.g., LLMs more
often produce efficient code), the dangers are exacerbated in others (e.g.,
LLMs refuse to share diverse opinions). As LLMs are increasingly used in
high-impact settings such as education and web search, careful maintenance of
LLM output diversity is essential to ensure a variety of facts and perspectives
are preserved over time. We experimentally demonstrate the prevalence of
generative monoculture through analysis of book review and code generation
tasks, and find that simple countermeasures such as altering sampling or
prompting strategies are insufficient to mitigate the behavior. Moreover, our
results suggest that the root causes of generative monoculture are likely
embedded within the LLM's alignment processes, suggesting a need for developing
fine-tuning paradigms that preserve or promote diversity.

摘要：<paragraph>我們介紹了「生成單一文化」，這是一種在大型語言模型 (LLM) 中觀察到的行為，其特徵是模型輸出多樣性相對於給定任務的可用訓練資料顯著變窄：例如，只為評價褒貶不一的書籍生成正面的書評。雖然在某些情況下，生成單一文化會增強效能（例如，LLM 更常產生高效的程式碼），但其危險性在其他情況下會加劇（例如，LLM 拒絕分享不同的意見）。由於 LLM 愈來愈多地用於教育和網路搜尋等高影響力的環境中，仔細維護 LLM 輸出的多樣性對於確保隨著時間推移，各種事實和觀點都能被保留下來至關重要。我們透過分析書評和程式碼生成任務，以實驗方式證明了生成單一文化的普遍性，並發現簡單的對策，例如改變抽樣或提示策略，不足以減輕這種行為。此外，我們的結果表明，生成單一文化的根本原因可能存在於 LLM 的比對過程中，這表明需要開發能保留或促進多樣性的微調範例。</paragraph>

##### **How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise on Machine Translation**
2407.02208v1 by Yan Meng, Di Wu, Christof Monz

The massive amounts of web-mined parallel data contain large amounts of
noise. Semantic misalignment, as the primary source of the noise, poses a
challenge for training machine translation systems. In this paper, we first
study the impact of real-world hard-to-detect misalignment noise by proposing a
process to simulate the realistic misalignment controlled by semantic
similarity. After quantitatively analyzing the impact of simulated misalignment
on machine translation, we show the limited effectiveness of widely used
pre-filters to improve the translation performance, underscoring the necessity
of more fine-grained ways to handle data noise. By observing the increasing
reliability of the model's self-knowledge for distinguishing misaligned and
clean data at the token-level, we propose a self-correction approach which
leverages the model's prediction distribution to revise the training
supervision from the ground-truth data over training time. Through
comprehensive experiments, we show that our self-correction method not only
improves translation performance in the presence of simulated misalignment
noise but also proves effective for real-world noisy web-mined datasets across
eight translation tasks.

摘要：大量的網路挖掘平行資料包含大量的雜訊。語意錯位，作為雜訊的主要來源，對訓練機器翻譯系統構成挑戰。在本文中，我們首先透過提出一個模擬由語意相似性控制的現實錯位過程，來研究真實世界難以偵測的錯位雜訊的影響。在對模擬錯位對機器翻譯的影響進行量化分析後，我們展示了廣泛使用的預過濾器在改善翻譯性能方面的有限效果，強調了處理資料雜訊需要更細緻的方式。透過觀察模型自我知識在區分錯位和乾淨資料的可靠性在代幣層級上不斷提高，我們提出了一個自我修正方法，該方法利用模型的預測分佈來修正訓練時間內來自真實資料的訓練監督。透過全面的實驗，我們展示了我們的自我修正方法不僅改善了在模擬錯位雜訊存在下的翻譯性能，而且對於八項翻譯任務的真實世界嘈雜網路挖掘資料集也證明了其有效性。

##### **Automatic Adaptation Rule Optimization via Large Language Models**
2407.02203v1 by Yusei Ishimizu, Jialong Li, Jinglue Xu, Jinyu Cai, Hitoshi Iba, Kenji Tei

Rule-based adaptation is a foundational approach to self-adaptation,
characterized by its human readability and rapid response. However, building
high-performance and robust adaptation rules is often a challenge because it
essentially involves searching the optimal design in a complex (variables)
space. In response, this paper attempt to employ large language models (LLMs)
as a optimizer to construct and optimize adaptation rules, leveraging the
common sense and reasoning capabilities inherent in LLMs. Preliminary
experiments conducted in SWIM have validated the effectiveness and limitation
of our method.

摘要：<paragraph>基於規則的適應是自適應的一種基礎方法，其特點是人類可讀性和快速反應。然而，建立高性能和強大的適應規則通常是一個挑戰，因為它本質上涉及在一個複雜的（變數）空間中搜尋最佳設計。作為回應，本文嘗試使用大型語言模型 (LLM) 作為優化器來建構和優化適應規則，利用 LLM 中固有的常識和推理能力。在 SWIM 中進行的初步實驗驗證了我們方法的有效性和局限性。</paragraph>

##### **Research on Reliable and Safe Occupancy Grid Prediction in Underground Parking Lots**
2407.02197v1 by JiaQi Luo

Against the backdrop of advancing science and technology, autonomous vehicle
technology has emerged as a focal point of intense scrutiny within the academic
community. Nevertheless, the challenge persists in guaranteeing the safety and
reliability of this technology when navigating intricate scenarios. While a
substantial portion of autonomous driving research is dedicated to testing in
open-air environments, such as urban roads and highways, where the myriad
variables at play are meticulously examined, enclosed indoor spaces like
underground parking lots have, to a significant extent, been overlooked in the
scholarly discourse. This discrepancy highlights a gap in derstanding the
unique challenges these confined settings pose for autonomous navigation
systems.
  This study tackles indoor autonomous driving, particularly in overlooked
spaces like underground parking lots. Using CARLA's simulation platform, a
realistic parking model is created for data gathering. An occupancy grid
network then processes this data to predict vehicle paths and obstacles,
enhancing the system's perception in complex indoor environments. Ultimately,
this strategy improves safety in autonomous parking operations. The paper
meticulously evaluates the model's predictive capabilities, validating its
efficacy in the context of underground parking. Our findings confirm that the
proposed strategy successfully enhances autonomous vehicle performance in these
complex indoor settings. It equips autonomous systems with improved adaptation
to underground lots, reinforcing safety measures and dependability. This work
paves the way for future advancements and applications by addressing the
research shortfall concerning indoor parking environments, serving as a pivotal
reference point.

摘要：在科學技術進步的背景下，自動駕駛技術已成為學術界關注的焦點。然而，在應對複雜場景時，保證該技術的安全性和可靠性仍然是一個挑戰。雖然自動駕駛研究的很大一部分專注於在戶外環境（例如城市道路和高速公路）中進行測試，並仔細檢查了眾多變量，但地下停車場等封閉的室內空間在學術論述中在很大程度上被忽視了。這種差異突顯了理解這些受限環境對自動導航系統構成的獨特挑戰方面的差距。
本研究解決了室內自動駕駛問題，特別是在地下停車場等被忽視的空間中。使用 CARLA 的模擬平台，創建了一個逼真的停車模型以收集數據。然後，佔用網格網路處理此數據以預測車輛路徑和障礙物，從而增強系統在複雜室內環境中的感知。最終，此策略提高了自動停車操作的安全性。本文仔細評估了該模型的預測能力，驗證了其在地下停車場中的功效。我們的研究結果證實，所提出的策略成功地增強了自動駕駛汽車在這些複雜的室內環境中的性能。它使自動系統能夠更好地適應地下停車場，加強安全措施和可靠性。這項工作通過解決有關室內停車環境的研究不足，為未來的進步和應用鋪平了道路，並作為一個關鍵參考點。

##### **Attack-Aware Noise Calibration for Differential Privacy**
2407.02191v1 by Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Flavio du Pin Calmon, Carmela Troncoso

Differential privacy (DP) is a widely used approach for mitigating privacy
risks when training machine learning models on sensitive data. DP mechanisms
add noise during training to limit the risk of information leakage. The scale
of the added noise is critical, as it determines the trade-off between privacy
and utility. The standard practice is to select the noise scale in terms of a
privacy budget parameter $\epsilon$. This parameter is in turn interpreted in
terms of operational attack risk, such as accuracy, or sensitivity and
specificity of inference attacks against the privacy of the data. We
demonstrate that this two-step procedure of first calibrating the noise scale
to a privacy budget $\epsilon$, and then translating $\epsilon$ to attack risk
leads to overly conservative risk assessments and unnecessarily low utility. We
propose methods to directly calibrate the noise scale to a desired attack risk
level, bypassing the intermediate step of choosing $\epsilon$. For a target
attack risk, our approach significantly decreases noise scale, leading to
increased utility at the same level of privacy. We empirically demonstrate that
calibrating noise to attack sensitivity/specificity, rather than $\epsilon$,
when training privacy-preserving ML models substantially improves model
accuracy for the same risk level. Our work provides a principled and practical
way to improve the utility of privacy-preserving ML without compromising on
privacy.

摘要：差分隱私 (DP) 是一種廣泛用於在敏感數據上訓練機器學習模型時降低隱私風險的方法。DP 機制會在訓練過程中加入雜訊，以限制資訊洩露的風險。加入雜訊的規模至關重要，因為它決定了隱私和效用之間的權衡。標準做法是根據隱私預算參數 $\epsilon$ 選擇雜訊規模。這個參數會進一步根據操作攻擊風險進行解釋，例如準確性，或對資料隱私的推論攻擊的敏感性和特異性。我們證明了這個兩步驟程序，首先將雜訊規模校準到隱私預算 $\epsilon$，然後將 $\epsilon$ 轉換為攻擊風險，會導致過於保守的風險評估和不必要的低效用。我們提出方法，將雜訊規模直接校準到所需的攻擊風險等級，略過選擇 $\epsilon$ 的中間步驟。對於目標攻擊風險，我們的做法會顯著降低雜訊規模，在相同的隱私等級下提高效用。我們經驗性地證明，在訓練隱私保護 ML 模型時，將雜訊校準到攻擊敏感性/特異性，而不是 $\epsilon$，會大幅改善在相同風險等級下的模型準確度。我們的研究提供了一個有原則且實用的方法，可以在不影響隱私的情況下提高隱私保護 ML 的效用。

##### **LlamAr & GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning**
2407.02147v1 by Hasna Chouikhi, Manel Aloui, Cyrine Ben Hammou, Ghaith Chaabane, Haithem Kchaou, Chehir Dhaouadi

Large language models (LLMs) have greatly impacted the natural language
processing (NLP) field, particularly for the English language. These models
have demonstrated capabilities in understanding and generating human-like text.
The success of language models largely depends on the availability of
high-quality instruction datasets, which consist of detailed task descriptions
and corresponding responses that are essential for training the models to
accurately address a variety of prompts. However, the availability and quality
of these resources vary by language. While models perform well in English, they
often struggle with languages like Arabic, due to the lack of datasets for
fine-tuning Arabic-specific tasks. To address this issue, we introduce
InstAr-500k, a new Arabic instruction dataset created by generating and
collecting content that covers several domains and instruction types. We then
assess this dataset by fine-tuning two open-source models, Llama-3-8B-Instruct
and Gemma-7B-IT, on several downstream tasks to scale improvements in their
functionality. Based on multiple evaluations, our fine-tuned models achieve
state-of-the-art performance on several Arabic NLP benchmarks. These outcomes
emphasize the effectiveness of our dataset in elevating the capabilities of
language models for Arabic. Our instruction dataset bridges the performance gap
between English and Arabic language models by providing resources that amplify
Arabic NLP development. Building on this foundation, we developed two
state-of-the-art models, LlamAr-8B and GemmAr-7B, which are specifically tuned
to excel at a wide range of Arabic NLP tasks.

摘要：大型語言模型 (LLM) 極大地影響了自然語言處理 (NLP) 領域，特別是對英語而言。這些模型已證明在理解和產生類人文本方面具有能力。語言模型的成功在很大程度上取決於高品質教學資料集的可用性，其中包含詳細的任務說明和相應的回應，這些說明和回應對於訓練模型以準確處理各種提示至關重要。然而，這些資源的可用性和品質因語言而異。儘管模型在英語方面表現良好，但由於缺乏用於微調阿拉伯語特定任務的資料集，它們通常難以處理阿拉伯語等語言。為了解決這個問題，我們引入了 InstAr-500k，這是一個新的阿拉伯語教學資料集，它是透過產生和收集涵蓋多個領域和教學類型的內容而建立的。然後，我們透過微調兩個開源模型，即 Llama-3-8B-Instruct 和 Gemma-7B-IT，在幾個下游任務中評估這個資料集，以擴展其功能的改進。根據多項評估，我們微調的模型在幾個阿拉伯語 NLP 基準上達到了最先進的效能。這些結果強調了我們的資料集在提升阿拉伯語語言模型能力方面的有效性。我們的教學資料集透過提供擴大阿拉伯語 NLP 開發的資源，彌補了英語和阿拉伯語語言模型之間的效能差距。在此基礎上，我們開發了兩個最先進的模型，即 LlamAr-8B 和 GemmAr-7B，它們經過特別調整，可以在廣泛的阿拉伯語 NLP 任務中表現出色。

##### **Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks**
2407.02138v1 by Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe

Trustworthy prediction in Deep Neural Networks (DNNs), including Pre-trained
Language Models (PLMs) is important for safety-critical applications in the
real world. However, DNNs often suffer from uncertainty estimation, such as
miscalibration. In particular, approaches that require multiple stochastic
inference can mitigate this problem, but the expensive cost of inference makes
them impractical. In this study, we propose $k$-Nearest Neighbor Uncertainty
Estimation ($k$NN-UE), which is an uncertainty estimation method that uses the
distances from the neighbors and label-existence ratio of neighbors.
Experiments on sentiment analysis, natural language inference, and named entity
recognition show that our proposed method outperforms the baselines or recent
density-based methods in confidence calibration, selective prediction, and
out-of-distribution detection. Moreover, our analyses indicate that introducing
dimension reduction or approximate nearest neighbor search inspired by recent
$k$NN-LM studies reduces the inference overhead without significantly degrading
estimation performance when combined them appropriately.

摘要：在深度神经網路（DNN）中，包括預先訓練的語言模型（PLM），可信賴的預測對於現實世界中的安全關鍵應用非常重要。然而，DNN 經常會遇到不確定性估計的問題，例如校準不良。特別是，需要多次隨機推論的方法可以減輕這個問題，但昂貴的推論成本讓它們不切實際。在這項研究中，我們提出 $k$-最近鄰不確定性估計（$k$NN-UE），這是一種使用鄰近距離和鄰近標籤存在比的不確定性估計方法。在情緒分析、自然語言推論和命名實體識別的實驗中，我們提出的方法在信心校準、選擇性預測和分布外偵測方面優於基準或最近的基於密度的模型。此外，我們的分析表明，在適當地結合它們時，引入最近鄰語言模型（$k$NN-LM）研究中啟發的降維或近似最近鄰搜尋，可以減少推論開銷，而不會顯著降低估計效能。

##### **Black Big Boxes: Do Language Models Hide a Theory of Adjective Order?**
2407.02136v1 by Jaap Jumelet, Lisa Bylinina, Willem Zuidema, Jakub Szymanik

In English and other languages, multiple adjectives in a complex noun phrase
show intricate ordering patterns that have been a target of much linguistic
theory. These patterns offer an opportunity to assess the ability of language
models (LMs) to learn subtle rules of language involving factors that cross the
traditional divisions of syntax, semantics, and pragmatics. We review existing
hypotheses designed to explain Adjective Order Preferences (AOPs) in humans and
develop a setup to study AOPs in LMs: we present a reusable corpus of adjective
pairs and define AOP measures for LMs. With these tools, we study a series of
LMs across intermediate checkpoints during training. We find that all models'
predictions are much closer to human AOPs than predictions generated by factors
identified in theoretical linguistics. At the same time, we demonstrate that
the observed AOPs in LMs are strongly correlated with the frequency of the
adjective pairs in the training data and report limited generalization to
unseen combinations. This highlights the difficulty in establishing the link
between LM performance and linguistic theory. We therefore conclude with a road
map for future studies our results set the stage for, and a discussion of key
questions about the nature of knowledge in LMs and their ability to generalize
beyond the training sets.

摘要：在英語和其他語言中，複雜名詞短語中的多個形容詞顯示出錯綜複雜的排序模式，一直是許多語言理論的目標。這些模式提供了一個機會來評估語言模型 (LM) 學習語言微妙規則的能力，涉及跨越句法、語義和語用學傳統劃分的因素。我們回顧了旨在解釋人類形容詞順序偏好 (AOP) 的現有假設，並制定了一個在 LM 中研究 AOP 的設置：我們提供了一個可重複使用的形容詞對語料庫，並定義了 LM 的 AOP 測量。利用這些工具，我們在訓練期間研究了一系列跨中間檢查點的 LM。我們發現，所有模型的預測都比理論語言學中確定的因素產生的預測更接近人類 AOP。同時，我們證明了在 LM 中觀察到的 AOP 與訓練數據中形容詞對的頻率密切相關，並報告了對未見組合的有限概括。這突出了在建立 LM 性能與語言理論之間的聯繫方面的難度。因此，我們以我們的研究結果設定的未來研究路線圖作為結論，並討論了有關 LM 中知識的性質及其在訓練集之外概括的能力的關鍵問題。

##### **Fake News Detection: It's All in the Data!**
2407.02122v1 by Soveatin Kuntur, Anna Wróblewska, Marcin Paprzycki, Maria Ganzha

This comprehensive survey serves as an indispensable resource for researchers
embarking on the journey of fake news detection. By highlighting the pivotal
role of dataset quality and diversity, it underscores the significance of these
elements in the effectiveness and robustness of detection models. The survey
meticulously outlines the key features of datasets, various labeling systems
employed, and prevalent biases that can impact model performance. Additionally,
it addresses critical ethical issues and best practices, offering a thorough
overview of the current state of available datasets. Our contribution to this
field is further enriched by the provision of GitHub repository, which
consolidates publicly accessible datasets into a single, user-friendly portal.
This repository is designed to facilitate and stimulate further research and
development efforts aimed at combating the pervasive issue of fake news.

摘要：這份全面的調查是研究人員踏上辨識假新聞之旅時不可或缺的資源。它強調了資料集品質與多樣性的關鍵角色，突顯出這些元素對於偵測模型的有效性和穩健性的重要性。這份調查仔細概述了資料集的主要特徵、所使用的各種標籤系統，以及可能影響模型效能的普遍偏差。此外，它還探討了關鍵的道德問題和最佳實務，提供了可用資料集現況的全面概述。我們對這個領域的貢獻，進一步透過提供 GitHub 儲存庫而得到豐富，該儲存庫將公開可存取的資料集整合到一個單一的、使用者友善的入口網站中。這個儲存庫旨在促進和激勵進一步的研究和開發工作，目標是打擊假新聞這個普遍的問題。

##### **Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning**
2407.02119v1 by Yifang Chen, Shuohang Wang, Ziyi Yang, Hiteshi Sharma, Nikos Karampatziakis, Donghan Yu, Kevin Jamieson, Simon Shaolei Du, Yelong Shen

Reinforcement learning with human feedback (RLHF), as a widely adopted
approach in current large language model pipelines, is \textit{bottlenecked by
the size of human preference data}. While traditional methods rely on offline
preference dataset constructions, recent approaches have shifted towards online
settings, where a learner uses a small amount of labeled seed data and a large
pool of unlabeled prompts to iteratively construct new preference data through
self-generated responses and high-quality reward/preference feedback. However,
most current online algorithms still focus on preference labeling during policy
model updating with given feedback oracles, which incurs significant expert
query costs. \textit{We are the first to explore cost-effective proxy reward
oracles construction strategies for further labeling preferences or rewards
with extremely limited labeled data and expert query budgets}. Our approach
introduces two key innovations: (1) on-policy query to avoid OOD and imbalance
issues in seed data, and (2) active learning to select the most informative
data for preference queries. Using these methods, we train a evaluation model
with minimal expert-labeled data, which then effectively labels nine times more
preference pairs for further RLHF training. For instance, our model using
Direct Preference Optimization (DPO) gains around over 1% average improvement
on AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our
methodology is orthogonal to other direct expert query-based strategies and
therefore might be integrated with them to further reduce query costs.

摘要：強化學習搭配人類回饋 (RLHF) 作為當前大型語言模型管線中廣泛採用的方法，受到人類偏好資料規模的限制。傳統方法依賴於線下偏好資料集建構，而最近的方法已轉向線上設定，其中學習者使用少量標籤種子資料和大量的未標籤提示，透過自我產生的回應和高品質獎勵/偏好回饋，反覆建構新的偏好資料。然而，大多數目前的線上演算法仍專注於在給定回饋神諭的政策模型更新期間進行偏好標籤，這會產生大量的專家查詢成本。我們率先探討了具有成本效益的代理獎勵神諭建構策略，以進一步標記偏好或獎勵，且標籤資料和專家查詢預算極為有限。我們的做法引入了兩項關鍵創新：(1) 策略查詢，以避免種子資料中的 OOD 和不平衡問題，以及 (2) 主動學習，以選擇最具資訊性的資料進行偏好查詢。使用這些方法，我們訓練了一個評估模型，其使用最少的專家標籤資料，然後有效標記了多達九倍的偏好對，以進行進一步的 RLHF 訓練。例如，我們的模型使用直接偏好最佳化 (DPO)，在 AlpacaEval2、MMLU-5shot 和 MMLU-0shot 上獲得了超過 1% 的平均改進，查詢成本僅為 1.7K。我們的做法與其他基於直接專家查詢的策略正交，因此可以與這些策略整合，以進一步降低查詢成本。

##### **Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale**
2407.02118v1 by Wenzhen Zheng, Wenbo Pan, Xu Xu, Libo Qin, Li Yue, Ming Zhou

In recent years, Large Language Models (LLMs) have made significant strides
towards Artificial General Intelligence. However, training these models from
scratch requires substantial computational resources and vast amounts of text
data. In this paper, we explore an alternative approach to constructing an LLM
for a new language by continually pretraining (CPT) from existing pretrained
LLMs, instead of using randomly initialized parameters. Based on parallel
experiments on 40 model sizes ranging from 40M to 5B parameters, we find that
1) CPT converges faster and saves significant resources in a scalable manner;
2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)
with a joint data-parameter scaling term; 3) The compute-optimal data-parameter
allocation for CPT markedly differs based on our estimated scaling factors; 4)
The effectiveness of transfer at scale is influenced by training duration and
linguistic properties, while robust to data replaying, a method that
effectively mitigates catastrophic forgetting in CPT. We hope our findings
provide deeper insights into the transferability of LLMs at scale for the
research community.

摘要：近年來，大型語言模型（LLM）在人工通用智能方面取得了重大進展。然而，從頭訓練這些模型需要大量的計算資源和大量的文本數據。在本文中，我們探討了一種替代方法，通過持續預訓練（CPT）現有的預訓練 LLM，而不是使用隨機初始化的參數，來構建一種新語言的 LLM。基於對 40 個模型大小（範圍從 40M 到 5B 參數）的並行實驗，我們發現 1) CPT 收斂得更快，並以可擴展的方式節省了大量資源；2) CPT 遵循 Hoffmann 等人（2022）推導出的擴展縮放定律，並帶有一個聯合數據參數縮放項；3) 根據我們估計的縮放因子，CPT 的計算最佳數據參數分配顯著不同；4) 大規模轉移的有效性受訓練持續時間和語言屬性的影響，同時對數據重放具有魯棒性，這是一種有效減輕 CPT 中災難性遺忘的方法。我們希望我們的發現能為研究界提供對大規模 LLM 可轉移性的更深入見解。

##### **A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data**
2407.02112v1 by Andrej Tschalzev, Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt

Tabular data is prevalent in real-world machine learning applications, and
new models for supervised learning of tabular data are frequently proposed.
Comparative studies assessing the performance of models typically consist of
model-centric evaluation setups with overly standardized data preprocessing.
This paper demonstrates that such model-centric evaluations are biased, as
real-world modeling pipelines often require dataset-specific preprocessing and
feature engineering. Therefore, we propose a data-centric evaluation framework.
We select 10 relevant datasets from Kaggle competitions and implement
expert-level preprocessing pipelines for each dataset. We conduct experiments
with different preprocessing pipelines and hyperparameter optimization (HPO)
regimes to quantify the impact of model selection, HPO, feature engineering,
and test-time adaptation. Our main findings are: 1. After dataset-specific
feature engineering, model rankings change considerably, performance
differences decrease, and the importance of model selection reduces. 2. Recent
models, despite their measurable progress, still significantly benefit from
manual feature engineering. This holds true for both tree-based models and
neural networks. 3. While tabular data is typically considered static, samples
are often collected over time, and adapting to distribution shifts can be
important even in supposedly static data. These insights suggest that research
efforts should be directed toward a data-centric perspective, acknowledging
that tabular data requires feature engineering and often exhibits temporal
characteristics.

摘要：表格資料在現實世界的機器學習應用中很普遍，
且經常提出表格資料監督式學習的新模型。
評估模型效能的比較研究通常包含
以模型為中心的評估設定，以及過度標準化的資料前處理。
本文證明此類以模型為中心的評估是有偏見的，
因為現實世界的建模管道通常需要針對資料集進行特定前處理和
特徵工程。因此，我們提出以資料為中心的評估架構。
我們從 Kaggle 競賽中選出 10 個相關資料集，並針對每個資料集實作
專家級別的前處理管道。我們執行實驗
使用不同的前處理管道和超參數最佳化 (HPO)
機制來量化模型選擇、HPO、特徵工程和
測試時間適應的影響。我們的發現重點如下：1. 在進行特定於資料集的
特徵工程後，模型排名會大幅改變，效能
差異會減少，而模型選擇的重要性會降低。2. 近期
模型儘管有顯著進展，但仍會顯著受益於
手動特徵工程。這對基於樹的模型和
神經網路都成立。3. 雖然表格資料通常被視為靜態的，但樣本
通常會隨著時間收集，而適應分佈轉移甚至在假設的靜態資料中也很重要。這些見解表明研究
工作應朝向以資料為中心的觀點，承認
表格資料需要特徵工程，且通常會展現時間特性。

##### **HRSAM: Efficiently Segment Anything in High-Resolution Images**
2407.02109v1 by You Huang, Wenbin Lai, Jiayi Ji, Liujuan Cao, Shengchuan Zhang, Rongrong Ji

The Segment Anything Model (SAM) has significantly advanced interactive
segmentation but struggles with high-resolution images crucial for
high-precision segmentation. This is primarily due to the quadratic space
complexity of SAM-implemented attention and the length extrapolation issue in
common global attention. This study proposes HRSAM that integrates Flash
Attention and incorporates Plain, Shifted and newly proposed Cycle-scan Window
(PSCWin) attention to address these issues. The shifted window attention is
redesigned with padding to maintain consistent window sizes, enabling effective
length extrapolation. The cycle-scan window attention adopts the recently
developed State Space Models (SSMs) to ensure global information exchange with
minimal computational overhead. Such window-based attention allows HRSAM to
perform effective attention computations on scaled input images while
maintaining low latency. Moreover, we further propose HRSAM++ that additionally
employs a multi-scale strategy to enhance HRSAM's performance. The experiments
on the high-precision segmentation datasets HQSeg44K and DAVIS show that
high-resolution inputs enable the SAM-distilled HRSAM models to outperform the
teacher model while maintaining lower latency. Compared to the SOTAs, HRSAM
achieves a 1.56 improvement in interactive segmentation's NoC95 metric with
only 31% of the latency. HRSAM++ further enhances the performance, achieving a
1.63 improvement in NoC95 with just 38% of the latency.

摘要：分段任何模型 (SAM) 已大幅提升互動式分段，但對於高解析度影像仍有困擾，而高解析度影像對於高精準分段至關重要。這主要是由於 SAM 實作注意力的二次空間複雜度和一般全域注意力的長度外推問題。本研究提出 HRSAM，整合 Flash 注意力，並納入 Plain、Shifted 和新提出的循環掃描視窗 (PSCWin) 注意力來解決這些問題。已重新設計帶有內補的移位視窗注意力，以維持一致的視窗大小，進而能有效長度外推。循環掃描視窗注意力採用最近開發的狀態空間模型 (SSM)，以確保全域資訊交換，且運算負擔極小。此類視窗式注意力讓 HRSAM 能對縮放輸入影像執行有效注意力運算，同時維持低延遲。此外，我們進一步提出 HRSAM++，它另外採用多重縮放策略來提升 HRSAM 的效能。在高精準度分段資料集 HQSeg44K 和 DAVIS 上的實驗顯示，高解析度輸入讓 SAM 提煉的 HRSAM 模型能優於教師模型，同時維持較低延遲。與 SOTAs 相較，HRSAM 在互動式分段的 NoC95 指標上提升了 1.56，但延遲僅有 31%。HRSAM++ 進一步提升效能，在 NoC95 上提升了 1.63，但延遲僅有 38%。

##### **Automated Knowledge Graph Learning in Industrial Processes**
2407.02106v1 by Lolitta Ammann, Jorge Martinez-Gil, Michael Mayr, Georgios C. Chasparis

Industrial processes generate vast amounts of time series data, yet
extracting meaningful relationships and insights remains challenging. This
paper introduces a framework for automated knowledge graph learning from time
series data, specifically tailored for industrial applications. Our framework
addresses the complexities inherent in industrial datasets, transforming them
into knowledge graphs that improve decision-making, process optimization, and
knowledge discovery. Additionally, it employs Granger causality to identify key
attributes that can inform the design of predictive models. To illustrate the
practical utility of our approach, we also present a motivating use case
demonstrating the benefits of our framework in a real-world industrial
scenario. Further, we demonstrate how the automated conversion of time series
data into knowledge graphs can identify causal influences or dependencies
between important process parameters.

摘要：工業程序會產生大量時間序列資料，然而要萃取出有意義的關係和見解仍然具有挑戰性。本文介紹一個自動化知識圖譜學習的架構，從時間序列資料中學習，特別針對工業應用量身打造。我們的架構處理工業資料集固有的複雜性，將其轉換成知識圖譜，以改善決策、流程最佳化和知識發現。此外，它採用 Granger 因果關係來識別關鍵屬性，這些屬性可以提供預測模型的設計資訊。為了說明我們方法的實用性，我們也提出一個激勵人心的使用案例，展示我們架構在真實世界的工業情境中的優點。此外，我們展示如何將時間序列資料自動轉換成知識圖譜，可以識別重要流程參數之間的因果影響或依賴關係。

##### **Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior**
2407.02099v1 by Pedro Henrique Luz de Araujo, Benjamin Roth

One way to personalize and steer generations from large language models (LLM)
is to assign a persona: a role that describes how the user expects the LLM to
behave (e.g., a helpful assistant, a teacher, a woman). This paper investigates
how personas affect diverse aspects of model behavior. We assign to seven LLMs
162 personas from 12 categories spanning variables like gender, sexual
orientation, and occupation. We prompt them to answer questions from five
datasets covering objective (e.g., questions about math and history) and
subjective tasks (e.g., questions about beliefs and values). We also compare
persona's generations to two baseline settings: a control persona setting with
30 paraphrases of "a helpful assistant" to control for models' prompt
sensitivity, and an empty persona setting where no persona is assigned. We find
that for all models and datasets, personas show greater variability than the
control setting and that some measures of persona behavior generalize across
models.

摘要：一種從大型語言模型 (LLM) 中個人化和引導世代的方法是指定一個角色：一個描述使用者期望 LLM 如何表現的角色（例如，一個有用的助理、一個老師、一個女人）。這篇論文探討角色如何影響模型行為的不同面向。我們將 162 個來自 12 個類別的角色分配給七個 LLM，這些類別涵蓋性別、性取向和職業等變數。我們提示他們回答來自五個資料集的問題，這些資料集涵蓋客觀（例如，關於數學和歷史的問題）和主觀任務（例如，關於信念和價值觀的問題）。我們還將角色的世代與兩個基準設定進行比較：一個控制角色設定，其中包含 30 個「一個有用的助理」的改述，以控制模型提示的敏感性，以及一個未指定角色的空角色設定。我們發現，對於所有模型和資料集，角色顯示出的變異性都比控制設定大，並且某些角色行為的測量值會概括到所有模型。

##### **Latent Diffusion Model for Generating Ensembles of Climate Simulations**
2407.02070v1 by Johannes Meuer, Maximilian Witte, Claudia Timmreck, Thomas Ludwig, Christopher Kadow

Obtaining accurate estimates of uncertainty in climate scenarios often
requires generating large ensembles of high-resolution climate simulations, a
computationally expensive and memory intensive process. To address this
challenge, we train a novel generative deep learning approach on extensive sets
of climate simulations. The model consists of two components: a variational
autoencoder for dimensionality reduction and a denoising diffusion
probabilistic model that generates multiple ensemble members. We validate our
model on the Max Planck Institute Grand Ensemble and show that it achieves good
agreement with the original ensemble in terms of variability. By leveraging the
latent space representation, our model can rapidly generate large ensembles
on-the-fly with minimal memory requirements, which can significantly improve
the efficiency of uncertainty quantification in climate simulations.

摘要：取得氣候情境中不確定性的準確估計通常需要產生大量高解析度氣候模擬，這是一個計算成本昂貴且記憶體密集的過程。為了應對這項挑戰，我們在廣泛的氣候模擬中訓練一種新穎的生成式深度學習方法。該模型包含兩個組成部分：用於降維的變分自動編碼器和產生多個集合成員的去噪擴散概率模型。我們在 Max Planck Institute Grand Ensemble 上驗證了我們的模型，並表明它在變異性方面與原始集合達到了良好的吻合。通過利用潛在空間表示，我們的模型可以快速產生大量集合，且記憶體需求極小，這可以顯著提高氣候模擬中不確定性量化的效率。

##### **Crossroads of Continents: Automated Artifact Extraction for Cultural Adaptation with Large Multimodal Models**
2407.02067v1 by Anjishnu Mukherjee, Ziwei Zhu, Antonios Anastasopoulos

In this work, we present a comprehensive three-phase study to examine (1) the
effectiveness of large multimodal models (LMMs) in recognizing cultural
contexts; (2) the accuracy of their representations of diverse cultures; and
(3) their ability to adapt content across cultural boundaries. We first
introduce Dalle Street, a large-scale dataset generated by DALL-E 3 and
validated by humans, containing 9,935 images of 67 countries and 10 concept
classes. We reveal disparities in cultural understanding at the sub-region
level with both open-weight (LLaVA) and closed-source (GPT-4V) models on Dalle
Street and other existing benchmarks. Next, we assess models' deeper culture
understanding by an artifact extraction task and identify over 18,000 artifacts
associated with different countries. Finally, we propose a highly composable
pipeline, CultureAdapt, to adapt images from culture to culture. Our findings
reveal a nuanced picture of the cultural competence of LMMs, highlighting the
need to develop culture-aware systems. Dataset and code are available at
https://github.com/iamshnoo/crossroads

摘要：在這項工作中，我們提出了一項全面的三階段研究，以檢視 (1) 大型多模態模型 (LMM) 在識別文化背景方面的有效性；(2) 它們對不同文化表徵的準確性；以及 (3) 它們跨文化界線調整內容的能力。我們首先介紹 Dalle Street，一個由 DALL-E 3 生成的、並經由人類驗證的大規模資料集，包含 67 個國家和 10 個概念類別的 9,935 張圖片。我們揭露了在 Dalle Street 和其他現有基準上，開放權重 (LLaVA) 和閉源 (GPT-4V) 模型在次區域層級的文化理解差異。接下來，我們透過人工製品萃取任務評估模型更深入的文化理解，並識別出超過 18,000 件與不同國家相關的人工製品。最後，我們提出了一個高度可組合的管道，CultureAdapt，以調整不同文化中的圖片。我們的研究結果揭露了 LMM 文化能力的細微差別，強調了開發文化感知系統的必要性。資料集和程式碼可在 https://github.com/iamshnoo/crossroads 取得

##### **BiasDora: Exploring Hidden Biased Associations in Vision-Language Models**
2407.02066v1 by Chahat Raj, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu

Existing works examining Vision Language Models (VLMs) for social biases
predominantly focus on a limited set of documented bias associations, such as
gender:profession or race:crime. This narrow scope often overlooks a vast range
of unexamined implicit associations, restricting the identification and, hence,
mitigation of such biases. We address this gap by probing VLMs to (1) uncover
hidden, implicit associations across 9 bias dimensions. We systematically
explore diverse input and output modalities and (2) demonstrate how biased
associations vary in their negativity, toxicity, and extremity. Our work (3)
identifies subtle and extreme biases that are typically not recognized by
existing methodologies. We make the Dataset of retrieved associations, (Dora),
publicly available here https://github.com/chahatraj/BiasDora.

摘要：現有的研究探討視覺語言模型 (VLM) 的社會偏見，主要關注於一組有限的文件偏見關聯，例如性別：職業或種族：犯罪。這種狹窄的範圍經常忽略了大量未經檢驗的隱含關聯，限制了這些偏見的識別和因此而採取的緩解措施。我們通過探測 VLM 來解決這個差距，以 (1) 揭示 9 個偏見向度的隱藏、隱含關聯。我們系統性地探索不同的輸入和輸出模式，並 (2) 說明有偏見的關聯在負面性、毒性和極端性方面的變化。我們的研究 (3) 識別出通常無法通過現有方法識別的微妙和極端偏見。我們在此公開檢索關聯的資料集 (Dora) https://github.com/chahatraj/BiasDora。

##### **Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?**
2407.02062v1 by Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe

This work investigates the impact of data augmentation on confidence
calibration and uncertainty estimation in Named Entity Recognition (NER) tasks.
For the future advance of NER in safety-critical fields like healthcare and
finance, it is essential to achieve accurate predictions with calibrated
confidence when applying Deep Neural Networks (DNNs), including Pre-trained
Language Models (PLMs), as a real-world application. However, DNNs are prone to
miscalibration, which limits their applicability. Moreover, existing methods
for calibration and uncertainty estimation are computational expensive. Our
investigation in NER found that data augmentation improves calibration and
uncertainty in cross-genre and cross-lingual setting, especially in-domain
setting. Furthermore, we showed that the calibration for NER tends to be more
effective when the perplexity of the sentences generated by data augmentation
is lower, and that increasing the size of the augmentation further improves
calibration and uncertainty.

摘要：本研究探討資料擴充對命名實體辨識 (NER) 任務中信心校準和不確定性估計的影響。對於 NER 在醫療保健和金融等安全關鍵領域的未來進展而言，在應用深度神經網路 (DNN)，包括預先訓練的語言模型 (PLM)，作為實際應用時，必須達成校準信心和準確預測至關重要。然而，DNN 容易發生校準不佳，這限制了它們的適用性。此外，現有的校準和不確定性估計方法在運算上很昂貴。我們對 NER 的研究發現，資料擴充改善了跨類型和跨語言設定中的校準和不確定性，特別是在領域內設定中。此外，我們證明了當資料擴充產生的句子的困惑度較低時，NER 的校準往往更有效，而且增加擴充大小進一步改善了校準和不確定性。

##### **Terminating Differentiable Tree Experts**
2407.02060v1 by Jonathan Thomm, Michael Hersche, Giacomo Camposampiero, Aleksandar Terzić, Bernhard Schölkopf, Abbas Rahimi

We advance the recently proposed neuro-symbolic Differentiable Tree Machine,
which learns tree operations using a combination of transformers and Tensor
Product Representations. We investigate the architecture and propose two key
components. We first remove a series of different transformer layers that are
used in every step by introducing a mixture of experts. This results in a
Differentiable Tree Experts model with a constant number of parameters for any
arbitrary number of steps in the computation, compared to the previous method
in the Differentiable Tree Machine with a linear growth. Given this flexibility
in the number of steps, we additionally propose a new termination algorithm to
provide the model the power to choose how many steps to make automatically. The
resulting Terminating Differentiable Tree Experts model sluggishly learns to
predict the number of steps without an oracle. It can do so while maintaining
the learning capabilities of the model, converging to the optimal amount of
steps.

摘要：我們進一步發展最近提出的神經符號可微分樹機，
它使用Transformer和張量乘積表示的組合來學習樹操作。我們研究架構並提出兩個關鍵組成部分。我們首先移除一系列不同的Transformer層，這些層在每一步中通過引入專家組合使用。與可微分樹機中採用線性增長的先前方法相比，這導致可微分樹專家模型在任意數量的計算步驟中具有恆定的參數數量。由於步驟數具有這種靈活性，我們另外提出一個新的終止演算法，讓模型能夠自動選擇執行多少個步驟。由此產生的終止可微分樹專家模型緩慢地學習預測步驟數，而無需預言機。它可以在執行此操作的同時維持模型的學習能力，收斂到最佳步驟數。

##### **Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation**
2407.02056v1 by Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Boyuan Pan, Heda Wang, Yao Hu, Kan Li

Self-consistency (SC), leveraging multiple samples from LLMs, shows
significant gains on various reasoning tasks but struggles with free-form
generation due to the difficulty of aggregating answers. Its variants, UCS and
USC, rely on sample selection or voting mechanisms to improve output quality.
These methods, however, face limitations due to their inability to fully
utilize the nuanced consensus knowledge present within multiple candidate
samples, often resulting in suboptimal outputs. We propose Fine-Grained
Self-Consistency (FSC) to addresses these limitations by extracting and
integrating segment-level commonalities from candidate samples, enhancing the
performance of LLMs both in open-ended and reasoning tasks. Based on this, we
present two additional strategies: candidate filtering, which enhances overall
quality by identifying highly similar candidate sets, and merging, which
reduces input token requirements by combining similar samples. The
effectiveness of FSC is demonstrated through extensive experiments on various
tasks, including summarization, code generation, and mathematical reasoning,
using GPT-3.5-turbo and GPT-4. The results indicate significant improvements
over baseline methods, showcasing the potential of FSC to optimize output
quality by effectively synthesizing fine-grained consensus knowledge from
multiple samples.

摘要：自我一致性 (SC)，利用来自 LLM 的多个样本，在各种推理任务中显示出显著的增益，但由于汇总答案的难度，在自由形式生成中遇到了困难。它的变体 UCS 和 USC 依赖于样本选择或投票机制来提高输出质量。然而，这些方法由于无法充分利用多个候选样本中存在的细致共识知识而面临限制，通常导致次优输出。我们提出细粒度自我一致性 (FSC) 来解决这些限制，通过从候选样本中提取和集成片段级共性，增强 LLM 在开放式和推理任务中的性能。基于此，我们提出了两种附加策略：候选过滤，通过识别高度相似的候选集来提高整体质量；合并，通过合并相似的样本来减少输入令牌需求。FSC 的有效性通过在各种任务上的广泛实验得到证明，包括摘要、代码生成和数学推理，使用 GPT-3.5-turbo 和 GPT-4。结果表明与基线方法相比有显着改进，展示了 FSC 通过有效合成来自多个样本的细粒度共识知识来优化输出质量的潜力。

##### **Abstract Dialectical Frameworks are Boolean Networks (full version)**
2407.02055v1 by Jesse Heyninck, Matthias Knorr, João Leite

Dialectical frameworks are a unifying model of formal argumentation, where
argumentative relations between arguments are represented by assigning
acceptance conditions to atomic arguments. Their generality allow them to cover
a number of different approaches with varying forms of representing the
argumentation structure. Boolean regulatory networks are used to model the
dynamics of complex biological processes, taking into account the interactions
of biological compounds, such as proteins or genes. These models have proven
highly useful for comprehending such biological processes, allowing to
reproduce known behaviour and testing new hypotheses and predictions in silico,
for example in the context of new medical treatments. While both these
approaches stem from entirely different communities, it turns out that there
are striking similarities in their appearence. In this paper, we study the
relation between these two formalisms revealing their communalities as well as
their differences, and introducing a correspondence that allows to establish
novel results for the individual formalisms.

摘要：辯證框架是形式論證的統一模型，其中論證之間的論證關係是透過指派接受條件給原子論證來表示。它們的普遍性允許它們涵蓋許多不同的方法，並以不同的形式表示論證結構。布林法規網路用於模擬複雜生物過程的動態，考量生物化合物（例如蛋白質或基因）的交互作用。這些模型已被證明對於理解此類生物過程非常有用，允許複製已知的行為並在電腦模擬中測試新的假設和預測，例如在新的醫療治療的背景下。儘管這兩種方法完全來自不同的社群，但事實證明它們的外觀有驚人的相似性。在本文中，我們研究這兩種形式主義之間的關係，揭示它們的共性以及它們的差異，並引入一種對應關係，允許為個別形式主義建立新的結果。

##### **Accompanied Singing Voice Synthesis with Fully Text-controlled Melody**
2407.02049v1 by Ruiqi Li, Zhiqing Hong, Yongqi Wang, Lichao Zhang, Rongjie Huang, Siqi Zheng, Zhou Zhao

Text-to-song (TTSong) is a music generation task that synthesizes accompanied
singing voices. Current TTSong methods, inherited from singing voice synthesis
(SVS), require melody-related information that can sometimes be impractical,
such as music scores or MIDI sequences. We present MelodyLM, the first TTSong
model that generates high-quality song pieces with fully text-controlled
melodies, achieving minimal user requirements and maximum control flexibility.
MelodyLM explicitly models MIDI as the intermediate melody-related feature and
sequentially generates vocal tracks in a language model manner, conditioned on
textual and vocal prompts. The accompaniment music is subsequently synthesized
by a latent diffusion model with hybrid conditioning for temporal alignment.
With minimal requirements, users only need to input lyrics and a reference
voice to synthesize a song sample. For full control, just input textual prompts
or even directly input MIDI. Experimental results indicate that MelodyLM
achieves superior performance in terms of both objective and subjective
metrics. Audio samples are available at https://melodylm666.github.io.

摘要：文字轉歌曲 (TTSong) 是一種音樂生成任務，可以合成伴奏的歌聲。目前的 TTSong 方法承襲自歌聲合成 (SVS)，需要與旋律相關的資訊，例如樂譜或 MIDI 序列，有時這可能不切實際。我們提出 MelodyLM，這是第一個 TTSong 模型，可以生成高品質的歌曲片段，並完全由文字控制旋律，達到最少的使用者需求和最大的控制靈活性。MelodyLM 明確地將 MIDI 建模為中間的旋律相關特徵，並以語言模型的方式循序漸進地產生人聲軌道，以文字和人聲提示為條件。伴奏音樂隨後由潛在擴散模型合成，並進行混合條件處理以進行時間對齊。使用者只需要輸入歌詞和參考人聲即可合成歌曲範例，需求極低。若要完全控制，只需輸入文字提示，甚至直接輸入 MIDI。實驗結果表明，MelodyLM 在客觀和主觀指標方面都取得了優異的表現。音訊範例可在 https://melodylm666.github.io/ 取得。

##### **Concise and Precise Context Compression for Tool-Using Language Models**
2407.02043v1 by Yang Xu, Yunlong Feng, Honglin Mu, Yutai Hou, Yitong Li, Xinghao Wang, Wanjun Zhong, Zhongyang Li, Dandan Tu, Qingfu Zhu, Min Zhang, Wanxiang Che

Through reading the documentation in the context, tool-using language models
can dynamically extend their capability using external tools. The cost is that
we have to input lengthy documentation every time the model needs to use the
tool, occupying the input window as well as slowing down the decoding process.
  Given the progress in general-purpose compression, soft context compression
is a suitable approach to alleviate the problem. However, when compressing tool
documentation, existing methods suffer from the weaknesses of key information
loss (specifically, tool/parameter name errors) and difficulty in adjusting the
length of compressed sequences based on documentation lengths.
  To address these problems, we propose two strategies for compressing tool
documentation into concise and precise summary sequences for tool-using
language models. 1) Selective compression strategy mitigates key information
loss by deliberately retaining key information as raw text tokens. 2) Block
compression strategy involves dividing tool documentation into short chunks and
then employing a fixed-length compression model to achieve variable-length
compression. This strategy facilitates the flexible adjustment of the
compression ratio.
  Results on API-Bank and APIBench show that our approach reaches a performance
comparable to the upper-bound baseline under up to 16x compression ratio.

摘要：<paragraph>透過閱讀脈絡中的文件，使用工具的語言模型
可以使用外部工具動態擴展其功能。代價是
每次模型需要使用工具時，我們都必須輸入冗長的說明文件，佔用輸入視窗並減慢解碼過程。
考量到通用壓縮的進度，軟性脈絡壓縮是一種緩解問題的合適方法。然而，在壓縮工具
文件時，現有方法會因為關鍵資訊遺失（特別是工具/參數名稱錯誤）和難以根據文件長度調整壓縮序列長度的弱點而受限。
為了解決這些問題，我們提出兩種策略，將工具文件壓縮成簡潔且精確的摘要序列，以供使用工具的
語言模型使用。1) 選擇性壓縮策略透過刻意保留關鍵資訊作為原始文字符號，來減輕關鍵資訊遺失。2) 區塊
壓縮策略包含將工具文件分成小塊，然後使用固定長度壓縮模型來達成可變長度壓縮。此策略促進壓縮比的彈性調整。
API-Bank 和 APIBench 的結果顯示，我們的做法在高達 16 倍壓縮比下，達到與上限基準相當的效能。</paragraph>

##### **Fake News Detection and Manipulation Reasoning via Large Vision-Language Models**
2407.02042v1 by Ruihan Jin, Ruibo Fu, Zhengqi Wen, Shuai Zhang, Yukun Liu, Jianhua Tao

Fake news becomes a growing threat to information security and public opinion
with the rapid sprawl of media manipulation. Therefore, fake news detection
attracts widespread attention from academic community. Traditional fake news
detection models demonstrate remarkable performance on authenticity binary
classification but their ability to reason detailed faked traces based on the
news content remains under-explored. Furthermore, due to the lack of external
knowledge, the performance of existing methods on fact-related news is
questionable, leaving their practical implementation unclear. In this paper, we
propose a new multi-media research topic, namely manipulation reasoning.
Manipulation reasoning aims to reason manipulations based on news content. To
support the research, we introduce a benchmark for fake news detection and
manipulation reasoning, referred to as Human-centric and Fact-related Fake News
(HFFN). The benchmark highlights the centrality of human and the high factual
relevance, with detailed manual annotations. HFFN encompasses four realistic
domains with fake news samples generated through three manipulation approaches.
Moreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is
presented not only to judge on the authenticity of multi-modal news, but also
raise analytical reasoning about potential manipulations. On the feature
extraction level, a cross-attention mechanism is employed to extract
fine-grained fusion features from multi-modal inputs. On the reasoning level, a
large vision-language model (LVLM) serves as the backbone to facilitate
fact-related reasoning. A two-stage training framework is deployed to better
activate the capacity of identification and reasoning. Comprehensive
experiments demonstrate that our model outperforms state-of-the-art (SOTA) fake
news detection models and powerful LVLMs like GPT-4 and LLaVA.

摘要：<paragraph>隨著媒體操弄的快速蔓延，假新聞已成為資訊安全和輿論日益嚴重的威脅。因此，假新聞偵測吸引了學術界的廣泛關注。傳統的假新聞偵測模型在真偽二元分類上表現出色，但它們根據新聞內容推論詳細偽造痕跡的能力仍未得到充分探討。此外，由於缺乏外部知識，現有方法在與事實相關的新聞上的表現令人質疑，這讓它們的實際應用不明朗。在本文中，我們提出了一個新的多媒體研究主題，即操弄推理。操弄推理旨在根據新聞內容推論操弄。為了支持這項研究，我們引入了假新聞偵測和操弄推理的基準，稱為以人為中心且與事實相關的假新聞 (HFFN)。該基準強調了人類的中心性和高度的事實相關性，並附有詳細的手動註解。HFFN 涵蓋了四個現實領域，其中假新聞範例是透過三種操弄方法產生的。此外，我們提出了一個多模態新聞偵測和推理語言模型 (M-DRUM)，它不僅可以判斷多模態新聞的真偽，還能對潛在操弄提出分析推理。在特徵提取層面，我們採用了一個交叉注意力機制，從多模態輸入中提取細粒度的融合特徵。在推理層面，一個大型視覺語言模型 (LVLM) 作為主幹，以利於與事實相關的推理。我們部署了一個兩階段訓練架構，以更好地啟動識別和推理能力。全面的實驗表明，我們的模型優於最先進 (SOTA) 的假新聞偵測模型和強大的 LVLMs，例如 GPT-4 和 LLaVA。</paragraph>

##### **ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation**
2407.02040v1 by Zhiyuan Ma, Yuxiang Wei, Yabin Zhang, Xiangyu Zhu, Zhen Lei, Lei Zhang

By leveraging the text-to-image diffusion priors, score distillation can
synthesize 3D contents without paired text-3D training data. Instead of
spending hours of online optimization per text prompt, recent studies have been
focused on learning a text-to-3D generative network for amortizing multiple
text-3D relations, which can synthesize 3D contents in seconds. However,
existing score distillation methods are hard to scale up to a large amount of
text prompts due to the difficulties in aligning pretrained diffusion prior
with the distribution of rendered images from various text prompts. Current
state-of-the-arts such as Variational Score Distillation finetune the
pretrained diffusion model to minimize the noise prediction error so as to
align the distributions, which are however unstable to train and will impair
the model's comprehension capability to numerous text prompts. Based on the
observation that the diffusion models tend to have lower noise prediction
errors at earlier timesteps, we propose Asynchronous Score Distillation (ASD),
which minimizes the noise prediction error by shifting the diffusion timestep
to earlier ones. ASD is stable to train and can scale up to 100k prompts. It
reduces the noise prediction error without changing the weights of pre-trained
diffusion model, thus keeping its strong comprehension capability to prompts.
We conduct extensive experiments across different 2D diffusion models,
including Stable Diffusion and MVDream, and text-to-3D generators, including
Hyper-iNGP, 3DConv-Net and Triplane-Transformer. The results demonstrate ASD's
effectiveness in stable 3D generator training, high-quality 3D content
synthesis, and its superior prompt-consistency, especially under large prompt
corpus.

摘要：<paragraph>透過利用文本到影像擴散先驗，分數蒸餾可以在沒有成對文本 3D 訓練資料的情況下，合成 3D 內容。最近的研究不再花費數小時對每個文本提示進行線上最佳化，而是專注於學習文本到 3D 生成網路，以攤銷多個文本 3D 關係，可以在幾秒鐘內合成 3D 內容。然而，現有的分數蒸餾方法很難擴充到大量的文本提示，因為難以將預訓練的擴散先驗與來自各種文本提示的渲染影像分佈對齊。目前的技術，例如變異分數蒸餾，會微調預訓練的擴散模型，以最小化雜訊預測誤差，進而對齊分佈，然而這些分佈的訓練並不穩定，而且會損害模型對大量文本提示的理解能力。根據擴散模型在較早時間步長中往往具有較低雜訊預測誤差的觀察，我們提出了非同步分數蒸餾 (ASD)，它透過將擴散時間步長轉移到較早的時間步長來最小化雜訊預測誤差。ASD 訓練穩定，且可以擴充到 100k 個提示。它在不改變預訓練擴散模型權重的情況下降低了雜訊預測誤差，從而保持了其對提示的強大理解能力。我們對不同的 2D 擴散模型（包括 Stable Diffusion 和 MVDream）以及文本到 3D 生成器（包括 Hyper-iNGP、3DConv-Net 和 Triplane-Transformer）進行了廣泛的實驗。結果證明了 ASD 在穩定的 3D 生成器訓練、高品質 3D 內容合成以及其優異的提示一致性（尤其是在大型提示語料庫中）方面的有效性。</paragraph>

##### **Prompt Stability Scoring for Text Annotation with Large Language Models**
2407.02039v1 by Christopher Barrie, Elli Palaiologou, Petter Törnberg

Researchers are increasingly using language models (LMs) for text annotation.
These approaches rely only on a prompt telling the model to return a given
output according to a set of instructions. The reproducibility of LM outputs
may nonetheless be vulnerable to small changes in the prompt design. This calls
into question the replicability of classification routines. To tackle this
problem, researchers have typically tested a variety of semantically similar
prompts to determine what we call "prompt stability." These approaches remain
ad-hoc and task specific. In this article, we propose a general framework for
diagnosing prompt stability by adapting traditional approaches to intra- and
inter-coder reliability scoring. We call the resulting metric the Prompt
Stability Score (PSS) and provide a Python package PromptStability for its
estimation. Using six different datasets and twelve outcomes, we classify >150k
rows of data to: a) diagnose when prompt stability is low; and b) demonstrate
the functionality of the package. We conclude by providing best practice
recommendations for applied researchers.

摘要：研究人員越來越常使用語言模型 (LM) 進行文字註解。
這些方法僅依賴提示，指示模型根據一組說明回傳給定的輸出。
儘管如此，LM 輸出的可複製性仍可能受到提示設計中的微小變更影響。
這對分類例程的可重複性提出質疑。為了解決這個問題，研究人員通常會測試各種語義相似的提示，以確定我們所說的「提示穩定性」。這些方法仍然是臨時且特定於任務的。在本文中，我們提出了一個通用框架，用於透過調整傳統方法來診斷提示穩定性，以進行組內和組間編碼器可靠性評分。我們將產生的指標稱為提示穩定性分數 (PSS)，並提供一個 Python 套件 PromptStability 來估計它。我們使用六個不同的資料集和十二個結果，將超過 150k 列的資料分類為：a) 診斷提示穩定性低的情況；以及 b) 展示套件的功能。我們最後提供最佳實務建議，供應用研究人員參考。

##### **SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules**
2407.02031v1 by Suyi Li, Lingyun Yang, Xiaoxiao Jiang, Hanfeng Lu, Zhipeng Di, Weiyi Lu, Jiawei Chen, Kan Liu, Yinghao Yu, Tao Lan, Guodong Yang, Lin Qu, Liping Zhang, Wei Wang

This paper documents our characterization study and practices for serving
text-to-image requests with stable diffusion models in production. We first
comprehensively analyze inference request traces for commercial text-to-image
applications. It commences with our observation that add-on modules, i.e.,
ControlNets and LoRAs, that augment the base stable diffusion models, are
ubiquitous in generating images for commercial applications. Despite their
efficacy, these add-on modules incur high loading overhead, prolong the serving
latency, and swallow up expensive GPU resources. Driven by our characterization
study, we present SwiftDiffusion, a system that efficiently generates
high-quality images using stable diffusion models and add-on modules. To
achieve this, SwiftDiffusion reconstructs the existing text-to-image serving
workflow by identifying the opportunities for parallel computation and
distributing ControlNet computations across multiple GPUs. Further,
SwiftDiffusion thoroughly analyzes the dynamics of image generation and
develops techniques to eliminate the overhead associated with LoRA loading and
patching while preserving the image quality. Last, SwiftDiffusion proposes
specialized optimizations in the backbone architecture of the stable diffusion
models, which are also compatible with the efficient serving of add-on modules.
Compared to state-of-the-art text-to-image serving systems, SwiftDiffusion
reduces serving latency by up to 5x and improves serving throughput by up to 2x
without compromising image quality.

摘要：本文档记录了我们的表征研究和实践，用于在生产中使用稳定扩散模型提供文本到图像请求。我们首先全面分析了商业文本到图像应用程序的推理请求跟踪。它从我们的观察开始，即附加模块（即 ControlNets 和 LoRAs）增强了基本的稳定扩散模型，在为商业应用程序生成图像时无处不在。尽管它们有效，但这些附加模块会产生高加载开销，延长服务延迟，并吞噬昂贵的 GPU 资源。在我们的表征研究的推动下，我们提出了 SwiftDiffusion，这是一个使用稳定扩散模型和附加模块高效生成高质量图像的系统。为此，SwiftDiffusion 通过识别并行计算的机会和在多个 GPU 上分布 ControlNet 计算，重建了现有的文本到图像服务工作流。此外，SwiftDiffusion 彻底分析了图像生成的动态，并开发了消除与 LoRA 加载和修补相关开销的技术，同时保留了图像质量。最后，SwiftDiffusion 在稳定扩散模型的主干架构中提出了专门的优化，这些优化也与附加模块的高效服务兼容。与最先进的文本到图像服务系统相比，SwiftDiffusion 将服务延迟降低了 5 倍，并将服务吞吐量提高了 2 倍，同时不影响图像质量。

##### **Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis**
2407.02030v1 by Chahat Raj, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu

Large Language Models (LLMs) perpetuate social biases, reflecting prejudices
in their training data and reinforcing societal stereotypes and inequalities.
Our work explores the potential of the Contact Hypothesis, a concept from
social psychology for debiasing LLMs. We simulate various forms of social
contact through LLM prompting to measure their influence on the model's biases,
mirroring how intergroup interactions can reduce prejudices in social contexts.
We create a dataset of 108,000 prompts following a principled approach
replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and
NousHermes) across 13 social bias dimensions. We propose a unique debiasing
technique, Social Contact Debiasing (SCD), that instruction-tunes these models
with unbiased responses to prompts. Our research demonstrates that LLM
responses exhibit social biases when subject to contact probing, but more
importantly, these biases can be significantly reduced by up to 40% in 1 epoch
of instruction tuning LLaMA 2 following our SCD strategy. Our code and data are
available at https://github.com/chahatraj/breakingbias.

摘要：大型語言模型 (LLM) 延續了社會偏見，反映了其訓練數據中的偏見，並加強了社會刻板印象和不平等現象。我們的研究探討了接觸假說的可能性，這是一個來自社會心理學的概念，用於消除 LLM 的偏見。我們模擬了各種形式的社會接觸，通過 LLM 提示來衡量它們對模型偏見的影響，反映了群際互動如何減少社會環境中的偏見。我們根據一個遵循原則性方法的 108,000 個提示建立了一個數據集，複製社會接觸來衡量三個 LLM（LLaMA 2、Tulu 和 NousHermes）在 13 個社會偏見維度上的偏見。我們提出了一種獨特的去偏見技術，即社會接觸去偏見 (SCD)，它使用無偏見的提示對這些模型進行指令調整。我們的研究表明，當受到接觸探測時，LLM 的回應會表現出社會偏見，但更重要的是，這些偏見可以在 LLaMA 2 按照我們的 SCD 策略進行 1 個時期的指令調整後顯著減少多達 40%。我們的程式碼和資料可在 https://github.com/chahatraj/breakingbias 取得。

##### **Why does in-context learning fail sometimes? Evaluating in-context learning on open and closed questions**
2407.02028v1 by Xiang Li, Haoran Tang, Siyu Chen, Ziwei Wang, Ryan Chen, Marcin Abram

We measure the performance of in-context learning as a function of task
novelty and difficulty for open and closed questions. For that purpose, we
created a novel benchmark consisting of hard scientific questions, each paired
with a context of various relevancy. We show that counter-intuitively, a
context that is more aligned with the topic does not always help more than a
less relevant context. This effect is especially visible for open questions and
questions of high difficulty or novelty. This result reveals a fundamental
difference between the treatment of close-form and open-form questions by
large-language models and shows a need for a more robust evaluation of
in-context learning on the variety of different types of questions. It also
poses a new question of how to optimally select a context for large language
models, especially in the context of Retrieval Augmented Generation (RAG)
systems. Our results suggest that the answer to this question can be highly
application-dependent and might be contingent on factors including the format
of the question, the perceived difficulty level of the questions, and the
novelty or popularity of the information we seek.

摘要：我們測量情境學習的表現，作為開放式和封閉式問題的任務新穎性和難度的函數。為此目的，我們建立了一個由困難的科學問題組成的全新基準，每個問題都與各種相關性的情境配對。我們展示出與主題更一致的情境並非總是有助於比相關性較低的情境更多。這種效應對於開放式問題和高難度或新穎性的問題特別明顯。此結果揭示了大型語言模型對封閉形式和開放形式問題的處理之間的根本區別，並顯示需要對各種不同類型的問題進行更強健的情境學習評估。它還提出了如何為大型語言模型最佳選擇情境的全新問題，特別是在檢索增強生成 (RAG) 系統的情境中。我們的結果表明，這個問題的答案可能高度依賴應用程式，並且可能取決於包括問題格式、問題的感知難度等級以及我們尋求的資訊的新穎性或流行性等因素。

##### **On the Expressive Power of Sparse Geometric MPNNs**
2407.02025v1 by Yonatan Sverdlov, Nadav Dym

Motivated by applications in chemistry and other sciences, we study the
expressive power of message-passing neural networks for geometric graphs, whose
node features correspond to 3-dimensional positions. Recent work has shown that
such models can separate generic pairs of non-equivalent geometric graphs,
though they may fail to separate some rare and complicated instances. However,
these results assume a fully connected graph, where each node possesses
complete knowledge of all other nodes. In contrast, often, in application,
every node only possesses knowledge of a small number of nearest neighbors.
This paper shows that generic pairs of non-equivalent geometric graphs can be
separated by message-passing networks with rotation equivariant features as
long as the underlying graph is connected. When only invariant intermediate
features are allowed, generic separation is guaranteed for generically globally
rigid graphs. We introduce a simple architecture, EGENNET, which achieves our
theoretical guarantees and compares favorably with alternative architecture on
synthetic and chemical benchmarks.

摘要：受化学和其他科学应用的启发，我们研究消息传递神经网络在几何图中的表达能力，其节点特征对应于三维位置。最近的研究表明，此类模型可以分离非等价几何图的一般对，尽管它们可能无法分离一些罕见且复杂的实例。然而，这些结果假设了一个完全连接的图，其中每个节点都拥有所有其他节点的完整知识。相比之下，在应用中，每个节点通常只拥有少数最近邻居的知识。本文表明，只要底层图是连通的，具有旋转等变特征的消息传递网络就可以分离非等价几何图的一般对。当仅允许不变中间特征时，对于一般全局刚性图，可以保证一般分离。我们引入了一个简单的架构 EGENNET，它实现了我们的理论保证，并且在合成和化学基准上与替代架构进行了有利的比较。

##### **An End-to-End Speech Summarization Using Large Language Model**
2407.02005v1 by Hengchao Shang, Zongyao Li, Jiaxin Guo, Shaojun Li, Zhiqiang Rao, Yuanchang Luo, Daimeng Wei, Hao Yang

Abstractive Speech Summarization (SSum) aims to generate human-like text
summaries from spoken content. It encounters difficulties in handling long
speech input and capturing the intricate cross-modal mapping between long
speech inputs and short text summaries. Research on large language models
(LLMs) and multimodal information fusion has provided new insights for
addressing these challenges. In this paper, we propose an end-to-end SSum model
that utilizes Q-Former as a connector for the audio-text modality and employs
LLMs to generate text summaries directly from speech features. We adopt a
multi-stage training approach that includes LLM based ASR and Text
Summarization (TSum) tasks as auxiliary tasks. ASR tasks are used to align
feature spaces and enhance the LLM's ability to handle longer speech. Then, we
utilize a curriculum learning strategy to facilitate the model's transition
from TSum to SSum. Finally, our model achieves competitive performance on the
How-2 dataset.

摘要：摘要式語音摘要 (SSum) 的目標是從口說內容產生類似人類的文字摘要。它在處理長語音輸入和擷取長語音輸入與短文字摘要之間複雜的跨模態對應時會遇到困難。關於大型語言模型 (LLM) 和多模態資訊融合的研究為了解決這些挑戰提供了新的見解。在本文中，我們提出了一個端到端的 SSum 模型，它利用 Q-Former 作為音訊文字模態的連接器，並使用 LLM 直接從語音特徵產生文字摘要。我們採用多階段訓練方法，其中包括基於 LLM 的 ASR 和文字摘要 (TSum) 任務作為輔助任務。ASR 任務用於對齊特徵空間並增強 LLM 處理較長語音的能力。然後，我們利用課程學習策略來促進模型從 TSum 過渡到 SSum。最後，我們的模型在 How-2 資料集上取得了有競爭力的表現。

##### **SAVE: Segment Audio-Visual Easy way using Segment Anything Model**
2407.02004v1 by Khanh-Binh Nguyen, Chae Jung Park

The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify
and locate auditory elements within visual scenes by accurately predicting
segmentation masks at the pixel level. Achieving this involves comprehensively
considering data and model aspects to address this task effectively. This study
presents a lightweight approach, SAVE, which efficiently adapts the pre-trained
segment anything model (SAM) to the AVS task. By incorporating an image encoder
adapter into the transformer blocks to better capture the distinct dataset
information and proposing a residual audio encoder adapter to encode the audio
features as a sparse prompt, our proposed model achieves effective audio-visual
fusion and interaction during the encoding stage. Our proposed method
accelerates the training and inference speed by reducing the input resolution
from 1024 to 256 pixels while achieving higher performance compared with the
previous SOTA. Extensive experimentation validates our approach, demonstrating
that our proposed model outperforms other SOTA methods significantly. Moreover,
leveraging the pre-trained model on synthetic data enhances performance on real
AVSBench data, achieving 84.59 mIoU on the S4 (V1S) subset and 70.28 mIoU on
the MS3 (V1M) set with only 256 pixels for input images. This increases up to
86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with inputs of 1024
pixels.

摘要：視覺-聽覺分割 (AVS) 的主要目標是透過精準預測像素層級的分割遮罩，精準識別並定位視覺場景中的聽覺元素。為了有效處理這項任務，需要全面考量資料和模型面向。本研究提出一個輕量級方法 SAVE，它能有效地將預先訓練好的任何模型區段 (SAM) 調整至 AVS 任務。透過將影像編碼器轉接器納入Transformer區塊，以更好地擷取不同的資料集資訊，並提出一個殘差音訊編碼器轉接器，將音訊特徵編碼為一個稀疏提示，我們提出的模型在編碼階段實現了有效的視覺-聽覺融合和互動。我們提出的方法透過將輸入解析度從 1024 降至 256 像素來加速訓練和推論速度，同時與先前的 SOTA 相比，達到了更高的效能。廣泛的實驗驗證了我們的方法，證明我們提出的模型顯著優於其他 SOTA 方法。此外，在合成資料上利用預先訓練好的模型，可以提升在真實 AVSBench 資料上的效能，在 S4 (V1S) 子集中達到 84.59 mIoU，在 MS3 (V1M) 集合中達到 70.28 mIoU，輸入影像僅有 256 像素。使用 1024 像素的輸入，在 S4 (V1S) 上增加到 86.16 mIoU，在 MS3 (V1M) 上增加到 70.83 mIoU。

##### **Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion**
2407.01994v1 by Ananjan Nandi, Navdeep Kaur, Parag Singla, Mausam

High-quality and high-coverage rule sets are imperative to the success of
Neuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form
the basis of all symbolic inferences. Recent literature builds neural models
for generating rule sets, however, preliminary experiments show that they
struggle with maintaining high coverage. In this work, we suggest three simple
augmentations to existing rule sets: (1) transforming rules to their abductive
forms, (2) generating equivalent rules that use inverse forms of constituent
relations and (3) random walks that propose new rules. Finally, we prune
potentially low quality rules. Experiments over four datasets and five
ruleset-baseline settings suggest that these simple augmentations consistently
improve results, and obtain up to 7.1 pt MRR and 8.5 pt Hits@1 gains over using
rules without augmentations.

摘要：高品質且高覆蓋率的規則集對於神經符號知識圖譜完成功能 (NS-KGC) 模型的成功至關重要，因為它們形成所有符號推論的基礎。最近的文獻為產生規則集建立神經模型，然而，初步實驗顯示它們在維持高覆蓋率方面有困難。在這項工作中，我們建議對現有規則集進行三個簡單的擴充：(1) 將規則轉換為它們的演繹形式，(2) 產生使用構成關係的逆形式的等效規則，以及 (3) 提出新規則的隨機遊走。最後，我們修剪潛在的低品質規則。在四個資料集和五個規則集基準設定上的實驗表明，這些簡單的擴充持續改善結果，並獲得比不使用擴充的規則多達 7.1 pt MRR 和 8.5 pt Hits@1 的增益。

##### **Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?**
2407.01992v1 by Nishant Balepur, Rachel Rudinger

Recent work shows that large language models (LLMs) can answer
multiple-choice questions using only the choices, but does this mean that MCQA
leaderboard rankings of LLMs are largely influenced by abilities in
choices-only settings? To answer this, we use a contrast set that probes if
LLMs over-rely on choices-only shortcuts in MCQA. While previous works build
contrast sets via expensive human annotations or model-generated data which can
be biased, we employ graph mining to extract contrast sets from existing MCQA
datasets. We use our method on UnifiedQA, a group of six commonsense reasoning
datasets with high choices-only accuracy, to build an 820-question contrast
set. After validating our contrast set, we test 12 LLMs, finding that these
models do not exhibit reliance on choice-only shortcuts when given both the
question and choices. Thus, despite the susceptibility~of MCQA to high
choices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA
leaderboards just due to their ability to exploit choices-only shortcuts.

摘要：最近的研究表明，大型语言模型 (LLM) 仅使用选项就能回答多项选择题，但这是否表示多项选择问答 (MCQA) 排行榜上的 LLM 主要受限于仅选项设置中的能力？为了回答这个问题，我们使用对比集来探查 LLM 在 MCQA 中是否过度依赖仅选项捷径。虽然先前的研究通过昂贵的人工注释或可能存在偏差的模型生成数据来构建对比集，但我们采用图挖掘从现有 MCQA 数据集中提取对比集。我们使用我们的方法在 UnifiedQA 上，这是一个由六个具有高仅选项准确率的常识推理数据集组成的组，构建了一个 820 题的对比集。在验证我们的对比集后，我们测试了 12 个 LLM，发现当同时给出问题和选项时，这些模型不会表现出对仅选项捷径的依赖。因此，尽管 MCQA 容易受到高仅选项准确率的影响，但我们认为 LLM 在 MCQA 排行榜上获得高排名并非仅仅因为它们利用仅选项捷径的能力。

##### **A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding**
2407.01976v1 by Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei Yang, Binghong Wu, Qi Liu, Hao Feng, Han Wang, Hao Liu, Can Huang

Recently, many studies have demonstrated that exclusively incorporating
OCR-derived text and spatial layouts with large language models (LLMs) can be
highly effective for document understanding tasks. However, existing methods
that integrate spatial layouts with text have limitations, such as producing
overly long text sequences or failing to fully leverage the autoregressive
traits of LLMs. In this work, we introduce Interleaving Layout and Text in a
Large Language Model (LayTextLLM)} for document understanding. In particular,
LayTextLLM projects each bounding box to a single embedding and interleaves it
with text, efficiently avoiding long sequence issues while leveraging
autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction
of layout and textual data but also shows enhanced performance in Key
Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive
benchmark evaluations reveal significant improvements, with a 27.0% increase on
KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document
understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based
LLMs on KIE tasks.

摘要：最近，許多研究已經證明，將僅透過 OCR 衍生的文字和空間佈局與大型語言模型 (LLM) 整合，對於文件理解任務可能非常有效。然而，現有整合空間佈局與文字的方法有其限制，例如產生過長的文字序列，或是無法充分利用 LLM 的自迴歸特質。在這項工作中，我們在大型語言模型中引入了交錯佈局和文字（LayTextLLM），用於文件理解。特別是，LayTextLLM 將每個邊界框投影到單一嵌入中，並將其與文字交錯，有效避免長序列問題，同時利用 LLM 的自迴歸特質。LayTextLLM 不僅簡化了佈局和文字資料的互動，還展現出在關鍵資訊萃取 (KIE) 和視覺問答 (VQA) 中的效能提升。全面的基準評估顯示出顯著的進步，在 KIE 任務上比現有最先進的文件理解 MLLM 提升 27.0%，在 VQA 任務上提升 24.1%，並且在 KIE 任務上比其他 SOTA 基於 OCR 的 LLM 提升 15.5%。

##### **MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation**
2407.01972v1 by Zijie J. Wang, Duen Horng Chau

Retrieval-augmented text generation (RAG) addresses the common limitations of
large language models (LLMs), such as hallucination, by retrieving information
from an updatable external knowledge base. However, existing approaches often
require dedicated backend servers for data storage and retrieval, thereby
limiting their applicability in use cases that require strict data privacy,
such as personal finance, education, and medicine. To address the pressing need
for client-side dense retrieval, we introduce MeMemo, the first open-source
JavaScript toolkit that adapts the state-of-the-art approximate nearest
neighbor search technique HNSW to browser environments. Developed with modern
and native Web technologies, such as IndexedDB and Web Workers, our toolkit
leverages client-side hardware capabilities to enable researchers and
developers to efficiently search through millions of high-dimensional vectors
in the browser. MeMemo enables exciting new design and research opportunities,
such as private and personalized content creation and interactive prototyping,
as demonstrated in our example application RAG Playground. Reflecting on our
work, we discuss the opportunities and challenges for on-device dense
retrieval. MeMemo is available at https://github.com/poloclub/mememo.

摘要：檢索增強文字生成 (RAG) 藉由從可更新的外部知識庫中檢索資訊，來解決大型語言模型 (LLM) 常見的限制，例如幻覺。然而，現有的方法通常需要專用的後端伺服器來儲存和檢索資料，因此限制了它們在需要嚴格資料隱私的用例中的適用性，例如個人理財、教育和醫療。為了滿足對用戶端密集檢索的迫切需求，我們推出了 MeMemo，這是第一個開源的 JavaScript 工具包，它將最先進的近似最近鄰搜尋技術 HNSW 調整到瀏覽器環境中。我們的工具包使用現代且原生的網路技術（例如 IndexedDB 和 Web Workers）開發，它利用用戶端硬體功能，讓研究人員和開發人員能夠在瀏覽器中有效率地搜尋數百萬個高維向量。MeMemo 開啟了令人興奮的新設計和研究機會，例如私人和個人化內容建立以及互動式原型製作，如我們範例應用程式 RAG Playground 所示。回顧我們的作品，我們討論了裝置上密集檢索的機會和挑戰。MeMemo 可在 https://github.com/poloclub/mememo 取得。

##### **AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment**
2407.01965v1 by Yilong Lai, Jialong Wu, Congzhi Zhang, Haowen Sun, Deyu Zhou

Conversational Query Reformulation (CQR) has significantly advanced in
addressing the challenges of conversational search, particularly those stemming
from the latent user intent and the need for historical context. Recent works
aimed to boost the performance of CRQ through alignment. However, they are
designed for one specific retrieval system, which potentially results in poor
generalization. To overcome this limitation, we present a novel framework
AdaCQR. By aligning reformulation models with both term-based and
semantic-based retrieval systems, AdaCQR enhances the generalizability of
information-seeking queries across diverse retrieval environments through a
dual-phase training strategy. We also developed two effective approaches for
acquiring superior labels and diverse input candidates, boosting the efficiency
and robustness of the framework. Experimental evaluations on the TopiOCQA and
QReCC datasets demonstrate that AdaCQR significantly outperforms existing
methods, offering both quantitative and qualitative improvements in
conversational query reformulation.

摘要：對話式查詢改寫 (CQR) 在解決對話式搜尋的挑戰方面已取得顯著進展，特別是那些源自潛在使用者意圖和對歷史脈絡需求的挑戰。最近的研究旨在透過對齊提升 CQR 的效能。然而，它們是針對一個特定的檢索系統所設計，這可能會導致不佳的概括化。為了克服這個限制，我們提出了一個新的架構 AdaCQR。透過將改寫模型與基於術語和基於語意的檢索系統對齊，AdaCQR 透過雙階段訓練策略，增強了資訊查詢在不同檢索環境中的概括性。我們還開發了兩種有效的方法，用於獲取優異的標籤和多樣化的輸入候選，提升了這個架構的效率和穩健性。在 TopiOCQA 和 QReCC 資料集上的實驗評估顯示，AdaCQR 明顯優於現有方法，在對話式查詢改寫中提供了量化和質化的改進。

##### **Enabling Discriminative Reasoning in Large Language Models for Legal Judgment Prediction**
2407.01964v1 by Chenlong Deng, Kelong Mao, Yuyao Zhang, Zhicheng Dou

Legal judgment prediction is essential for enhancing judicial efficiency. In
this work, we identify that existing large language models (LLMs) underperform
in this domain due to challenges in understanding case complexities and
distinguishing between similar charges. To adapt LLMs for effective legal
judgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT)
reasoning framework inspired by human judicial reasoning. ADAPT involves
decomposing case facts, discriminating among potential charges, and predicting
the final judgment. We further enhance LLMs through fine-tuning with multi-task
synthetic trajectories to improve legal judgment prediction accuracy and
efficiency under our ADAPT framework. Extensive experiments conducted on two
widely-used datasets demonstrate the superior performance of our framework in
legal judgment prediction, particularly when dealing with complex and confusing
charges.

摘要：法律判決預測對於提升司法效率至關重要。在本文中，我們發現現有的大型語言模型 (LLM) 在此領域表現不佳，原因在於難以理解案件的複雜性以及區分類似的指控。為了讓 LLM 適應有效的法律判決預測，我們引入了受人類司法推理啟發的詢問-區分-預測 (ADAPT) 推理框架。ADAPT 涉及分解案件事實、區分潛在指控以及預測最終判決。我們進一步透過微調多任務合成軌跡來增強 LLM，以提升在 ADAPT 框架下的法律判決預測準確度和效率。在兩個廣泛使用的資料集上進行的廣泛實驗證明了我們框架在法律判決預測中的卓越效能，特別是在處理複雜且令人困惑的指控時。

##### **S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models**
2407.01955v1 by Parsa Kavehzadeh, Mohammadreza Pourreza, Mojtaba Valipour, Tinashu Zhu, Haoli Bai, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh

Deployment of autoregressive large language models (LLMs) is costly, and as
these models increase in size, the associated costs will become even more
considerable. Consequently, different methods have been proposed to accelerate
the token generation process and reduce costs. Speculative decoding (SD) is
among the most promising approaches to speed up the LLM decoding process by
verifying multiple tokens in parallel and using an auxiliary smaller draft
model to generate the possible tokens. In SD, usually, one draft model is used
to serve a specific target model; however, in practice, LLMs are diverse, and
we might need to deal with many target models or more than one target model
simultaneously. In this scenario, it is not clear which draft model should be
used for which target model, and searching among different draft models or
training customized draft models can further increase deployment costs. In this
paper, we first introduce a novel multi-target scenario for the deployment of
draft models for faster inference. Then, we present a novel, more efficient
sorted speculative decoding mechanism that outperforms regular baselines in
multi-target settings. We evaluated our method on Spec-Bench in different
settings, including base models such as Vicuna 7B, 13B, and LLama Chat 70B. Our
results suggest that our draft models perform better than baselines for
multiple target models at the same time.

摘要：自迴歸大型語言模型 (LLM) 的部署成本很高，隨著這些模型的規模越來越大，相關成本將變得更加可觀。因此，人們提出了不同的方法來加速記號產生過程並降低成本。推測性解碼 (SD) 是加速 LLM 解碼過程最有前途的方法之一，它通過並行驗證多個記號並使用輔助小型草稿模型來生成可能的記號。在 SD 中，通常使用一個草稿模型來服務一個特定的目標模型；然而，在實務中，LLM 是多樣化的，我們可能需要同時處理多個目標模型或多於一個目標模型。在這種情況下，尚不清楚應為哪個目標模型使用哪個草稿模型，並且在不同的草稿模型中搜尋或訓練自訂草稿模型可能會進一步增加部署成本。在本文中，我們首先介紹了一個用於部署草稿模型以加快推理的多目標場景的新穎方法。然後，我們提出了一種新穎、更有效率的排序推測性解碼機制，它在多目標設定中優於常規基準。我們在不同的設定中對 Spec-Bench 評估了我們的方法，包括基礎模型，例如 Vicuna 7B、13B 和 LLama Chat 70B。我們的結果表明，我們的草稿模型同時針對多個目標模型的表現優於基準。

##### **CatMemo at the FinLLM Challenge Task: Fine-Tuning Large Language Models using Data Fusion in Financial Applications**
2407.01953v1 by Yupeng Cao, Zhiyuan Yao, Zhi Chen, Zhiyang Deng

The integration of Large Language Models (LLMs) into financial analysis has
garnered significant attention in the NLP community. This paper presents our
solution to IJCAI-2024 FinLLM challenge, investigating the capabilities of LLMs
within three critical areas of financial tasks: financial classification,
financial text summarization, and single stock trading. We adopted Llama3-8B
and Mistral-7B as base models, fine-tuning them through Parameter Efficient
Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) approaches. To enhance model
performance, we combine datasets from task 1 and task 2 for data fusion. Our
approach aims to tackle these diverse tasks in a comprehensive and integrated
manner, showcasing LLMs' capacity to address diverse and complex financial
tasks with improved accuracy and decision-making capabilities.

摘要：大型語言模型 (LLM) 整合到財務分析中已在自然語言處理社群中獲得極大的關注。本文提出我們對 IJCAI-2024 FinLLM 挑戰的解決方案，探討 LLM 在財務任務的三個關鍵領域中的能力：財務分類、財務文字摘要和單一股票交易。我們採用 Llama3-8B 和 Mistral-7B 作為基礎模型，透過參數高效微調 (PEFT) 和低秩適應 (LoRA) 方法進行微調。為了增強模型效能，我們結合任務 1 和任務 2 的資料集進行資料融合。我們的做法旨在全面且整合地處理這些不同的任務，展示 LLM 處理多樣且複雜的財務任務的能力，並提升準確度和決策能力。

##### **LDP: A Local Diffusion Planner for Efficient Robot Navigation and Collision Avoidance**
2407.01950v1 by Wenhao Yu, Jie Peng, Huanyu Yang, Junrui Zhang, Yifan Duan, Jianmin Ji, Yanyong Zhang

The conditional diffusion model has been demonstrated as an efficient tool
for learning robot policies, owing to its advancement to accurately model the
conditional distribution of policies. The intricate nature of real-world
scenarios, characterized by dynamic obstacles and maze-like structures,
underscores the complexity of robot local navigation decision-making as a
conditional distribution problem. Nevertheless, leveraging the diffusion model
for robot local navigation is not trivial and encounters several under-explored
challenges: (1) Data Urgency. The complex conditional distribution in local
navigation needs training data to include diverse policy in diverse real-world
scenarios; (2) Myopic Observation. Due to the diversity of the perception
scenarios, diffusion decisions based on the local perspective of robots may
prove suboptimal for completing the entire task, as they often lack foresight.
In certain scenarios requiring detours, the robot may become trapped. To
address these issues, our approach begins with an exploration of a diverse data
generation mechanism that encompasses multiple agents exhibiting distinct
preferences through target selection informed by integrated global-local
insights. Then, based on this diverse training data, a diffusion agent is
obtained, capable of excellent collision avoidance in diverse scenarios.
Subsequently, we augment our Local Diffusion Planner, also known as LDP by
incorporating global observations in a lightweight manner. This enhancement
broadens the observational scope of LDP, effectively mitigating the risk of
becoming ensnared in local optima and promoting more robust navigational
decisions.

摘要：條件擴散模型已被證明是一種有效的機器人策略學習工具，因為它能準確地模擬策略的條件分佈。現實世界場景的複雜性，其特徵在於動態障礙物和迷宮般的結構，強調了機器人局部導航決策制定作為條件分佈問題的複雜性。儘管如此，利用擴散模型進行機器人局部導航並非易事，並會遇到一些尚未充分探索的挑戰：(1) 數據緊迫性。局部導航中的複雜條件分佈需要訓練數據，以納入在不同現實世界場景中的不同策略；(2) 近視觀察。由於感知場景的多樣性，基於機器人局部觀點的擴散決策可能會被證明對於完成整個任務而言次優，因為它們通常缺乏遠見。在某些需要繞行的場景中，機器人可能會受困。為了解決這些問題，我們的做法從探索多樣化數據生成機制開始，該機制包含通過綜合全局-局部見解告知目標選擇而表現出不同偏好的多個代理。然後，基於這些多樣化的訓練數據，獲得了一個擴散代理，它能夠在不同的場景中出色地避免碰撞。隨後，我們通過以輕量級的方式納入全局觀察，來擴充我們的局部擴散規劃器，也稱為 LDP。此增強擴展了 LDP 的觀察範圍，有效地減輕了陷入局部最優的風險，並促進了更強大的導航決策。

##### **Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation**
2407.01948v1 by Pablo Messina, René Vidal, Denis Parra, Álvaro Soto, Vladimir Araujo

Advancing representation learning in specialized fields like medicine remains
challenging due to the scarcity of expert annotations for text and images. To
tackle this issue, we present a novel two-stage framework designed to extract
high-quality factual statements from free-text radiology reports in order to
improve the representations of text encoders and, consequently, their
performance on various downstream tasks. In the first stage, we propose a
\textit{Fact Extractor} that leverages large language models (LLMs) to identify
factual statements from well-curated domain-specific datasets. In the second
stage, we introduce a \textit{Fact Encoder} (CXRFE) based on a BERT model
fine-tuned with objective functions designed to improve its representations
using the extracted factual data. Our framework also includes a new
embedding-based metric (CXRFEScore) for evaluating chest X-ray text generation
systems, leveraging both stages of our approach. Extensive evaluations show
that our fact extractor and encoder outperform current state-of-the-art methods
in tasks such as sentence ranking, natural language inference, and label
extraction from radiology reports. Additionally, our metric proves to be more
robust and effective than existing metrics commonly used in the radiology
report generation literature. The code of this project is available at
\url{https://github.com/PabloMessina/CXR-Fact-Encoder}.

摘要：<paragraph>由於缺乏文本和影像的專業註解，在醫學等專業領域推進表徵學習仍然具有挑戰性。為了解決這個問題，我們提出一個新穎的兩階段架構，旨在從自由文本放射科報告中擷取高品質的事實陳述，以改善文本編碼器的表徵，並進而改善它們在各種下游任務上的效能。在第一階段，我們提出一個利用大型語言模型 (LLM) 從經過精心策劃的特定領域資料集識別事實陳述的「事實萃取器」。在第二階段，我們基於 BERT 模型引入一個微調後的「事實編碼器」(CXRFE)，使用旨在利用萃取事實資料改善其表徵的目標函數。我們的架構還包括一個新的基於嵌入的指標 (CXRFEScore)，用於評估胸部 X 光文本生成系統，利用我們方法的兩個階段。廣泛的評估顯示，我們的事實萃取器和編碼器在句子排序、自然語言推論和從放射科報告中萃取標籤等任務中優於目前的最新方法。此外，我們的指標證明比放射科報告生成文獻中常用的現有指標更強大且有效。此專案的程式碼可在 \url{https://github.com/PabloMessina/CXR-Fact-Encoder} 取得。</paragraph>

##### **Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness**
2407.01942v1 by Khyathi Raghavi Chandu, Linjie Li, Anas Awadalla, Ximing Lu, Jae Sung Park, Jack Hessel, Lijuan Wang, Yejin Choi

The ability to acknowledge the inevitable uncertainty in their knowledge and
reasoning is a prerequisite for AI systems to be truly truthful and reliable.
In this paper, we present a taxonomy of uncertainty specific to vision-language
AI systems, distinguishing between epistemic uncertainty (arising from a lack
of information) and aleatoric uncertainty (due to inherent unpredictability),
and further explore finer categories within. Based on this taxonomy, we
synthesize a benchmark dataset, CertainlyUncertain, featuring 178K visual
question answering (VQA) samples as contrastive pairs. This is achieved by 1)
inpainting images to make previously answerable questions into unanswerable
ones; and 2) using image captions to prompt large language models for both
answerable and unanswerable questions. Additionally, we introduce a new metric
confidence-weighted accuracy, that is well correlated with both accuracy and
calibration error, to address the shortcomings of existing metrics.

摘要：辨識知識和推理中不可避免的不確定性的能力是 AI 系統真正誠實可靠的先決條件。在本文中，我們提出了一個專門針對視覺語言 AI 系統的不確定性分類法，區分認識論不確定性（源於資訊不足）和隨機不確定性（由於固有的不可預測性），並進一步探討其中的更精細類別。基於這個分類法，我們綜合了一個基準資料集 CertainlyUncertain，其中包含 178K 個視覺問答 (VQA) 範例作為對比對。這是透過 1) 修補影像，將先前可回答的問題變成不可回答的問題；以及 2) 使用影像標題提示大型語言模型，以回答可回答和不可回答的問題來實現。此外，我們還引入了一個新的指標信心加權準確度，它與準確度和校準誤差都有很好的相關性，以解決現有指標的缺點。

##### **Efficient-Empathy: Towards Efficient and Effective Selection of Empathy Data**
2407.01937v1 by Linzhuang Sun, Hao Liang, Jingxuan Wei, Linkun Sun, Bihui Yu, Bin Cui, Wentao Zhang

In recent years, with the rapid advancements in large language models (LLMs),
achieving excellent empathetic response capability has become a crucial
prerequisite. Consequently, managing and understanding large-scale video
datasets has gained increasing importance. However, empathetic data are
typically trained without any quality selection, leading to inefficient data
usage and wasted computational resources. Additionally, using raw data can
result in low performance in empathetic dialogues. In this work, we present
Efficient-Empathy, a sensibility and rationality score-based data selection
algorithm that automatically selects sensibility and rationality data while
discarding low-quality data. With only the sensibility data (59% of the full
dataset), our trained sensibility model efficiently achieves state-of-the-art
(SoTA) performance. Furthermore, with multiple data selection hyperparameters,
the sensibility model demonstrates SoTA performance, showcasing the robustness
of our method. By integrating sensibility and rationality data with a MoE
structure, we achieve even higher performance, demonstrating the effectiveness
of our Efficient-Empathy algorithm.

摘要：近年来，随着大型语言模型 (LLM) 的快速发展，实现出色的同理心响应能力已成为一项关键先决条件。因此，管理和理解大规模视频数据集变得越来越重要。然而，同理心数据通常在没有任何质量选择的情况下进行训练，导致数据使用效率低下和计算资源浪费。此外，使用原始数据会导致同理心对话中的性能低下。在这项工作中，我们提出了 Efficient-Empathy，一种基于敏感性和合理性分数的数据选择算法，它可以自动选择敏感性和合理性数据，同时丢弃低质量数据。仅使用敏感性数据（完整数据集的 59%），我们训练的敏感性模型有效地实现了最先进 (SoTA) 性能。此外，通过多种数据选择超参数，敏感性模型展示了 SoTA 性能，展示了我们方法的稳健性。通过将敏感性和合理性数据与 MoE 结构集成，我们取得了更高的性能，证明了我们 Efficient-Empathy 算法的有效性。

##### **What We Talk About When We Talk About LMs: Implicit Paradigm Shifts and the Ship of Language Models**
2407.01929v1 by Shengqi Zhu, Jeffrey M. Rzeszotarski

The term Language Models (LMs), as a time-specific collection of models of
interest, is constantly reinvented, with its referents updated much like the
$\textit{Ship of Theseus}$ replaces its parts but remains the same ship in
essence. In this paper, we investigate this $\textit{Ship of Language Models}$
problem, wherein scientific evolution takes the form of continuous, implicit
retrofits of key existing terms. We seek to initiate a novel perspective of
scientific progress, in addition to the more well-studied emergence of new
terms. To this end, we construct the data infrastructure based on recent NLP
publications. Then, we perform a series of text-based analyses toward a
detailed, quantitative understanding of the use of Language Models as a term of
art. Our work highlights how systems and theories influence each other in
scientific discourse, and we call for attention to the transformation of this
Ship that we all are contributing to.

摘要：語言模型（LM）一詞作為特定時間對感興趣模型的集合，不斷地被重新定義，其所指就像忒修斯之船一樣更換零件，但本質上仍然是同一艘船。在本文中，我們探討了這個「語言模型之船」問題，其中科學演化採取了對關鍵現有術語持續、隱含的改造形式。我們尋求開啟一個科學進步的新觀點，除了對新術語出現的更深入研究之外。為此，我們根據最近的 NLP 出版物構建了數據基礎架構。然後，我們對文本進行一系列基於文本的分析，以詳細、量化地理解語言模型作為術語的使用。我們的研究重點說明了系統和理論如何在科學論述中相互影響，並且我們呼籲關注我們所有人都在促成的這艘船的轉變。

##### **To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models**
2407.01920v1 by Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang

Large Language Models (LLMs) trained on extensive corpora inevitably retain
sensitive data, such as personal privacy information and copyrighted material.
Recent advancements in knowledge unlearning involve updating LLM parameters to
erase specific knowledge. However, current unlearning paradigms are mired in
vague forgetting boundaries, often erasing knowledge indiscriminately. In this
work, we introduce KnowUnDo, a benchmark containing copyrighted content and
user privacy domains to evaluate if the unlearning process inadvertently erases
essential knowledge. Our findings indicate that existing unlearning methods
often suffer from excessive unlearning. To address this, we propose a simple
yet effective method, MemFlex, which utilizes gradient information to precisely
target and unlearn sensitive parameters. Experimental results show that MemFlex
is superior to existing methods in both precise knowledge unlearning and
general knowledge retaining of LLMs. Code and dataset will be released at
https://github.com/zjunlp/KnowUnDo.

摘要：大型語言模型 (LLM) 在廣泛的語料庫上進行訓練，不可避免地會保留敏感資料，例如個人隱私資訊和受版權保護的資料。最近在知識遺忘方面的進展包括更新 LLM 參數以抹除特定知識。然而，目前的遺忘範例陷於模糊的遺忘界線，常常不加區別地抹除知識。在這項工作中，我們引入了 KnowUnDo，一個包含受版權保護的內容和使用者隱私網域的基準，以評估遺忘過程是否會無意間抹除必要的知識。我們的研究結果表明，現有的遺忘方法常常會過度遺忘。為了解決這個問題，我們提出了一個簡單但有效的方法 MemFlex，它利用梯度資訊精確地鎖定並遺忘敏感參數。實驗結果顯示，MemFlex 在精確的知識遺忘和 LLM 的一般知識保留方面都優於現有方法。程式碼和資料集將在 https://github.com/zjunlp/KnowUnDo 上發布。

##### **A Method to Facilitate Membership Inference Attacks in Deep Learning Models**
2407.01919v1 by Zitao Chen, Karthik Pattabiraman

Modern machine learning (ML) ecosystems offer a surging number of ML
frameworks and code repositories that can greatly facilitate the development of
ML models. Today, even ordinary data holders who are not ML experts can apply
off-the-shelf codebase to build high-performance ML models on their data, many
of which are sensitive in nature (e.g., clinical records).
  In this work, we consider a malicious ML provider who supplies model-training
code to the data holders, does not have access to the training process, and has
only black-box query access to the resulting model. In this setting, we
demonstrate a new form of membership inference attack that is strictly more
powerful than prior art. Our attack empowers the adversary to reliably
de-identify all the training samples (average >99% attack TPR@0.1% FPR), and
the compromised models still maintain competitive performance as their
uncorrupted counterparts (average <1% accuracy drop). Moreover, we show that
the poisoned models can effectively disguise the amplified membership leakage
under common membership privacy auditing, which can only be revealed by a set
of secret samples known by the adversary.
  Overall, our study not only points to the worst-case membership privacy
leakage, but also unveils a common pitfall underlying existing privacy auditing
methods, which calls for future efforts to rethink the current practice of
auditing membership privacy in machine learning models.

摘要：現代機器學習 (ML) 生態系統提供了大量的 ML 框架和程式碼儲存庫，可以極大地促進 ML 模型的開發。如今，即使不是 ML 專家的普通資料持有者也可以套用現成的程式碼庫，根據其資料建立高性能 ML 模型，其中許多資料本質上很敏感（例如：臨床紀錄）。
  在這項工作中，我們考慮了一個惡意的 ML 提供者，他向資料持有者提供模型訓練程式碼，無法存取訓練程序，而且只能透過黑盒子查詢存取產生的模型。在此設定中，我們展示了一種新的成員推論攻擊形式，它比先前的技術更強大。我們的攻擊讓對手能夠可靠地取消識別所有訓練範例（平均 >99% 攻擊 TPR@0.1% FPR），而且受損的模型仍然保持與未受損的模型一樣的競爭力（平均 <1% 準確度下降）。此外，我們展示了中毒的模型可以有效地隱藏在常見成員隱私稽核下的擴增成員洩漏，這只能由對手知道的秘密範例集揭露。
  總的來說，我們的研究不僅指出最壞情況的成員隱私洩漏，還揭示了現有隱私稽核方法中的一個常見陷阱，這需要未來的努力來重新思考目前在機器學習模型中稽核成員隱私的做法。

##### **Sequential Manipulation Against Rank Aggregation: Theory and Algorithm**
2407.01916v1 by Ke Ma, Qianqian Xu, Jinshan Zeng, Wei Liu, Xiaochun Cao, Yingfei Sun, Qingming Huang

Rank aggregation with pairwise comparisons is widely encountered in
sociology, politics, economics, psychology, sports, etc . Given the enormous
social impact and the consequent incentives, the potential adversary has a
strong motivation to manipulate the ranking list. However, the ideal attack
opportunity and the excessive adversarial capability cause the existing methods
to be impractical. To fully explore the potential risks, we leverage an online
attack on the vulnerable data collection process. Since it is independent of
rank aggregation and lacks effective protection mechanisms, we disrupt the data
collection process by fabricating pairwise comparisons without knowledge of the
future data or the true distribution. From the game-theoretic perspective, the
confrontation scenario between the online manipulator and the ranker who takes
control of the original data source is formulated as a distributionally robust
game that deals with the uncertainty of knowledge. Then we demonstrate that the
equilibrium in the above game is potentially favorable to the adversary by
analyzing the vulnerability of the sampling algorithms such as Bernoulli and
reservoir methods. According to the above theoretical analysis, different
sequential manipulation policies are proposed under a Bayesian decision
framework and a large class of parametric pairwise comparison models. For
attackers with complete knowledge, we establish the asymptotic optimality of
the proposed policies. To increase the success rate of the sequential
manipulation with incomplete knowledge, a distributionally robust estimator,
which replaces the maximum likelihood estimation in a saddle point problem,
provides a conservative data generation solution. Finally, the corroborating
empirical evidence shows that the proposed method manipulates the results of
rank aggregation methods in a sequential manner.

摘要：配對比較中的排名聚合廣泛存在於社會學、政治學、經濟學、心理學、體育等領域。由於其巨大的社會影響和由此產生的誘因，潛在的對手有強烈的動機去操縱排名列表。然而，理想的攻擊機會和過度的對抗能力導致現有方法不切實際。為了充分探索潛在風險，我們利用線上攻擊對脆弱的數據收集過程進行攻擊。由於它獨立於排名聚合，並且缺乏有效的保護機制，我們在不了解未來數據或真實分佈的情況下，通過捏造配對比較來破壞數據收集過程。從博弈論的角度來看，線上操縱者和控制原始數據源的排名者之間的對抗場景被表述為一個分佈穩健博弈，用於應對知識的不確定性。然後，我們通過分析伯努利和水庫方法等抽樣演算法的脆弱性，證明上述博弈中的均衡對對手可能有利。根據上述理論分析，在貝葉斯決策框架和一大類參數化配對比較模型下，提出了不同的順序操縱策略。對於完全知識的攻擊者，我們建立了所提出策略的漸近最優性。為了提高不完全知識的順序操縱的成功率，一個分佈穩健的估計器（它在鞍點問題中取代了最大似然估計）提供了一個保守的數據生成解決方案。最後，佐證的實證證據表明，所提出的方法以順序的方式操縱排名聚合方法的結果。

##### **Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model**
2407.01911v1 by Yu-Kuan Fu, Cheng-Kuang Lee, Hsiu-Hsuan Wang, Hung-yi Lee

Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue
without the need for direct transcription, thereby preserving the wealth of
non-textual information inherent in speech. However, this approach faces a
challenge when speakers talk simultaneously, requiring stereo dialogue data
with speakers recorded on separate channels, a notably scarce resource. To
address this, we have developed an innovative pipeline capable of transforming
single-channel dialogue data into pseudo-stereo data. This expanded our
training dataset from a mere 2,000 to an impressive 17,600 hours, significantly
enriching the diversity and quality of the training examples available. The
inclusion of this pseudo-stereo data has proven to be effective in improving
the performance of spoken dialogue language models. Additionally, we explored
the use of discrete units of different speech foundation models for spoken
dialogue generation.

摘要：最近在口語對話模型的努力旨在合成口語對話，而無需直接轉錄，從而保留語音中固有的豐富非文字資訊。然而，當講者同時說話時，這種方法面臨挑戰，需要立體聲對話資料，並在不同的頻道上錄製講者，這是一個非常稀缺的資源。為了解決這個問題，我們開發了一個創新的管道，能夠將單聲道對話資料轉換為偽立體聲資料。這將我們的訓練資料集從僅有的 2,000 小時擴展到令人印象深刻的 17,600 小時，顯著豐富了可用訓練範例的多樣性和品質。納入這個偽立體聲資料已被證明可以有效改善口語對話語言模型的效能。此外，我們探討了將不同語音基礎模型的離散單元用於口語對話生成。

##### **MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation**
2407.01910v1 by Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, Yingyan, Lin

Large Language Models (LLMs) have recently shown promise in streamlining
hardware design processes by encapsulating vast amounts of domain-specific
data. In addition, they allow users to interact with the design processes
through natural language instructions, thus making hardware design more
accessible to developers. However, effectively leveraging LLMs in hardware
design necessitates providing domain-specific data during inference (e.g.,
through in-context learning), fine-tuning, or pre-training. Unfortunately,
existing publicly available hardware datasets are often limited in size,
complexity, or detail, which hinders the effectiveness of LLMs in hardware
design tasks. To address this issue, we first propose a set of criteria for
creating high-quality hardware datasets that can effectively enhance
LLM-assisted hardware design. Based on these criteria, we propose a
Multi-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at
various levels of detail and corresponding code samples. To benefit the broader
hardware design community, we have developed an open-source infrastructure that
facilitates easy access, integration, and extension of the dataset to meet
specific project needs. Furthermore, to fully exploit the potential of the
MG-Verilog dataset, which varies in complexity and detail, we introduce a
balanced fine-tuning scheme. This scheme serves as a unique use case to
leverage the diverse levels of detail provided by the dataset. Extensive
experiments demonstrate that the proposed dataset and fine-tuning scheme
consistently improve the performance of LLMs in hardware design tasks.

摘要：<paragraph>大型語言模型 (LLM) 最近在簡化硬體設計流程方面展現了希望，它們封裝了大量特定領域的資料。此外，它們允許使用者透過自然語言指令與設計流程互動，進而讓開發人員更容易存取硬體設計。然而，要在硬體設計中有效利用 LLM，需要在推理（例如透過情境學習）、微調或預訓練期間提供特定領域的資料。不幸的是，現有的公開硬體資料集通常在大小、複雜度或細節上受到限制，這會阻礙 LLM 在硬體設計任務中的效果。為了解決此問題，我們首先提出了一組建立高品質硬體資料集的標準，這些資料集可以有效提升 LLM 輔助的硬體設計。根據這些標準，我們提出了一個多粒度 Verilog (MG-Verilog) 資料集，其中包含各種細節層級的說明和對應的程式碼範例。為了讓更廣泛的硬體設計社群受益，我們開發了一個開放原始碼基礎架構，讓使用者可以輕鬆存取、整合和延伸資料集以滿足特定專案需求。此外，為了充分發揮 MG-Verilog 資料集的潛力（其複雜度和細節程度不一），我們引入了一種平衡的微調架構。這個架構作為一個獨特的用例，用於利用資料集提供的不同細節層級。廣泛的實驗證明，所提出的資料集和微調架構持續提升了 LLM 在硬體設計任務中的效能。</paragraph>

