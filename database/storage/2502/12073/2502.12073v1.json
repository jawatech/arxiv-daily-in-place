{"2502.12073": {"publish_time": "2025-02-17", "title": "Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation", "paper_summary": "Social media enables dynamic user engagement with trending topics, and recent\nresearch has explored the potential of large language models (LLMs) for\nresponse generation. While some studies investigate LLMs as agents for\nsimulating user behavior on social media, their focus remains on practical\nviability and scalability rather than a deeper understanding of how well LLM\naligns with human behavior. This paper analyzes LLMs' ability to simulate\nsocial media engagement through action guided response generation, where a\nmodel first predicts a user's most likely engagement action-retweet, quote, or\nrewrite-towards a trending post before generating a personalized response\nconditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and\nDeepSeek-R1 in social media engagement simulation regarding a major societal\nevent discussed on X. Our findings reveal that zero-shot LLMs underperform BERT\nin action prediction, while few-shot prompting initially degrades the\nprediction accuracy of LLMs with limited examples. However, in response\ngeneration, few-shot LLMs achieve stronger semantic alignment with ground truth\nposts.", "paper_summary_zh": "\u793e\u4ea4\u5a92\u9ad4\u8b93\u4f7f\u7528\u8005\u80fd\u5920\u52d5\u614b\u53c3\u8207\u71b1\u9580\u8a71\u984c\uff0c\u800c\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u56de\u61c9\u751f\u6210\u65b9\u9762\u7684\u6f5b\u529b\u3002\u5118\u7ba1\u6709\u4e9b\u7814\u7a76\u5c07 LLM \u8996\u70ba\u6a21\u64ec\u793e\u4ea4\u5a92\u9ad4\u4f7f\u7528\u8005\u884c\u70ba\u7684\u4ee3\u7406\uff0c\u4f46\u5176\u91cd\u9ede\u4ecd\u653e\u5728\u5be6\u52d9\u53ef\u884c\u6027\u548c\u53ef\u64f4\u5145\u6027\uff0c\u800c\u975e\u6df1\u5165\u4e86\u89e3 LLM \u5982\u4f55\u8207\u4eba\u985e\u884c\u70ba\u76f8\u7b26\u3002\u672c\u6587\u5206\u6790\u4e86 LLM \u900f\u904e\u52d5\u4f5c\u5f15\u5c0e\u56de\u61c9\u751f\u6210\u4f86\u6a21\u64ec\u793e\u4ea4\u5a92\u9ad4\u53c3\u8207\u7684\u80fd\u529b\uff0c\u5176\u4e2d\u4e00\u500b\u6a21\u578b\u9996\u5148\u9810\u6e2c\u4f7f\u7528\u8005\u6700\u6709\u53ef\u80fd\u7684\u53c3\u8207\u52d5\u4f5c\uff08\u8f49\u63a8\u3001\u5f15\u7528\u6216\u6539\u5beb\uff09\u5c0d\u71b1\u9580\u8cbc\u6587\u7684\u53c3\u8207\uff0c\u7136\u5f8c\u6839\u64da\u9810\u6e2c\u7684\u52d5\u4f5c\u7522\u751f\u500b\u4eba\u5316\u56de\u61c9\u3002\u6211\u5011\u5728 X \u4e0a\u8a0e\u8ad6\u7684\u4e00\u500b\u91cd\u5927\u793e\u6703\u4e8b\u4ef6\u4e2d\uff0c\u5c0d GPT-4o-mini\u3001O1-mini \u548c DeepSeek-R1 \u9032\u884c\u793e\u4ea4\u5a92\u9ad4\u53c3\u8207\u6a21\u64ec\u7684\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u96f6\u6b21\u5b78\u7fd2 LLM \u5728\u52d5\u4f5c\u9810\u6e2c\u65b9\u9762\u8868\u73fe\u4e0d\u5982 BERT\uff0c\u800c\u5c11\u6b21\u5b78\u7fd2\u63d0\u793a\u6700\u521d\u6703\u964d\u4f4e\u7bc4\u4f8b\u6709\u9650\u7684 LLM \u9810\u6e2c\u6e96\u78ba\u5ea6\u3002\u7136\u800c\uff0c\u5728\u56de\u61c9\u751f\u6210\u65b9\u9762\uff0c\u5c11\u6b21\u5b78\u7fd2 LLM \u8207\u771f\u5be6\u8cbc\u6587\u9054\u5230\u4e86\u66f4\u5f37\u7684\u8a9e\u7fa9\u5c0d\u9f4a\u3002", "author": "Zhongyi Qiu et.al.", "authors": "Zhongyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo", "id": "2502.12073v1", "paper_url": "http://arxiv.org/abs/2502.12073v1", "repo": "null"}}