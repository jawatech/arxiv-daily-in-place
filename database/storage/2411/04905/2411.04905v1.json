{"2411.04905": {"publish_time": "2024-11-07", "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models", "paper_summary": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u5404\u7a2e\u9818\u57df\u4e0d\u53ef\u6216\u7f3a\u7684\u7a0b\u5f0f\u78bc\uff0c\u5305\u62ec\u7a0b\u5f0f\u78bc\u751f\u6210\u3001\u63a8\u7406\u4efb\u52d9\u548c\u4ee3\u7406\u7cfb\u7d71\u3002\u96d6\u7136\u958b\u653e\u53d6\u7528\u7684\u7a0b\u5f0f\u78bc LLM \u9010\u6f38\u63a5\u8fd1\u5c08\u6709\u6a21\u578b\u7684\u6548\u80fd\u6c34\u6e96\uff0c\u4f46\u9069\u5408\u56b4\u8b39\u79d1\u5b78\u8abf\u67e5\u7684\u9ad8\u54c1\u8cea\u7a0b\u5f0f\u78bc LLM\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u5177\u6709\u53ef\u8907\u88fd\u8cc7\u6599\u8655\u7406\u7ba1\u7dda\u548c\u900f\u660e\u8a13\u7df4\u5354\u5b9a\u7684\u7a0b\u5f0f\u78bc LLM\uff0c\u4ecd\u7136\u6709\u9650\u3002\u9019\u7a2e\u7a00\u7f3a\u6027\u662f\u7531\u65bc\u5404\u7a2e\u6311\u6230\uff0c\u5305\u62ec\u8cc7\u6e90\u9650\u5236\u3001\u502b\u7406\u8003\u91cf\u4ee5\u53ca\u4fdd\u6301\u6a21\u578b\u5148\u9032\u7684\u7af6\u722d\u512a\u52e2\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 OpenCoder\uff0c\u9019\u662f\u4e00\u500b\u9802\u7d1a\u7a0b\u5f0f\u78bc LLM\uff0c\u4e0d\u50c5\u80fd\u9054\u5230\u8207\u9818\u5148\u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\uff0c\u9084\u80fd\u4f5c\u70ba\u7814\u7a76\u793e\u7fa4\u7684\u300c\u958b\u653e\u98df\u8b5c\u300d\u3002\u8207\u5927\u591a\u6578\u5148\u524d\u7684\u52aa\u529b\u4e0d\u540c\uff0c\u6211\u5011\u4e0d\u50c5\u91cb\u51fa\u6a21\u578b\u6b0a\u91cd\u548c\u63a8\u8ad6\u7a0b\u5f0f\u78bc\uff0c\u9084\u91cb\u51fa\u53ef\u8907\u88fd\u7684\u8a13\u7df4\u8cc7\u6599\u3001\u5b8c\u6574\u7684\u8cc7\u6599\u8655\u7406\u7ba1\u7dda\u3001\u56b4\u8b39\u7684\u5be6\u9a57\u6d88\u878d\u7d50\u679c\uff0c\u4ee5\u53ca\u958b\u653e\u79d1\u5b78\u7814\u7a76\u7684\u8a73\u7d30\u8a13\u7df4\u5354\u5b9a\u3002\u900f\u904e\u9019\u500b\u5168\u9762\u7684\u91cb\u51fa\uff0c\u6211\u5011\u627e\u51fa\u5efa\u7acb\u9802\u7d1a\u7a0b\u5f0f\u78bc LLM \u7684\u95dc\u9375\u8981\u7d20\uff1a(1) \u8cc7\u6599\u6e05\u7406\u7684\u6700\u4f73\u5316\u555f\u767c\u5f0f\u898f\u5247\u548c\u8cc7\u6599\u91cd\u8907\u6d88\u9664\u7684\u65b9\u6cd5\uff0c(2) \u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u7684\u6587\u5b57\u8a9e\u6599\u5eab\u7684\u53ec\u56de\uff0c\u4ee5\u53ca (3) \u9000\u706b\u548c\u76e3\u7763\u5fae\u8abf\u968e\u6bb5\u4e2d\u7684\u9ad8\u54c1\u8cea\u5408\u6210\u8cc7\u6599\u3002\u900f\u904e\u63d0\u4f9b\u9019\u7a2e\u7a0b\u5ea6\u7684\u958b\u653e\u6027\uff0c\u6211\u5011\u65e8\u5728\u64f4\u5927\u5c0d\u9802\u7d1a\u7a0b\u5f0f\u78bc LLM \u5404\u500b\u65b9\u9762\u7684\u4f7f\u7528\uff0c\u8b93 OpenCoder \u540c\u6642\u6210\u70ba\u5f37\u5927\u7684\u6a21\u578b\u548c\u958b\u653e\u57fa\u790e\uff0c\u4ee5\u52a0\u901f\u7814\u7a76\uff0c\u4e26\u4fc3\u9032\u7a0b\u5f0f\u78bc AI \u7684\u53ef\u8907\u88fd\u9032\u5c55\u3002", "author": "Siming Huang et.al.", "authors": "Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, Wei Chu", "id": "2411.04905v1", "paper_url": "http://arxiv.org/abs/2411.04905v1", "repo": "null"}}