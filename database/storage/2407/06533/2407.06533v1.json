{"2407.06533": {"publish_time": "2024-07-09", "title": "LETS-C: Leveraging Language Embedding for Time Series Classification", "paper_summary": "Recent advancements in language modeling have shown promising results when\napplied to time series data. In particular, fine-tuning pre-trained large\nlanguage models (LLMs) for time series classification tasks has achieved\nstate-of-the-art (SOTA) performance on standard benchmarks. However, these\nLLM-based models have a significant drawback due to the large model size, with\nthe number of trainable parameters in the millions. In this paper, we propose\nan alternative approach to leveraging the success of language modeling in the\ntime series domain. Instead of fine-tuning LLMs, we utilize a language\nembedding model to embed time series and then pair the embeddings with a simple\nclassification head composed of convolutional neural networks (CNN) and\nmultilayer perceptron (MLP). We conducted extensive experiments on\nwell-established time series classification benchmark datasets. We demonstrated\nLETS-C not only outperforms the current SOTA in classification accuracy but\nalso offers a lightweight solution, using only 14.5% of the trainable\nparameters on average compared to the SOTA model. Our findings suggest that\nleveraging language encoders to embed time series data, combined with a simple\nyet effective classification head, offers a promising direction for achieving\nhigh-performance time series classification while maintaining a lightweight\nmodel architecture.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u5728\u8bed\u8a00\u5efa\u6a21\u65b9\u9762\u7684\u8fdb\u6b65\u5df2\u5728\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u5c55\u73b0\u51fa\u53ef\u559c\u7684\u6210\u679c\u3002\u7279\u522b\u662f\uff0c\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u57fa\u4e8e LLM \u7684\u6a21\u578b\u7531\u4e8e\u6a21\u578b\u89c4\u6a21\u5e9e\u5927\u800c\u5b58\u5728\u4e00\u4e2a\u663e\u8457\u7684\u7f3a\u70b9\uff0c\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\u4ee5\u767e\u4e07\u8ba1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u5efa\u6a21\u5728\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7684\u6210\u529f\u3002\u6211\u4eec\u4e0d\u5fae\u8c03 LLM\uff0c\u800c\u662f\u5229\u7528\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u6765\u5d4c\u5165\u65f6\u95f4\u5e8f\u5217\uff0c\u7136\u540e\u5c06\u5d4c\u5165\u4e0e\u7531\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u548c\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u7ec4\u6210\u7684\u4e00\u4e2a\u7b80\u5355\u5206\u7c7b\u5934\u914d\u5bf9\u3002\u6211\u4eec\u5728\u6210\u719f\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u6211\u4eec\u8bc1\u660e LETS-C \u4e0d\u4ec5\u5728\u5206\u7c7b\u51c6\u786e\u5ea6\u4e0a\u4f18\u4e8e\u5f53\u524d\u7684 SOTA\uff0c\u800c\u4e14\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0e SOTA \u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747\u53ea\u4f7f\u7528\u4e86 14.5% \u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u8bed\u8a00\u7f16\u7801\u5668\u6765\u5d4c\u5165\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5206\u7c7b\u5934\uff0c\u4e3a\u5b9e\u73b0\u9ad8\u6027\u80fd\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002</paragraph>", "author": "Rachneet Kaur et.al.", "authors": "Rachneet Kaur, Zhen Zeng, Tucker Balch, Manuela Veloso", "id": "2407.06533v1", "paper_url": "http://arxiv.org/abs/2407.06533v1", "repo": "null"}}