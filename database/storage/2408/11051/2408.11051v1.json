{"2408.11051": {"publish_time": "2024-08-20", "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments", "paper_summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u8996\u89ba\u548c\u8a9e\u8a00\u5c0e\u822a (VLN) \u4efb\u52d9\u4e2d\u5c55\u73fe\u6f5b\u529b\uff0c\u4f46\u76ee\u524d\u7684\u61c9\u7528\u9762\u81e8\u6311\u6230\u3002\u96d6\u7136 LLM \u5728\u4e00\u822c\u5c0d\u8a71\u5834\u666f\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b83\u5011\u5728\u5c08\u696d\u5c0e\u822a\u4efb\u52d9\u4e2d\u537b\u986f\u5f97\u5403\u529b\uff0c\u8207\u5c08\u696d VLN \u6a21\u578b\u76f8\u6bd4\uff0c\u7522\u751f\u7684\u6548\u80fd\u4e26\u4e0d\u7406\u60f3\u3002\u6211\u5011\u5f15\u5165\u4e86 FLAME\uff08FLAMingo-Architected Embodied Agent\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u591a\u6a21\u614b LLM \u57fa\u790e\u4ee3\u7406\u548c\u67b6\u69cb\uff0c\u5c08\u70ba\u6709\u6548\u8655\u7406\u591a\u91cd\u89c0\u5bdf\u7684\u57ce\u5e02 VLN \u4efb\u52d9\u800c\u8a2d\u8a08\u3002\u6211\u5011\u7684\u505a\u6cd5\u5be6\u65bd\u4e86\u4e00\u500b\u4e09\u968e\u6bb5\u8abf\u6574\u6280\u8853\uff0c\u4ee5\u6709\u6548\u9069\u61c9\u5c0e\u822a\u4efb\u52d9\uff0c\u5305\u62ec\u91dd\u5c0d\u8857\u666f\u63cf\u8ff0\u7684\u55ae\u4e00\u611f\u77e5\u8abf\u6574\u3001\u91dd\u5c0d\u8ecc\u8de1\u6458\u8981\u7684\u591a\u91cd\u611f\u77e5\u8abf\u6574\uff0c\u4ee5\u53ca\u91dd\u5c0d VLN \u8cc7\u6599\u96c6\u7684\u7aef\u5c0d\u7aef\u8a13\u7df4\u3002\u589e\u5f37\u7684\u8cc7\u6599\u96c6\u6703\u81ea\u52d5\u5408\u6210\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 FLAME \u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u5728 Touchdown \u8cc7\u6599\u96c6\u4e0a\u4efb\u52d9\u5b8c\u6210\u7387\u63d0\u9ad8\u4e86 7.3%\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002\u9019\u9805\u5de5\u4f5c\u5c55\u793a\u4e86\u591a\u6a21\u614b LLM (MLLM) \u5728\u8907\u96dc\u5c0e\u822a\u4efb\u52d9\u4e2d\u7684\u6f5b\u529b\uff0c\u4ee3\u8868\u4e86 MLLM \u5728\u5177\u8eab AI \u4e2d\u5be6\u969b\u61c9\u7528\u7684\u4e00\u9805\u9032\u5c55\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://flame-sjtu.github.io", "author": "Yunzhe Xu et.al.", "authors": "Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang", "id": "2408.11051v1", "paper_url": "http://arxiv.org/abs/2408.11051v1", "repo": "null"}}