{"2408.08313": {"publish_time": "2024-08-15", "title": "Can Large Language Models Understand Symbolic Graphics Programs?", "paper_summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.", "paper_summary_zh": "<paragraph>\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u529f\u80fd\u901a\u5e38\u5177\u6709\u6311\u6230\u6027\uff0c\u90e8\u5206\u539f\u56e0\u662f\u96e3\u4ee5\u627e\u5230\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u672a\u63a5\u89f8\u904e\u7684\u4efb\u52d9\u3002\u6211\u5011\u900f\u904e\u5c08\u6ce8\u65bc\u7b26\u865f\u5716\u5f62\u7a0b\u5f0f\u9019\u500b\u65b0\u4efb\u52d9\u4f86\u89e3\u6c7a\u6b64\u6311\u6230\uff0c\u7b26\u865f\u5716\u5f62\u7a0b\u5f0f\u662f\u5716\u5f62\u5167\u5bb9\u7684\u71b1\u9580\u8868\u793a\u5f62\u5f0f\uff0c\u53ef\u5faa\u5e8f\u7522\u751f\u8996\u89ba\u8cc7\u6599\u3002LLM \u5df2\u5728\u7a0b\u5f0f\u5408\u6210\u65b9\u9762\u5c55\u73fe\u4ee4\u4eba\u8208\u596e\u7684\u524d\u666f\uff0c\u4f46\u5b83\u5011\u662f\u5426\u4e86\u89e3\u7b26\u865f\u5716\u5f62\u7a0b\u5f0f\uff1f\u8207\u50b3\u7d71\u7a0b\u5f0f\u4e0d\u540c\uff0c\u7b26\u865f\u5716\u5f62\u7a0b\u5f0f\u53ef\u4ee5\u8f49\u63db\u70ba\u5716\u5f62\u5167\u5bb9\u3002\u5728\u6b64\uff0c\u6211\u5011\u6839\u64da LLM \u56de\u7b54\u8207\u5716\u5f62\u5167\u5bb9\u76f8\u95dc\u554f\u984c\u7684\u80fd\u529b\uff0c\u4f86\u63cf\u8ff0\u5176\u5c0d\u7b26\u865f\u7a0b\u5f0f\u7684\u7406\u89e3\u3002\u9019\u9805\u4efb\u52d9\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u50c5\u5f9e\u7b26\u865f\u7a0b\u5f0f\u4e2d\u5f88\u96e3\u56de\u7b54\u9019\u4e9b\u554f\u984c\uff0c\u4f46\u5f9e\u5c0d\u61c9\u7684\u5716\u5f62\u5167\u5bb9\u4e2d\u5f88\u5bb9\u6613\u5c31\u80fd\u56de\u7b54\uff0c\u6211\u5011\u900f\u904e\u4eba\u9ad4\u5be6\u9a57\u9a57\u8b49\u4e86\u9019\u4e00\u9ede\u3002\u82e5\u8981\u4e86\u89e3\u7b26\u865f\u7a0b\u5f0f\uff0cLLM \u53ef\u80fd\u9700\u8981\u5177\u5099\u60f3\u50cf\u5c0d\u61c9\u5716\u5f62\u5167\u5bb9\u5916\u89c0\u7684\u80fd\u529b\uff0c\u800c\u7121\u9700\u76f4\u63a5\u5b58\u53d6\u5df2\u6e32\u67d3\u7684\u8996\u89ba\u5167\u5bb9\u3002\u6211\u5011\u4f7f\u7528\u6b64\u4efb\u52d9\u900f\u904e\u70ba\u7b26\u865f\u5716\u5f62\u7a0b\u5f0f\u7684\u8a9e\u610f\u7406\u89e3\u5efa\u7acb\u5927\u578b\u57fa\u6e96\uff0c\u4f86\u8a55\u4f30 LLM\u3002\u6b64\u57fa\u6e96\u662f\u900f\u904e\u7a0b\u5f0f\u5716\u5f62\u5c0d\u61c9\u5efa\u7acb\uff0c\u56e0\u6b64\u9700\u8981\u6700\u5c11\u7684\u4eba\u529b\u3002\u6211\u5011\u5728\u57fa\u6e96\u4e0a\u8a55\u4f30\u76ee\u524d\u7684 LLM\uff0c\u4ee5\u95e1\u660e\u5176\u5f9e\u7a0b\u5f0f\u4e2d\u63a8\u8ad6\u8996\u89ba\u5834\u666f\u7684\u80fd\u529b\u7684\u521d\u6b65\u8a55\u4f30\u3002\u6211\u5011\u767c\u73fe\u6b64\u4efb\u52d9\u5340\u5206\u4e86\u73fe\u6709\u7684 LLM\uff0c\u4e26\u4e14\u88ab\u8a8d\u70ba\u64c5\u9577\u63a8\u7406\u7684\u6a21\u578b\u8868\u73fe\u5f97\u66f4\u597d\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u7b26\u865f\u6307\u4ee4\u8abf\u6574 (SIT) \u4f86\u63d0\u5347\u6b64\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528\u7b26\u865f\u7a0b\u5f0f\u7522\u751f\u7684\u554f\u984c\u548c\u5f71\u50cf\u67e5\u8a62 GPT4-o\u3002\u6b64\u985e\u8cc7\u6599\u96a8\u5f8c\u7528\u65bc\u5fae\u8abf LLM\u3002\u6211\u5011\u4e5f\u767c\u73fe SIT \u8cc7\u6599\u53ef\u4ee5\u63d0\u5347 LLM \u7684\u4e00\u822c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</paragraph>", "author": "Zeju Qiu et.al.", "authors": "Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Sch\u00f6lkopf", "id": "2408.08313v1", "paper_url": "http://arxiv.org/abs/2408.08313v1", "repo": "null"}}