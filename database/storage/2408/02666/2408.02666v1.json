{"2408.02666": {"publish_time": "2024-08-05", "title": "Self-Taught Evaluators", "paper_summary": "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.", "paper_summary_zh": "\u57fa\u65bc\u6a21\u578b\u7684\u8a55\u4f30\u662f\u6210\u529f\u6a21\u578b\u958b\u767c\u7684\u6838\u5fc3\uff0c\u4f5c\u70ba\u8a13\u7df4\u7684\u734e\u52f5\u6a21\u578b\uff0c\u4ee5\u53ca\u4f5c\u70ba\u4eba\u5de5\u8a55\u4f30\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u70ba\u4e86\u8a13\u7df4\u6b64\u985e\u8a55\u4f30\u5668\uff0c\u6a19\u6e96\u65b9\u6cd5\u662f\u6536\u96c6\u5927\u91cf\u4eba\u985e\u504f\u597d\u5224\u65b7\uff0c\u4ee5\u5c0d\u6a21\u578b\u53cd\u61c9\u9032\u884c\u8a55\u4f30\uff0c\u9019\u65e2\u6602\u8cb4\uff0c\u800c\u4e14\u96a8\u8457\u6a21\u578b\u7684\u6539\u9032\uff0c\u6578\u64da\u4e5f\u6703\u8b8a\u5f97\u9673\u820a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65e8\u5728\u5728\u6c92\u6709\u4eba\u5de5\u8a3b\u89e3\u7684\u60c5\u6cc1\u4e0b\u6539\u9032\u8a55\u4f30\u5668\u7684\u65b9\u6cd5\uff0c\u50c5\u4f7f\u7528\u5408\u6210\u8a13\u7df4\u6578\u64da\u3002\u5f9e\u672a\u6a19\u8a18\u7684\u8aaa\u660e\u958b\u59cb\uff0c\u6211\u5011\u7684\u8fed\u4ee3\u5f0f\u81ea\u6211\u6539\u9032\u65b9\u6848\u6703\u7522\u751f\u5c0d\u6bd4\u7684\u6a21\u578b\u8f38\u51fa\uff0c\u4e26\u8a13\u7df4 LLM-as-a-Judge \u4ee5\u7522\u751f\u63a8\u7406\u8ffd\u8e64\u548c\u6700\u7d42\u5224\u65b7\uff0c\u5728\u6bcf\u6b21\u65b0\u7684\u8fed\u4ee3\u4e2d\u4f7f\u7528\u6539\u9032\u7684\u9810\u6e2c\u91cd\u8907\u6b64\u8a13\u7df4\u3002\u5728\u6c92\u6709\u4efb\u4f55\u6a19\u8a18\u7684\u504f\u597d\u6578\u64da\u7684\u60c5\u6cc1\u4e0b\uff0c\u6211\u5011\u7684\u81ea\u5b78\u8a55\u4f30\u5668\u53ef\u4ee5\u5c07\u5f37\u5927\u7684 LLM\uff08Llama3-70B-Instruct\uff09\u5f9e RewardBench \u4e0a\u7684 75.4 \u63d0\u5347\u5230 88.3\uff08\u591a\u6578\u6295\u7968\u70ba 88.7\uff09\u3002\u9019\u512a\u65bc\u5e38\u7528\u7684 LLM \u8a55\u5be9\uff0c\u4f8b\u5982 GPT-4\uff0c\u4e26\u4e14\u8207\u4f7f\u7528\u6a19\u8a18\u7bc4\u4f8b\u8a13\u7df4\u7684\u6548\u80fd\u6700\u4f73\u7684\u734e\u52f5\u6a21\u578b\u7684\u6548\u80fd\u76f8\u5339\u914d\u3002", "author": "Tianlu Wang et.al.", "authors": "Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li", "id": "2408.02666v1", "paper_url": "http://arxiv.org/abs/2408.02666v1", "repo": "null"}}