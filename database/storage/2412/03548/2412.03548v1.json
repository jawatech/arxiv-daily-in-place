{"2412.03548": {"publish_time": "2024-12-04", "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models", "paper_summary": "Multimodal language models (MLMs) still face challenges in fundamental visual\nperception tasks where specialized models excel. Tasks requiring reasoning\nabout 3D structures benefit from depth estimation, and reasoning about 2D\nobject instances benefits from object detection. Yet, MLMs can not produce\nintermediate depth or boxes to reason over. Finetuning MLMs on relevant data\ndoesn't generalize well and outsourcing computation to specialized vision tools\nis too compute-intensive and memory-inefficient. To address this, we introduce\nPerception Tokens, intrinsic image representations designed to assist reasoning\ntasks where language is insufficient. Perception tokens act as auxiliary\nreasoning tokens, akin to chain-of-thought prompts in language models. For\nexample, in a depth-related task, an MLM augmented with perception tokens can\nreason by generating a depth map as tokens, enabling it to solve the problem\neffectively. We propose AURORA, a training method that augments MLMs with\nperception tokens for improved reasoning over visual inputs. AURORA leverages a\nVQVAE to transform intermediate image representations, such as depth maps into\na tokenized format and bounding box tokens, which is then used in a multi-task\ntraining framework. AURORA achieves notable improvements across counting\nbenchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench,\noutperforming finetuning approaches in generalization across datasets. It also\nimproves on relative depth: over +6% on BLINK. With perception tokens, AURORA\nexpands the scope of MLMs beyond language-based reasoning, paving the way for\nmore effective visual reasoning capabilities.", "paper_summary_zh": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b (MLM) \u5728\u4e13\u95e8\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u7684\u57fa\u672c\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e2d\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u5bf9 3D \u7ed3\u6784\u8fdb\u884c\u63a8\u7406\u7684\u4efb\u52a1\u53d7\u76ca\u4e8e\u6df1\u5ea6\u4f30\u8ba1\uff0c\u800c\u5bf9 2D \u5bf9\u8c61\u5b9e\u4f8b\u8fdb\u884c\u63a8\u7406\u53d7\u76ca\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u3002\u7136\u800c\uff0cMLM \u65e0\u6cd5\u4ea7\u751f\u4e2d\u95f4\u6df1\u5ea6\u6216\u6846\u6765\u8fdb\u884c\u63a8\u7406\u3002\u9488\u5bf9\u76f8\u5173\u6570\u636e\u5fae\u8c03 MLM \u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5c06\u8ba1\u7b97\u5916\u5305\u7ed9\u4e13\u95e8\u7684\u89c6\u89c9\u5de5\u5177\u5219\u8fc7\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u611f\u77e5\u6807\u8bb0\uff0c\u8fd9\u662f\u65e8\u5728\u8f85\u52a9\u8bed\u8a00\u4e0d\u8db3\u7684\u63a8\u7406\u4efb\u52a1\u7684\u5185\u5728\u56fe\u50cf\u8868\u793a\u3002\u611f\u77e5\u6807\u8bb0\u5145\u5f53\u8f85\u52a9\u63a8\u7406\u6807\u8bb0\uff0c\u7c7b\u4f3c\u4e8e\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u601d\u60f3\u94fe\u63d0\u793a\u3002\u4f8b\u5982\uff0c\u5728\u4e0e\u6df1\u5ea6\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u611f\u77e5\u6807\u8bb0\u589e\u5f3a\u540e\u7684 MLM \u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u6df1\u5ea6\u56fe\u4f5c\u4e3a\u6807\u8bb0\u6765\u8fdb\u884c\u63a8\u7406\uff0c\u4ece\u800c\u6709\u6548\u5730\u89e3\u51b3\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86 AURORA\uff0c\u8fd9\u662f\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u611f\u77e5\u6807\u8bb0\u589e\u5f3a MLM\uff0c\u4ee5\u6539\u8fdb\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u63a8\u7406\u3002AURORA \u5229\u7528 VQVAE \u5c06\u4e2d\u95f4\u56fe\u50cf\u8868\u793a\uff08\u4f8b\u5982\u6df1\u5ea6\u56fe\uff09\u8f6c\u6362\u4e3a\u6807\u8bb0\u5316\u683c\u5f0f\u548c\u8fb9\u754c\u6846\u6807\u8bb0\uff0c\u7136\u540e\u5728\u591a\u4efb\u52a1\u8bad\u7ec3\u6846\u67b6\u4e2d\u4f7f\u7528\u3002AURORA \u5728\u8ba1\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\uff1aBLINK \u63d0\u9ad8\u4e86 +10.8%\uff0cCVBench \u63d0\u9ad8\u4e86 +11.3%\uff0cSEED-Bench \u63d0\u9ad8\u4e86 +8.3%\uff0c\u5728\u6570\u636e\u96c6\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u5fae\u8c03\u65b9\u6cd5\u3002\u5b83\u8fd8\u6539\u8fdb\u4e86\u76f8\u5bf9\u6df1\u5ea6\uff1aBLINK \u63d0\u9ad8\u4e86 +6% \u4ee5\u4e0a\u3002\u6709\u4e86\u611f\u77e5\u6807\u8bb0\uff0cAURORA \u5c06 MLM \u7684\u8303\u56f4\u6269\u5c55\u5230\u4e86\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u4e4b\u5916\uff0c\u4e3a\u66f4\u6709\u6548\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u94fa\u5e73\u4e86\u9053\u8def\u3002", "author": "Mahtab Bigverdi et.al.", "authors": "Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G. Shapiro, Ranjay Krishna", "id": "2412.03548v1", "paper_url": "http://arxiv.org/abs/2412.03548v1", "repo": "null"}}