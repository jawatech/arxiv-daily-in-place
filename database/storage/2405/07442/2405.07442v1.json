{"2405.07442": {"publish_time": "2024-05-13", "title": "Rene: A Pre-trained Multi-modal Architecture for Auscultation of Respiratory Diseases", "paper_summary": "This study presents a novel methodology utilizing a pre-trained speech\nrecognition model for processing respiratory sound data. By incorporating\nmedical record information, we introduce an innovative multi-modal\ndeep-learning architecture, named Rene, which addresses the challenges of poor\ninterpretability and underperformance in real-time clinical diagnostic response\nobserved in previous respiratory disease-focused models. The proposed Rene\narchitecture demonstrated significant improvements of 10.24%, 16.15%, 15.29%,\nand 18.90% respectively, compared to the baseline across four tasks related to\nrespiratory event detection and audio record classification on the SPRSound\ndatabase. In patient disease prediction tests on the ICBHI database, the\narchitecture exhibited improvements of 23% in the mean of average score and\nharmonic score compared to the baseline. Furthermore, we developed a real-time\nrespiratory sound discrimination system based on the Rene architecture,\nfeaturing a dual-thread design and compressed model parameters for simultaneous\nmicrophone recording and real-time dynamic decoding. Employing state-of-the-art\nEdge AI technology, this system enables rapid and accurate responses for\nrespiratory sound auscultation, facilitating deployment on wearable clinical\ndetection devices to capture incremental data, which can be synergistically\nevolved with large-scale models deployed on cloud servers for downstream tasks.", "paper_summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u9810\u8a13\u7df4\u7684\u8a9e\u97f3\u8b58\u5225\u6a21\u578b\u4f86\u8655\u7406\u547c\u5438\u97f3\u6578\u64da\u3002\u901a\u904e\u6574\u5408\u75c5\u6b77\u4fe1\u606f\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u591a\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\uff0c\u540d\u70ba Rene\uff0c\u5b83\u61c9\u5c0d\u4e86\u5728\u5148\u524d\u7684\u547c\u5438\u9053\u75be\u75c5\u6a21\u578b\u4e2d\u89c0\u5bdf\u5230\u7684\u5be6\u6642\u81e8\u5e8a\u8a3a\u65b7\u97ff\u61c9\u4e2d\u53ef\u89e3\u91cb\u6027\u5dee\u548c\u6027\u80fd\u4e0d\u4f73\u7684\u6311\u6230\u3002\u8207 SPRSound \u6578\u64da\u5eab\u4e2d\u8207\u547c\u5438\u4e8b\u4ef6\u6aa2\u6e2c\u548c\u97f3\u983b\u8a18\u9304\u5206\u985e\u76f8\u95dc\u7684\u56db\u9805\u4efb\u52d9\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684 Rene \u67b6\u69cb\u5206\u5225\u5c55\u793a\u4e86 10.24%\u300116.15%\u300115.29% \u548c 18.90% \u7684\u986f\u8457\u6539\u9032\u3002\u5728 ICBHI \u6578\u64da\u5eab\u4e0a\u7684\u60a3\u8005\u75be\u75c5\u9810\u6e2c\u6e2c\u8a66\u4e2d\uff0c\u8207\u57fa\u6e96\u76f8\u6bd4\uff0c\u8a72\u67b6\u69cb\u5728\u5e73\u5747\u5206\u548c\u8ae7\u6ce2\u5206\u4e0a\u8868\u73fe\u51fa 23% \u7684\u6539\u9032\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u57fa\u65bc Rene \u67b6\u69cb\u7684\u5be6\u6642\u547c\u5438\u97f3\u8b58\u5225\u7cfb\u7d71\uff0c\u5177\u6709\u96d9\u7dda\u7a0b\u8a2d\u8a08\u548c\u58d3\u7e2e\u6a21\u578b\u53c3\u6578\uff0c\u7528\u65bc\u540c\u6642\u9ea5\u514b\u98a8\u9304\u97f3\u548c\u5be6\u6642\u52d5\u614b\u89e3\u78bc\u3002\u63a1\u7528\u6700\u5148\u9032\u7684 Edge AI \u6280\u8853\uff0c\u8a72\u7cfb\u7d71\u80fd\u5920\u5c0d\u547c\u5438\u97f3\u807d\u8a3a\u9032\u884c\u5feb\u901f\u6e96\u78ba\u7684\u97ff\u61c9\uff0c\u4fc3\u9032\u5728\u53ef\u7a7f\u6234\u5f0f\u81e8\u5e8a\u6aa2\u6e2c\u8a2d\u5099\u4e0a\u90e8\u7f72\u4ee5\u6355\u7372\u589e\u91cf\u6578\u64da\uff0c\u9019\u4e9b\u6578\u64da\u53ef\u4ee5\u8207\u90e8\u7f72\u5728\u96f2\u670d\u52d9\u5668\u4e0a\u7684\u5927\u898f\u6a21\u6a21\u578b\u5354\u540c\u9032\u5316\uff0c\u4ee5\u57f7\u884c\u4e0b\u6e38\u4efb\u52d9\u3002", "author": "Pengfei Zhang et.al.", "authors": "Pengfei Zhang, Zhihang Zheng, Shichen Zhang, Minghao Yang, Shaojun Tang", "id": "2405.07442v1", "paper_url": "http://arxiv.org/abs/2405.07442v1", "repo": "null"}}