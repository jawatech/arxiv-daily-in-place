{"2405.05496": {"publish_time": "2024-05-09", "title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "paper_summary": "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment\nanalysis, which aims to extract the aspects and predict their sentiments. Most\nexisting studies focus on improving the performance of the target domain by\nfine-tuning domain-specific models (trained on source domains) based on the\ntarget domain dataset. Few works propose continual learning tasks for ABSA,\nwhich aim to learn the target domain's ability while maintaining the history\ndomains' abilities. In this paper, we propose a Large Language Model-based\nContinual Learning (\\texttt{LLM-CL}) model for ABSA. First, we design a domain\nknowledge decoupling module to learn a domain-invariant adapter and separate\ndomain-variant adapters dependently with an orthogonal constraint. Then, we\nintroduce a domain knowledge warmup strategy to align the representation\nbetween domain-invariant and domain-variant knowledge. In the test phase, we\nindex the corresponding domain-variant knowledge via domain positioning to not\nrequire each sample's domain ID. Extensive experiments over 19 datasets\nindicate that our \\texttt{LLM-CL} model obtains new state-of-the-art\nperformance.", "paper_summary_zh": "\u57fa\u65bc\u9762\u5411\u65b9\u9762\u7684\u89c0\u9ede\u5206\u6790 (ABSA) \u662f\u89c0\u9ede\u5206\u6790\u4e2d\u4e00\u500b\u91cd\u8981\u7684\u5b50\u4efb\u52d9\uff0c\u65e8\u5728\u8403\u53d6\u9762\u5411\u4e26\u9810\u6e2c\u5176\u89c0\u9ede\u3002\u5927\u591a\u6578\u73fe\u6709\u7814\u7a76\u5c08\u6ce8\u65bc\u900f\u904e\u6839\u64da\u76ee\u6a19\u9818\u57df\u8cc7\u6599\u96c6\u5fae\u8abf\u7279\u5b9a\u9818\u57df\u6a21\u578b\uff08\u5728\u4f86\u6e90\u9818\u57df\u4e2d\u8a13\u7df4\uff09\u4f86\u63d0\u5347\u76ee\u6a19\u9818\u57df\u7684\u6548\u80fd\u3002\u5f88\u5c11\u6709\u7814\u7a76\u63d0\u51fa ABSA \u7684\u6301\u7e8c\u5b78\u7fd2\u4efb\u52d9\uff0c\u5176\u76ee\u6a19\u662f\u5728\u7dad\u6301\u6b77\u53f2\u9818\u57df\u80fd\u529b\u7684\u540c\u6642\u5b78\u7fd2\u76ee\u6a19\u9818\u57df\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u6301\u7e8c\u5b78\u7fd2 (LLM-CL) \u6a21\u578b\uff0c\u7528\u65bc ABSA\u3002\u9996\u5148\uff0c\u6211\u5011\u8a2d\u8a08\u4e00\u500b\u9818\u57df\u77e5\u8b58\u89e3\u8026\u6a21\u7d44\uff0c\u4ee5\u5b78\u7fd2\u4e00\u500b\u4e0d\u8b8a\u9818\u57df\u7684\u9069\u914d\u5668\uff0c\u4e26\u900f\u904e\u6b63\u4ea4\u7d04\u675f\u5206\u5225\u5206\u96e2\u51fa\u8b8a\u7570\u9818\u57df\u7684\u9069\u914d\u5668\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u9818\u57df\u77e5\u8b58\u71b1\u8eab\u7b56\u7565\uff0c\u4ee5\u8abf\u6574\u4e0d\u8b8a\u9818\u57df\u548c\u8b8a\u7570\u9818\u57df\u77e5\u8b58\u4e4b\u9593\u7684\u8868\u5fb5\u3002\u5728\u6e2c\u8a66\u968e\u6bb5\uff0c\u6211\u5011\u900f\u904e\u9818\u57df\u5b9a\u4f4d\u7d22\u5f15\u5c0d\u61c9\u7684\u8b8a\u7570\u9818\u57df\u77e5\u8b58\uff0c\u4ee5\u4e0d\u9700\u8981\u6bcf\u500b\u6a23\u672c\u7684\u9818\u57df ID\u3002\u5728 19 \u500b\u8cc7\u6599\u96c6\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684 LLM-CL \u6a21\u578b\u7372\u5f97\u4e86\u65b0\u7684\u6700\u5148\u9032\u6548\u80fd\u3002", "author": "Xuanwen Ding et.al.", "authors": "Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, Liang He", "id": "2405.05496v1", "paper_url": "http://arxiv.org/abs/2405.05496v1", "repo": "null"}}