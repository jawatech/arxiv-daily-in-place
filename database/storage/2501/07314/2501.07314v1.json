{"2501.07314": {"publish_time": "2025-01-13", "title": "FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering", "paper_summary": "Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area.", "paper_summary_zh": "\u8cc7\u6599\u54c1\u8cea\u5c0d\u65bc\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81f3\u95dc\u91cd\u8981\u3002\n\u50b3\u7d71\u7684\u555f\u767c\u5f0f\u7be9\u9078\u5668\u5e38\u5e38\u6703\u932f\u904e\u4f4e\u54c1\u8cea\u6587\u5b57\u6216\u932f\u8aa4\u5730\u79fb\u9664\u6709\u50f9\u503c\u7684\u5167\u5bb9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u57fa\u65bc LLM \u7684\u884c\u7d1a\u7be9\u9078\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f37\u8a13\u7df4\u8cc7\u6599\u54c1\u8cea\u3002\u6211\u5011\u4f7f\u7528 GPT-4o mini \u5728\u884c\u7d1a\u6a19\u8a18\u4e86\u4f86\u81ea FineWeb \u7684 20,000 \u4efd\u6587\u4ef6\u7bc4\u4f8b\uff0c\u8b93\u6a21\u578b\u80fd\u70ba\u4f4e\u54c1\u8cea\u884c\u5efa\u7acb\u63cf\u8ff0\u6027\u6a19\u7c64\u3002\u9019\u4e9b\u6a19\u7c64\u88ab\u5206\u70ba\u4e5d\u5927\u985e\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b DeBERTa-v3 \u5206\u985e\u5668\uff0c\u5c07\u7be9\u9078\u7e2e\u653e\u5230 FineWeb \u7684 10B-token \u5b50\u96c6\u3002\u70ba\u4e86\u6e2c\u8a66\u6211\u5011\u7be9\u9078\u7684\u5f71\u97ff\uff0c\u6211\u5011\u5728\u539f\u59cb\u8cc7\u6599\u96c6\u548c\u7be9\u9078\u5f8c\u7684\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u4e86 GPT-2 \u6a21\u578b\u3002\u7d50\u679c\u986f\u793a\uff0c\u5728\u7be9\u9078\u5f8c\u7684\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u5728 HellaSwag \u57fa\u6e96\u4e0a\u7372\u5f97\u4e86\u66f4\u9ad8\u7684\u6e96\u78ba\u5ea6\uff0c\u4e26\u66f4\u5feb\u5730\u9054\u5230\u4e86\u6548\u80fd\u76ee\u6a19\uff0c\u5373\u4f7f\u8cc7\u6599\u6e1b\u5c11\u4e86 25%\u3002\u9019\u8b49\u660e\u4e86\u57fa\u65bc LLM \u7684\u884c\u7d1a\u7be9\u9078\u53ef\u4ee5\u986f\u8457\u6539\u5584\u8cc7\u6599\u54c1\u8cea\uff0c\u4e26\u63d0\u9ad8 LLM \u7684\u8a13\u7df4\u6548\u7387\u3002\u6211\u5011\u91cb\u51fa\u4e86\u6211\u5011\u54c1\u8cea\u8a3b\u89e3\u7684\u8cc7\u6599\u96c6 FinerWeb-10BT\uff0c\u4ee5\u53ca\u7a0b\u5f0f\u78bc\u5eab\uff0c\u4ee5\u652f\u6301\u6b64\u9818\u57df\u7684\u5f8c\u7e8c\u5de5\u4f5c\u3002", "author": "Erik Henriksson et.al.", "authors": "Erik Henriksson, Otto Tarkka, Filip Ginter", "id": "2501.07314v1", "paper_url": "http://arxiv.org/abs/2501.07314v1", "repo": "https://github.com/turkunlp/finerweb-10bt"}}