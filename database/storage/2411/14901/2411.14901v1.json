{"2411.14901": {"publish_time": "2024-11-22", "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos", "paper_summary": "Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u64c5\u9577\u5f9e\u5197\u9577\u7684\u6587\u5b57\u4e2d\u64f7\u53d6\u8cc7\u8a0a\uff0c\u4f46\u5176\u8996\u89ba\u8a9e\u8a00\u5c0d\u61c9\u6a21\u578b (VLM) \u5728\u8655\u7406\u9577\u9054\u4e00\u5c0f\u6642\u7684\u5f71\u7247\u6642\u6703\u9047\u5230\u56f0\u96e3\uff0c\u7279\u5225\u662f\u5728\u6642\u9593\u5b9a\u4f4d\u65b9\u9762\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u9019\u4e9b VLM \u53d7\u9650\u65bc\u5f71\u683c\u9650\u5236\uff0c\u7d93\u5e38\u907a\u5931\u7cbe\u78ba\u5b9a\u4f4d\u5ef6\u4f38\u5f71\u7247\u5167\u5bb9\u4e2d\u4e8b\u4ef6\u6240\u9700\u7684\u57fa\u672c\u6642\u9593\u7d30\u7bc0\u3002\u6211\u5011\u63d0\u51fa ReVisionLLM\uff0c\u4e00\u7a2e\u905e\u8ff4\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u5b9a\u4f4d\u9577\u9054\u4e00\u5c0f\u6642\u7684\u5f71\u7247\u4e2d\u7684\u4e8b\u4ef6\u3002\u6211\u5011\u7684\u6a21\u578b\u4ee5\u4eba\u985e\u641c\u5c0b\u7b56\u7565\u70ba\u9748\u611f\uff0c\u6700\u521d\u9396\u5b9a\u5ee3\u6cdb\u7684\u8208\u8da3\u7247\u6bb5\uff0c\u9010\u6f38\u4fee\u6b63\u5176\u7126\u9ede\u4ee5\u7cbe\u78ba\u627e\u51fa\u6642\u9593\u754c\u7dda\u3002\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u7121\u7e2b\u8655\u7406\u9577\u5ea6\u5dee\u7570\u6975\u5927\u7684\u5f71\u7247\uff0c\u5f9e\u5e7e\u5206\u9418\u5230\u5e7e\u5c0f\u6642\u4e0d\u7b49\u3002\u6211\u5011\u9084\u5f15\u5165\u4e00\u7a2e\u5206\u5c64\u8a13\u7df4\u7b56\u7565\uff0c\u5f9e\u77ed\u7247\u6bb5\u958b\u59cb\u64f7\u53d6\u4e0d\u540c\u7684\u4e8b\u4ef6\uff0c\u4e26\u9010\u6b65\u5ef6\u4f38\u5230\u66f4\u9577\u7684\u5f71\u7247\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cReVisionLLM \u662f\u7b2c\u4e00\u500b\u6709\u80fd\u529b\u5728\u9577\u9054\u4e00\u5c0f\u6642\u7684\u5f71\u7247\u4e2d\u9032\u884c\u6642\u9593\u5b9a\u4f4d\u7684 VLM\uff0c\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u5927\u5e45\u512a\u65bc\u5148\u524d\u7684\u6700\u65b0\u65b9\u6cd5\uff08\u5728 MAD \u4e0a +2.6% R1@0.1\uff09\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/Tanveer81/ReVisionLLM \u53d6\u5f97\u3002", "author": "Tanveer Hannan et.al.", "authors": "Tanveer Hannan, Md Mohaiminul Islam, Jindong Gu, Thomas Seidl, Gedas Bertasius", "id": "2411.14901v1", "paper_url": "http://arxiv.org/abs/2411.14901v1", "repo": "null"}}