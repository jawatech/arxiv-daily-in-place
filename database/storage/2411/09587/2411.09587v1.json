{"2411.09587": {"publish_time": "2024-11-14", "title": "BabyLM Challenge: Exploring the Effect of Variation Sets on Language Model Training Efficiency", "paper_summary": "While current large language models have achieved a remarkable success, their\ndata efficiency remains a challenge to overcome. Recently it has been suggested\nthat child-directed speech (CDS) can improve training data efficiency of modern\nlanguage models based on Transformer neural networks. However, it is not yet\nunderstood which specific properties of CDS are effective for training these\nmodels. In the context of the BabyLM Challenge, we focus on Variation Sets\n(VSs), sets of consecutive utterances expressing a similar intent with slightly\ndifferent words and structures, which are ubiquitous in CDS. To assess the\nimpact of VSs on training data efficiency, we augment CDS data with different\nproportions of artificial VSs and use these datasets to train an\nauto-regressive model, GPT-2. We find that the best proportion of VSs depends\non the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of\nVSs, but EWOK scores do not. Additionally, the results vary depending on\nmultiple factors such as the number of epochs and the order of utterance\npresentation. Taken together, these findings suggest that VSs can have a\nbeneficial influence on language models, while leaving room for further\ninvestigation.", "paper_summary_zh": "\u5118\u7ba1\u76ee\u524d\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u5df2\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\uff0c\u4f46\u5176\u8cc7\u6599\u6548\u7387\u4ecd\u662f\u4e00\u9805\u5f85\u514b\u670d\u7684\u6311\u6230\u3002\u6700\u8fd1\u6709\u4eba\u63d0\u51fa\uff0c\u4ee5\u5152\u7ae5\u70ba\u5c0d\u8c61\u7684\u8a9e\u8a00\uff08CDS\uff09\u53ef\u4ee5\u63d0\u5347\u57fa\u65bc Transformer \u795e\u7d93\u7db2\u8def\u7684\u73fe\u4ee3\u8a9e\u8a00\u6a21\u578b\u7684\u8a13\u7df4\u8cc7\u6599\u6548\u7387\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u672a\u4e86\u89e3 CDS \u7684\u54ea\u4e9b\u7279\u5b9a\u5c6c\u6027\u6709\u52a9\u65bc\u8a13\u7df4\u9019\u4e9b\u6a21\u578b\u3002\u5728 BabyLM \u6311\u6230\u7684\u80cc\u666f\u4e0b\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u8b8a\u7570\u96c6 (VS)\uff0c\u5373\u8868\u9054\u985e\u4f3c\u610f\u5716\u7684\u4e00\u7d44\u9023\u7e8c\u8a71\u8a9e\uff0c\u5176\u5b57\u8a5e\u548c\u7d50\u69cb\u7565\u6709\u4e0d\u540c\uff0c\u9019\u5728 CDS \u4e2d\u7121\u6240\u4e0d\u5728\u3002\u70ba\u4e86\u8a55\u4f30 VS \u5c0d\u8a13\u7df4\u8cc7\u6599\u6548\u7387\u7684\u5f71\u97ff\uff0c\u6211\u5011\u4f7f\u7528\u4e0d\u540c\u6bd4\u4f8b\u7684\u4eba\u5de5 VS \u64f4\u5145 CDS \u8cc7\u6599\uff0c\u4e26\u4f7f\u7528\u9019\u4e9b\u8cc7\u6599\u96c6\u8a13\u7df4\u81ea\u8ff4\u6b78\u6a21\u578b GPT-2\u3002\u6211\u5011\u767c\u73fe VS \u7684\u6700\u4f73\u6bd4\u4f8b\u53d6\u6c7a\u65bc\u8a55\u4f30\u57fa\u6e96\uff1aBLiMP \u548c GLUE \u5206\u6578\u53d7\u76ca\u65bc VS \u7684\u5b58\u5728\uff0c\u4f46 EWOK \u5206\u6578\u5247\u4e0d\u7136\u3002\u6b64\u5916\uff0c\u7d50\u679c\u6703\u56e0\u591a\u91cd\u56e0\u7d20\u800c\u7570\uff0c\u4f8b\u5982\u6642\u4ee3\u6578\u548c\u8a71\u8a9e\u5448\u73fe\u9806\u5e8f\u3002\u7d9c\u5408\u800c\u8a00\uff0c\u9019\u4e9b\u767c\u73fe\u8868\u660e VS \u53ef\u80fd\u5c0d\u8a9e\u8a00\u6a21\u578b\u7522\u751f\u6709\u76ca\u7684\u5f71\u97ff\uff0c\u540c\u6642\u4e5f\u70ba\u9032\u4e00\u6b65\u7684\u7814\u7a76\u7559\u4e0b\u4e86\u7a7a\u9593\u3002", "author": "Akari Haga et.al.", "authors": "Akari Haga, Akiyo Fukatsu, Miyu Oba, Arianna Bisazza, Yohei Oseki", "id": "2411.09587v1", "paper_url": "http://arxiv.org/abs/2411.09587v1", "repo": "null"}}