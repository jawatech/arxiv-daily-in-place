{"2303.12641": {"publish_time": "2023-03-22", "title": "Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models", "paper_summary": "State-of-the-art machine learning models often learn spurious correlations\nembedded in the training data. This poses risks when deploying these models for\nhigh-stake decision-making, such as in medical applications like skin cancer\ndetection. To tackle this problem, we propose Reveal to Revise (R2R), a\nframework entailing the entire eXplainable Artificial Intelligence (XAI) life\ncycle, enabling practitioners to iteratively identify, mitigate, and\n(re-)evaluate spurious model behavior with a minimal amount of human\ninteraction. In the first step (1), R2R reveals model weaknesses by finding\noutliers in attributions or through inspection of latent concepts learned by\nthe model. Secondly (2), the responsible artifacts are detected and spatially\nlocalized in the input data, which is then leveraged to (3) revise the model\nbehavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model\ncorrection, and (4) (re-)evaluate the model's performance and remaining\nsensitivity towards the artifact. Using two medical benchmark datasets for\nMelanoma detection and bone age estimation, we apply our R2R framework to VGG,\nResNet and EfficientNet architectures and thereby reveal and correct real\ndataset-intrinsic artifacts, as well as synthetic variants in a controlled\nsetting. Completing the XAI life cycle, we demonstrate multiple R2R iterations\nto mitigate different biases. Code is available on\nhttps://github.com/maxdreyer/Reveal2Revise.", "paper_summary_zh": "", "author": "Frederik Pahde et.al.", "authors": "Frederik Pahde,Maximilian Dreyer,Wojciech Samek,Sebastian Lapuschkin", "id": "2303.12641v2", "paper_url": "http://arxiv.org/abs/2303.12641v2", "repo": "https://github.com/maxdreyer/reveal2revise"}}