{"2301.02080": {"publish_time": "2023-01-05", "title": "Semantic match: Debugging feature attribution methods in XAI for healthcare", "paper_summary": "The recent spike in certified Artificial Intelligence (AI) tools for\nhealthcare has renewed the debate around adoption of this technology. One\nthread of such debate concerns Explainable AI (XAI) and its promise to render\nAI devices more transparent and trustworthy. A few voices active in the medical\nAI space have expressed concerns on the reliability of Explainable AI\ntechniques and especially feature attribution methods, questioning their use\nand inclusion in guidelines and standards. Despite valid concerns, we argue\nthat existing criticism on the viability of post-hoc local explainability\nmethods throws away the baby with the bathwater by generalizing a problem that\nis specific to image data. We begin by characterizing the problem as a lack of\nsemantic match between explanations and human understanding. To understand when\nfeature importance can be used reliably, we introduce a distinction between\nfeature importance of low- and high-level features. We argue that for data\ntypes where low-level features come endowed with a clear semantics, such as\ntabular data like Electronic Health Records (EHRs), semantic match can be\nobtained, and thus feature attribution methods can still be employed in a\nmeaningful and useful way. Finally, we sketch a procedure to test whether\nsemantic match has been achieved.", "paper_summary_zh": "", "author": "Giovanni Cin\u00e0 et.al.", "authors": "Giovanni Cin\u00e0,Tabea E. R\u00f6ber,Rob Goedhart,\u015e. \u0130lker Birbil", "id": "2301.02080v3", "paper_url": "http://arxiv.org/abs/2301.02080v3", "repo": "null"}}