{"2502.02770": {"publish_time": "2025-02-04", "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning", "paper_summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.", "paper_summary_zh": "\u5229\u7528\u6ce8\u610f\u529b\u7a00\u758f\u6027\u4f86\u52a0\u901f\u9577\u6587\u672c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e00\u76f4\u662f\u71b1\u9580\u7684\u7814\u7a76\u4e3b\u984c\u3002\u7136\u800c\uff0c\u7576\u524d\u6f14\u7b97\u6cd5\uff08\u4f8b\u5982\u7a00\u758f\u6ce8\u610f\u529b\u6216\u9375\u503c (KV) \u5feb\u53d6\u58d3\u7e2e\uff09\u5f80\u5f80\u4f7f\u7528\u56fa\u5b9a\u9810\u7b97\uff0c\u9019\u5728\u90e8\u7f72\u671f\u9593\u6703\u9020\u6210\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u5b83\u7121\u6cd5\u8003\u91cf\u771f\u5be6\u4e16\u754c\u5834\u666f\u7684\u52d5\u614b\u7279\u6027\uff0c\u5176\u4e2d\u6e96\u78ba\u5ea6\u548c\u6548\u7387\u4e4b\u9593\u7684\u6700\u4f73\u5e73\u8861\u53ef\u80fd\u5dee\u7570\u5f88\u5927\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u5c07\u9802\u90e8-$p$ \u53d6\u6a23\uff08\u6838\u53d6\u6a23\uff09\u501f\u7528\u7d66\u7a00\u758f\u6ce8\u610f\u529b\u53ef\u4ee5\u9a5a\u4eba\u5730\u5be6\u73fe\u81ea\u9069\u61c9\u9810\u7b97\u7de8\u5217\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u63d0\u51fa Twilight\uff0c\u4e00\u500b\u67b6\u69cb\uff0c\u7528\u65bc\u5c07\u81ea\u9069\u61c9\u7a00\u758f\u6027\u5e36\u5165\u4efb\u4f55\u73fe\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6f14\u7b97\u6cd5\uff0c\u800c\u4e0d\u6703\u72a7\u7272\u5176\u6e96\u78ba\u5ea6\u3002\u5be6\u8b49\u7d50\u679c\u986f\u793a\uff0cTwilight \u53ef\u4ee5\u81ea\u9069\u61c9\u5730\u526a\u9664\u6700\u591a 98% \u7684\u5197\u9918\u4ee3\u78bc\uff0c\u5c0e\u81f4\u81ea\u6ce8\u610f\u529b\u904b\u7b97\u52a0\u901f $15.4\\times$\uff0c\u4ee5\u53ca\u9577\u6587\u672c LLM \u89e3\u78bc\u7684\u7aef\u5c0d\u7aef\u6bcf\u500b\u4ee3\u78bc\u5ef6\u9072\u52a0\u901f $3.9\\times$\u3002", "author": "Chaofan Lin et.al.", "authors": "Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao", "id": "2502.02770v1", "paper_url": "http://arxiv.org/abs/2502.02770v1", "repo": "null"}}