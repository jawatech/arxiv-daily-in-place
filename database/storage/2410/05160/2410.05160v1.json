{"2410.05160": {"publish_time": "2024-10-07", "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "paper_summary": "Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.", "paper_summary_zh": "<paragraph>\u5d4c\u5165\u6a21\u578b\u5c0d\u65bc\u652f\u63f4\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\uff0c\u4f8b\u5982\u8a9e\u7fa9\u76f8\u4f3c\u6027\u3001\u8cc7\u8a0a\u6aa2\u7d22\u548c\u5206\u985e\u3002\u6700\u8fd1\uff0c\u958b\u767c\u901a\u7528\u6587\u5b57\u5d4c\u5165\u6a21\u578b\u7684\u8208\u8da3\u6fc0\u589e\uff0c\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u8de8\u4efb\u52d9\u9032\u884c\u6982\u62ec\uff08\u4f8b\u5982 MTEB\uff09\u3002\u7136\u800c\uff0c\u5118\u7ba1\u901a\u7528\u591a\u6a21\u614b\u5d4c\u5165\u6a21\u578b\u5f88\u91cd\u8981\uff0c\u4f46\u5b78\u7fd2\u9032\u5ea6\u76f8\u5c0d\u7de9\u6162\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u63a2\u8a0e\u5efa\u7acb\u80fd\u5920\u8655\u7406\u5ee3\u6cdb\u4e0b\u6e38\u4efb\u52d9\u7684\u901a\u7528\u5d4c\u5165\u7684\u53ef\u80fd\u6027\u3002\u6211\u5011\u7684\u8ca2\u737b\u6709\u5169\u65b9\u9762\uff1a(1) MMEB (\u5927\u898f\u6a21\u591a\u6a21\u614b\u5d4c\u5165\u57fa\u6e96)\uff0c\u6db5\u84cb 4 \u500b\u5143\u4efb\u52d9\uff08\u5373\u5206\u985e\u3001\u8996\u89ba\u554f\u984c\u89e3\u7b54\u3001\u591a\u6a21\u614b\u6aa2\u7d22\u548c\u8996\u89ba\u57fa\u790e\uff09\u548c 36 \u500b\u8cc7\u6599\u96c6\uff0c\u5305\u62ec 20 \u500b\u8a13\u7df4\u8cc7\u6599\u96c6\u548c 16 \u500b\u8a55\u4f30\u8cc7\u6599\u96c6\uff0c\u4ee5\u53ca (2) VLM2Vec (\u8996\u89ba\u8a9e\u8a00\u6a21\u578b -> \u5411\u91cf)\uff0c\u4e00\u7a2e\u5c0d\u6bd4\u8a13\u7df4\u6846\u67b6\uff0c\u900f\u904e\u5728 MMEB \u4e0a\u8a13\u7df4\u5c07\u4efb\u4f55\u6700\u5148\u9032\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u8f49\u63db\u70ba\u5d4c\u5165\u6a21\u578b\u3002\u8207 CLIP \u548c BLIP \u7b49\u5148\u524d\u7684\u6a21\u578b\u4e0d\u540c\uff0cVLM2Vec \u53ef\u4ee5\u8655\u7406\u4efb\u4f55\u5716\u50cf\u548c\u6587\u5b57\u7d44\u5408\uff0c\u4ee5\u6839\u64da\u4efb\u52d9\u8aaa\u660e\u7522\u751f\u56fa\u5b9a\u7dad\u5ea6\u7684\u5411\u91cf\u3002\u6211\u5011\u5728 Phi-3.5-V \u4e0a\u5efa\u7acb\u4e86\u4e00\u7cfb\u5217 VLM2Vec \u6a21\u578b\uff0c\u4e26\u5728 MMEB \u7684\u8a55\u4f30\u5206\u5272\u4e0a\u5c0d\u5b83\u5011\u9032\u884c\u8a55\u4f30\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\\model \u5728 MMEB \u7684\u5206\u4f48\u5167\u548c\u5206\u4f48\u5916\u8cc7\u6599\u96c6\u4e0a\uff0c\u6bd4\u73fe\u6709\u7684\u591a\u6a21\u614b\u5d4c\u5165\u6a21\u578b\u53d6\u5f97\u4e86 10% \u5230 20% \u7684\u7d55\u5c0d\u5e73\u5747\u6539\u9032\u3002</paragraph>", "author": "Ziyan Jiang et.al.", "authors": "Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen", "id": "2410.05160v1", "paper_url": "http://arxiv.org/abs/2410.05160v1", "repo": "null"}}