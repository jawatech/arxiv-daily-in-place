{"2501.02573": {"publish_time": "2025-01-05", "title": "LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations", "paper_summary": "The machine learning and data science community has made significant while\ndispersive progress in accelerating transformer-based large language models\n(LLMs), and one promising approach is to replace the original causal attention\nin a generative pre-trained transformer (GPT) with \\emph{exponentially decaying\ncausal linear attention}. In this paper, we present LeetDecoding, which is the\nfirst Python package that provides a large set of computation routines for this\nfundamental operator. The launch of LeetDecoding was motivated by the current\nlack of (1) clear understanding of the complexity regarding this operator, (2)\na comprehensive collection of existing computation methods (usually spread in\nseemingly unrelated fields), and (3) CUDA implementations for fast inference on\nGPU. LeetDecoding's design is easy to integrate with existing linear-attention\nLLMs, and allows for researchers to benchmark and evaluate new computation\nmethods for exponentially decaying causal linear attention. The usage of\nLeetDecoding does not require any knowledge of GPU programming and the\nunderlying complexity analysis, intentionally making LeetDecoding accessible to\nLLM practitioners. The source code of LeetDecoding is provided at\n\\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this\nGitHub repository}, and users can simply install LeetDecoding by the command\n\\texttt{pip install leet-decoding}.", "paper_summary_zh": "\u6a5f\u5668\u5b78\u7fd2\u548c\u8cc7\u6599\u79d1\u5b78\u793e\u7fa4\u5728\u52a0\u901f\u4ee5 Transformer \u70ba\u57fa\u790e\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u65b9\u9762\u53d6\u5f97\u4e86\u986f\u8457\u4f46\u5206\u6563\u7684\u9032\u5c55\uff0c\u5176\u4e2d\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\u662f\u5c07\u751f\u6210\u5f0f\u9810\u8a13\u7df4 Transformer (GPT) \u4e2d\u7684\u539f\u59cb\u56e0\u679c\u6ce8\u610f\u529b\u66ff\u63db\u70ba\u300c\u6307\u6578\u8870\u6e1b\u56e0\u679c\u7dda\u6027\u6ce8\u610f\u529b\u300d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 LeetDecoding\uff0c\u9019\u662f\u7b2c\u4e00\u500b Python \u5957\u4ef6\uff0c\u70ba\u9019\u500b\u57fa\u672c\u904b\u7b97\u5b50\u63d0\u4f9b\u4e86\u4e00\u7d44\u5927\u91cf\u7684\u904b\u7b97\u5e38\u5f0f\u3002LeetDecoding \u7684\u63a8\u51fa\u662f\u57fa\u65bc\u76ee\u524d (1) \u7f3a\u4e4f\u5c0d\u9019\u500b\u904b\u7b97\u5b50\u8907\u96dc\u5ea6\u7684\u6e05\u695a\u7406\u89e3\u3001(2) \u73fe\u6709\u904b\u7b97\u65b9\u6cd5\u7684\u5168\u9762\u6536\u96c6\uff08\u901a\u5e38\u5206\u6563\u5728\u770b\u4f3c\u4e0d\u76f8\u95dc\u7684\u9818\u57df\u4e2d\uff09\uff0c\u4ee5\u53ca (3) CUDA \u5be6\u4f5c\uff0c\u4ee5\u5728 GPU \u4e0a\u9032\u884c\u5feb\u901f\u63a8\u8ad6\u3002LeetDecoding \u7684\u8a2d\u8a08\u5f88\u5bb9\u6613\u8207\u73fe\u6709\u7684\u7dda\u6027\u6ce8\u610f\u529b LLM \u6574\u5408\uff0c\u4e26\u5141\u8a31\u7814\u7a76\u4eba\u54e1\u5c0d\u6307\u6578\u8870\u6e1b\u56e0\u679c\u7dda\u6027\u6ce8\u610f\u529b\u7684\u65b0\u904b\u7b97\u65b9\u6cd5\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u548c\u8a55\u4f30\u3002\u4f7f\u7528 LeetDecoding \u4e0d\u9700\u8981\u4efb\u4f55 GPU \u7a0b\u5f0f\u8a2d\u8a08\u548c\u57fa\u790e\u8907\u96dc\u5ea6\u5206\u6790\u7684\u77e5\u8b58\uff0c\u6709\u610f\u5730\u8b93 LLM \u5be6\u52d9\u4eba\u54e1\u53ef\u4ee5\u4f7f\u7528 LeetDecoding\u3002LeetDecoding \u7684\u539f\u59cb\u7a0b\u5f0f\u78bc\u63d0\u4f9b\u65bc \\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{\u9019\u500b GitHub \u5132\u5b58\u5eab}\uff0c\u4f7f\u7528\u8005\u53ef\u4ee5\u900f\u904e\u6307\u4ee4 \\texttt{pip install leet-decoding} \u8f15\u9b06\u5b89\u88dd LeetDecoding\u3002", "author": "Jiaping Wang et.al.", "authors": "Jiaping Wang, Simiao Zhang, Qiao-Chu He, Yifan Chen", "id": "2501.02573v1", "paper_url": "http://arxiv.org/abs/2501.02573v1", "repo": "https://github.com/computational-machine-intelligence/leetdecoding"}}