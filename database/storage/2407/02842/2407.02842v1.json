{"2407.02842": {"publish_time": "2024-07-03", "title": "MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition and Analysis", "paper_summary": "Multimodal Large Language Models (MLLM) have made significant progress in the\nfield of document analysis. Despite this, existing benchmarks typically focus\nonly on extracting text and simple layout information, neglecting the complex\ninteractions between elements in structured documents such as mind maps and\nflowcharts. To address this issue, we introduce the new benchmark named\nMindBench, which not only includes meticulously constructed bilingual authentic\nor synthetic images, detailed annotations, evaluation metrics and baseline\nmodels, but also specifically designs five types of structured understanding\nand parsing tasks. These tasks include full parsing, partial parsing,\nposition-related parsing, structured Visual Question Answering (VQA), and\nposition-related VQA, covering key areas such as text recognition, spatial\nawareness, relationship discernment, and structured parsing. Extensive\nexperimental results demonstrate the substantial potential and significant room\nfor improvement in current models' ability to handle structured document\ninformation. We anticipate that the launch of MindBench will significantly\nadvance research and application development in structured document analysis\ntechnology. MindBench is available at:\nhttps://miasanlei.github.io/MindBench.github.io/.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u6587\u4ef6\u5206\u6790\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u73b0\u6709\u7684\u57fa\u51c6\u901a\u5e38\u53ea\u4e13\u6ce8\u4e8e\u63d0\u53d6\u6587\u672c\u548c\u7b80\u5355\u7684\u5e03\u5c40\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u7ed3\u6784\u5316\u6587\u4ef6\uff08\u5982\u601d\u7ef4\u5bfc\u56fe\u548c\u6d41\u7a0b\u56fe\uff09\u4e2d\u5143\u7d20\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u540d\u4e3a MindBench \u7684\u65b0\u57fa\u51c6\uff0c\u5b83\u4e0d\u4ec5\u5305\u62ec\u7cbe\u5fc3\u6784\u5efa\u7684\u53cc\u8bed\u771f\u5b9e\u6216\u5408\u6210\u56fe\u50cf\u3001\u8be6\u7ec6\u6ce8\u91ca\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd8\u4e13\u95e8\u8bbe\u8ba1\u4e86\u4e94\u79cd\u7c7b\u578b\u7684\u7ed3\u6784\u5316\u7406\u89e3\u548c\u89e3\u6790\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u5305\u62ec\u5b8c\u5168\u89e3\u6790\u3001\u90e8\u5206\u89e3\u6790\u3001\u4f4d\u7f6e\u76f8\u5173\u89e3\u6790\u3001\u7ed3\u6784\u5316\u89c6\u89c9\u95ee\u7b54 (VQA) \u548c\u4f4d\u7f6e\u76f8\u5173 VQA\uff0c\u6db5\u76d6\u6587\u672c\u8bc6\u522b\u3001\u7a7a\u95f4\u611f\u77e5\u3001\u5173\u7cfb\u8fa8\u522b\u548c\u7ed3\u6784\u5316\u89e3\u6790\u7b49\u5173\u952e\u9886\u57df\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5f53\u524d\u6a21\u578b\u5904\u7406\u7ed3\u6784\u5316\u6587\u6863\u4fe1\u606f\u7684\u80fd\u529b\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u548c\u663e\u7740\u7684\u6539\u8fdb\u7a7a\u95f4\u3002\u6211\u4eec\u9884\u8ba1 MindBench \u7684\u63a8\u51fa\u5c06\u6781\u5927\u5730\u63a8\u8fdb\u7ed3\u6784\u5316\u6587\u6863\u5206\u6790\u6280\u672f\u7684\u7814\u7a76\u548c\u5e94\u7528\u5f00\u53d1\u3002MindBench \u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1a\nhttps://miasanlei.github.io/MindBench.github.io/\u3002", "author": "Lei Chen et.al.", "authors": "Lei Chen, Feng Yan, Yujie Zhong, Shaoxiang Chen, Zequn Jie, Lin Ma", "id": "2407.02842v1", "paper_url": "http://arxiv.org/abs/2407.02842v1", "repo": "null"}}