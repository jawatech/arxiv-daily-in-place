{"2409.07790": {"publish_time": "2024-09-12", "title": "Full-text Error Correction for Chinese Speech Recognition with Large Language Model", "paper_summary": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5728\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u4e2d\u9032\u884c\u932f\u8aa4\u6821\u6b63\u7684\u5de8\u5927\u6f5b\u529b\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u7814\u7a76\u90fd\u8457\u91cd\u65bc\u77ed\u6642\u8a9e\u97f3\u9304\u97f3\u4e2d\u7684\u8a9e\u53e5\uff0c\u800c\u9019\u662f\u76e3\u7763\u5f0f ASR \u8a13\u7df4\u4e2d\u4e3b\u8981\u7684\u8a9e\u97f3\u8cc7\u6599\u5f62\u5f0f\u3002\u672c\u6587\u63a2\u8a0e\u4e86 LLM \u5728 ASR \u7cfb\u7d71\u7522\u751f\u7684\u9577\u7bc7\u8a9e\u97f3\u9304\u97f3\u5168\u6587\u5b57\u932f\u8aa4\u6821\u6b63\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f8b\u5982\u64ad\u5ba2\u3001\u65b0\u805e\u5ee3\u64ad\u548c\u6703\u8b70\u7684\u9010\u5b57\u7a3f\u3002\u9996\u5148\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u540d\u70ba ChFT \u7684\u4e2d\u6587\u5168\u6587\u5b57\u932f\u8aa4\u6821\u6b63\u8cc7\u6599\u96c6\uff0c\u5229\u7528\u4e86\u4e00\u500b\u5305\u542b\u6587\u5b57\u8f49\u8a9e\u97f3\u5408\u6210\u3001ASR \u548c\u932f\u8aa4\u6821\u6b63\u914d\u5c0d\u8403\u53d6\u5668\u7684\u7ba1\u9053\u3002\u9019\u500b\u8cc7\u6599\u96c6\u8b93\u6211\u5011\u80fd\u5920\u8de8\u8108\u7d61\uff08\u5305\u62ec\u5168\u6587\u5b57\u548c\u7247\u6bb5\uff09\u6821\u6b63\u932f\u8aa4\uff0c\u4e26\u89e3\u6c7a\u66f4\u5ee3\u6cdb\u7684\u932f\u8aa4\u985e\u578b\uff0c\u4f8b\u5982\u6a19\u9ede\u7b26\u865f\u9084\u539f\u548c\u53cd\u5411\u6587\u5b57\u6b63\u898f\u5316\uff0c\u5f9e\u800c\u4f7f\u6821\u6b63\u904e\u7a0b\u66f4\u5168\u9762\u3002\u5176\u6b21\uff0c\u6211\u5011\u4f7f\u7528\u4e00\u5957\u4e0d\u540c\u7684\u63d0\u793a\u548c\u76ee\u6a19\u683c\u5f0f\uff0c\u91dd\u5c0d\u5efa\u69cb\u7684\u8cc7\u6599\u96c6\u5fae\u8abf\u4e00\u500b\u9810\u5148\u8a13\u7df4\u7684 LLM\uff0c\u4e26\u8a55\u4f30\u5176\u5728\u5168\u6587\u5b57\u932f\u8aa4\u6821\u6b63\u4e2d\u7684\u8868\u73fe\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u6839\u64da\u5168\u6587\u5b57\u548c\u7247\u6bb5\u8a2d\u8a08\u63d0\u793a\uff0c\u8003\u91cf\u5404\u7a2e\u8f38\u51fa\u683c\u5f0f\uff0c\u4f8b\u5982\u76f4\u63a5\u6821\u6b63\u7684\u6587\u5b57\u548c\u57fa\u65bc JSON \u7684\u932f\u8aa4\u6821\u6b63\u914d\u5c0d\u3002\u900f\u904e\u5404\u7a2e\u6e2c\u8a66\u8a2d\u5b9a\uff0c\u5305\u62ec\u540c\u8cea\u3001\u6700\u65b0\u548c\u56f0\u96e3\u7684\u6e2c\u8a66\u96c6\uff0c\u6211\u5011\u767c\u73fe\u5fae\u8abf\u5f8c\u7684 LLM \u5728\u5168\u6587\u5b57\u8a2d\u5b9a\u4e2d\u8868\u73fe\u826f\u597d\uff0c\u4e26\u63a1\u7528\u4e0d\u540c\u7684\u63d0\u793a\uff0c\u6bcf\u500b\u63d0\u793a\u90fd\u5c55\u73fe\u51fa\u81ea\u5df1\u7684\u512a\u52e2\u548c\u52a3\u52e2\u3002\u9019\u70ba\u9032\u4e00\u6b65\u7684\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u57fa\u6e96\u3002\u8a72\u8cc7\u6599\u96c6\u53ef\u5728\u7db2\u7ad9\u4e0a\u53d6\u5f97\u3002</paragraph>", "author": "Zhiyuan Tang et.al.", "authors": "Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang", "id": "2409.07790v1", "paper_url": "http://arxiv.org/abs/2409.07790v1", "repo": "null"}}