{"2411.06681": {"publish_time": "2024-11-11", "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models", "paper_summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u4f46\u7121\u7dda\u7db2\u8def\u5728\u652f\u63f4 LLM \u4e2d\u7684\u89d2\u8272\u5c1a\u672a\u88ab\u5fb9\u5e95\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7121\u7dda\u5206\u6563\u5f0f\u5c08\u5bb6\u6df7\u5408 (WDMoE) \u67b6\u69cb\uff0c\u4ee5\u5728\u57fa\u5730\u53f0 (BS) \u7684\u908a\u7de3\u4f3a\u670d\u5668\u548c\u7121\u7dda\u7db2\u8def\u4e2d\u7684\u884c\u52d5\u88dd\u7f6e\u4e0a\u5be6\u73fe LLM \u7684\u5354\u4f5c\u90e8\u7f72\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u900f\u904e\u5c07\u9598\u63a7\u7db2\u8def\u548c\u524d\u7f6e\u795e\u7d93\u7db2\u8def\u5c64\u7f6e\u65bc BS\uff0c\u540c\u6642\u5c07\u5c08\u5bb6\u7db2\u8def\u5206\u4f48\u5728\u88dd\u7f6e\u4e2d\uff0c\u4f86\u5206\u89e3 LLM \u4e2d\u7684 MoE \u5c64\u3002\u6b64\u90e8\u7f72\u5229\u7528\u4e86\u884c\u52d5\u88dd\u7f6e\u4e0a\u5c08\u5bb6\u7db2\u8def\u7684\u4e26\u884c\u63a8\u8ad6\u80fd\u529b\uff0c\u6709\u6548\u5730\u5229\u7528\u4e86\u9019\u4e9b\u88dd\u7f6e\u6709\u9650\u7684\u904b\u7b97\u548c\u5feb\u53d6\u8cc7\u6e90\u3002\u56e0\u6b64\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u57fa\u65bc WDMoE \u7684 LLM \u6548\u80fd\u6307\u6a19\uff0c\u5b83\u540c\u6642\u8003\u91cf\u4e86\u6a21\u578b\u80fd\u529b\u548c\u5ef6\u9072\u3002\u70ba\u4e86\u5728\u7dad\u6301\u7cbe\u6e96\u5ea6\u7684\u540c\u6642\u5c07\u5ef6\u9072\u964d\u81f3\u6700\u4f4e\uff0c\u6211\u5011\u6839\u64da\u6548\u80fd\u6307\u6a19\u5171\u540c\u6700\u4f73\u5316\u5c08\u5bb6\u9078\u64c7\u548c\u983b\u5bec\u914d\u7f6e\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528 NVIDIA Jetson \u5957\u4ef6\u5efa\u69cb\u786c\u9ad4\u6e2c\u8a66\u5e73\u53f0\uff0c\u4ee5\u9a57\u8b49 WDMoE \u7684\u6548\u80fd\u3002\u7406\u8ad6\u6a21\u64ec\u548c\u5be6\u969b\u786c\u9ad4\u5be6\u9a57\u90fd\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u5f71\u97ff LLM \u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\uff0c\u986f\u8457\u964d\u4f4e\u5ef6\u9072\u3002", "author": "Nan Xue et.al.", "authors": "Nan Xue, Yaping Sun, Zhiyong Chen, Meixia Tao, Xiaodong Xu, Liang Qian, Shuguang Cui, Wenjun Zhang, Ping Zhang", "id": "2411.06681v1", "paper_url": "http://arxiv.org/abs/2411.06681v1", "repo": "null"}}