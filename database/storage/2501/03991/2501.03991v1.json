{"2501.03991": {"publish_time": "2025-01-07", "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles", "paper_summary": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications.", "paper_summary_zh": "\u6821\u6e96\uff0c\u6a21\u578b\u4fe1\u5fc3\u8207\u9810\u6e2c\u6e96\u78ba\u5ea6\u4e4b\u9593\u7684\u5c0d\u9f4a\uff0c\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u53ef\u9760\u90e8\u7f72\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u8861\u91cf\u5176\u65b9\u6cd5\u5c0d\u5176\u4ed6\u63d0\u793a\u6a23\u5f0f\u548c\u4e0d\u540c\u898f\u6a21 LLM \u7684\u6982\u62ec\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5b9a\u7fa9\u4e86\u4e00\u500b\u53d7\u63a7\u5be6\u9a57\u8a2d\u5b9a\uff0c\u6db5\u84cb 12 \u500b LLM \u548c\u56db\u7a2e\u63d0\u793a\u6a23\u5f0f\u3002\u6211\u5011\u9032\u4e00\u6b65\u7814\u7a76\u4e86\u662f\u5426\u7d0d\u5165\u591a\u500b LLM \u7684\u56de\u61c9\u5354\u8b70\u548c\u9069\u7576\u7684\u640d\u5931\u51fd\u6578\u53ef\u4ee5\u6539\u5584\u6821\u6e96\u6027\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u69cb\u5efa\u4e86 Calib-n\uff0c\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u7528\u65bc\u8a13\u7df4\u4e00\u500b\u8f14\u52a9\u6a21\u578b\u4ee5\u9032\u884c\u4fe1\u5fc3\u4f30\u8a08\uff0c\u8a72\u6a21\u578b\u5f59\u7e3d\u4f86\u81ea\u591a\u500b LLM \u7684\u56de\u61c9\u4ee5\u6355\u6349\u6a21\u578b\u9593\u5354\u8b70\u3002\u70ba\u4e86\u512a\u5316\u6821\u6e96\uff0c\u6211\u5011\u6574\u5408\u4e86\u7126\u9ede\u548c AUC \u66ff\u4ee3\u640d\u5931\u4ee5\u53ca\u4e8c\u5143\u4ea4\u53c9\u71b5\u3002\u5728\u56db\u500b\u6578\u64da\u96c6\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u56de\u61c9\u5354\u8b70\u548c\u7126\u9ede\u640d\u5931\u90fd\u6539\u5584\u4e86\u57fa\u6e96\u6821\u6e96\u3002\u6211\u5011\u767c\u73fe\uff0c\u5c11\u6b21\u63d0\u793a\u5c0d\u65bc\u57fa\u65bc\u8f14\u52a9\u6a21\u578b\u7684\u65b9\u6cd5\u6700\u6709\u6548\uff0c\u4e26\u4e14\u8f14\u52a9\u6a21\u578b\u5728\u6e96\u78ba\u6027\u8b8a\u5316\u4e2d\u5c55\u793a\u4e86\u7a69\u5065\u7684\u6821\u6e96\u6027\u80fd\uff0c\u512a\u65bc LLM \u7684\u5167\u90e8\u6982\u7387\u548c\u8a00\u8a9e\u5316\u4fe1\u5fc3\u3002\u9019\u4e9b\u898b\u89e3\u52a0\u6df1\u4e86\u5c0d LLM \u6821\u6e96\u4e2d\u5f71\u97ff\u56e0\u7d20\u7684\u7406\u89e3\uff0c\u652f\u6301\u4e86\u5b83\u5011\u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u3002", "author": "Yuxi Xia et.al.", "authors": "Yuxi Xia, Pedro Henrique Luz de Araujo, Klim Zaporojets, Benjamin Roth", "id": "2501.03991v1", "paper_url": "http://arxiv.org/abs/2501.03991v1", "repo": "null"}}