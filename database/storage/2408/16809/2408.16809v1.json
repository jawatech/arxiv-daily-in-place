{"2408.16809": {"publish_time": "2024-08-29", "title": "See or Guess: Counterfactually Regularized Image Captioning", "paper_summary": "Image captioning, which generates natural language descriptions of the visual\ninformation in an image, is a crucial task in vision-language research.\nPrevious models have typically addressed this task by aligning the generative\ncapabilities of machines with human intelligence through statistical fitting of\nexisting datasets. While effective for normal images, they may struggle to\naccurately describe those where certain parts of the image are obscured or\nedited, unlike humans who excel in such cases. These weaknesses they exhibit,\nincluding hallucinations and limited interpretability, often hinder performance\nin scenarios with shifted association patterns. In this paper, we present a\ngeneric image captioning framework that employs causal inference to make\nexisting models more capable of interventional tasks, and counterfactually\nexplainable. Our approach includes two variants leveraging either total effect\nor natural direct effect. Integrating them into the training process enables\nmodels to handle counterfactual scenarios, increasing their generalizability.\nExtensive experiments on various datasets show that our method effectively\nreduces hallucinations and improves the model's faithfulness to images,\ndemonstrating high portability across both small-scale and large-scale\nimage-to-text models. The code is available at\nhttps://github.com/Aman-4-Real/See-or-Guess.", "paper_summary_zh": "\u5f71\u50cf\u6a19\u984c\uff0c\u7522\u751f\u5f71\u50cf\u4e2d\u8996\u89ba\u8cc7\u8a0a\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\uff0c\u662f\u8996\u89ba\u8a9e\u8a00\u7814\u7a76\u4e2d\u7684\u4e00\u9805\u91cd\u8981\u4efb\u52d9\u3002\n\u5148\u524d\u7684\u6a21\u578b\u901a\u5e38\u900f\u904e\u7d71\u8a08\u64ec\u5408\u73fe\u6709\u8cc7\u6599\u96c6\uff0c\u5c07\u6a5f\u5668\u751f\u6210\u80fd\u529b\u8207\u4eba\u985e\u667a\u6167\u7d50\u5408\uff0c\u4f86\u8655\u7406\u9019\u9805\u4efb\u52d9\u3002\u96d6\u7136\u5c0d\u4e00\u822c\u5f71\u50cf\u6709\u6548\uff0c\u4f46\u8207\u64c5\u9577\u6b64\u985e\u60c5\u6cc1\u7684\u4eba\u985e\u4e0d\u540c\uff0c\u9019\u4e9b\u6a21\u578b\u53ef\u80fd\u96e3\u4ee5\u7cbe\u78ba\u63cf\u8ff0\u5f71\u50cf\u4e2d\u67d0\u4e9b\u90e8\u5206\u88ab\u906e\u853d\u6216\u7de8\u8f2f\u7684\u5f71\u50cf\u3002\u9019\u4e9b\u6a21\u578b\u5c55\u73fe\u7684\u5f31\u9ede\uff0c\u5305\u62ec\u5e7b\u89ba\u548c\u6709\u9650\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u901a\u5e38\u6703\u963b\u7919\u5728\u95dc\u806f\u6a21\u5f0f\u8f49\u79fb\u7684\u60c5\u6cc1\u4e0b\u7684\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u901a\u7528\u5f71\u50cf\u6a19\u984c\u6846\u67b6\uff0c\u5b83\u63a1\u7528\u56e0\u679c\u63a8\u8ad6\uff0c\u8b93\u73fe\u6709\u6a21\u578b\u66f4\u80fd\u57f7\u884c\u4ecb\u5165\u4efb\u52d9\uff0c\u4e26\u53cd\u4e8b\u5be6\u5730\u89e3\u91cb\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u5169\u7a2e\u8b8a\u9ad4\uff0c\u5229\u7528\u7e3d\u9ad4\u6548\u61c9\u6216\u81ea\u7136\u76f4\u63a5\u6548\u61c9\u3002\u5c07\u9019\u4e9b\u8b8a\u9ad4\u6574\u5408\u5230\u8a13\u7df4\u904e\u7a0b\u4e2d\uff0c\u80fd\u8b93\u6a21\u578b\u8655\u7406\u53cd\u4e8b\u5be6\u5834\u666f\uff0c\u9032\u800c\u63d0\u5347\u6a21\u578b\u7684\u6982\u62ec\u6027\u3002\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6709\u6548\u6e1b\u5c11\u5e7b\u89ba\uff0c\u4e26\u63d0\u5347\u6a21\u578b\u5c0d\u5f71\u50cf\u7684\u5fe0\u5be6\u5ea6\uff0c\u8b49\u660e\u5728\u5c0f\u898f\u6a21\u548c\u5927\u578b\u5f71\u50cf\u8f49\u6587\u5b57\u6a21\u578b\u4e2d\u90fd\u5177\u6709\u9ad8\u5ea6\u53ef\u79fb\u690d\u6027\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/Aman-4-Real/See-or-Guess \u53d6\u5f97\u3002", "author": "Qian Cao et.al.", "authors": "Qian Cao, Xu Chen, Ruihua Song, Xiting Wang, Xinting Huang, Yuchen Ren", "id": "2408.16809v1", "paper_url": "http://arxiv.org/abs/2408.16809v1", "repo": "https://github.com/aman-4-real/see-or-guess"}}