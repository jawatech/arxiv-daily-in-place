{"2406.18495": {"publish_time": "2024-06-26", "title": "WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs", "paper_summary": "We introduce WildGuard -- an open, light-weight moderation tool for LLM\nsafety that achieves three goals: (1) identifying malicious intent in user\nprompts, (2) detecting safety risks of model responses, and (3) determining\nmodel refusal rate. Together, WildGuard serves the increasing needs for\nautomatic safety moderation and evaluation of LLM interactions, providing a\none-stop tool with enhanced accuracy and broad coverage across 13 risk\ncategories. While existing open moderation tools such as Llama-Guard2 score\nreasonably well in classifying straightforward model interactions, they lag far\nbehind a prompted GPT-4, especially in identifying adversarial jailbreaks and\nin evaluating models' refusals, a key measure for evaluating safety behaviors\nin model responses.\n  To address these challenges, we construct WildGuardMix, a large-scale and\ncarefully balanced multi-task safety moderation dataset with 92K labeled\nexamples that cover vanilla (direct) prompts and adversarial jailbreaks, paired\nwith various refusal and compliance responses. WildGuardMix is a combination of\nWildGuardTrain, the training data of WildGuard, and WildGuardTest, a\nhigh-quality human-annotated moderation test set with 5K labeled items covering\nbroad risk scenarios. Through extensive evaluations on WildGuardTest and ten\nexisting public benchmarks, we show that WildGuard establishes state-of-the-art\nperformance in open-source safety moderation across all the three tasks\ncompared to ten strong existing open-source moderation models (e.g., up to\n26.4% improvement on refusal detection). Importantly, WildGuard matches and\nsometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt\nharmfulness identification). WildGuard serves as a highly effective safety\nmoderator in an LLM interface, reducing the success rate of jailbreak attacks\nfrom 79.8% to 2.4%.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa WildGuard\u2014\u2014\u4e00\u7a2e\u958b\u653e\u3001\u8f15\u91cf\u7d1a\u7684 LLM \u5b89\u5168\u6027\u5be9\u6838\u5de5\u5177\uff0c\u53ef\u9054\u6210\u4e09\u500b\u76ee\u6a19\uff1a(1) \u8b58\u5225\u4f7f\u7528\u8005\u63d0\u793a\u4e2d\u7684\u60e1\u610f\u610f\u5716\uff0c(2) \u5075\u6e2c\u6a21\u578b\u56de\u61c9\u7684\u5b89\u5168\u6027\u98a8\u96aa\uff0c\u4ee5\u53ca (3) \u6c7a\u5b9a\u6a21\u578b\u62d2\u7d55\u7387\u3002WildGuard \u7d50\u5408\u4e86\u5c0d LLM \u4e92\u52d5\u9032\u884c\u81ea\u52d5\u5b89\u5168\u6027\u5be9\u6838\u548c\u8a55\u4f30\u65e5\u76ca\u589e\u9577\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u4e00\u7ad9\u5f0f\u5de5\u5177\uff0c\u5728 13 \u500b\u98a8\u96aa\u985e\u5225\u4e2d\u5177\u5099\u66f4\u9ad8\u7684\u6e96\u78ba\u5ea6\u548c\u5ee3\u6cdb\u7684\u6db5\u84cb\u7bc4\u570d\u3002\u5118\u7ba1\u73fe\u6709\u7684\u958b\u653e\u5f0f\u5be9\u6838\u5de5\u5177\uff08\u4f8b\u5982 Llama-Guard2\uff09\u5728\u5c0d\u76f4\u63a5\u7684\u6a21\u578b\u4e92\u52d5\u9032\u884c\u5206\u985e\u65b9\u9762\u8868\u73fe\u5f97\u76f8\u7576\u4e0d\u932f\uff0c\u4f46\u5b83\u5011\u9060\u9060\u843d\u5f8c\u65bc\u63d0\u793a\u5f0f GPT-4\uff0c\u7279\u5225\u662f\u5728\u8b58\u5225\u5c0d\u6297\u6027\u8d8a\u7344\u548c\u8a55\u4f30\u6a21\u578b\u62d2\u7d55\u65b9\u9762\uff0c\u800c\u62d2\u7d55\u662f\u8a55\u4f30\u6a21\u578b\u56de\u61c9\u4e2d\u5b89\u5168\u6027\u884c\u70ba\u7684\u4e00\u9805\u95dc\u9375\u6307\u6a19\u3002\n\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u69cb\u5efa\u4e86 WildGuardMix\uff0c\u9019\u662f\u4e00\u500b\u5927\u578b\u4e14\u7d93\u904e\u4ed4\u7d30\u5e73\u8861\u7684\u591a\u4efb\u52d9\u5b89\u5168\u6027\u5be9\u6838\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b 92K \u500b\u6a19\u8a18\u7bc4\u4f8b\uff0c\u6db5\u84cb\u4e86\u539f\u59cb (\u76f4\u63a5) \u63d0\u793a\u548c\u5c0d\u6297\u6027\u8d8a\u7344\uff0c\u4e26\u914d\u5c0d\u4e86\u5404\u7a2e\u62d2\u7d55\u548c\u670d\u5f9e\u56de\u61c9\u3002WildGuardMix \u7d50\u5408\u4e86 WildGuard \u7684\u8a13\u7df4\u8cc7\u6599 WildGuardTrain\uff0c\u4ee5\u53ca WildGuardTest\uff0c\u4e00\u500b\u9ad8\u54c1\u8cea\u7684\u4eba\u5de5\u6a19\u8a18\u5be9\u6838\u6e2c\u8a66\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6db5\u84cb\u5ee3\u6cdb\u98a8\u96aa\u60c5\u5883\u7684 5K \u500b\u6a19\u8a18\u9805\u76ee\u3002\u900f\u904e\u5c0d WildGuardTest \u548c\u5341\u500b\u73fe\u6709\u516c\u958b\u57fa\u6e96\u9032\u884c\u5ee3\u6cdb\u7684\u8a55\u4f30\uff0c\u6211\u5011\u8868\u660e WildGuard \u5728\u958b\u653e\u539f\u59cb\u78bc\u5b89\u5168\u6027\u5be9\u6838\u65b9\u9762\u5efa\u7acb\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5728\u6240\u6709\u4e09\u9805\u4efb\u52d9\u4e0a\u8207\u5341\u500b\u5f37\u5927\u7684\u73fe\u6709\u958b\u653e\u539f\u59cb\u78bc\u5be9\u6838\u6a21\u578b\u76f8\u6bd4\uff08\u4f8b\u5982\uff0c\u62d2\u7d55\u5075\u6e2c\u7684\u6539\u9032\u5e45\u5ea6\u9ad8\u9054 26.4%\uff09\u3002\u91cd\u8981\u7684\u662f\uff0cWildGuard \u5339\u914d\u4e26\u6709\u6642\u8d85\u904e GPT-4 \u7684\u6548\u80fd\uff08\u4f8b\u5982\uff0c\u63d0\u793a\u5371\u5bb3\u8b58\u5225\u7684\u6539\u9032\u5e45\u5ea6\u9ad8\u9054 3.9%\uff09\u3002WildGuard \u5728 LLM \u4ecb\u9762\u4e2d\u4f5c\u70ba\u4e00\u500b\u9ad8\u6548\u80fd\u7684\u5b89\u5168\u6027\u5be9\u6838\u5668\uff0c\u5c07\u8d8a\u7344\u653b\u64ca\u7684\u6210\u529f\u7387\u5f9e 79.8% \u964d\u4f4e\u5230 2.4%\u3002</paragraph>", "author": "Seungju Han et.al.", "authors": "Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, Nouha Dziri", "id": "2406.18495v1", "paper_url": "http://arxiv.org/abs/2406.18495v1", "repo": "null"}}