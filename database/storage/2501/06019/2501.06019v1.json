{"2501.06019": {"publish_time": "2025-01-10", "title": "BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response", "paper_summary": "Disaster events occur around the world and cause significant damage to human\nlife and property. Earth observation (EO) data enables rapid and comprehensive\nbuilding damage assessment (BDA), an essential capability in the aftermath of a\ndisaster to reduce human casualties and to inform disaster relief efforts.\nRecent research focuses on the development of AI models to achieve accurate\nmapping of unseen disaster events, mostly using optical EO data. However,\nsolutions based on optical data are limited to clear skies and daylight hours,\npreventing a prompt response to disasters. Integrating multimodal (MM) EO data,\nparticularly the combination of optical and SAR imagery, makes it possible to\nprovide all-weather, day-and-night disaster responses. Despite this potential,\nthe development of robust multimodal AI models has been constrained by the lack\nof suitable benchmark datasets. In this paper, we present a BDA dataset using\nveRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based\nall-weather disaster response. To the best of our knowledge, BRIGHT is the\nfirst open-access, globally distributed, event-diverse MM dataset specifically\ncurated to support AI-based disaster response. It covers five types of natural\ndisasters and two types of man-made disasters across 12 regions worldwide, with\na particular focus on developing countries where external assistance is most\nneeded. The optical and SAR imagery in BRIGHT, with a spatial resolution\nbetween 0.3-1 meters, provides detailed representations of individual\nbuildings, making it ideal for precise BDA. In our experiments, we have tested\nseven advanced AI models trained with our BRIGHT to validate the\ntransferability and robustness. The dataset and code are available at\nhttps://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official\ndataset for the 2025 IEEE GRSS Data Fusion Contest.", "paper_summary_zh": "<paragraph>\u5168\u7403\u5404\u5730\u90fd\u6703\u767c\u751f\u707d\u5bb3\u4e8b\u4ef6\uff0c\u5c0d\u4eba\u985e\u751f\u547d\u548c\u8ca1\u7522\u9020\u6210\u91cd\u5927\u640d\u5bb3\u3002\u5730\u7403\u89c0\u6e2c (EO) \u8cc7\u6599\u80fd\u5feb\u901f\u4e14\u5168\u9762\u5730\u9032\u884c\u5efa\u7bc9\u7269\u640d\u5bb3\u8a55\u4f30 (BDA)\uff0c\u9019\u662f\u707d\u5f8c\u6e1b\u5c11\u4eba\u54e1\u50b7\u4ea1\u548c\u63d0\u4f9b\u6551\u707d\u8cc7\u8a0a\u7684\u91cd\u8981\u80fd\u529b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u91cd\u9ede\u5728\u65bc\u958b\u767c AI \u6a21\u578b\uff0c\u4ee5\u6e96\u78ba\u7e6a\u88fd\u672a\u898b\u707d\u5bb3\u4e8b\u4ef6\u7684\u5730\u5716\uff0c\u5927\u591a\u4f7f\u7528\u5149\u5b78 EO \u8cc7\u6599\u3002\u7136\u800c\uff0c\u57fa\u65bc\u5149\u5b78\u8cc7\u6599\u7684\u89e3\u6c7a\u65b9\u6848\u50c5\u9650\u65bc\u6674\u6717\u7684\u5929\u7a7a\u548c\u767d\u5929\u6642\u6bb5\uff0c\u7121\u6cd5\u5c0d\u707d\u5bb3\u505a\u51fa\u53ca\u6642\u53cd\u61c9\u3002\u6574\u5408\u591a\u6a21\u5f0f (MM) EO \u8cc7\u6599\uff0c\u7279\u5225\u662f\u5149\u5b78\u548c SAR \u5f71\u50cf\u7684\u7d44\u5408\uff0c\u5c31\u80fd\u63d0\u4f9b\u5168\u5929\u5019\u3001\u665d\u591c\u4e0d\u9593\u65b7\u7684\u707d\u5bb3\u61c9\u8b8a\u80fd\u529b\u3002\u5118\u7ba1\u6709\u6b64\u6f5b\u529b\uff0c\u4f46\u5408\u9069\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5065\u5168\u591a\u6a21\u5f0f AI \u6a21\u578b\u7684\u767c\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u4f7f\u7528\u6975\u9ad8\u89e3\u6790\u5ea6\u5149\u5b78\u548c SAR \u5f71\u50cf (BRIGHT) \u7684 BDA \u8cc7\u6599\u96c6\uff0c\u4ee5\u652f\u63f4\u57fa\u65bc AI \u7684\u5168\u5929\u5019\u707d\u5bb3\u61c9\u8b8a\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cBRIGHT \u662f\u7b2c\u4e00\u500b\u958b\u653e\u53d6\u7528\u3001\u5168\u7403\u5206\u4f48\u3001\u4e8b\u4ef6\u591a\u6a23\u7684 MM \u8cc7\u6599\u96c6\uff0c\u5c08\u9580\u7b56\u5283\u7528\u65bc\u652f\u63f4\u57fa\u65bc AI \u7684\u707d\u5bb3\u61c9\u8b8a\u3002\u5b83\u6db5\u84cb\u4e86\u5168\u7403 12 \u500b\u5730\u5340\u7684\u4e94\u7a2e\u985e\u578b\u81ea\u7136\u707d\u5bb3\u548c\u5169\u7a2e\u4eba\u70ba\u707d\u5bb3\uff0c\u7279\u5225\u95dc\u6ce8\u6700\u9700\u8981\u5916\u90e8\u63f4\u52a9\u7684\u958b\u767c\u4e2d\u570b\u5bb6\u3002BRIGHT \u4e2d\u7684\u5149\u5b78\u548c SAR \u5f71\u50cf\u5177\u6709 0.3-1 \u516c\u5c3a\u4e4b\u9593\u7684\u7a7a\u9593\u89e3\u6790\u5ea6\uff0c\u63d0\u4f9b\u500b\u5225\u5efa\u7bc9\u7269\u7684\u8a73\u7d30\u8868\u793a\uff0c\u975e\u5e38\u9069\u5408\u7cbe\u78ba\u7684 BDA\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u6e2c\u8a66\u4e86\u4e03\u500b\u4f7f\u7528\u6211\u5011\u7684 BRIGHT \u8a13\u7df4\u7684\u9ad8\u968e AI \u6a21\u578b\uff0c\u4ee5\u9a57\u8b49\u5176\u53ef\u8f49\u79fb\u6027\u548c\u5065\u5168\u6027\u3002\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/ChenHongruixuan/BRIGHT \u53d6\u5f97\u3002BRIGHT \u4e5f\u4f5c\u70ba 2025 IEEE GRSS \u8cc7\u6599\u878d\u5408\u7af6\u8cfd\u7684\u5b98\u65b9\u8cc7\u6599\u96c6\u3002</paragraph>", "author": "Hongruixuan Chen et.al.", "authors": "Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya", "id": "2501.06019v1", "paper_url": "http://arxiv.org/abs/2501.06019v1", "repo": "https://github.com/chenhongruixuan/bright"}}