{"2405.08553": {"publish_time": "2024-05-14", "title": "Improving Transformers with Dynamically Composable Multi-Head Attention", "paper_summary": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA,\nattention heads work independently, causing problems such as low-rank\nbottleneck of attention score matrices and head redundancy. We propose\nDynamically Composable Multi-Head Attention (DCMHA), a parameter and\ncomputation efficient attention architecture that tackles the shortcomings of\nMHA and increases the expressive power of the model by dynamically composing\nattention heads. At the core of DCMHA is a $\\it{Compose}$ function that\ntransforms the attention score and weight matrices in an input-dependent way.\nDCMHA can be used as a drop-in replacement of MHA in any transformer\narchitecture to obtain the corresponding DCFormer. DCFormer significantly\noutperforms Transformer on different architectures and model scales in language\nmodeling, matching the performance of models with ~1.7x-2.0x compute. For\nexample, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining\nperplexity and downstream task evaluation. The code and models are available at\nhttps://github.com/Caiyun-AI/DCFormer.", "paper_summary_zh": "\u591a\u982d\u6ce8\u610f\u529b (MHA) \u662f Transformer \u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u3002\u5728 MHA \u4e2d\uff0c\u6ce8\u610f\u529b\u982d\u90e8\u7368\u7acb\u904b\u4f5c\uff0c\u5c0e\u81f4\u6ce8\u610f\u529b\u5206\u6578\u77e9\u9663\u7684\u4f4e\u79e9\u74f6\u9838\u548c\u982d\u90e8\u5197\u9918\u7b49\u554f\u984c\u3002\u6211\u5011\u63d0\u51fa\u52d5\u614b\u53ef\u7d44\u6210\u591a\u982d\u6ce8\u610f\u529b (DCMHA)\uff0c\u9019\u662f\u4e00\u7a2e\u53c3\u6578\u548c\u8a08\u7b97\u6548\u7387\u9ad8\u7684\u6ce8\u610f\u529b\u67b6\u69cb\uff0c\u53ef\u89e3\u6c7a MHA \u7684\u7f3a\u9ede\u4e26\u900f\u904e\u52d5\u614b\u7d44\u6210\u6ce8\u610f\u529b\u982d\u90e8\u4f86\u589e\u52a0\u6a21\u578b\u7684\u8868\u9054\u80fd\u529b\u3002DCMHA \u7684\u6838\u5fc3\u662f\u4e00\u500b $\\it{Compose}$ \u51fd\u6578\uff0c\u5b83\u4ee5\u8f38\u5165\u4f9d\u8cf4\u7684\u65b9\u5f0f\u8f49\u63db\u6ce8\u610f\u529b\u5206\u6578\u548c\u6b0a\u91cd\u77e9\u9663\u3002DCMHA \u53ef\u7528\u65bc\u53d6\u4ee3\u4efb\u4f55 Transformer \u67b6\u69cb\u4e2d\u7684 MHA\uff0c\u4ee5\u53d6\u5f97\u5c0d\u61c9\u7684 DCFormer\u3002DCFormer \u5728\u4e0d\u540c\u7684\u67b6\u69cb\u548c\u8a9e\u8a00\u5efa\u6a21\u4e2d\u7684\u6a21\u578b\u898f\u6a21\u4e0a\u986f\u8457\u512a\u65bc Transformer\uff0c\u5176\u6548\u80fd\u8207\u904b\u7b97\u91cf\u7d04\u70ba 1.7x-2.0x \u7684\u6a21\u578b\u76f8\u5339\u914d\u3002\u4f8b\u5982\uff0cDCPythia-6.9B \u5728\u9810\u8a13\u7df4\u56f0\u60d1\u5ea6\u548c\u4e0b\u6e38\u4efb\u52d9\u8a55\u4f30\u4e2d\u90fd\u512a\u65bc\u958b\u6e90 Pythia-12B\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u5728 https://github.com/Caiyun-AI/DCFormer \u53d6\u5f97\u3002", "author": "Da Xiao et.al.", "authors": "Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan", "id": "2405.08553v1", "paper_url": "http://arxiv.org/abs/2405.08553v1", "repo": "https://github.com/caiyun-ai/dcformer"}}