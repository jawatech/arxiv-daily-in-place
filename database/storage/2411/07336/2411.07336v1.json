{"2411.07336": {"publish_time": "2024-11-11", "title": "SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models", "paper_summary": "Set theory is foundational to mathematics and, when sets are finite, to\nreasoning about the world. An intelligent system should perform set operations\nconsistently, regardless of superficial variations in the operands. Initially\ndesigned for semantically-oriented NLP tasks, large language models (LLMs) are\nnow being evaluated on algorithmic tasks. Because sets are comprised of\narbitrary symbols (e.g. numbers, words), they provide an opportunity to test,\nsystematically, the invariance of LLMs' algorithmic abilities under simple\nlexical or semantic variations. To this end, we present the SetLexSem\nChallenge, a synthetic benchmark that evaluates the performance of LLMs on set\noperations. SetLexSem assesses the robustness of LLMs' instruction-following\nabilities under various conditions, focusing on the set operations and the\nnature and construction of the set members. Evaluating seven LLMs with\nSetLexSem, we find that they exhibit poor robustness to variation in both\noperation and operands. We show -- via the framework's systematic sampling of\nset members along lexical and semantic dimensions -- that LLMs are not only not\nrobust to variation along these dimensions but demonstrate unique failure modes\nin particular, easy-to-create semantic groupings of \"deceptive\" sets. We find\nthat rigorously measuring language model robustness to variation in frequency\nand length is challenging and present an analysis that measures them\nindependently. The code for reproducing the results of this paper, and for\ngenerating the SetLexSem Challenge dataset, is available at\n\\href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge}.", "paper_summary_zh": "\u96c6\u5408\u8ad6\u662f\u6578\u5b78\u7684\u57fa\u790e\uff0c\u7576\u96c6\u5408\u662f\u6709\u9650\u6642\uff0c\u5b83\u7528\u65bc\u63a8\u7406\u4e16\u754c\u3002\u4e00\u500b\u667a\u80fd\u7cfb\u7d71\u61c9\u59cb\u7d42\u5982\u4e00\u5730\u57f7\u884c\u96c6\u5408\u904b\u7b97\uff0c\u800c\u4e0d\u7ba1\u904b\u7b97\u5143\u8868\u9762\u7684\u8b8a\u5316\u3002\u6700\u521d\u8a2d\u8a08\u7528\u65bc\u8a9e\u7fa9\u5c0e\u5411\u7684 NLP \u4efb\u52d9\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u73fe\u5728\u6b63\u5728\u6f14\u7b97\u6cd5\u4efb\u52d9\u4e0a\u9032\u884c\u8a55\u4f30\u3002\u7531\u65bc\u96c6\u5408\u7531\u4efb\u610f\u7b26\u865f\uff08\u4f8b\u5982\u6578\u5b57\u3001\u5b57\u8a5e\uff09\u7d44\u6210\uff0c\u56e0\u6b64\u5b83\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u6a5f\u6703\uff0c\u53ef\u4ee5\u7cfb\u7d71\u6027\u5730\u6e2c\u8a66 LLM \u7684\u6f14\u7b97\u6cd5\u80fd\u529b\u5728\u7c21\u55ae\u7684\u8a5e\u5f59\u6216\u8a9e\u7fa9\u8b8a\u5316\u4e0b\u7684\u4e0d\u8b8a\u6027\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SetLexSem \u6311\u6230\uff0c\u9019\u662f\u4e00\u500b\u7d9c\u5408\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u5728\u96c6\u5408\u904b\u7b97\u4e0a\u7684\u6548\u80fd\u3002SetLexSem \u8a55\u4f30 LLM \u5728\u5404\u7a2e\u689d\u4ef6\u4e0b\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u7684\u7a69\u5065\u6027\uff0c\u91cd\u9ede\u95dc\u6ce8\u96c6\u5408\u904b\u7b97\u4ee5\u53ca\u96c6\u5408\u6210\u54e1\u7684\u6027\u8cea\u548c\u5efa\u69cb\u3002\u4f7f\u7528 SetLexSem \u8a55\u4f30\u4e03\u500b LLM\uff0c\u6211\u5011\u767c\u73fe\u5b83\u5011\u5c0d\u904b\u7b97\u548c\u904b\u7b97\u5143\u4e2d\u7684\u8b8a\u5316\u8868\u73fe\u51fa\u8f03\u5dee\u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u900f\u904e\u8a72\u6846\u67b6\u6cbf\u8457\u8a5e\u5f59\u548c\u8a9e\u7fa9\u7dad\u5ea6\u5c0d\u96c6\u5408\u6210\u54e1\u9032\u884c\u7cfb\u7d71\u6027\u62bd\u6a23\uff0c\u8868\u660e LLM \u4e0d\u50c5\u5c0d\u9019\u4e9b\u7dad\u5ea6\u4e2d\u7684\u8b8a\u5316\u4e0d\u7a69\u5065\uff0c\u800c\u4e14\u8868\u73fe\u51fa\u7368\u7279\u7684\u5931\u6557\u6a21\u5f0f\uff0c\u7279\u5225\u662f\u300c\u5177\u6b3a\u9a19\u6027\u7684\u300d\u96c6\u5408\u7684\u6613\u65bc\u5efa\u7acb\u7684\u8a9e\u7fa9\u7fa4\u7d44\u3002\u6211\u5011\u767c\u73fe\uff0c\u56b4\u683c\u6e2c\u91cf\u8a9e\u8a00\u6a21\u578b\u5c0d\u983b\u7387\u548c\u9577\u5ea6\u8b8a\u5316\u7684\u7a69\u5065\u6027\u5177\u6709\u6311\u6230\u6027\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u7a2e\u7368\u7acb\u6e2c\u91cf\u5b83\u5011\u7684\u5206\u6790\u3002\u7528\u65bc\u91cd\u73fe\u672c\u6587\u7d50\u679c\u548c\u751f\u6210 SetLexSem \u6311\u6230\u8cc7\u6599\u96c6\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 \\href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge} \u53d6\u5f97\u3002", "author": "Bardiya Akhbari et.al.", "authors": "Bardiya Akhbari, Manish Gawali, Nicholas A. Dronen", "id": "2411.07336v1", "paper_url": "http://arxiv.org/abs/2411.07336v1", "repo": "https://github.com/amazon-science/setlexsem-challenge"}}