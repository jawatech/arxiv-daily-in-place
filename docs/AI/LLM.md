
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-11-22**|**Measuring Bullshit in the Language Games played by ChatGPT**|Alessandro Trevisan et.al.|[2411.15129v1](http://arxiv.org/abs/2411.15129v1)|null|
|**2024-11-22**|**Health AI Developer Foundations**|Atilla P. Kiraly et.al.|[2411.15128v1](http://arxiv.org/abs/2411.15128v1)|null|
|**2024-11-22**|**TÜLU 3: Pushing Frontiers in Open Language Model Post-Training**|Nathan Lambert et.al.|[2411.15124v1](http://arxiv.org/abs/2411.15124v1)|null|
|**2024-11-22**|**ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation**|Xiaoman Zhang et.al.|[2411.15122v1](http://arxiv.org/abs/2411.15122v1)|null|
|**2024-11-22**|**VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement**|Daeun Lee et.al.|[2411.15115v1](http://arxiv.org/abs/2411.15115v1)|null|
|**2024-11-22**|**RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts**|Hjalmar Wijk et.al.|[2411.15114v1](http://arxiv.org/abs/2411.15114v1)|null|
|**2024-11-22**|**Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion**|Samarth N Ramesh et.al.|[2411.15113v1](http://arxiv.org/abs/2411.15113v1)|null|
|**2024-11-22**|**About Time: Advances, Challenges, and Outlooks of Action Understanding**|Alexandros Stergiou et.al.|[2411.15106v1](http://arxiv.org/abs/2411.15106v1)|null|
|**2024-11-22**|**XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**|Yixin Dong et.al.|[2411.15100v1](http://arxiv.org/abs/2411.15100v1)|null|
|**2024-11-22**|**Context-Aware Multimodal Pretraining**|Karsten Roth et.al.|[2411.15099v1](http://arxiv.org/abs/2411.15099v1)|null|
|**2024-11-22**|**OminiControl: Minimal and Universal Control for Diffusion Transformer**|Zhenxiong Tan et.al.|[2411.15098v1](http://arxiv.org/abs/2411.15098v1)|[link](https://github.com/Yuanshi9815/OminiControl)|
|**2024-11-22**|**RED: Effective Trajectory Representation Learning with Comprehensive Information**|Silin Zhou et.al.|[2411.15096v1](http://arxiv.org/abs/2411.15096v1)|null|
|**2024-11-22**|**Instance-Aware Generalized Referring Expression Segmentation**|E-Ro Nguyen et.al.|[2411.15087v1](http://arxiv.org/abs/2411.15087v1)|null|
|**2024-11-22**|**Towards Speaker Identification with Minimal Dataset and Constrained Resources using 1D-Convolution Neural Network**|Irfan Nafiz Shahan et.al.|[2411.15082v1](http://arxiv.org/abs/2411.15082v1)|[link](https://github.com/irfannafiz/recme)|
|**2024-11-22**|**Locating the Leading Edge of Cultural Change**|Sarah Griebel et.al.|[2411.15068v1](http://arxiv.org/abs/2411.15068v1)|[link](https://github.com/IllinoisLiteraryLab/novelty)|
|**2024-11-22**|**Financial Risk Assessment via Long-term Payment Behavior Sequence Folding**|Yiran Qiao et.al.|[2411.15056v1](http://arxiv.org/abs/2411.15056v1)|null|
|**2024-11-22**|**Fantastic Biases (What are They) and Where to Find Them**|Valentin Barriere et.al.|[2411.15051v1](http://arxiv.org/abs/2411.15051v1)|null|
|**2024-11-22**|**Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications**|Hong Ding et.al.|[2411.15042v1](http://arxiv.org/abs/2411.15042v1)|null|
|**2024-11-22**|**mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA**|Tao Zhang et.al.|[2411.15041v1](http://arxiv.org/abs/2411.15041v1)|null|
|**2024-11-22**|**One to rule them all: natural language to bind communication, perception and action**|Simone Colombani et.al.|[2411.15033v1](http://arxiv.org/abs/2411.15033v1)|null|
|**2024-11-22**|**Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot**|Simone Colombani et.al.|[2411.15027v1](http://arxiv.org/abs/2411.15027v1)|null|
|**2024-11-22**|**Evolutionary Automata and Deep Evolutionary Computation**|Eugene Eberbach et.al.|[2411.15008v1](http://arxiv.org/abs/2411.15008v1)|null|
|**2024-11-22**|**FTA generation using GenAI with an Autonomy sensor Usecase**|Sneha Sudhir Shetiya et.al.|[2411.15007v1](http://arxiv.org/abs/2411.15007v1)|null|
|**2024-11-22**|**ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**|Junhong Shen et.al.|[2411.15004v1](http://arxiv.org/abs/2411.15004v1)|[link](https://github.com/colonylabs/ScribeAgent)|
|**2024-11-22**|**Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution**|Jonas Gösgens et.al.|[2411.14995v1](http://arxiv.org/abs/2411.14995v1)|null|
|**2024-11-22**|**Free Energy Projective Simulation (FEPS): Active inference with interpretability**|Joséphine Pazem et.al.|[2411.14991v1](http://arxiv.org/abs/2411.14991v1)|null|
|**2024-11-22**|**Large Multi-modal Models Can Interpret Features in Large Multi-modal Models**|Kaichen Zhang et.al.|[2411.14982v1](http://arxiv.org/abs/2411.14982v1)|[link](https://github.com/EvolvingLMMs-Lab/multimodal-sae)|
|**2024-11-22**|**Exploring Foundation Models Fine-Tuning for Cytology Classification**|Manon Dausort et.al.|[2411.14975v1](http://arxiv.org/abs/2411.14975v1)|[link](https://github.com/mdausort/Cytology-fine-tuning)|
|**2024-11-22**|**Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models**|Alec Wright et.al.|[2411.14972v1](http://arxiv.org/abs/2411.14972v1)|null|
|**2024-11-22**|**SwissADT: An Audio Description Translation System for Swiss Languages**|Lukas Fischer et.al.|[2411.14967v1](http://arxiv.org/abs/2411.14967v1)|null|
|**2024-11-22**|**LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents**|Hitesh Laxmichand Patel et.al.|[2411.14962v1](http://arxiv.org/abs/2411.14962v1)|null|
|**2024-11-22**|**Design-o-meter: Towards Evaluating and Refining Graphic Designs**|Sahil Goyal et.al.|[2411.14959v1](http://arxiv.org/abs/2411.14959v1)|null|
|**2024-11-22**|**Information Extraction from Heterogenous Documents without Ground Truth Labels using Synthetic Label Generation and Knowledge Distillation**|Aniket Bhattacharyya et.al.|[2411.14957v1](http://arxiv.org/abs/2411.14957v1)|null|
|**2024-11-22**|**Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing**|Miriam Alber et.al.|[2411.14953v1](http://arxiv.org/abs/2411.14953v1)|[link](https://github.com/visiontransformerad/vit-ad)|
|**2024-11-22**|**Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning**|Junjie Shan et.al.|[2411.14937v1](http://arxiv.org/abs/2411.14937v1)|[link](https://github.com/HKU-TASR/Geminio)|
|**2024-11-22**|**LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure Cooperation**|Zhenwei Yang et.al.|[2411.14927v1](http://arxiv.org/abs/2411.14927v1)|null|
|**2024-11-22**|**Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot**|Linqi Lu et.al.|[2411.14925v1](http://arxiv.org/abs/2411.14925v1)|null|
|**2024-11-22**|**GOT4Rec: Graph of Thoughts for Sequential Recommendation**|Zewen Long et.al.|[2411.14922v1](http://arxiv.org/abs/2411.14922v1)|null|
|**2024-11-22**|**DAIRHuM: A Platform for Directly Aligning AI Representations with Human Musical Judgments applied to Carnatic Music**|Prashanth Thattai Ravikumar et.al.|[2411.14907v1](http://arxiv.org/abs/2411.14907v1)|null|
|**2024-11-22**|**ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos**|Tanveer Hannan et.al.|[2411.14901v1](http://arxiv.org/abs/2411.14901v1)|null|
|**2024-11-22**|**Evaluating LLM Prompts for Data Augmentation in Multi-label Classification of Ecological Texts**|Anna Glazkova et.al.|[2411.14896v1](http://arxiv.org/abs/2411.14896v1)|null|
|**2024-11-22**|**Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation**|Yuheng Xu et.al.|[2411.14883v1](http://arxiv.org/abs/2411.14883v1)|null|
|**2024-11-22**|**Leveraging Hierarchical Prototypes as the Verbalizer for Implicit Discourse Relation Recognition**|Wanqiu Long et.al.|[2411.14880v1](http://arxiv.org/abs/2411.14880v1)|null|
|**2024-11-22**|**Astro-HEP-BERT: A bidirectional language model for studying the meanings of concepts in astrophysics and high energy physics**|Arno Simons et.al.|[2411.14877v1](http://arxiv.org/abs/2411.14877v1)|null|
|**2024-11-22**|**Prioritize Denoising Steps on Diffusion Model Preference Alignment via Explicit Denoised Distribution Estimation**|Dingyuan Shi et.al.|[2411.14871v1](http://arxiv.org/abs/2411.14871v1)|null|
|**2024-11-22**|**BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence**|Xuewu Lin et.al.|[2411.14869v1](http://arxiv.org/abs/2411.14869v1)|null|
|**2024-11-22**|**Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired Image-to-Image Translation**|Jeongsol Kim et.al.|[2411.14863v1](http://arxiv.org/abs/2411.14863v1)|null|
|**2024-11-22**|**Domain and Range Aware Synthetic Negatives Generation for Knowledge Graph Embedding Models**|Alberto Bernardi et.al.|[2411.14858v1](http://arxiv.org/abs/2411.14858v1)|null|
|**2024-11-22**|**Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models**|Wanqi Yang et.al.|[2411.14842v1](http://arxiv.org/abs/2411.14842v1)|null|
|**2024-11-22**|**VisGraphVar: A Benchmark Generator for Assessing Variability in Graph Analysis Using Large Vision-Language Models**|Camilo Chacón Sartori et.al.|[2411.14832v1](http://arxiv.org/abs/2411.14832v1)|null|
|**2024-11-22**|**Physically Interpretable Probabilistic Domain Characterization**|Anaïs Halin et.al.|[2411.14827v1](http://arxiv.org/abs/2411.14827v1)|null|
|**2024-11-22**|**Fine-Grained Alignment in Vision-and-Language Navigation through Bayesian Optimization**|Yuhang Song et.al.|[2411.14811v1](http://arxiv.org/abs/2411.14811v1)|null|
|**2024-11-22**|**High-Resolution Image Synthesis via Next-Token Prediction**|Dengsheng Chen et.al.|[2411.14808v1](http://arxiv.org/abs/2411.14808v1)|null|
|**2024-11-22**|**Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension**|Luca Parolari et.al.|[2411.14807v1](http://arxiv.org/abs/2411.14807v1)|null|
|**2024-11-22**|**Continual SFT Matches Multimodal RLHF with Negative Supervision**|Ke Zhu et.al.|[2411.14797v1](http://arxiv.org/abs/2411.14797v1)|null|
|**2024-11-22**|**De-biased Multimodal Electrocardiogram Analysis**|Haitao Li et.al.|[2411.14795v1](http://arxiv.org/abs/2411.14795v1)|null|
|**2024-11-22**|**VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection**|Songhao Han et.al.|[2411.14794v1](http://arxiv.org/abs/2411.14794v1)|null|
|**2024-11-22**|**KBAda: Efficient Self Adaptation on Specific Knowledge Bases**|Zheni Zeng et.al.|[2411.14790v1](http://arxiv.org/abs/2411.14790v1)|null|
|**2024-11-22**|**Resolution-Agnostic Transformer-based Climate Downscaling**|Declan Curran et.al.|[2411.14774v1](http://arxiv.org/abs/2411.14774v1)|null|
|**2024-11-22**|**Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology**|Qian Liang et.al.|[2411.14773v1](http://arxiv.org/abs/2411.14773v1)|null|
|**2024-11-22**|**Grid and Road Expressions Are Complementary for Trajectory Representation Learning**|Silin Zhou et.al.|[2411.14768v1](http://arxiv.org/abs/2411.14768v1)|null|
|**2024-11-22**|**Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction**|Huiwon Jang et.al.|[2411.14762v1](http://arxiv.org/abs/2411.14762v1)|null|
|**2024-11-22**|**TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior**|Sen Yang et.al.|[2411.14751v1](http://arxiv.org/abs/2411.14751v1)|null|
|**2024-11-22**|**Point Cloud Understanding via Attention-Driven Contrastive Learning**|Yi Wang et.al.|[2411.14744v1](http://arxiv.org/abs/2411.14744v1)|null|
|**2024-11-22**|**FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification**|Zhengrui Guo et.al.|[2411.14743v1](http://arxiv.org/abs/2411.14743v1)|[link](https://github.com/dddavid4real/focus)|
|**2024-11-22**|**TEXGen: a Generative Diffusion Model for Mesh Textures**|Xin Yu et.al.|[2411.14740v1](http://arxiv.org/abs/2411.14740v1)|[link](https://github.com/CVMI-Lab/TEXGen)|
|**2024-11-22**|**IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search**|Simon Lupart et.al.|[2411.14739v1](http://arxiv.org/abs/2411.14739v1)|null|
|**2024-11-22**|**Universal and Context-Independent Triggers for Precise Control of LLM Outputs**|Jiashuo Liang et.al.|[2411.14738v1](http://arxiv.org/abs/2411.14738v1)|null|
|**2024-11-22**|**Evaluating and Advancing Multimodal Large Language Models in Ability Lens**|Feng Chen et.al.|[2411.14725v1](http://arxiv.org/abs/2411.14725v1)|null|
|**2024-11-22**|**MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts**|Jiatong Li et.al.|[2411.14721v1](http://arxiv.org/abs/2411.14721v1)|null|
|**2024-11-22**|**Optimizing Social Media Annotation of HPV Vaccine Skepticism and Misinformation Using Large Language Models: An Experimental Evaluation of In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models**|Luhang Sun et.al.|[2411.14720v1](http://arxiv.org/abs/2411.14720v1)|null|
|**2024-11-22**|**FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data**|Binqian Xu et.al.|[2411.14717v1](http://arxiv.org/abs/2411.14717v1)|[link](https://github.com/1xbq1/fedmllm)|
|**2024-11-22**|**LIBER: Lifelong User Behavior Modeling Based on Large Language Models**|Chenxu Zhu et.al.|[2411.14713v1](http://arxiv.org/abs/2411.14713v1)|null|
|**2024-11-22**|**Understanding LLM Embeddings for Regression**|Eric Tang et.al.|[2411.14708v1](http://arxiv.org/abs/2411.14708v1)|null|
|**2024-11-22**|**Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation**|Xunyu Zhu et.al.|[2411.14698v1](http://arxiv.org/abs/2411.14698v1)|null|
|**2024-11-22**|**Whats in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning**|AJ Piergiovanni et.al.|[2411.14688v1](http://arxiv.org/abs/2411.14688v1)|null|
|**2024-11-22**|**Cross Group Attention and Group-wise Rolling for Multimodal Medical Image Synthesis**|Tao Song et.al.|[2411.14684v1](http://arxiv.org/abs/2411.14684v1)|null|
|**2024-11-22**|**Multiverse of Greatness: Generating Story Branches with LLMs**|Pittawat Taveekitworachai et.al.|[2411.14672v1](http://arxiv.org/abs/2411.14672v1)|null|
|**2024-11-22**|**Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective**|Jinming Xing et.al.|[2411.14654v1](http://arxiv.org/abs/2411.14654v1)|null|
|**2024-11-22**|**Social Media Algorithms Can Shape Affective Polarization via Exposure to Antidemocratic Attitudes and Partisan Animosity**|Tiziano Piccardi et.al.|[2411.14652v1](http://arxiv.org/abs/2411.14652v1)|null|
|**2024-11-22**|**Benchmarking Multimodal Models for Ukrainian Language Understanding Across Academic and Cultural Domains**|Yurii Paniv et.al.|[2411.14647v1](http://arxiv.org/abs/2411.14647v1)|null|
|**2024-11-21**|**Evaluating Representational Similarity Measures from the Lens of Functional Correspondence**|Yiqing Bo et.al.|[2411.14633v1](http://arxiv.org/abs/2411.14633v1)|null|
|**2024-11-21**|**Unveiling the Hidden: A Comprehensive Evaluation of Underwater Image Enhancement and Its Impact on Object Detection**|Ali Awad et.al.|[2411.14626v1](http://arxiv.org/abs/2411.14626v1)|null|
|**2024-11-21**|**Predictive Analytics of Air Alerts in the Russian-Ukrainian War**|Demian Pavlyshenko et.al.|[2411.14625v1](http://arxiv.org/abs/2411.14625v1)|null|
|**2024-11-21**|**Exploiting Boosting in Hyperdimensional Computing for Enhanced Reliability in Healthcare**|SungHeon Jeong et.al.|[2411.14612v1](http://arxiv.org/abs/2411.14612v1)|null|
|**2024-11-21**|**G-RAG: Knowledge Expansion in Material Science**|Radeen Mostafa et.al.|[2411.14592v1](http://arxiv.org/abs/2411.14592v1)|[link](https://github.com/RadeenXALNW/G-RAG_1.0)|
|**2024-11-21**|**SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world Human-Machine Interactions**|Yaqi Wang et.al.|[2411.14574v1](http://arxiv.org/abs/2411.14574v1)|null|
|**2024-11-21**|**Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective**|Shenglai Zeng et.al.|[2411.14572v1](http://arxiv.org/abs/2411.14572v1)|null|
|**2024-11-21**|**Assessment of LLM Responses to End-user Security Questions**|Vijay Prakash et.al.|[2411.14571v1](http://arxiv.org/abs/2411.14571v1)|null|
|**2024-11-21**|**An Experimental Study on Data Augmentation Techniques for Named Entity Recognition on Low-Resource Domains**|Arthur Elwing Torres et.al.|[2411.14551v1](http://arxiv.org/abs/2411.14551v1)|null|
|**2024-11-21**|**The importance of the clustering model to detect new types of intrusion in data traffic**|Noor Saud Abd et.al.|[2411.14550v1](http://arxiv.org/abs/2411.14550v1)|null|
|**2024-11-21**|**Whack-a-Chip: The Futility of Hardware-Centric Export Controls**|Ritwik Gupta et.al.|[2411.14425v1](http://arxiv.org/abs/2411.14425v1)|null|
|**2024-11-21**|**Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions**|Yu Zhao et.al.|[2411.14405v1](http://arxiv.org/abs/2411.14405v1)|null|
|**2024-11-21**|**Resolving Multiple-Dynamic Model Uncertainty in Hypothesis-Driven Belief-MDPs**|Ofer Dagan et.al.|[2411.14404v1](http://arxiv.org/abs/2411.14404v1)|null|
|**2024-11-21**|**Landing Trajectory Prediction for UAS Based on Generative Adversarial Network**|Jun Xiang et.al.|[2411.14403v1](http://arxiv.org/abs/2411.14403v1)|null|
|**2024-11-21**|**Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings**|Aaron Zheng et.al.|[2411.14398v1](http://arxiv.org/abs/2411.14398v1)|null|
|**2024-11-21**|**POS-tagging to highlight the skeletal structure of sentences**|Grigorii Churakov et.al.|[2411.14393v1](http://arxiv.org/abs/2411.14393v1)|[link](https://github.com/disk0Dancer/rubert-finetuned-pos)|
|**2024-11-21**|**Using Formal Models, Safety Shields and Certified Control to Validate AI-Based Train Systems**|Jan Gruteser et.al.|[2411.14374v1](http://arxiv.org/abs/2411.14374v1)|null|
|**2024-11-21**|**Synthesising Robust Controllers for Robot Collectives with Recurrent Tasks: A Case Study**|Till Schnittka et.al.|[2411.14371v1](http://arxiv.org/abs/2411.14371v1)|null|
|**2024-11-21**|**Contrasting local and global modeling with machine learning and satellite data: A case study estimating tree canopy height in African savannas**|Esther Rolf et.al.|[2411.14354v1](http://arxiv.org/abs/2411.14354v1)|null|

#### Abstracts
##### **Measuring Bullshit in the Language Games played by ChatGPT**
2411.15129v1 by Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell

Generative large language models (LLMs), which create text without direct
correspondence to truth value, are widely understood to resemble the uses of
language described in Frankfurt's popular monograph On Bullshit. In this paper,
we offer a rigorous investigation of this topic, identifying how the phenomenon
has arisen, and how it might be analysed. In this paper, we elaborate on this
argument to propose that LLM-based chatbots play the 'language game of
bullshit'. We use statistical text analysis to investigate the features of this
Wittgensteinian language game, based on a dataset constructed to contrast the
language of 1,000 scientific publications with typical pseudo-scientific text
generated by ChatGPT. We then explore whether the same language features can be
detected in two well-known contexts of social dysfunction: George Orwell's
critique of politics and language, and David Graeber's characterisation of
bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a
statistical model of the language of bullshit can reliably relate the
Frankfurtian artificial bullshit of ChatGPT to the political and workplace
functions of bullshit as observed in natural human language.

摘要：生成式大型語言模型（LLM）會產生沒有直接對應真值的文章，廣泛被認為類似於法蘭克福的暢銷專著《論扯淡》中所描述的語言用法。在本文中，我們對此主題進行嚴謹的探討，找出這種現象如何產生，以及如何分析。在本文中，我們進一步闡述這個論點，提出基於 LLM 的聊天機器人玩「扯淡的語言遊戲」。我們使用統計文本分析來探討這個維根斯坦語言遊戲的特徵，根據一個建構來對比 1,000 篇科學出版物語言與 ChatGPT 所產生典型偽科學文本的資料集。接著我們探討是否可以在兩個著名的社會失能情境中偵測到相同的語言特徵：喬治·歐威爾對政治與語言的批判，以及大衛·格雷伯對扯淡工作的描述。使用簡單的假設檢定方法，我們證明了扯淡語言的統計模型可以可靠地將 ChatGPT 的法蘭克福式人工扯淡與在自然人類語言中觀察到的扯淡的政治和職場功能關聯起來。

##### **Health AI Developer Foundations**
2411.15128v1 by Atilla P. Kiraly, Sebastien Baur, Kenneth Philbrick, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Nick George, Fayaz Jamil, Jing Tang, Kai Bailey, Faruk Ahmed, Akshay Goel, Abbi Ward, Lin Yang, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Shekoofeh Azizi, David F. Steiner, Yun Liu, Tim Thelin, Rory Pilgrim, Can Kirmizibayrak

Robust medical Machine Learning (ML) models have the potential to
revolutionize healthcare by accelerating clinical research, improving workflows
and outcomes, and producing novel insights or capabilities. Developing such ML
models from scratch is cost prohibitive and requires substantial compute, data,
and time (e.g., expert labeling). To address these challenges, we introduce
Health AI Developer Foundations (HAI-DEF), a suite of pre-trained,
domain-specific foundation models, tools, and recipes to accelerate building ML
for health applications. The models cover various modalities and domains,
including radiology (X-rays and computed tomography), histopathology,
dermatological imaging, and audio. These models provide domain specific
embeddings that facilitate AI development with less labeled data, shorter
training times, and reduced computational costs compared to traditional
approaches. In addition, we utilize a common interface and style across these
models, and prioritize usability to enable developers to integrate HAI-DEF
efficiently. We present model evaluations across various tasks and conclude
with a discussion of their application and evaluation, covering the importance
of ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and
specifically the foundation models lower the barrier to entry for ML in
healthcare, we emphasize the importance of validation with problem- and
population-specific data for each desired usage setting. This technical report
will be updated over time as more modalities and features are added.

摘要：強大的醫療機器學習 (ML) 模型有潛力透過加速臨床研究、改善工作流程和成果，以及產生新見解或能力，從而徹底改變醫療保健。從頭開始開發此類 ML 模型在成本上過於昂貴，且需要大量的運算、資料和時間 (例如，專家標籤)。為了應對這些挑戰，我們推出了 Health AI Developer Foundations (HAI-DEF)，這是一套預先訓練好的、特定於領域的基礎模型、工具和範例，用於加速建立醫療保健應用的 ML。這些模型涵蓋各種模式和領域，包括放射學 (X 光和電腦斷層掃描)、組織病理學、皮膚影像學和音訊。這些模型提供特定於領域的嵌入，與傳統方法相比，它們有助於使用較少的標記資料、縮短訓練時間和降低運算成本來進行 AI 開發。此外，我們在這些模型中使用通用介面和樣式，並優先考慮可用性，以使開發人員能夠有效整合 HAI-DEF。我們針對各種任務展示模型評估，並以討論其應用和評估作為結論，涵蓋確保效能、公平性和公正性的重要性。最後，儘管 HAI-DEF 和特別是基礎模型降低了醫療保健中 ML 的進入門檻，但我們強調驗證對於每個所需的用法設定來說具有問題和特定於人群資料的重要性。隨著更多模式和功能的加入，這份技術報告將會隨著時間更新。

##### **TÜLU 3: Pushing Frontiers in Open Language Model Post-Training**
2411.15124v1 by Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi

Language model post-training is applied to refine behaviors and unlock new
skills across a wide range of recent language models, but open recipes for
applying these techniques lag behind proprietary ones. The underlying training
data and recipes for post-training are simultaneously the most important pieces
of the puzzle and the portion with the least transparency. To bridge this gap,
we introduce T\"ULU 3, a family of fully-open state-of-the-art post-trained
models, alongside its data, code, and training recipes, serving as a
comprehensive guide for modern post-training techniques. T\"ULU 3, which builds
on Llama 3.1 base models, achieves results surpassing the instruct versions of
Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and
Claude 3.5-Haiku. The training algorithms for our models include supervised
finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we
call Reinforcement Learning with Verifiable Rewards (RLVR). With T\"ULU 3, we
introduce a multi-task evaluation scheme for post-training recipes with
development and unseen evaluations, standard benchmark implementations, and
substantial decontamination of existing open datasets on said benchmarks. We
conclude with analysis and discussion of training methods that did not reliably
improve performance.
  In addition to the T\"ULU 3 model weights and demo, we release the complete
recipe -- including datasets for diverse core skills, a robust toolkit for data
curation and evaluation, the training code and infrastructure, and, most
importantly, a detailed report for reproducing and further adapting the T\"ULU
3 approach to more domains.

摘要：語言模型後訓練用於改善行為並解鎖各種最新語言模型的新技能，但應用這些技術的公開配方落後於專有配方。基礎訓練數據和後訓練配方同時是拼圖中最重要的一部分，也是透明度最低的部分。為了彌補這一差距，我們引入了 T\"ULU 3，這是一個完全開放的最新後訓練模型系列，連同其數據、代碼和訓練配方，作為現代後訓練技術的綜合指南。T\"ULU 3 建構在 Llama 3.1 基礎模型上，其成果超越了 Llama 3.1、Qwen 2.5、Mistral，甚至 GPT-4o-mini 和 Claude 3.5-Haiku 等封閉模型的指令版本。我們模型的訓練演算法包括監督微調 (SFT)、直接偏好最佳化 (DPO)，以及我們稱之為可驗證獎勵強化學習 (RLVR) 的新方法。有了 T\"ULU 3，我們引入了多任務評估方案，用於後訓練配方，其中包含開發和未見評估、標準基準實作，以及對所述基準上現有開放式數據集的大量去汙。我們以對訓練方法的分析和討論作為結論，這些方法並未可靠地改善效能。除了 T\"ULU 3 模型權重和示範之外，我們還發布了完整的配方，包括用於各種核心技能的數據集、用於數據整理和評估的強大工具包、訓練代碼和基礎架構，最重要的是，一份用於重現和進一步調整 T\"ULU 3 方法以適應更多領域的詳細報告。

##### **ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation**
2411.15122v1 by Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Julián N. Acosta, Josh Miller, Ouwen Huang, Pranav Rajpurkar

AI-driven models have demonstrated significant potential in automating
radiology report generation for chest X-rays. However, there is no standardized
benchmark for objectively evaluating their performance. To address this, we
present ReXrank, https://rexrank.ai, a public leaderboard and challenge for
assessing AI-powered radiology report generation. Our framework incorporates
ReXGradient, the largest test dataset consisting of 10,000 studies, and three
public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation
assessment. ReXrank employs 8 evaluation metrics and separately assesses models
capable of generating only findings sections and those providing both findings
and impressions sections. By providing this standardized evaluation framework,
ReXrank enables meaningful comparisons of model performance and offers crucial
insights into their robustness across diverse clinical settings. Beyond its
current focus on chest X-rays, ReXrank's framework sets the stage for
comprehensive evaluation of automated reporting across the full spectrum of
medical imaging.

摘要：人工智能驅動的模型已證明在自動化胸部 X 射線放射報告生成方面具有顯著的潛力。然而，沒有標準化的基準來客觀評估其性能。為了解決這個問題，我們提出了 ReXrank，https://rexrank.ai，一個公共排行榜和挑戰，用於評估 AI 驅動的放射報告生成。我們的框架包含 ReXGradient，這是由 10,000 項研究組成的最大測試數據集，以及三個公共數據集（MIMIC-CXR、IU-Xray、CheXpert Plus），用於報告生成評估。ReXrank 採用 8 項評估指標，並分別評估只能生成結果部分的模型和同時提供結果和印象部分的模型。通過提供這個標準化的評估框架，ReXrank 能夠對模型性能進行有意義的比較，並提供對其在不同臨床環境中穩健性的關鍵見解。除了目前關注胸部 X 射線之外，ReXrank 的框架還為跨越整個醫學影像範圍的自動化報告的全面評估奠定了基礎。

##### **VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement**
2411.15115v1 by Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal

Recent text-to-video (T2V) diffusion models have demonstrated impressive
generation capabilities across various domains. However, these models often
generate videos that have misalignments with text prompts, especially when the
prompts describe complex scenes with multiple objects and attributes. To
address this, we introduce VideoRepair, a novel model-agnostic, training-free
video refinement framework that automatically identifies fine-grained
text-video misalignments and generates explicit spatial and textual feedback,
enabling a T2V diffusion model to perform targeted, localized refinements.
VideoRepair consists of four stages: In (1) video evaluation, we detect
misalignments by generating fine-grained evaluation questions and answering
those questions with MLLM. In (2) refinement planning, we identify accurately
generated objects and then create localized prompts to refine other areas in
the video. Next, in (3) region decomposition, we segment the correctly
generated area using a combined grounding module. We regenerate the video by
adjusting the misaligned regions while preserving the correct regions in (4)
localized refinement. On two popular video generation benchmarks (EvalCrafter
and T2V-CompBench), VideoRepair substantially outperforms recent baselines
across various text-video alignment metrics. We provide a comprehensive
analysis of VideoRepair components and qualitative examples.

摘要：最近的文字轉影片 (T2V) 擴散模型已展示出跨各種領域的令人印象深刻的生成能力。然而，這些模型經常會產生與文字提示不一致的影片，特別是在提示描述具有多個物件和屬性的複雜場景時。為了解決這個問題，我們引入了 VideoRepair，一種新穎的與模型無關、無需訓練的影片精煉架構，它會自動識別細微的文字影片不一致之處，並產生明確的空間和文字回饋，讓 T2V 擴散模型執行有針對性的局部精煉。VideoRepair 包含四個階段：在 (1) 影片評估中，我們透過產生細微的評估問題並使用 MLLM 回答這些問題來偵測不一致之處。在 (2) 精煉規劃中，我們識別準確生成的物件，然後建立局部提示來精煉影片中的其他區域。接著，在 (3) 區域分解中，我們使用結合接地模組來區隔正確生成的區域。我們在 (4) 局部精煉中調整不一致的區域，同時保留正確的區域，藉此重新產生影片。在兩個流行的影片生成基準 (EvalCrafter 和 T2V-CompBench) 上，VideoRepair 在各種文字影片對齊指標方面都大幅優於最近的基準。我們提供了 VideoRepair 組成部分的全面分析和定性範例。

##### **RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts**
2411.15114v1 by Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes

Frontier AI safety policies highlight automation of AI research and
development (R&D) by AI agents as an important capability to anticipate.
However, there exist few evaluations for AI R&D capabilities, and none that are
highly realistic and have a direct comparison to human performance. We
introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7
challenging, open-ended ML research engineering environments and data from 71
8-hour attempts by 61 distinct human experts. We confirm that our experts make
progress in the environments given 8 hours, with 82% of expert attempts
achieving a non-zero score and 24% matching or exceeding our strong reference
solutions. We compare humans to several public frontier models through
best-of-k with varying time budgets and agent designs, and find that the best
AI agents achieve a score 4x higher than human experts when both are given a
total time budget of 2 hours per environment. However, humans currently display
better returns to increasing time budgets, narrowly exceeding the top AI agent
scores given an 8-hour budget, and achieving 2x the score of the top AI agent
when both are given 32 total hours (across different attempts). Qualitatively,
we find that modern AI agents possess significant expertise in many ML topics
-- e.g. an agent wrote a faster custom Triton kernel than any of our human
experts' -- and can generate and test solutions over ten times faster than
humans, at much lower cost. We open-source the evaluation environments, human
expert data, analysis code and agent trajectories to facilitate future
research.

摘要：前沿 AI 安全政策重点关注 AI 代理对 AI 研究和开发 (R&D) 的自动化，将其视为一项重要的预期能力。然而，很少有针对 AI 研发能力的评估，而且没有一项评估具有高度的现实性，并与人类表现进行直接比较。我们引入了 RE-Bench（研究工程基准，v1），它包含 7 个具有挑战性的、开放式的 ML 研究工程环境和 61 位不同人类专家进行的 71 次 8 小时尝试的数据。我们确认，我们的专家在给定的 8 小时内在环境中取得了进展，82% 的专家尝试获得了非零分数，24% 达到或超过了我们强大的参考解决方案。我们通过具有不同时间预算和代理设计的最佳 k 值将人类与几个公共前沿模型进行比较，发现最好的 AI 代理在每种环境中获得的时间预算总计为 2 小时时，其分数比人类专家高 4 倍。然而，人类目前在增加时间预算方面表现出更好的回报，在给定 8 小时预算的情况下略微超过顶级 AI 代理分数，并且在总共给定 32 小时（在不同的尝试中）的情况下，其分数达到顶级 AI 代理的两倍。从质量上讲，我们发现现代 AI 代理在许多 ML 主题中拥有重要的专业知识——例如，一位代理编写了一个比我们任何一位人类专家都更快的自定义 Triton 内核——并且能够生成和测试解决方案的速度比人类快十倍以上，成本也低得多。我们开源评估环境、人类专家数据、分析代码和代理轨迹，以促进未来的研究。

##### **Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion**
2411.15113v1 by Samarth N Ramesh, Zhixue Zhao

As text-to-image models grow increasingly powerful and complex, their
burgeoning size presents a significant obstacle to widespread adoption,
especially on resource-constrained devices. This paper presents a pioneering
study on post-training pruning of Stable Diffusion 2, addressing the critical
need for model compression in text-to-image domain. Our study tackles the
pruning techniques for the previously unexplored multi-modal generation models,
and particularly examines the pruning impact on the textual component and the
image generation component separately. We conduct a comprehensive comparison on
pruning the model or the single component of the model in various sparsities.
Our results yield previously undocumented findings. For example, contrary to
established trends in language model pruning, we discover that simple magnitude
pruning outperforms more advanced techniques in text-to-image context.
Furthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%
sparsity with minimal quality loss, achieving a significant reduction in model
size. We propose an optimal pruning configuration that prunes the text encoder
to 47.5% and the diffusion generator to 35%. This configuration maintains image
generation quality while substantially reducing computational requirements. In
addition, our work uncovers intriguing questions about information encoding in
text-to-image models: we observe that pruning beyond certain thresholds leads
to sudden performance drops (unreadable images), suggesting that specific
weights encode critical semantics information. This finding opens new avenues
for future research in model compression, interoperability, and bias
identification in text-to-image models. By providing crucial insights into the
pruning behavior of text-to-image models, our study lays the groundwork for
developing more efficient and accessible AI-driven image generation systems

摘要：隨著文字轉圖像模型越來越強大且複雜，其龐大的規模對廣泛採用構成重大障礙，特別是在資源受限的裝置上。本文針對 Stable Diffusion 2 的訓練後剪枝提出開創性的研究，探討文字轉圖像領域中模型壓縮的關鍵需求。我們的研究探討先前未探索的多模式生成模型的剪枝技術，並特別分別探討剪枝對文字元件和影像生成元件的影響。我們對各種稀疏度的模型或模型單一元件剪枝進行全面比較。我們的結果產生了先前未記錄的發現。例如，與語言模型剪枝中的既定趨勢相反，我們發現簡單的幅度剪枝在文字轉圖像的背景下優於更進階的技術。此外，我們的結果顯示 Stable Diffusion 2 可以剪枝到 38.5% 的稀疏度，且品質損失極小，大幅縮小模型規模。我們提出最佳剪枝配置，將文字編碼器剪枝到 47.5%，擴散生成器剪枝到 35%。此配置維持影像生成品質，同時大幅降低運算需求。此外，我們的研究也揭露文字轉圖像模型中資訊編碼的有趣問題：我們觀察到剪枝超過特定閾值會導致效能突然下降（無法辨識的影像），這表示特定的權重編碼關鍵的語意資訊。此發現為未來在文字轉圖像模型中的模型壓縮、互操作性以及偏差識別的研究開啟新途徑。透過提供對文字轉圖像模型剪枝行為的重要見解，我們的研究為開發更有效率且易於取得的 AI 驅動影像生成系統奠定基礎

##### **About Time: Advances, Challenges, and Outlooks of Action Understanding**
2411.15106v1 by Alexandros Stergiou, Ronald Poppe

We have witnessed impressive advances in video action understanding.
Increased dataset sizes, variability, and computation availability have enabled
leaps in performance and task diversification. Current systems can provide
coarse- and fine-grained descriptions of video scenes, extract segments
corresponding to queries, synthesize unobserved parts of videos, and predict
context. This survey comprehensively reviews advances in uni- and multi-modal
action understanding across a range of tasks. We focus on prevalent challenges,
overview widely adopted datasets, and survey seminal works with an emphasis on
recent advances. We broadly distinguish between three temporal scopes: (1)
recognition tasks of actions observed in full, (2) prediction tasks for ongoing
partially observed actions, and (3) forecasting tasks for subsequent unobserved
action. This division allows us to identify specific action modeling and video
representation challenges. Finally, we outline future directions to address
current shortcomings.

摘要：我們見證了影片動作理解的顯著進展。
資料集大小、變異性與運算可用性的增加，促成了效能的躍進與任務的多樣化。目前的系統能提供影片場景的粗略和細緻描述、萃取與查詢相符的片段、綜合影片中未觀察到的部分，以及預測脈絡。這份調查全面回顧了單模態和多模態動作理解在各種任務上的進展。我們專注於普遍的挑戰、概觀廣泛採用的資料集，並調查具有開創性意義的作品，特別強調最近的進展。我們廣泛區分三種時間範圍：(1) 完整觀察動作的辨識任務、(2) 持續進行的部分觀察動作的預測任務，以及 (3) 後續未觀察動作的預測任務。這種區分讓我們能找出特定動作建模和影片表示的挑戰。最後，我們概述了未來的方向，以解決目前的缺點。

##### **XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models**
2411.15100v1 by Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen

The applications of LLM Agents are becoming increasingly complex and diverse,
leading to a high demand for structured outputs that can be parsed into code,
structured function calls, and embodied agent commands. These developments
bring significant demands for structured generation in LLM inference.
Context-free grammar is a flexible approach to enable structured generation via
constrained decoding. However, executing context-free grammar requires going
through several stack states over all tokens in vocabulary during runtime,
bringing non-negligible overhead for structured generation. In this paper, we
propose XGrammar, a flexible and efficient structure generation engine for
large language models. XGrammar accelerates context-free grammar execution by
dividing the vocabulary into context-independent tokens that can be prechecked
and context-dependent tokens that need to be interpreted during runtime. We
further build transformations to expand the grammar context and reduce the
number of context-independent tokens. Additionally, we build an efficient
persistent stack to accelerate the context-dependent token checks. Finally, we
co-design the grammar engine with LLM inference engine to overlap grammar
computation with GPU executions. Evaluation results show that XGrammar can
achieve up to 100x speedup over existing solutions. Combined with an LLM
inference engine, it can generate near-zero overhead structure generation in
end-to-end low-LLM serving.

摘要：LLM 代理的應用越來越複雜且多元，
導致對可解析成程式碼、結構化函式呼叫和具體代理指令的結構化輸出的需求很高。這些發展對 LLM 推論中的結構化產生顯著需求。
無上下文語法是一種靈活的方法，可透過受限解碼啟用結構化產生。然而，執行無上下文語法需要在執行期間遍歷詞彙表中所有符號的幾個堆疊狀態，為結構化產生帶來不可忽視的負擔。在本文中，我們提出 XGrammar，這是一個大型語言模型的靈活且高效的結構產生引擎。XGrammar 透過將詞彙表分為可在預先檢查的與語境無關的符號和需要在執行期間解釋的與語境相關的符號，加速無上下文語法的執行。我們進一步建立轉換以擴充語法語境並減少與語境無關的符號數量。此外，我們建立一個高效的持續堆疊，以加速與語境相關的符號檢查。最後，我們將語法引擎與 LLM 推論引擎共同設計，讓語法運算與 GPU 執行重疊。評估結果顯示，XGrammar 可以比現有解決方案快上 100 倍。結合 LLM 推論引擎，它可以在端到端的低 LLM 服務中產生近乎零負擔的結構產生。

##### **Context-Aware Multimodal Pretraining**
2411.15099v1 by Karsten Roth, Zeynep Akata, Dima Damen, Ivana Balažević, Olivier J. Hénaff

Large-scale multimodal representation learning successfully optimizes for
zero-shot transfer at test time. Yet the standard pretraining paradigm
(contrastive learning on large amounts of image-text data) does not explicitly
encourage representations to support few-shot adaptation. In this work, we
propose a simple, but carefully designed extension to multimodal pretraining
which enables representations to accommodate additional context. Using this
objective, we show that vision-language models can be trained to exhibit
significantly increased few-shot adaptation: across 21 downstream tasks, we
find up to four-fold improvements in test-time sample efficiency, and average
few-shot adaptation gains of over 5%, while retaining zero-shot generalization
performance across model scales and training durations. In particular, equipped
with simple, training-free, metric-based adaptation mechanisms, our
representations easily surpass more complex and expensive optimization-based
schemes, vastly simplifying generalization to new domains.

摘要：大規模多模態表徵學習成功針對測試時間的零次學習傳輸進行最佳化。然而，標準預訓練範例（對大量影像文字資料進行對比學習）並未明確鼓勵表徵支援少次學習適應。在這項工作中，我們提出一個簡單但經過仔細設計的多模態預訓練延伸，讓表徵能夠容納額外的內容。使用此目標，我們展示了視覺語言模型可以訓練成展現大幅增加的少次學習適應：在 21 個下游任務中，我們發現測試時間樣本效率提升多達四倍，且平均少次學習適應增益超過 5%，同時在模型規模和訓練持續時間中保留零次學習概化效能。特別是，我們的表徵配備簡單、無需訓練的基於指標的適應機制，輕易超越更複雜且昂貴的基於最佳化的方案，大幅簡化對新領域的概化。

##### **OminiControl: Minimal and Universal Control for Diffusion Transformer**
2411.15098v1 by Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang

In this paper, we introduce OminiControl, a highly versatile and
parameter-efficient framework that integrates image conditions into pre-trained
Diffusion Transformer (DiT) models. At its core, OminiControl leverages a
parameter reuse mechanism, enabling the DiT to encode image conditions using
itself as a powerful backbone and process them with its flexible multi-modal
attention processors. Unlike existing methods, which rely heavily on additional
encoder modules with complex architectures, OminiControl (1) effectively and
efficiently incorporates injected image conditions with only ~0.1% additional
parameters, and (2) addresses a wide range of image conditioning tasks in a
unified manner, including subject-driven generation and spatially-aligned
conditions such as edges, depth, and more. Remarkably, these capabilities are
achieved by training on images generated by the DiT itself, which is
particularly beneficial for subject-driven generation. Extensive evaluations
demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted
models in both subject-driven and spatially-aligned conditional generation.
Additionally, we release our training dataset, Subjects200K, a diverse
collection of over 200,000 identity-consistent images, along with an efficient
data synthesis pipeline to advance research in subject-consistent generation.

摘要：在本文中，我們介紹 OminiControl，一個高度靈活且參數效率高的框架，它將影像條件整合到預先訓練的擴散轉換器 (DiT) 模型中。OminiControl 的核心利用參數重複使用機制，讓 DiT 能夠將影像條件編碼，並使用自己作為強大的主幹，並用其靈活的多模式注意處理器處理它們。與依賴具有複雜架構的額外編碼器模組的現有方法不同，OminiControl (1) 有效且高效地整合注入的影像條件，僅需額外約 0.1% 的參數，以及 (2) 以統一的方式處理廣泛的影像條件化任務，包括受主體驅動的生成和空間對齊的條件，例如邊緣、深度等。值得注意的是，這些功能是透過訓練 DiT 本身產生的影像來實現的，這對受主體驅動的生成特別有益。廣泛的評估證明，OminiControl 在受主體驅動和空間對齊條件生成中都優於現有的基於 UNet 和 DiT 適用的模型。此外，我們發布了我們的訓練資料集 Subjects200K，一個包含超過 200,000 個身份一致影像的多元化集合，以及一個高效的資料合成管道，以推進主體一致生成的研究。

##### **RED: Effective Trajectory Representation Learning with Comprehensive Information**
2411.15096v1 by Silin Zhou, Shuo Shang, Lisi Chen, Christian S. Jensen, Panos Kalnis

Trajectory representation learning (TRL) maps trajectories to vectors that
can then be used for various downstream tasks, including trajectory similarity
computation, trajectory classification, and travel-time estimation. However,
existing TRL methods often produce vectors that, when used in downstream tasks,
yield insufficiently accurate results. A key reason is that they fail to
utilize the comprehensive information encompassed by trajectories. We propose a
self-supervised TRL framework, called RED, which effectively exploits multiple
types of trajectory information. Overall, RED adopts the Transformer as the
backbone model and masks the constituting paths in trajectories to train a
masked autoencoder (MAE). In particular, RED considers the moving patterns of
trajectories by employing a Road-aware masking strategy} that retains key paths
of trajectories during masking, thereby preserving crucial information of the
trajectories. RED also adopts a spatial-temporal-user joint Embedding scheme to
encode comprehensive information when preparing the trajectories as model
inputs. To conduct training, RED adopts Dual-objective task learning}: the
Transformer encoder predicts the next segment in a trajectory, and the
Transformer decoder reconstructs the entire trajectory. RED also considers the
spatial-temporal correlations of trajectories by modifying the attention
mechanism of the Transformer. We compare RED with 9 state-of-the-art TRL
methods for 4 downstream tasks on 3 real-world datasets, finding that RED can
usually improve the accuracy of the best-performing baseline by over 5%.

摘要：軌跡表示學習 (TRL) 將軌跡對應到向量，然後可將這些向量用於各種下游任務，包括軌跡相似性運算、軌跡分類和旅遊時間估計。然而，現有的 TRL 方法通常會產生向量，當用於下游任務時，產生的結果準確度不足。一個關鍵原因是它們未能利用軌跡所包含的綜合資訊。我們提出一個自監督 TRL 框架，稱為 RED，它有效利用多種類型的軌跡資訊。總的來說，RED 採用 Transformer 作為主幹模型，並遮蔽軌跡中的構成路徑來訓練遮蔽自動編碼器 (MAE)。特別是，RED 透過採用道路感知遮蔽策略來考慮軌跡的移動模式，該策略在遮蔽過程中保留軌跡的主要路徑，從而保留軌跡的關鍵資訊。RED 還採用時空使用者聯合嵌入架構，在準備軌跡作為模型輸入時編碼綜合資訊。為了進行訓練，RED 採用雙目標任務學習：Transformer 編碼器預測軌跡中的下一個區段，而 Transformer 解碼器重建整個軌跡。RED 還透過修改 Transformer 的注意力機制來考慮軌跡的時空關聯性。我們將 RED 與 9 種最先進的 TRL 方法比較，針對 3 個真實世界資料集的 4 個下游任務進行比較，發現 RED 通常可以將效能最佳的基準準確度提升超過 5%。

##### **Instance-Aware Generalized Referring Expression Segmentation**
2411.15087v1 by E-Ro Nguyen, Hieu Le, Dimitris Samaras, Michael Ryoo

Recent works on Generalized Referring Expression Segmentation (GRES) struggle
with handling complex expressions referring to multiple distinct objects. This
is because these methods typically employ an end-to-end foreground-background
segmentation and lack a mechanism to explicitly differentiate and associate
different object instances to the text query. To this end, we propose
InstAlign, a method that incorporates object-level reasoning into the
segmentation process. Our model leverages both text and image inputs to extract
a set of object-level tokens that capture both the semantic information in the
input prompt and the objects within the image. By modeling the text-object
alignment via instance-level supervision, each token uniquely represents an
object segment in the image, while also aligning with relevant semantic
information from the text. Extensive experiments on the gRefCOCO and Ref-ZOM
benchmarks demonstrate that our method significantly advances state-of-the-art
performance, setting a new standard for precise and flexible GRES.

摘要：近期關於廣義指涉表達式分割 (GRES) 的研究在處理指涉多個不同物體的複雜表達式時遇到困難。這是因為這些方法通常採用端對端的背景前景分割，並且缺乏明確區分和將不同的物件實例與文字查詢關聯的機制。為此，我們提出了 InstAlign，這是一種將物件層級推理納入分割過程的方法。我們的模型同時利用文字和影像輸入，以萃取一組物件層級的標記，這些標記擷取輸入提示中的語意資訊和影像中的物件。透過實例層級監督對文字物件比對進行建模，每個標記在影像中唯一代表一個物件區段，同時也與文字中的相關語意資訊對齊。在 gRefCOCO 和 Ref-ZOM 基準上的廣泛實驗顯示，我們的模型顯著提升了現有技術的效能，為精確且彈性的 GRES 設定了新的標準。

##### **Towards Speaker Identification with Minimal Dataset and Constrained Resources using 1D-Convolution Neural Network**
2411.15082v1 by Irfan Nafiz Shahan, Pulok Ahmed Auvi

Voice recognition and speaker identification are vital for applications in
security and personal assistants. This paper presents a lightweight
1D-Convolutional Neural Network (1D-CNN) designed to perform speaker
identification on minimal datasets. Our approach achieves a validation accuracy
of 97.87%, leveraging data augmentation techniques to handle background noise
and limited training samples. Future improvements include testing on larger
datasets and integrating transfer learning methods to enhance generalizability.
We provide all code, the custom dataset, and the trained models to facilitate
reproducibility. These resources are available on our GitHub repository:
https://github.com/IrfanNafiz/RecMe.

摘要：語音辨識和說話者辨識對於安全和個人助理應用程式至關重要。本文提出了一個輕量級的 1D 卷積神經網路 (1D-CNN)，用於對最小的資料集執行說話者辨識。我們的做法達到了 97.87% 的驗證準確度，利用資料擴充技術來處理背景噪音和有限的訓練樣本。未來的改進包括在更大的資料集上進行測試，並整合遷移學習方法來增強泛化能力。我們提供所有程式碼、自訂資料集和訓練好的模型，以利於重現性。這些資源可在我們的 GitHub 儲存庫中取得：
https://github.com/IrfanNafiz/RecMe。

##### **Locating the Leading Edge of Cultural Change**
2411.15068v1 by Sarah Griebel, Becca Cohen, Lucian Li, Jaihyun Park, Jiayu Liu, Jana Perkins, Ted Underwood

Measures of textual similarity and divergence are increasingly used to study
cultural change. But which measures align, in practice, with social evidence
about change? We apply three different representations of text (topic models,
document embeddings, and word-level perplexity) to three different corpora
(literary studies, economics, and fiction). In every case, works by
highly-cited authors and younger authors are textually ahead of the curve. We
don't find clear evidence that one representation of text is to be preferred
over the others. But alignment with social evidence is strongest when texts are
represented through the top quartile of passages, suggesting that a text's
impact may depend more on its most forward-looking moments than on sustaining a
high level of innovation throughout.

摘要：文本相似性和差异性测量值正越来越多地用于研究文化变迁。但在实践中，哪些测量值与关于变迁的社会证据相符？我们对三个不同的语料库（文学研究、经济学和小说）应用了文本的三种不同表征（主题模型、文档嵌入和词级困惑度）。在每种情况下，被高度引用的作者和年轻作者的作品在文本上都领先于曲线。我们没有找到明确的证据表明文本的一种表征优于其他表征。但当文本通过前四分之一段落进行表征时，与社会证据的一致性最强，这表明文本的影响可能更多地取决于其最具前瞻性的时刻，而不是在整个过程中维持高水平的创新。

##### **Financial Risk Assessment via Long-term Payment Behavior Sequence Folding**
2411.15056v1 by Yiran Qiao, Yateng Tang, Xiang Ao, Qi Yuan, Ziming Liu, Chen Shen, Xuehao Zheng

Online inclusive financial services encounter significant financial risks due
to their expansive user base and low default costs. By real-world practice, we
reveal that utilizing longer-term user payment behaviors can enhance models'
ability to forecast financial risks. However, learning long behavior sequences
is non-trivial for deep sequential models. Additionally, the diverse fields of
payment behaviors carry rich information, requiring thorough exploitation.
These factors collectively complicate the task of long-term user behavior
modeling. To tackle these challenges, we propose a Long-term Payment Behavior
Sequence Folding method, referred to as LBSF. In LBSF, payment behavior
sequences are folded based on merchants, using the merchant field as an
intrinsic grouping criterion, which enables informative parallelism without
reliance on external knowledge. Meanwhile, we maximize the utility of payment
details through a multi-field behavior encoding mechanism. Subsequently,
behavior aggregation at the merchant level followed by relational learning
across merchants facilitates comprehensive user financial representation. We
evaluate LBSF on the financial risk assessment task using a large-scale
real-world dataset. The results demonstrate that folding long behavior
sequences based on internal behavioral cues effectively models long-term
patterns and changes, thereby generating more accurate user financial profiles
for practical applications.

摘要：線上包容性金融服務因其廣大的使用者基礎和低違約成本而面臨顯著的金融風險。根據實際操作，我們揭露了利用較長期使用者付款行為，可以提升模型預測金融風險的能力。然而，對於深度遞歸模型來說，學習長行為序列並非易事。此外，付款行為的多元領域承載豐富資訊，需要徹底利用。這些因素共同使長期使用者行為建模的任務複雜化。為了應對這些挑戰，我們提出長期付款行為序列摺疊方法，稱為 LBSF。在 LBSF 中，付款行為序列會根據商家摺疊，使用商家欄位作為內在的分組標準，這能實現有意義的平行運算，而無需依賴外部知識。同時，我們透過多欄位行為編碼機制，最大化付款明細的效用。隨後，在商家層級進行行為彙總，接著進行跨商家的關係學習，促成全面的使用者財務表徵。我們使用大規模的實際資料集，對金融風險評估任務評估 LBSF。結果證明，根據內部行為線索摺疊長行為序列，能有效地建構長期模式和變化，從而為實際應用產生更準確的使用者財務概況。

##### **Fantastic Biases (What are They) and Where to Find Them**
2411.15051v1 by Valentin Barriere

Deep Learning models tend to learn correlations of patterns on huge datasets.
The bigger these systems are, the more complex are the phenomena they can
detect, and the more data they need for this. The use of Artificial
Intelligence (AI) is becoming increasingly ubiquitous in our society, and its
impact is growing everyday. The promises it holds strongly depend on their fair
and universal use, such as access to information or education for all. In a
world of inequalities, they can help to reach the most disadvantaged areas.
However, such a universal systems must be able to represent society, without
benefiting some at the expense of others. We must not reproduce the
inequalities observed throughout the world, but educate these IAs to go beyond
them. We have seen cases where these systems use gender, race, or even class
information in ways that are not appropriate for resolving their tasks. Instead
of real causal reasoning, they rely on spurious correlations, which is what we
usually call a bias. In this paper, we first attempt to define what is a bias
in general terms. It helps us to demystify the concept of bias, to understand
why we can find them everywhere and why they are sometimes useful. Second, we
focus over the notion of what is generally seen as negative bias, the one we
want to avoid in machine learning, before presenting a general zoology
containing the most common of these biases. We finally conclude by looking at
classical methods to detect them, by means of specially crafted datasets of
templates and specific algorithms, and also classical methods to mitigate them.

摘要：深度學習模型傾向於學習龐大資料集中的模式關聯性。
這些系統越大，它們可以偵測的現象就越複雜，並且需要更多資料。人工
智慧 (AI) 的使用在我們社會中正變得越來越普遍，而且它的
影響力也與日俱增。它所承諾的承諾在很大程度上取決於它們的公平和普遍使用，例如讓所有人獲得資訊或教育。在一個不平等的世界中，它們可以幫助接觸到最弱勢的地區。
然而，這樣一個普遍的系統必須能夠代表社會，而不會以犧牲他人為代價來使某些人受益。我們不能複製全世界觀察到的不平等現象，而必須教育這些 AI 超越它們。我們已經看到這些系統以不適當的方式使用性別、種族甚至階級資訊來解決其任務的情況。它們不是依賴於真正的因果推理，而是依賴於虛假的關聯性，這正是我們通常所說的偏見。在本文中，我們首先嘗試定義什麼是一般意義上的偏見。它有助於我們揭開偏見的概念的神秘面紗，了解為什麼我們可以在任何地方找到它們，以及為什麼它們有時是有用的。其次，我們專注於通常被視為負面偏見的概念，這是我們希望在機器學習中避免的，然後展示一個包含這些偏見中最常見的偏見的通用動物學。最後，我們通過查看專門製作的範本和特定演算法的資料集，以及減輕它們的經典方法來檢視偵測它們的經典方法。

##### **Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications**
2411.15042v1 by Hong Ding, Ziming Wang, Yi Ding, Hongjie Lin, SuYang Xi, Chia Chao Kang

Addressing the challenge of ensuring safety in ever-changing and
unpredictable environments, particularly in the swiftly advancing realm of
autonomous driving in today's 5G wireless communication world, we present
Navigation Secure (NavSecure). This vision-based navigation framework merges
the strengths of world models with crucial safety-focused decision-making
capabilities, enabling autonomous vehicles to navigate real-world complexities
securely. Our approach anticipates potential threats and formulates safer
routes by harnessing the predictive capabilities of world models, thus
significantly reducing the need for extensive real-world trial-and-error
learning. Additionally, our method empowers vehicles to autonomously learn and
develop through continuous practice, ensuring the system evolves and adapts to
new challenges. Incorporating radio frequency technology, NavSecure leverages
5G networks to enhance real-time data exchange, improving communication and
responsiveness. Validated through rigorous experiments under simulation-to-real
driving conditions, NavSecure has shown exceptional performance in
safety-critical scenarios, such as sudden obstacle avoidance. Results indicate
that NavSecure excels in key safety metrics, including collision prevention and
risk reduction, surpassing other end-to-end methodologies. This framework not
only advances autonomous driving safety but also demonstrates how world models
can enhance decision-making in critical applications. NavSecure sets a new
standard for developing more robust and trustworthy autonomous driving systems,
capable of handling the inherent dynamics and uncertainties of real-world
environments.

摘要：<paragraph>針對確保在瞬息萬變且不可預測的環境中安全性的挑戰，特別是在當今 5G 無線通訊世界的自動駕駛快速發展領域中，我們提出了「導航安全」(NavSecure)。這個基於視覺的導航架構結合了世界模型的優點，以及以安全為重點的關鍵決策制定能力，讓自動駕駛車輛能夠安全地應對真實世界的複雜性。我們的做法預測潛在威脅，並透過利用世界模型的預測能力制定更安全的路線，從而大幅減少對廣泛的真實世界試錯學習的需求。此外，我們的做法賦予車輛自主學習和發展的能力，透過持續練習，確保系統能夠演進並適應新的挑戰。NavSecure 結合射頻技術，利用 5G 網路來增強即時資料交換，改善通訊和回應能力。NavSecure 經過模擬到真實駕駛條件下的嚴格實驗驗證，已在安全關鍵情境中展現出非凡的效能，例如突然避開障礙物。結果顯示，NavSecure 在關鍵的安全指標中表現出色，包括防碰撞和降低風險，超越其他端到端的方法。這個架構不僅提升了自動駕駛的安全性，也展示了世界模型如何能增強關鍵應用中的決策制定。NavSecure 為開發更強大且值得信賴的自動駕駛系統樹立了新的標準，能夠應對真實世界環境中固有的動態和不確定性。</paragraph>

##### **mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA**
2411.15041v1 by Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, Weiming Hu

Advanced Multimodal Large Language Models (MLLMs) struggle with recent
Knowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their
limited and frozen knowledge scope, often leading to ambiguous and inaccurate
responses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally
introduced to provide MLLMs with comprehensive and up-to-date knowledge,
effectively expanding the knowledge scope. However, current mRAG methods have
inherent drawbacks, including: 1) Performing retrieval even when external
knowledge is not needed. 2) Lacking of identification of evidence that supports
the query. 3) Increasing model complexity due to additional information
filtering modules or rules. To address these shortcomings, we propose a novel
generalized framework called \textbf{m}ultimodal
\textbf{R}etrieval-\textbf{R}eflection-\textbf{A}ugmented \textbf{G}eneration
(mR$^2$AG), which achieves adaptive retrieval and useful information
localization to enable answers through two easy-to-implement reflection
operations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection
is designed to distinguish different user queries and avoids redundant
retrieval calls, and Relevance-Reflection is introduced to guide the MLLM in
locating beneficial evidence of the retrieved content and generating answers
accordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM
with efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset
(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,
GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while
maintaining the exceptional capabilities of base MLLMs across a wide range of
Visual-dependent tasks.

摘要：先進的多模態大型語言模型 (MLLM) 在基於知識的 VQA 任務中遇到困難，例如 INFOSEEK 和百科全書 VQA，原因在於它們的知識範圍有限且已凍結，這通常會導致模棱兩可且不準確的回應。因此，多模態檢索增強生成 (mRAG) 自然而然地被引入，以向 MLLM 提供全面且最新的知識，有效地擴展了知識範圍。然而，當前的 mRAG 方法有其固有的缺點，包括：1) 即使不需要外部知識，也會執行檢索。2) 缺乏對支持查詢證據的識別。3) 由於額外的資訊過濾模組或規則，導致模型複雜度增加。為了解決這些缺點，我們提出了一個新穎的通用架構，稱為 **m**ultimodal **R**etrieval-**R**eflection-**A**ugmented **G**eneration (mR$^2$AG)，它通過兩個易於實作的反射操作實現了適應性檢索和有用的資訊定位，以啟用答案，防止模型複雜度高。在 mR$^2$AG 中，檢索反射被設計為區分不同的使用者查詢，並避免重複的檢索呼叫，而相關性反射被引入以指導 MLLM 定位檢索內容的有益證據，並據此產生答案。此外，mR$^2$AG 可以整合到任何訓練有素的 MLLM 中，並在提議的 mR$^2$AG 指令微調資料集 (mR$^2$AG-IT) 上進行有效的微調。mR$^2$AG 在 INFOSEEK 和百科全書 VQA 上明顯優於最先進的 MLLM（例如 GPT-4v/o）和基於 RAG 的 MLLM，同時在廣泛的視覺依賴任務中保持了基礎 MLLM 的卓越能力。

##### **One to rule them all: natural language to bind communication, perception and action**
2411.15033v1 by Simone Colombani, Dimitri Ognibene, Giuseppe Boccignone

In recent years, research in the area of human-robot interaction has focused
on developing robots capable of understanding complex human instructions and
performing tasks in dynamic and diverse environments. These systems have a wide
range of applications, from personal assistance to industrial robotics,
emphasizing the importance of robots interacting flexibly, naturally and safely
with humans. This paper presents an advanced architecture for robotic action
planning that integrates communication, perception, and planning with Large
Language Models (LLMs). Our system is designed to translate commands expressed
in natural language into executable robot actions, incorporating environmental
information and dynamically updating plans based on real-time feedback. The
Planner Module is the core of the system where LLMs embedded in a modified
ReAct framework are employed to interpret and carry out user commands. By
leveraging their extensive pre-trained knowledge, LLMs can effectively process
user requests without the need to introduce new knowledge on the changing
environment. The modified ReAct framework further enhances the execution space
by providing real-time environmental perception and the outcomes of physical
actions. By combining robust and dynamic semantic map representations as graphs
with control components and failure explanations, this architecture enhances a
robot adaptability, task execution, and seamless collaboration with human users
in shared and dynamic environments. Through the integration of continuous
feedback loops with the environment the system can dynamically adjusts the plan
to accommodate unexpected changes, optimizing the robot ability to perform
tasks. Using a dataset of previous experience is possible to provide detailed
feedback about the failure. Updating the LLMs context of the next iteration
with suggestion on how to overcame the issue.

摘要：近年来，人机交互领域的研究重点
在于开发能够理解复杂人类指令并在动态和多样化环境中执行任务的机器人。这些系统具有广泛的应用，从个人助理到工业机器人，强调了机器人与人类灵活、自然和安全交互的重要性。本文提出了一种先进的机器人动作规划架构，该架构集成了通信、感知和规划与大型语言模型 (LLM)。我们的系统旨在将以自然语言表达的命令翻译成可执行的机器人动作，并结合环境信息并根据实时反馈动态更新计划。规划器模块是系统的核心，其中嵌入在修改后的 ReAct 框架中的 LLM 用于解释和执行用户命令。通过利用其广泛的预训练知识，LLM 可以有效处理用户请求，而无需引入有关不断变化的环境的新知识。修改后的 ReAct 框架通过提供实时环境感知和物理动作的结果进一步增强了执行空间。通过将鲁棒且动态语义地图表示与控制组件和故障解释相结合，该架构增强了机器人的适应性、任务执行以及与人类用户在共享和动态环境中的无缝协作。通过将连续反馈回路与环境相结合，系统可以动态调整计划以适应意外变化，从而优化机器人执行任务的能力。利用先前的经验数据集，可以提供有关故障的详细反馈。使用有关如何克服问题的建议更新下一个迭代的 LLM 上下文。

##### **Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot**
2411.15027v1 by Simone Colombani, Luca Brini, Dimitri Ognibene, Giuseppe Boccignone

Robots are increasingly being used in dynamic environments like workplaces,
hospitals, and homes. As a result, interactions with robots must be simple and
intuitive, with robots perception adapting efficiently to human-induced
changes. This paper presents a robot control architecture that addresses key
challenges in human-robot interaction, with a particular focus on the dynamic
creation and continuous update of the robot state representation. The
architecture uses Large Language Models to integrate diverse information
sources, including natural language commands, robotic skills representation,
real-time dynamic semantic mapping of the perceived scene. This enables
flexible and adaptive robotic behavior in complex, dynamic environments.
Traditional robotic systems often rely on static, pre-programmed instructions
and settings, limiting their adaptability to dynamic environments and real-time
collaboration. In contrast, this architecture uses LLMs to interpret complex,
high-level instructions and generate actionable plans that enhance human-robot
collaboration. At its core, the system Perception Module generates and
continuously updates a semantic scene graph using RGB-D sensor data, providing
a detailed and structured representation of the environment. A particle filter
is employed to ensure accurate object localization in dynamic, real-world
settings. The Planner Module leverages this up-to-date semantic map to break
down high-level tasks into sub-tasks and link them to robotic skills such as
navigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,
GOTO). By combining real-time perception, state tracking, and LLM-driven
communication and task planning, the architecture enhances adaptability, task
efficiency, and human-robot collaboration in dynamic environments.

摘要：<paragraph>機器人正越來越廣泛地應用於工作場所、醫院和家庭等動態環境中。因此，與機器人的互動必須簡單直觀，機器人的感知能力必須有效適應人類引發的變化。本文提出了一種機器人控制架構，用於解決人機互動中的關鍵挑戰，特別關注機器人狀態表示的動態建立和持續更新。該架構使用大型語言模型整合多種資訊來源，包括自然語言命令、機器人技能表示、感知場景的即時動態語義對應。這使得機器人在複雜的動態環境中能夠靈活適應。傳統的機器人系統通常依賴於靜態的、預先編程的指令和設定，這限制了它們對動態環境和即時協作的適應能力。相比之下，此架構使用 LLM 來詮釋複雜的高層級指令，並制定可行的計畫，以增強人機協作。在系統的核心，感知模組使用 RGB-D 感測器資料產生並持續更新語義場景圖，提供環境的詳細且結構化的表示。採用粒子濾波器以確保在動態的真實世界設定中準確定位物件。規劃模組利用這個最新的語義地圖，將高層級任務分解為子任務，並將它們連結到機器人技能，例如導航、物件操作（例如，取放）和移動（例如，前往）。透過結合即時感知、狀態追蹤和 LLM 驅動的溝通和任務規劃，此架構增強了動態環境中的適應能力、任務效率和人機協作。</paragraph>

##### **Evolutionary Automata and Deep Evolutionary Computation**
2411.15008v1 by Eugene Eberbach

Evolution by natural selection, which is one of the most compelling themes of
modern science, brought forth evolutionary algorithms and evolutionary
computation, applying mechanisms of evolution in nature to various problems
solved by computers. In this paper we concentrate on evolutionary automata that
constitute an analogous model of evolutionary computation compared to
well-known evolutionary algorithms. Evolutionary automata provide a more
complete dual model of evolutionary computation, similar like abstract automata
(e.g., Turing machines) form a more formal and precise model compared to
recursive algorithms and their subset - evolutionary algorithms. An
evolutionary automaton is an automaton that evolves performing evolutionary
computation perhaps using an infinite number of generations. This model allows
for a direct modeling evolution of evolution, and leads to tremendous
expressiveness of evolutionary automata and evolutionary computation. This also
gives the hint to the power of natural evolution that is self-evolving by
interactive feedback with the environment.

摘要：自然選擇的演化，這是現代科學最引人入勝的主題之一，催生了演化演算法和演化運算，將自然中的演化機制應用於電腦解決的各種問題。在本文中，我們專注於演化自動機，與眾所周知的演化演算法相比，它構成了一個類似的演化運算模型。演化自動機提供了一個更完整的演化運算雙重模型，類似於抽象自動機（例如，圖靈機）與遞迴演算法及其子集（演化演算法）相比，形成了更正式且精確的模型。演化自動機是一種自動機，它演化執行演化運算，可能使用無限代數。此模型允許直接建模演化的演化，並導致演化自動機和演化運算的巨大表現力。這也暗示了自然演化的力量，它通過與環境的互動回饋而自我演化。

##### **FTA generation using GenAI with an Autonomy sensor Usecase**
2411.15007v1 by Sneha Sudhir Shetiya, Divya Garikapati, Veeraja Sohoni

Functional safety forms an important aspect in the design of systems. Its
emphasis on the automotive industry has evolved significantly over the years.
Till date many methods have been developed to get appropriate FTA(Fault Tree
analysis) for various scenarios and features pertaining to Autonomous Driving.
This paper is an attempt to explore the scope of using Generative Artificial
Intelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use
case of malfunction for the Lidar sensor in mind. We explore various available
open source Large Language Models(LLM) models and then dive deep into one of
them to study its responses and provide our analysis. This paper successfully
shows the possibility to train existing Large Language models through Prompt
Engineering for fault tree analysis for any Autonomy usecase aided with
PlantUML tool.

摘要：功能安全在系統設計中形成了一個重要的面向。它對汽車產業的重視多年來已顯著演變。迄今為止，已經開發出許多方法來取得各種情境和與自動駕駛相關特性的適當 FTA（故障樹分析）。本文嘗試探討使用生成式人工智慧 (GenAI) 的範圍，以便開發故障樹分析 (FTA)，並以光達感測器的故障用例為考量。我們探討各種可用的開源大型語言模型 (LLM) 模型，然後深入探討其中一個模型以研究其回應並提供我們的分析。本文成功展示了透過提示工程訓練現有大型語言模型的可能性，以進行任何自主用例的故障樹分析，並輔以 PlantUML 工具。

##### **ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data**
2411.15004v1 by Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, Ameet Talwalkar

Large Language Model (LLM) agents are rapidly improving to handle
increasingly complex web-based tasks. Most of these agents rely on
general-purpose, proprietary models like GPT-4 and focus on designing better
prompts to improve their planning abilities. However, general-purpose LLMs are
not specifically trained to understand specialized web contexts such as HTML,
and they often struggle with long-horizon planning. We explore an alternative
approach that fine-tunes open-source LLMs using production-scale workflow data
collected from over 250 domains corresponding to 6 billion tokens. This simple
yet effective approach shows substantial gains over prompting-based agents on
existing benchmarks -- ScribeAgent achieves state-of-the-art direct generation
performance on Mind2Web and improves the task success rate by 14.1% over the
previous best text-only web agents on WebArena. We further perform detailed
ablation studies on various fine-tuning design choices and provide insights
into LLM selection, training recipes, context window optimization, and effect
of dataset sizes.

摘要：大型語言模型 (LLM) 代理正在快速改進，以處理日益複雜的基於網路的任務。這些代理程式大多依賴於通用專有模型，例如 GPT-4，並專注於設計更好的提示，以提高其規劃能力。然而，通用 LLM 並未經過專門訓練來理解 HTML 等專業網路環境，而且它們通常難以進行長期規劃。我們探索了一種替代方法，該方法使用從超過 250 個網域收集的生產規模工作流程資料，對應於 60 億個代幣，對開源 LLM 進行微調。這種簡單但有效的方法在現有基準上顯示出比基於提示的代理顯著的進步——ScribeAgent 在 Mind2Web 上實現了最先進的直接生成效能，並將 WebArena 上先前最佳純文字網路代理的任務成功率提高了 14.1%。我們進一步對各種微調設計選項進行了詳細的消融研究，並提供了對 LLM 選擇、訓練配方、環境視窗最佳化和資料集大小影響的見解。

##### **Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution**
2411.14995v1 by Jonas Gösgens, Niklas Jansen, Hector Geffner

Learning STRIPS action models from action traces alone is a challenging
problem as it involves learning the domain predicates as well. In this work, a
novel approach is introduced which, like the well-known LOCM systems, is
scalable, but like SAT approaches, is sound and complete. Furthermore, the
approach is general and imposes no restrictions on the hidden domain or the
number or arity of the predicates. The new learning method is based on an
\emph{efficient, novel test} that checks whether the assumption that a
predicate is affected by a set of action patterns, namely, actions with
specific argument positions, is consistent with the traces. The predicates and
action patterns that pass the test provide the basis for the learned domain
that is then easily completed with preconditions and static predicates. The new
method is studied theoretically and experimentally. For the latter, the method
is evaluated on traces and graphs obtained from standard classical domains like
the 8-puzzle, which involve hundreds of thousands of states and transitions.
The learned representations are then verified on larger instances.

摘要：僅從動作軌跡學習 STRIPS 動作模型是一項具有挑戰性的問題，因為它涉及學習領域謂詞。在這項工作中，引入了一種新方法，就像眾所周知的 LOCM 系統一樣，它是可擴展的，但就像 SAT 方法一樣，它是健全且完整的。此外，該方法是通用的，對隱藏域或謂詞的數量或元數沒有限制。新的學習方法基於一個高效的新測試，該測試檢查假設謂詞受一組動作模式（即具有特定參數位置的動作）影響是否與軌跡一致。通過測試的謂詞和動作模式為學習的領域提供了基礎，然後可以輕鬆地用先決條件和靜態謂詞完成該領域。對新方法進行了理論和實驗研究。對於後者，該方法在從標準經典域（如涉及數十萬個狀態和轉換的 8-puzzle）獲得的軌跡和圖形上進行了評估。然後在更大的實例上驗證學習的表示。

##### **Free Energy Projective Simulation (FEPS): Active inference with interpretability**
2411.14991v1 by Joséphine Pazem, Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer, Hans J. Briegel

In the last decade, the free energy principle (FEP) and active inference
(AIF) have achieved many successes connecting conceptual models of learning and
cognition to mathematical models of perception and action. This effort is
driven by a multidisciplinary interest in understanding aspects of
self-organizing complex adaptive systems, including elements of agency. Various
reinforcement learning (RL) models performing active inference have been
proposed and trained on standard RL tasks using deep neural networks. Recent
work has focused on improving such agents' performance in complex environments
by incorporating the latest machine learning techniques. In this paper, we take
an alternative approach. Within the constraints imposed by the FEP and AIF, we
attempt to model agents in an interpretable way without deep neural networks by
introducing Free Energy Projective Simulation (FEPS). Using internal rewards
only, FEPS agents build a representation of their partially observable
environments with which they interact. Following AIF, the policy to achieve a
given task is derived from this world model by minimizing the expected free
energy. Leveraging the interpretability of the model, techniques are introduced
to deal with long-term goals and reduce prediction errors caused by erroneous
hidden state estimation. We test the FEPS model on two RL environments inspired
from behavioral biology: a timed response task and a navigation task in a
partially observable grid. Our results show that FEPS agents fully resolve the
ambiguity of both environments by appropriately contextualizing their
observations based on prediction accuracy only. In addition, they infer optimal
policies flexibly for any target observation in the environment.

摘要：<paragraph>在過去十年中，自由能原理 (FEP) 和主動推論 (AIF) 在將學習和認知的概念模型與感知和動作的數學模型連結起來方面取得許多成功。這項工作是由跨領域的興趣所驅動，目的是了解自組織複雜適應系統的各個方面，包括能動性元素。各種執行主動推論的強化學習 (RL) 模型已被提出，並使用深度神經網路在標準 RL 任務上進行訓練。最近的工作重點在於透過納入最新的機器學習技術來改善此類代理在複雜環境中的表現。在本文中，我們採取另一種方法。在 FEP 和 AIF 所施加的限制下，我們嘗試透過引入自由能投影模擬 (FEPS) 以可解釋的方式對代理進行建模，而無需使用深度神經網路。僅使用內部獎勵，FEPS 代理就能建立其部分可觀察環境的表徵，並與其互動。遵循 AIF，透過最小化預期的自由能，從這個世界模型中推導出達成特定任務的政策。運用模型的可解釋性，引入了應對長期目標和減少由錯誤隱藏狀態估計所造成的預測誤差的技術。我們在兩個受行為生物學啟發的 RL 環境中測試 FEPS 模型：一個計時反應任務和一個部分可觀察網格中的導航任務。我們的結果顯示，FEPS 代理僅根據預測準確性適當地將其觀察結果情境化，從而完全解決了這兩個環境的模糊性。此外，它們會靈活地推論環境中任何目標觀察的最佳政策。</paragraph>

##### **Large Multi-modal Models Can Interpret Features in Large Multi-modal Models**
2411.14982v1 by Kaichen Zhang, Yifei Shen, Bo Li, Ziwei Liu

Recent advances in Large Multimodal Models (LMMs) lead to significant
breakthroughs in both academia and industry. One question that arises is how
we, as humans, can understand their internal neural representations. This paper
takes an initial step towards addressing this question by presenting a
versatile framework to identify and interpret the semantics within LMMs.
Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the
representations into human understandable features. 2) We then present an
automatic interpretation framework to interpreted the open-semantic features
learned in SAE by the LMMs themselves. We employ this framework to analyze the
LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these
features can effectively steer the model's behavior. Our results contribute to
a deeper understanding of why LMMs excel in specific tasks, including EQ tests,
and illuminate the nature of their mistakes along with potential strategies for
their rectification. These findings offer new insights into the internal
mechanisms of LMMs and suggest parallels with the cognitive processes of the
human brain.

摘要：大型多模态模型 (LMM) 的最新进展为学术界和产业界带来了重大突破。一个出现的问题是我们作为人类如何理解其内部神经表征。本文通过提出一个通用的框架来识别和解释 LMM 中的语义，迈出了解决这个问题的第一步。具体来说，1) 我们首先应用稀疏自动编码器 (SAE) 将表征分解为人类可理解的特征。2) 然后，我们提出一个自动解释框架，由 LMM 本身解释在 SAE 中学习的开放语义特征。我们使用 LLaVA-OV-72B 模型分析 LLaVA-NeXT-8B 模型，证明这些特征可以有效地引导模型的行为。我们的研究结果有助于更深入地理解 LMM 在特定任务（包括情商测试）中表现出色的原因，并阐明其错误的本质以及潜在的纠正策略。这些发现为 LMM 的内部机制提供了新的见解，并暗示了与人脑认知过程的相似之处。

##### **Exploring Foundation Models Fine-Tuning for Cytology Classification**
2411.14975v1 by Manon Dausort, Tiffanie Godelaine, Maxime Zanella, Karim El Khoury, Isabelle Salmon, Benoît Macq

Cytology slides are essential tools in diagnosing and staging cancer, but
their analysis is time-consuming and costly. Foundation models have shown great
potential to assist in these tasks. In this paper, we explore how existing
foundation models can be applied to cytological classification. More
particularly, we focus on low-rank adaptation, a parameter-efficient
fine-tuning method suited to few-shot learning. We evaluated five foundation
models across four cytological classification datasets. Our results demonstrate
that fine-tuning the pre-trained backbones with LoRA significantly improves
model performance compared to fine-tuning only the classifier head, achieving
state-of-the-art results on both simple and complex classification tasks while
requiring fewer data samples.

摘要：細胞學載玻片是診斷和分期癌症的重要工具，但其分析既耗時又昂貴。基礎模型已顯示出在這些任務中提供協助的巨大潛力。在本文中，我們探討了如何將現有的基礎模型應用於細胞學分類。更具體地說，我們專注於低秩適應，這是一種適合於少次學習的參數高效微調方法。我們評估了五個基礎模型，涵蓋四個細胞學分類數據集。我們的結果表明，與僅微調分類器頭部相比，使用 LoRA 微調預訓練的骨幹顯著提高了模型性能，在簡單和複雜的分類任務上都取得了最先進的結果，同時需要更少數據樣本。

##### **Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models**
2411.14972v1 by Alec Wright, Alistair Carson, Lauri Juvela

This paper introduces Open-Amp, a synthetic data framework for generating
large-scale and diverse audio effects data. Audio effects are relevant to many
musical audio processing and Music Information Retrieval (MIR) tasks, such as
modelling of analog audio effects, automatic mixing, tone matching and
transcription. Existing audio effects datasets are limited in scope, usually
including relatively few audio effects processors and a limited amount of input
audio signals. Our proposed framework overcomes these issues, by crowdsourcing
neural network emulations of guitar amplifiers and effects, created by users of
open-source audio effects emulation software. This allows users of Open-Amp
complete control over the input signals to be processed by the effects models,
as well as providing high-quality emulations of hundreds of devices. Open-Amp
can render audio online during training, allowing great flexibility in data
augmentation. Our experiments show that using Open-Amp to train a guitar
effects encoder achieves new state-of-the-art results on multiple guitar
effects classification tasks. Furthermore, we train a one-to-many guitar
effects model using Open-Amp, and use it to emulate unseen analog effects via
manipulation of its learned latent space, indicating transferability to analog
guitar effects data.

摘要：本文介紹 Open-Amp，一種用於生成大規模且多樣化音效資料的合成資料架構。音效與許多音樂音訊處理和音樂資訊檢索 (MIR) 任務相關，例如類比音效建模、自動混音、音色匹配和轉錄。現有的音效資料集範圍有限，通常只包含相對較少的音效處理器和有限的輸入音訊訊號。我們提出的架構克服了這些問題，透過群眾外包由開源音效模擬軟體使用者建立的吉他擴大器和效果的神經網路模擬。這讓 Open-Amp 的使用者可以完全控制要由效果模型處理的輸入訊號，並提供數百種裝置的高品質模擬。Open-Amp 能在訓練期間線上渲染音訊，讓資料擴充極具彈性。我們的實驗顯示，使用 Open-Amp 訓練吉他效果編碼器在多個吉他效果分類任務中達成新的最先進成果。此外，我們使用 Open-Amp 訓練一個一對多的吉他效果模型，並使用它透過操作其學習到的潛在空間來模擬未見的類比效果，表示可轉移到類比吉他效果資料。

##### **SwissADT: An Audio Description Translation System for Swiss Languages**
2411.14967v1 by Lukas Fischer, Yingqiang Gao, Alexa Lintner, Sarah Ebling

Audio description (AD) is a crucial accessibility service provided to blind
persons and persons with visual impairment, designed to convey visual
information in acoustic form. Despite recent advancements in multilingual
machine translation research, the lack of well-crafted and time-synchronized AD
data impedes the development of audio description translation (ADT) systems
that address the needs of multilingual countries such as Switzerland.
Furthermore, since the majority of ADT systems rely solely on text, uncertainty
exists as to whether incorporating visual information from the corresponding
video clips can enhance the quality of ADT outputs. In this work, we present
SwissADT, the first ADT system implemented for three main Swiss languages and
English. By collecting well-crafted AD data augmented with video clips in
German, French, Italian, and English, and leveraging the power of Large
Language Models (LLMs), we aim to enhance information accessibility for diverse
language populations in Switzerland by automatically translating AD scripts to
the desired Swiss language. Our extensive experimental ADT results, composed of
both automatic and human evaluations of ADT quality, demonstrate the promising
capability of SwissADT for the ADT task. We believe that combining human
expertise with the generation power of LLMs can further enhance the performance
of ADT systems, ultimately benefiting a larger multilingual target population.

摘要：音訊描述 (AD) 是一項重要的無障礙服務，提供給失明和視力受損人士，旨在以聲音形式傳達視覺資訊。儘管多語言機器翻譯研究最近有進展，但缺乏精心製作且時間同步的 AD 資料，阻礙了音訊描述翻譯 (ADT) 系統的發展，而這些系統能滿足瑞士等多語言國家的需求。此外，由於大多數 ADT 系統僅依賴文字，因此無法確定是否納入對應影片剪輯的視覺資訊可以提升 ADT 輸出的品質。在這項工作中，我們提出 SwissADT，這是第一個為三種主要的瑞士語言和英語實作的 ADT 系統。透過收集以德語、法語、義大利語和英語為輔的精心製作 AD 資料，並利用大型語言模型 (LLM) 的強大功能，我們旨在透過自動將 AD 腳本翻譯成所需的瑞士語言，來提升瑞士不同語言族群的資訊無障礙性。我們的廣泛實驗性 ADT 結果，包括 ADT 品質的自動評估和人工評估，證明了 SwissADT 在 ADT 任務中具有令人期待的能力。我們相信，將人類專業知識與 LLM 的生成能力結合，可以進一步提升 ADT 系統的效能，最終讓更多元的目標多語言族群受益。

##### **LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents**
2411.14962v1 by Hitesh Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Karan Gupta, Priyaranjan Pattnayak

Accurate barcode detection and decoding in Identity documents is crucial for
applications like security, healthcare, and education, where reliable data
extraction and verification are essential. However, building robust detection
models is challenging due to the lack of diverse, realistic datasets an issue
often tied to privacy concerns and the wide variety of document formats.
Traditional tools like Faker rely on predefined templates, making them less
effective for capturing the complexity of real-world identity documents. In
this paper, we introduce a new approach to synthetic data generation that uses
LLMs to create contextually rich and realistic data without relying on
predefined field. Using the vast knowledge LLMs have about different documents
and content, our method creates data that reflects the variety found in real
identity documents. This data is then encoded into barcode and overlayed on
templates for documents such as Driver's licenses, Insurance cards, Student
IDs. Our approach simplifies the process of dataset creation, eliminating the
need for extensive domain knowledge or predefined fields. Compared to
traditional methods like Faker, data generated by LLM demonstrates greater
diversity and contextual relevance, leading to improved performance in barcode
detection models. This scalable, privacy-first solution is a big step forward
in advancing machine learning for automated document processing and identity
verification.

摘要：準確條碼辨識與解碼身分證件對於安全、醫療保健和教育等應用程式至關重要，在這些應用程式中，可靠的資料擷取和驗證至關重要。然而，由於缺乏多樣化、真實的資料集，建立強大的偵測模型具有挑戰性，而這個問題通常與隱私問題和文件格式的多樣性有關。像是 Faker 等傳統工具依賴於預定義的範本，這使得它們在捕捉真實世界身分證件的複雜性方面效率較低。在本文中，我們介紹一種新的合成資料生成方法，該方法使用 LLM 來建立豐富且真實的資料，而無需依賴預定義的欄位。利用 LLM 對不同文件和內容的廣泛知識，我們的技術建立了反映真實身分證件中多樣性的資料。然後將這些資料編碼成條碼，並疊加在駕照、保險卡、學生證等文件的範本上。我們的技術簡化了資料集建立的流程，消除了對廣泛領域知識或預定義欄位的需求。與 Faker 等傳統方法相比，LLM 生成的資料表現出更大的多樣性和上下文相關性，從而提升條碼偵測模型的效能。這個可擴充、以隱私為優先的解決方案是推進機器學習用於自動文件處理和身分驗證的重要一步。

##### **Design-o-meter: Towards Evaluating and Refining Graphic Designs**
2411.14959v1 by Sahil Goyal, Abhinav Mahajan, Swasti Mishra, Prateksha Udhayanan, Tripti Shukla, K J Joseph, Balaji Vasan Srinivasan

Graphic designs are an effective medium for visual communication. They range
from greeting cards to corporate flyers and beyond. Off-late, machine learning
techniques are able to generate such designs, which accelerates the rate of
content production. An automated way of evaluating their quality becomes
critical. Towards this end, we introduce Design-o-meter, a data-driven
methodology to quantify the goodness of graphic designs. Further, our approach
can suggest modifications to these designs to improve its visual appeal. To the
best of our knowledge, Design-o-meter is the first approach that scores and
refines designs in a unified framework despite the inherent subjectivity and
ambiguity of the setting. Our exhaustive quantitative and qualitative analysis
of our approach against baselines adapted for the task (including recent
Multimodal LLM-based approaches) brings out the efficacy of our methodology. We
hope our work will usher more interest in this important and pragmatic problem
setting.

摘要：平面設計是一種有效的視覺溝通媒介。其範圍從賀卡到企業傳單，甚至更多。最近，機器學習技術能夠產生此類設計，這加速了內容製作的速度。自動化評估其品質的方法變得至關重要。為此，我們引入了 Design-o-meter，這是一種數據驅動的方法，用於量化平面設計的優劣。此外，我們的方法可以建議修改這些設計，以提高其視覺吸引力。據我們所知，Design-o-meter 是第一種在統一框架中評分和改進設計的方法，儘管該設置具有固有的主觀性和模糊性。我們針對針對任務調整的基準（包括最近的多模態 LLM 基於方法）對我們的方法進行了詳盡的定量和定性分析，這突顯了我們方法論的有效性。我們希望我們的努力將為這個重要且務實的問題設置帶來更多興趣。

##### **Information Extraction from Heterogenous Documents without Ground Truth Labels using Synthetic Label Generation and Knowledge Distillation**
2411.14957v1 by Aniket Bhattacharyya, Anurag Tripathi

Invoices and receipts submitted by employees are visually rich documents
(VRDs) with textual, visual and layout information. To protect against the risk
of fraud and abuse, it is crucial for organizations to efficiently extract
desired information from submitted receipts. This helps in the assessment of
key factors such as appropriateness of the expense claim, adherence to spending
and transaction policies, the validity of the receipt, as well as downstream
anomaly detection at various levels. These documents are heterogenous, with
multiple formats and languages, uploaded with different image qualities, and
often do not contain ground truth labels for the efficient training of models.
In this paper we propose Task Aware Instruction-based Labelling (TAIL), a
method for synthetic label generation in VRD corpuses without labels, and
fine-tune a multimodal Visually Rich Document Understanding Model (VRDU) on
TAIL labels using response-based knowledge distillation without using the
teacher model's weights or training dataset to conditionally generate
annotations in the appropriate format. Using a benchmark external dataset where
ground truth labels are available, we demonstrate conditions under which our
approach performs at par with Claude 3 Sonnet through empirical studies. We
then show that the resulting model performs at par or better on the internal
expense documents of a large multinational organization than state-of-the-art
LMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and
~5X faster, and outperforms layout-aware baselines by more than 10% in Average
Normalized Levenshtein Similarity (ANLS) scores due to its ability to reason
and extract information from rare formats. Finally, we illustrate the usage of
our approach in overpayment prevention.

摘要：員工提交的發票和收據是視覺豐富文件 (VRD)，包含文字、視覺和版面資訊。為了防範詐欺和濫用風險，組織有效從已提交收據中萃取所需資訊至關重要。這有助於評估關鍵因素，例如費用申報是否適當、是否遵守支出和交易政策、收據的有效性，以及各種層級的下游異常偵測。這些文件是異質的，具有多種格式和語言，上傳時具有不同的影像品質，而且通常不包含有效標籤，無法有效訓練模型。在本文中，我們提出任務感知指令標籤 (TAIL)，這是一種在沒有標籤的 VRD 語料庫中進行合成標籤產生方法，並使用基於回應的知識蒸餾，微調多模態視覺豐富文件理解模型 (VRDU) 的 TAIL 標籤，而不用教師模型的權重或訓練資料集來有條件地產生適當格式的註解。使用基準外部資料集（其中有有效標籤），我們透過實證研究展示了在哪些條件下，我們的做法與 Claude 3 Sonnet 表現相當。接著我們展示，對於大型跨國組織的內部費用文件，最終產生的模型表現與最先進的大型多模態模型 (LMM) Claude 3 Sonnet 相當或更好，同時成本降低 85%，速度快約 5 倍，而且由於其推理和從罕見格式萃取資訊的能力，在平均標準化 Levenshtein 相似度 (ANLS) 分數上比考量版面的基準高出 10% 以上。最後，我們說明了在防止多付中使用我們做法的方式。

##### **Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing**
2411.14953v1 by Miriam Alber, Christoph Hönes, Patrick Baier

One of the most promising use-cases for machine learning in industrial
manufacturing is the early detection of defective products using a quality
control system. Such a system can save costs and reduces human errors due to
the monotonous nature of visual inspections. Today, a rich body of research
exists which employs machine learning methods to identify rare defective
products in unbalanced visual quality control datasets. These methods typically
rely on two components: A visual backbone to capture the features of the input
image and an anomaly detection algorithm that decides if these features are
within an expected distribution. With the rise of transformer architecture as
visual backbones of choice, there exists now a great variety of different
combinations of these two components, ranging all along the trade-off between
detection quality and inference time. Facing this variety, practitioners in the
field often have to spend a considerable amount of time on researching the
right combination for their use-case at hand. Our contribution is to help
practitioners with this choice by reviewing and evaluating current vision
transformer models together with anomaly detection methods. For this, we chose
SotA models of both disciplines, combined them and evaluated them towards the
goal of having small, fast and efficient anomaly detection models suitable for
industrial manufacturing. We evaluated the results of our experiments on the
well-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing
a suitable model architecture for a quality control system in practice,
considering given use-case and hardware constraints.

摘要：機器學習在工業製造中最有前景的應用案例之一，便是利用品質控管系統及早偵測出有缺陷的產品。此類系統可以節省成本，並減少由於視覺檢查的單調性而造成的人為錯誤。如今，已有一大批研究採用機器學習方法，用於識別不平衡視覺品質控管資料集中的罕見缺陷產品。這些方法通常仰賴兩個組成部分：一個視覺主幹，用於擷取輸入影像的特徵，以及一個異常偵測演算法，用於判斷這些特徵是否在預期的分佈中。隨著Transformer架構崛起，成為視覺主幹的首選，現在存在各種各樣的這兩個組成部分的組合，在偵測品質和推論時間之間進行權衡。面對這種多樣性，該領域的從業人員通常必須花費大量時間研究適合其手邊使用案例的正確組合。我們的貢獻是透過檢視和評估目前的視覺Transformer模型以及異常偵測方法，來協助從業人員做出此項選擇。為此，我們選擇了兩個領域的 SotA 模型，將它們結合起來，並針對小、快、高效的異常偵測模型的目標，對它們進行評估，以適用於工業製造。我們在著名的 MVTecAD 和 BTAD 資料集上評估了我們實驗的結果。此外，我們提供了在實務中為品質控管系統選擇合適模型架構的準則，考量既定的使用案例和硬體限制。

##### **Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning**
2411.14937v1 by Junjie Shan, Ziqi Zhao, Jialin Lu, Rui Zhang, Siu Ming Yiu, Ka-Ho Chow

Foundation models that bridge vision and language have made significant
progress, inspiring numerous life-enriching applications. However, their
potential for misuse to introduce new threats remains largely unexplored. This
paper reveals that vision-language models (VLMs) can be exploited to overcome
longstanding limitations in gradient inversion attacks (GIAs) within federated
learning (FL), where an FL server reconstructs private data samples from
gradients shared by victim clients. Current GIAs face challenges in
reconstructing high-resolution images, especially when the victim has a large
local data batch. While focusing reconstruction on valuable samples rather than
the entire batch is promising, existing methods lack the flexibility to allow
attackers to specify their target data. In this paper, we introduce Geminio,
the first approach to transform GIAs into semantically meaningful, targeted
attacks. Geminio enables a brand new privacy attack experience: attackers can
describe, in natural language, the types of data they consider valuable, and
Geminio will prioritize reconstruction to focus on those high-value samples.
This is achieved by leveraging a pretrained VLM to guide the optimization of a
malicious global model that, when shared with and optimized by a victim,
retains only gradients of samples that match the attacker-specified query.
Extensive experiments demonstrate Geminio's effectiveness in pinpointing and
reconstructing targeted samples, with high success rates across complex
datasets under FL and large batch sizes and showing resilience against existing
defenses.

摘要：<paragraph>將視覺與語言結合起來的基礎模型已經取得顯著進展，激發了許多豐富生活的應用。然而，其被濫用以產生新威脅的可能性在很大程度上仍未得到探討。本文揭示了視覺語言模型 (VLM) 可以被利用來克服聯合學習 (FL) 中梯度反轉攻擊 (GIA) 的長期限制，其中 FL 伺服器從受害者客戶端共享的梯度中重建私有數據樣本。目前的 GIA 在重建高解析度影像時面臨挑戰，特別是當受害者有大量的本地數據批次時。雖然將重建重點放在有價值的樣本，而非整個批次上是有希望的，但現有方法缺乏彈性，無法讓攻擊者指定其目標數據。在本文中，我們介紹了 Geminio，這是將 GIA 轉換成語義有意義的目標攻擊的第一種方法。Geminio 提供了全新的隱私攻擊體驗：攻擊者可以使用自然語言描述他們認為有價值的數據類型，而 Geminio 將優先進行重建，以專注於那些高價值樣本。這是透過利用預先訓練的 VLM 來引導惡意全域模型的最佳化來實現的，當與受害者共享並由受害者最佳化時，該模型只保留與攻擊者指定的查詢相符的樣本的梯度。大量的實驗證明了 Geminio 在精確定位和重建目標樣本方面的有效性，在 FL 和大型批次下跨越複雜的數據集，並顯示出對現有防禦措施的韌性。</paragraph>

##### **LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure Cooperation**
2411.14927v1 by Zhenwei Yang, Jilei Mao, Wenxian Yang, Yibo Ai, Yu Kong, Haibao Yu, Weidong Zhang

Temporal perception, the ability to detect and track objects over time, is
critical in autonomous driving for maintaining a comprehensive understanding of
dynamic environments. However, this task is hindered by significant challenges,
including incomplete perception caused by occluded objects and observational
blind spots, which are common in single-vehicle perception systems. To address
these issues, we introduce LET-VIC, a LiDAR-based End-to-End Tracking framework
for Vehicle-Infrastructure Cooperation (VIC). LET-VIC leverages
Vehicle-to-Everything (V2X) communication to enhance temporal perception by
fusing spatial and temporal data from both vehicle and infrastructure sensors.
First, it spatially integrates Bird's Eye View (BEV) features from vehicle-side
and infrastructure-side LiDAR data, creating a comprehensive view that
mitigates occlusions and compensates for blind spots. Second, LET-VIC
incorporates temporal context across frames, allowing the model to leverage
historical data for enhanced tracking stability and accuracy. To further
improve robustness, LET-VIC includes a Calibration Error Compensation (CEC)
module to address sensor misalignments and ensure precise feature alignment.
Experiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC significantly
outperforms baseline models, achieving at least a 13.7% improvement in mAP and
a 13.1% improvement in AMOTA without considering communication delays. This
work offers a practical solution and a new research direction for advancing
temporal perception in autonomous driving through vehicle-infrastructure
cooperation.

摘要：時間感知，即在時間中偵測和追蹤物件的能力，對於自動駕駛至關重要，因為它能維持對動態環境的全面理解。然而，這項任務受到重大挑戰的阻礙，包括由遮擋物體和觀測盲點造成的感知不完整，這些在單一車輛感知系統中很常見。為了解決這些問題，我們引入了 LET-VIC，一個基於 LiDAR 的端到端追蹤架構，用於車輛基礎設施合作 (VIC)。LET-VIC 利用車聯萬物 (V2X) 通訊，透過融合車輛和基礎設施感測器的空間和時間資料來增強時間感知。首先，它將車輛側和基礎設施側 LiDAR 資料中的鳥瞰圖 (BEV) 特徵進行空間整合，創造了一個全面的視圖，可減輕遮擋並補償盲點。其次，LET-VIC 整合了跨幀的時間背景，讓模型能利用歷史資料來增強追蹤穩定性和準確性。為了進一步提高穩健性，LET-VIC 包含了一個校正誤差補償 (CEC) 模組，用於處理感測器未對準的問題，並確保精確的特徵對齊。在 V2X-Seq-SPD 資料集上的實驗證明，LET-VIC 明顯優於基準模型，在 mAP 上的改進至少為 13.7%，在 AMOTA 上的改進為 13.1%，且未考慮通訊延遲。這項工作提供了一個實用的解決方案和一個新的研究方向，透過車輛基礎設施合作來提升自動駕駛中的時間感知。

##### **Purrfessor: A Fine-tuned Multimodal LLaVA Diet Health Chatbot**
2411.14925v1 by Linqi Lu, Yifan Deng, Chuan Tian, Sijia Yang, Dhavan Shah

This study introduces Purrfessor, an innovative AI chatbot designed to
provide personalized dietary guidance through interactive, multimodal
engagement. Leveraging the Large Language-and-Vision Assistant (LLaVA) model
fine-tuned with food and nutrition data and a human-in-the-loop approach,
Purrfessor integrates visual meal analysis with contextual advice to enhance
user experience and engagement. We conducted two studies to evaluate the
chatbot's performance and user experience: (a) simulation assessments and human
validation were conducted to examine the performance of the fine-tuned model;
(b) a 2 (Profile: Bot vs. Pet) by 3 (Model: GPT-4 vs. LLaVA vs. Fine-tuned
LLaVA) experiment revealed that Purrfessor significantly enhanced users'
perceptions of care ($\beta = 1.59$, $p = 0.04$) and interest ($\beta = 2.26$,
$p = 0.01$) compared to the GPT-4 bot. Additionally, user interviews
highlighted the importance of interaction design details, emphasizing the need
for responsiveness, personalization, and guidance to improve user engagement.

摘要：本研究介紹了 Purrfessor，這是一種創新的 AI 聊天機器人，旨在透過互動式多模式參與提供個人化的飲食指導。Purrfessor 採用經過食物和營養資料微調的大語言和視覺助理 (LLaVA) 模型，以及人工介入的方式，將視覺化餐點分析與情境建議整合，以增強使用者體驗和參與度。我們進行了兩項研究，以評估聊天機器人的效能和使用者體驗：(a) 進行模擬評估和人工驗證，以檢驗微調模型的效能；(b) 一項 2 (個人資料：機器人與寵物) x 3 (模型：GPT-4 與 LLaVA 與微調 LLaVA) 實驗顯示，與 GPT-4 機器人相比，Purrfessor 大幅提升了使用者對於關懷 ($\beta = 1.59$，$p = 0.04$) 和興趣 ($\beta = 2.26$，$p = 0.01$) 的觀感。此外，使用者訪談強調了互動設計細節的重要性，強調需要回應性、個人化和指導，以提升使用者參與度。

##### **GOT4Rec: Graph of Thoughts for Sequential Recommendation**
2411.14922v1 by Zewen Long, Liang Wang, Shu Wu, Qiang Liu, Liang Wang

With the advancement of large language models (LLMs), researchers have
explored various methods to optimally leverage their comprehension and
generation capabilities in sequential recommendation scenarios. However,
several challenges persist in this endeavor. Firstly, most existing approaches
rely on the input-output prompting paradigm, which can result in irrelevant or
inaccurate responses. Secondly, while there have been attempts to enhance LLMs
using prompting strategies such as chain-of-thought (CoT), these efforts have
not fully harnessed the reasoning abilities of LLMs or effectively captured the
multifaceted information contained within user sequences. To address these
limitations, we propose GOT4Rec, a sequential recommendation method that
utilizes the graph of thoughts (GoT) prompting strategy. Specifically, we
identify and utilize three key types of information within user history
sequences: short-term interests, long-term interests and collaborative
information from other users. Our approach enables LLMs to independently reason
and generate recommendations based on these distinct types of information,
subsequently aggregating the results within the GoT framework to derive the
final recommended items. This method allows LLMs, with enhanced reasoning
capabilities, to more effectively consider the diverse information within user
sequences, resulting in more accurate recommendations and more comprehensive
explanations. Extensive experiments on real-world datasets demonstrate the
effectiveness of GOT4Rec, indicating that it outperforms existing
state-of-the-art baselines. Our code is available at
https://anonymous.4open.science/r/GOT4Rec-ED99.

摘要：隨著大型語言模型 (LLM) 的進步，研究人員已探索各種方法，以最佳方式利用其理解和生成能力在順序推薦場景中。然而，在這個努力中仍存在一些挑戰。首先，大多數現有方法依賴於輸入輸出提示範例，這可能會導致不相關或不準確的回應。其次，雖然有人嘗試使用提示策略（例如思想鏈 (CoT)）來增強 LLM，但這些努力並未充分利用 LLM 的推理能力或有效擷取使用者序列中包含的多方面資訊。為了解決這些限制，我們提出 GOT4Rec，這是一種順序推薦方法，利用了思想圖 (GoT) 提示策略。具體來說，我們在使用者歷史序列中識別並利用三種類型的關鍵資訊：短期興趣、長期興趣和來自其他使用者的協作資訊。我們的方法使 LLM 能夠根據這些不同類型的資訊獨立推理並產生建議，然後在 GoT 框架內匯總結果以推導出最終推薦的項目。這種方法允許 LLM 在增強推理能力的同時，更有效地考慮使用者序列中的不同資訊，從而產生更準確的建議和更全面的說明。在真實世界資料集上的大量實驗證明了 GOT4Rec 的有效性，表明它優於現有的最先進基準。我們的程式碼可在 https://anonymous.4open.science/r/GOT4Rec-ED99 取得。

##### **DAIRHuM: A Platform for Directly Aligning AI Representations with Human Musical Judgments applied to Carnatic Music**
2411.14907v1 by Prashanth Thattai Ravikumar

Quantifying and aligning music AI model representations with human behavior
is an important challenge in the field of MIR. This paper presents a platform
for exploring the Direct alignment between AI music model Representations and
Human Musical judgments (DAIRHuM). It is designed to enable musicians and
experimentalists to label similarities in a dataset of music recordings, and
examine a pre-trained model's alignment with their labels using quantitative
scores and visual plots. DAIRHuM is applied to analyze alignment between NSynth
representations, and a rhythmic duet between two percussionists in a Carnatic
quartet ensemble, an example of a genre where annotated data is scarce and
assessing alignment is non-trivial. The results demonstrate significant
findings on model alignment with human judgments of rhythmic harmony, while
highlighting key differences in rhythm perception and music similarity
judgments specific to Carnatic music. This work is among the first efforts to
enable users to explore human-AI model alignment in Carnatic music and advance
MIR research in Indian music while dealing with data scarcity and cultural
specificity. The development of this platform provides greater accessibility to
music AI tools for under-represented genres.

摘要：量化和调整音乐 AI 模型表示与人类行为在 MIR 领域是一项重要的挑战。本文提供了一个平台，用于探索 AI 音乐模型表示与人类音乐判断（DAIRHuM）之间的直接对齐。它旨在使音乐家和实验主义者能够标记音乐录音数据集中的相似性，并使用定量分数和可视化图表检查预训练模型与其标签的对齐方式。DAIRHuM 用于分析 NSynth 表示之间的对齐，以及卡纳蒂克四重奏合奏中两位打击乐手之间的节奏二重奏，这是一个注释数据稀缺且评估对齐并非易事的流派示例。结果展示了模型与人类对节奏和声判断对齐的重大发现，同时突出了卡纳蒂克音乐特有的节奏感知和音乐相似性判断中的关键差异。这项工作是首次让用户探索卡纳蒂克音乐中的人工智能模型对齐并推进印度音乐的 MIR 研究，同时处理数据稀缺和文化特异性。该平台的开发为代表性不足的流派提供了更广泛的音乐 AI 工具的可访问性。

##### **ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos**
2411.14901v1 by Tanveer Hannan, Md Mohaiminul Islam, Jindong Gu, Thomas Seidl, Gedas Bertasius

Large language models (LLMs) excel at retrieving information from lengthy
text, but their vision-language counterparts (VLMs) face difficulties with
hour-long videos, especially for temporal grounding. Specifically, these VLMs
are constrained by frame limitations, often losing essential temporal details
needed for accurate event localization in extended video content. We propose
ReVisionLLM, a recursive vision-language model designed to locate events in
hour-long videos. Inspired by human search strategies, our model initially
targets broad segments of interest, progressively revising its focus to
pinpoint exact temporal boundaries. Our model can seamlessly handle videos of
vastly different lengths, from minutes to hours. We also introduce a
hierarchical training strategy that starts with short clips to capture distinct
events and progressively extends to longer videos. To our knowledge,
ReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,
outperforming previous state-of-the-art methods across multiple datasets by a
significant margin (+2.6% R1@0.1 on MAD). The code is available at
https://github.com/Tanveer81/ReVisionLLM.

摘要：大型語言模型 (LLM) 擅長從冗長的文字中擷取資訊，但其視覺語言對應模型 (VLM) 在處理長達一小時的影片時會遇到困難，特別是在時間定位方面。具體來說，這些 VLM 受限於影格限制，經常遺失精確定位延伸影片內容中事件所需的基本時間細節。我們提出 ReVisionLLM，一種遞迴視覺語言模型，旨在定位長達一小時的影片中的事件。我們的模型以人類搜尋策略為靈感，最初鎖定廣泛的興趣片段，逐漸修正其焦點以精確找出時間界線。我們的模型可以無縫處理長度差異極大的影片，從幾分鐘到幾小時不等。我們還引入一種分層訓練策略，從短片段開始擷取不同的事件，並逐步延伸到更長的影片。據我們所知，ReVisionLLM 是第一個有能力在長達一小時的影片中進行時間定位的 VLM，在多個資料集上大幅優於先前的最新方法（在 MAD 上 +2.6% R1@0.1）。程式碼可在 https://github.com/Tanveer81/ReVisionLLM 取得。

##### **Evaluating LLM Prompts for Data Augmentation in Multi-label Classification of Ecological Texts**
2411.14896v1 by Anna Glazkova, Olga Zakharova

Large language models (LLMs) play a crucial role in natural language
processing (NLP) tasks, improving the understanding, generation, and
manipulation of human language across domains such as translating, summarizing,
and classifying text. Previous studies have demonstrated that instruction-based
LLMs can be effectively utilized for data augmentation to generate diverse and
realistic text samples. This study applied prompt-based data augmentation to
detect mentions of green practices in Russian social media. Detecting green
practices in social media aids in understanding their prevalence and helps
formulate recommendations for scaling eco-friendly actions to mitigate
environmental issues. We evaluated several prompts for augmenting texts in a
multi-label classification task, either by rewriting existing datasets using
LLMs, generating new data, or combining both approaches. Our results revealed
that all strategies improved classification performance compared to the models
fine-tuned only on the original dataset, outperforming baselines in most cases.
The best results were obtained with the prompt that paraphrased the original
text while clearly indicating the relevant categories.

摘要：大型語言模型 (LLM) 在自然語言處理 (NLP) 任務中扮演著至關重要的角色，改善了跨越翻譯、摘要和分類文本等領域的人類語言理解、生成和操作。先前的研究已經證明，基於指令的 LLM 可以有效地用於資料擴充，以產生多樣化且逼真的文本範例。本研究應用基於提示的資料擴充，以偵測俄羅斯社群媒體中對綠色實務的提及。在社群媒體中偵測綠色實務有助於了解其盛行程度，並有助於制定建議，以擴大環保行動來減輕環境問題。我們評估了多標籤分類任務中擴充文本的幾個提示，方法是使用 LLM 改寫現有資料集、產生新資料，或結合這兩種方法。我們的結果顯示，與僅對原始資料集進行微調的模型相比，所有策略都改善了分類效能，在多數情況下都優於基準。最好的結果是透過提示獲得，它對原始文本進行了改寫，同時清楚地指出相關類別。

##### **Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation**
2411.14883v1 by Yuheng Xu, Taiping Zhang

Domain-invariant representation learning is a powerful method for domain
generalization. Previous approaches face challenges such as high computational
demands, training instability, and limited effectiveness with high-dimensional
data, potentially leading to the loss of valuable features. To address these
issues, we hypothesize that an ideal generalized representation should exhibit
similar pattern responses within the same channel across cross-domain images.
Based on this hypothesis, we use deep features from the source domain as
queries, and deep features from the generated domain as keys and values.
Through a cross-channel attention mechanism, the original deep features are
reconstructed into robust regularization representations, forming an explicit
constraint that guides the model to learn domain-invariant representations.
Additionally, style augmentation is another common method. However, existing
methods typically generate new styles through convex combinations of source
domains, which limits the diversity of training samples by confining the
generated styles to the original distribution. To overcome this limitation, we
propose an Adaptive Feature Blending (AFB) method that generates
out-of-distribution samples while exploring the in-distribution space,
significantly expanding the domain range. Extensive experimental results
demonstrate that our proposed methods achieve superior performance on two
standard domain generalization benchmarks for medical image segmentation.

摘要：領域不變表示學習是領域泛化的一種強大方法。先前的做法面臨諸如高計算需求、訓練不穩定以及對高維數據的有效性有限等挑戰，可能會導致有價值特徵的遺失。為了解決這些問題，我們假設一個理想的泛化表示應在跨領域影像中展現出相同通道內相似的模式反應。基於此假設，我們使用來自來源領域的深度特徵作為查詢，並使用來自生成領域的深度特徵作為鍵和值。透過跨通道注意力機制，原始深度特徵被重建為穩健的正則化表示，形成一個引導模型學習領域不變表示的明確約束。此外，樣式擴充是另一種常見方法。然而，現有方法通常透過來源領域的凸組合來生成新的樣式，這會將生成的樣式限制在原始分佈中，進而限制訓練樣本的多樣性。為了克服此限制，我們提出了一種自適應特徵混合 (AFB) 方法，該方法在探索分佈內空間的同時生成分佈外樣本，大幅擴展了領域範圍。廣泛的實驗結果證明，我們提出的方法在兩個標準醫學影像分割領域泛化基準上取得了卓越的效能。

##### **Leveraging Hierarchical Prototypes as the Verbalizer for Implicit Discourse Relation Recognition**
2411.14880v1 by Wanqiu Long, Bonnie Webber

Implicit discourse relation recognition involves determining relationships
that hold between spans of text that are not linked by an explicit discourse
connective. In recent years, the pre-train, prompt, and predict paradigm has
emerged as a promising approach for tackling this task. However, previous work
solely relied on manual verbalizers for implicit discourse relation
recognition, which suffer from issues of ambiguity and even incorrectness. To
overcome these limitations, we leverage the prototypes that capture certain
class-level semantic features and the hierarchical label structure for
different classes as the verbalizer. We show that our method improves on
competitive baselines. Besides, our proposed approach can be extended to enable
zero-shot cross-lingual learning, facilitating the recognition of discourse
relations in languages with scarce resources. These advancement validate the
practicality and versatility of our approach in addressing the issues of
implicit discourse relation recognition across different languages.

摘要：隱含話語關係辨識涉及判定未透過明確話語連接詞連結的文本範圍之間的關係。近年來，預訓練、提示和預測範例已成為處理這項任務的有前景方法。然而，先前的研究僅依賴手動言語化器進行隱含話語關係辨識，而這會產生模稜兩可甚至不正確的問題。為了克服這些限制，我們利用原型來擷取特定類別層級語意特徵和不同類別的階層標籤結構作為言語化器。我們展示我們的模型優於競爭基準線。此外，我們提出的方法可以延伸以啟用零次學習跨語言學習，促進辨識資源稀少的語言中的話語關係。這些進展驗證了我們的方法在解決不同語言中隱含話語關係辨識問題的實用性和多功能性。

##### **Astro-HEP-BERT: A bidirectional language model for studying the meanings of concepts in astrophysics and high energy physics**
2411.14877v1 by Arno Simons

I present Astro-HEP-BERT, a transformer-based language model specifically
designed for generating contextualized word embeddings (CWEs) to study the
meanings of concepts in astrophysics and high-energy physics. Built on a
general pretrained BERT model, Astro-HEP-BERT underwent further training over
three epochs using the Astro-HEP Corpus, a dataset I curated from 21.84 million
paragraphs extracted from more than 600,000 scholarly articles on arXiv, all
belonging to at least one of these two scientific domains. The project
demonstrates both the effectiveness and feasibility of adapting a bidirectional
transformer for applications in the history, philosophy, and sociology of
science (HPSS). The entire training process was conducted using freely
available code, pretrained weights, and text inputs, completed on a single
MacBook Pro Laptop (M2/96GB). Preliminary evaluations indicate that
Astro-HEP-BERT's CWEs perform comparably to domain-adapted BERT models trained
from scratch on larger datasets for domain-specific word sense disambiguation
and induction and related semantic change analyses. This suggests that
retraining general language models for specific scientific domains can be a
cost-effective and efficient strategy for HPSS researchers, enabling high
performance without the need for extensive training from scratch.

摘要：我展示了 Astro-HEP-BERT，這是一個基於轉換器的語言模型，專門設計用於產生語境化單字嵌入 (CWE)，以研究天體物理學和高能物理學中概念的含義。建立在預先訓練好的 BERT 模型之上，Astro-HEP-BERT 使用 Astro-HEP 語料庫進行了三個時期的進一步訓練，這個資料集是我從 arXiv 上 600,000 多篇學術文章中摘錄的 2184 萬段落整理而成的，所有文章都至少屬於這兩個科學領域之一。該專案展示了調整雙向轉換器在科學史、哲學和社會學 (HPSS) 中應用的有效性和可行性。整個訓練過程使用免費提供的程式碼、預訓練權重和文字輸入進行，並在單一的 MacBook Pro 筆記型電腦 (M2/96GB) 上完成。初步評估表明，Astro-HEP-BERT 的 CWE 與從頭開始在較大型資料集上訓練的領域適應 BERT 模型在領域特定的單字感官消歧、歸納和相關語義變更分析方面表現相當。這表明針對特定科學領域重新訓練一般語言模型可能是 HPSS 研究人員一種經濟有效且高效的策略，無需從頭開始進行大量訓練即可實現高性能。

##### **Prioritize Denoising Steps on Diffusion Model Preference Alignment via Explicit Denoised Distribution Estimation**
2411.14871v1 by Dingyuan Shi, Yong Wang, Hangyu Li, Xiangxiang Chu

Diffusion models have shown remarkable success in text-to-image generation,
making alignment methods for these models increasingly important. A key
challenge is the sparsity of preference labels, which are typically available
only at the terminal of denoising trajectories. This raises the issue of how to
assign credit across denoising steps based on these sparse labels. In this
paper, we propose Denoised Distribution Estimation (DDE), a novel method for
credit assignment. Unlike previous approaches that rely on auxiliary models or
hand-crafted schemes, DDE derives its strategy more explicitly. The proposed
DDE directly estimates the terminal denoised distribution from the perspective
of each step. It is equipped with two estimation strategies and capable of
representing the entire denoising trajectory with a single model inference.
Theoretically and empirically, we show that DDE prioritizes optimizing the
middle part of the denoising trajectory, resulting in a novel and effective
credit assignment scheme. Extensive experiments demonstrate that our approach
achieves superior performance, both quantitatively and qualitatively.

摘要：擴散模型在文字轉圖片生成方面展現了顯著的成功，使得這些模型的對齊方法日益重要。一個關鍵挑戰是偏好標籤的稀疏性，這些標籤通常只在去噪軌跡的終端處可用。這引發了一個問題，即如何根據這些稀疏標籤在去噪步驟中分配信用。在本文中，我們提出去噪分佈估計（DDE），這是一種用於信用分配的新方法。與依賴於輔助模型或手工製作方案的先前方法不同，DDE 更明確地推導出其策略。所提出的 DDE 直接從每個步驟的角度估計終端去噪分佈。它配備了兩種估計策略，並且能夠使用單一模型推論來表示整個去噪軌跡。在理論上和經驗上，我們表明 DDE 優先優化去噪軌跡的中間部分，從而產生一種新穎且有效的信用分配方案。大量的實驗表明，我們的做法在質量和數量上都取得了優異的表現。

##### **BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence**
2411.14869v1 by Xuewu Lin, Tianwei Lin, Lichao Huang, Hongyu Xie, Zhizhong Su

In embodied intelligence systems, a key component is 3D perception algorithm,
which enables agents to understand their surrounding environments. Previous
algorithms primarily rely on point cloud, which, despite offering precise
geometric information, still constrain perception performance due to inherent
sparsity, noise, and data scarcity. In this work, we introduce a novel
image-centric 3D perception model, BIP3D, which leverages expressive image
features with explicit 3D position encoding to overcome the limitations of
point-centric methods. Specifically, we leverage pre-trained 2D vision
foundation models to enhance semantic understanding, and introduce a spatial
enhancer module to improve spatial understanding. Together, these modules
enable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end
3D perception. In our experiments, BIP3D outperforms current state-of-the-art
results on the EmbodiedScan benchmark, achieving improvements of 5.69% in the
3D detection task and 15.25% in the 3D visual grounding task.

摘要：在具身智能系統中，一個關鍵組成部分是 3D 感知演算法，它使代理能夠理解其周圍的環境。先前的演算法主要依賴點雲，儘管提供精確的幾何資訊，但由於固有的稀疏性、雜訊和資料稀少，仍然會限制感知效能。在這項工作中，我們引入了一個新的以影像為中心的 3D 感知模型 BIP3D，它利用具有明確 3D 位置編碼的表達性影像特徵來克服以點為中心的限制方法。具體來說，我們利用預先訓練的 2D 視覺基礎模型來增強語意理解，並引入一個空間增強器模組來改善空間理解。這些模組共同使 BIP3D 能夠實現多視圖、多模式特徵融合和端到端的 3D 感知。在我們的實驗中，BIP3D 在 EmbodiedScan 基準上優於目前的最新結果，在 3D 偵測任務中改進了 5.69%，在 3D 視覺基礎任務中改進了 15.25%。

##### **Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired Image-to-Image Translation**
2411.14863v1 by Jeongsol Kim, Beomsu Kim, Jong Chul Ye

Diffusion models (DMs), which enable both image generation from noise and
inversion from data, have inspired powerful unpaired image-to-image (I2I)
translation algorithms. However, they often require a larger number of neural
function evaluations (NFEs), limiting their practical applicability. In this
paper, we tackle this problem with Schrodinger Bridges (SBs), which are
stochastic differential equations (SDEs) between distributions with minimal
transport cost. We analyze the probability flow ordinary differential equation
(ODE) formulation of SBs, and observe that we can decompose its vector field
into a linear combination of source predictor, target predictor, and noise
predictor. Inspired by this observation, we propose Latent Schrodinger Bridges
(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and
develop appropriate prompt optimization and change of variables formula to
match the training and inference between distributions. We demonstrate that our
algorithm successfully conduct competitive I2I translation in unsupervised
setting with only a fraction of computation cost required by previous DM-based
I2I methods.

摘要：擴散模型 (DM) 能夠從雜訊中產生影像，並從資料中進行反演，它啟發了強大的非配對影像到影像 (I2I) 翻譯演算法。然而，它們通常需要大量的網路函數評估 (NFE)，這限制了它們的實用性。在本文中，我們使用薛丁格橋樑 (SB) 來解決這個問題，薛丁格橋樑是具有最小傳輸成本的機率分佈之間的隨機微分方程式 (SDE)。我們分析了 SB 的機率流常微分方程式 (ODE) 公式，並觀察到我們可以將其向量場分解為源預測器、目標預測器和雜訊預測器的線性組合。受到這個觀察的啟發，我們提出了潛在薛丁格橋樑 (LSB)，它透過預先訓練的穩定擴散來近似 SB ODE，並開發適當的提示最佳化和變數轉換公式，以匹配分佈之間的訓練和推論。我們證明了我們的演算法成功地在非監督式設定中進行了具有競爭力的 I2I 翻譯，而運算成本僅為先前基於 DM 的 I2I 方法的一小部分。

##### **Domain and Range Aware Synthetic Negatives Generation for Knowledge Graph Embedding Models**
2411.14858v1 by Alberto Bernardi, Luca Costabello

Knowledge Graph Embedding models, representing entities and edges in a
low-dimensional space, have been extremely successful at solving tasks related
to completing and exploring Knowledge Graphs (KGs). One of the key aspects of
training most of these models is teaching to discriminate between true
statements positives and false ones (negatives). However, the way in which
negatives can be defined is not trivial, as facts missing from the KG are not
necessarily false and a set of ground truth negatives is hardly ever given.
This makes synthetic negative generation a necessity. Different generation
strategies can heavily affect the quality of the embeddings, making it a
primary aspect to consider. We revamp a strategy that generates corruptions
during training respecting the domain and range of relations, we extend its
capabilities and we show our methods bring substantial improvement (+10% MRR)
for standard benchmark datasets and over +150% MRR for a larger ontology-backed
dataset.

摘要：知識圖表嵌入模型，將實體和邊緣表示在低維空間中，在解決與完成和探索知識圖表 (KG) 相關的任務方面非常成功。訓練這些模型的主要方面之一是教導區分真實陳述的正例和虛假陳述的負例。然而，定義負例的方式並非微不足道，因為從 KG 中遺失的事實並不一定為假，而且幾乎從不給定一組真實的負例。這使得合成負例生成成為必要。不同的生成策略會嚴重影響嵌入的品質，使其成為要考慮的主要方面。我們改造了一種在訓練期間生成破壞的策略，尊重關係的網域和範圍，我們擴展了它的功能，並且我們展示了我們的方法為標準基準資料集帶來實質性的改進（+10% MRR），並且為一個較大的由本体支持的資料集帶來超過 +150% MRR。

##### **Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models**
2411.14842v1 by Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Tianyi Zhou, Ling Chen

Adversarial audio attacks pose a significant threat to the growing use of
large language models (LLMs) in voice-based human-machine interactions. While
existing research has primarily focused on model-specific adversarial methods,
real-world applications demand a more generalizable and universal approach to
audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks
(CAA) benchmark including four distinct types of audio attacks, which aims to
explore the the vulnerabilities of LLMs to these audio attacks in
conversational scenarios. To evaluate the robustness of LLMs, we propose three
evaluation strategies: Standard Evaluation, utilizing traditional metrics to
quantify model performance under attacks; GPT-4o-Based Evaluation, which
simulates real-world conversational complexities; and Human Evaluation,
offering insights into user perception and trust. We evaluate six
state-of-the-art LLMs with voice interaction capabilities, including
Gemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on
the CAA benchmark. Our comprehensive analysis reveals the impact of four types
of audio attacks on the performance of these models, demonstrating that GPT-4o
exhibits the highest level of resilience.

摘要：對抗式音訊攻擊對大型語言模型（LLM）在語音互動中的人機互動中日益廣泛的使用構成重大威脅。雖然現有研究主要集中於特定於模型的對抗方法，但實際應用需要一種更具概括性和普遍性的方法來進行音訊對抗攻擊。在本文中，我們介紹了聊天音訊攻擊（CAA）基準，其中包括四種類型的音訊攻擊，旨在探討 LLM 在對話場景中對這些音訊攻擊的脆弱性。為了評估 LLM 的穩健性，我們提出了三種評估策略：標準評估，利用傳統指標量化攻擊下的模型效能；基於 GPT-4o 的評估，模擬真實世界的對話複雜性；以及人類評估，提供對使用者感知和信任的見解。我們使用三種不同的評估方法在 CAA 基準上評估了六種具有語音互動功能的最新 LLM，包括 Gemini-1.5-Pro、GPT-4o 等。我們的綜合分析揭示了四種類型的音訊攻擊對這些模型效能的影響，證明 GPT-4o 展現出最高程度的韌性。

##### **VisGraphVar: A Benchmark Generator for Assessing Variability in Graph Analysis Using Large Vision-Language Models**
2411.14832v1 by Camilo Chacón Sartori, Christian Blum, Filippo Bistaffa

The fast advancement of Large Vision-Language Models (LVLMs) has shown
immense potential. These models are increasingly capable of tackling abstract
visual tasks. Geometric structures, particularly graphs with their inherent
flexibility and complexity, serve as an excellent benchmark for evaluating
these models' predictive capabilities. While human observers can readily
identify subtle visual details and perform accurate analyses, our investigation
reveals that state-of-the-art LVLMs exhibit consistent limitations in specific
visual graph scenarios, especially when confronted with stylistic variations.
In response to these challenges, we introduce VisGraphVar (Visual Graph
Variability), a customizable benchmark generator able to produce graph images
for seven distinct task categories (detection, classification, segmentation,
pattern recognition, link prediction, reasoning, matching), designed to
systematically evaluate the strengths and limitations of individual LVLMs. We
use VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing
two distinct prompting strategies, namely zero-shot and chain-of-thought. The
findings demonstrate that variations in visual attributes of images (e.g., node
labeling and layout) and the deliberate inclusion of visual imperfections, such
as overlapping nodes, significantly affect model performance. This research
emphasizes the importance of a comprehensive evaluation across graph-related
tasks, extending beyond reasoning alone. VisGraphVar offers valuable insights
to guide the development of more reliable and robust systems capable of
performing advanced visual graph analysis.

摘要：大型視覺語言模型 (LVLMs) 的快速進步已展現出巨大的潛力。這些模型越來越有能力處理抽象的視覺任務。幾何結構，特別是具有內在靈活性與複雜性的圖形，可用作評估這些模型預測能力的絕佳基準。人類觀察者可以輕易辨識微妙的視覺細節並執行準確的分析，但我們的調查顯示，最先進的 LVLMs 在特定的視覺圖形場景中表現出持續的限制，特別是在面對風格變化時。為了應對這些挑戰，我們引入了 VisGraphVar（視覺圖形變異），這是一個可自訂的基準產生器，能夠產生七個不同任務類別的圖形影像（偵測、分類、分割、模式辨識、連結預測、推理、配對），旨在系統性地評估個別 LVLMs 的優點和限制。我們使用 VisGraphVar 產生 990 個圖形影像並評估六個 LVLMs，採用兩種不同的提示策略，即零次學習和思維鏈。研究結果表明，影像視覺屬性的變化（例如節點標籤和版面）以及視覺瑕疵的故意加入（例如重疊節點）會顯著影響模型效能。這項研究強調了跨圖形相關任務進行全面評估的重要性，而不僅限於推理。VisGraphVar 提供了寶貴的見解，以指導更可靠且強大的系統的開發，這些系統能夠執行進階的視覺圖形分析。

##### **Physically Interpretable Probabilistic Domain Characterization**
2411.14827v1 by Anaïs Halin, Sébastien Piérard, Renaud Vandeghen, Benoît Gérin, Maxime Zanella, Martin Colot, Jan Held, Anthony Cioppa, Emmanuel Jean, Gianluca Bontempi, Saïd Mahmoudi, Benoît Macq, Marc Van Droogenbroeck

Characterizing domains is essential for models analyzing dynamic
environments, as it allows them to adapt to evolving conditions or to hand the
task over to backup systems when facing conditions outside their operational
domain. Existing solutions typically characterize a domain by solving a
regression or classification problem, which limits their applicability as they
only provide a limited summarized description of the domain. In this paper, we
present a novel approach to domain characterization by characterizing domains
as probability distributions. Particularly, we develop a method to predict the
likelihood of different weather conditions from images captured by
vehicle-mounted cameras by estimating distributions of physical parameters
using normalizing flows. To validate our proposed approach, we conduct
experiments within the context of autonomous vehicles, focusing on predicting
the distribution of weather parameters to characterize the operational domain.
This domain is characterized by physical parameters (absolute characterization)
and arbitrarily predefined domains (relative characterization). Finally, we
evaluate whether a system can safely operate in a target domain by comparing it
to multiple source domains where safety has already been established. This
approach holds significant potential, as accurate weather prediction and
effective domain adaptation are crucial for autonomous systems to adjust to
dynamic environmental conditions.

摘要：描述領域對於分析動態環境的模型來說至關重要，因為它允許模型適應不斷變化的條件，或在面對運作領域之外的條件時將任務移交給備份系統。現有的解決方案通常透過解決回歸或分類問題來描述領域，這會限制它們的適用性，因為它們只提供領域的有限摘要描述。在本文中，我們提出了一種新的領域描述方法，將領域描述為機率分佈。特別是，我們開發了一種方法，透過估計使用正規化流的物理參數分佈，從車載相機拍攝的影像預測不同天氣條件發生的機率。為了驗證我們提出的方法，我們在自動駕駛車輛的背景下進行實驗，重點在於預測天氣參數的分佈以描述運作領域。此領域的特徵在於物理參數（絕對描述）和任意預定義的領域（相對描述）。最後，我們評估系統是否可以在目標領域中安全地運作，方法是將其與已經建立安全性的多個來源領域進行比較。這種方法具有重要的潛力，因為準確的天氣預測和有效的領域適應對於自動系統適應動態環境條件至關重要。

##### **Fine-Grained Alignment in Vision-and-Language Navigation through Bayesian Optimization**
2411.14811v1 by Yuhang Song, Mario Gianni, Chenguang Yang, Kunyang Lin, Te-Chuan Chiu, Anh Nguyen, Chun-Yi Lee

This paper addresses the challenge of fine-grained alignment in
Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D
environments based on natural language instructions. Current approaches use
contrastive learning to align language with visual trajectory sequences.
Nevertheless, they encounter difficulties with fine-grained vision negatives.
To enhance cross-modal embeddings, we introduce a novel Bayesian
Optimization-based adversarial optimization framework for creating fine-grained
contrastive vision samples. To validate the proposed methodology, we conduct a
series of experiments to assess the effectiveness of the enriched embeddings on
fine-grained vision negatives. We conduct experiments on two common VLN
benchmarks R2R and REVERIE, experiments on the them demonstrate that these
embeddings benefit navigation, and can lead to a promising performance
enhancement. Our source code and trained models are available at:
https://anonymous.4open.science/r/FGVLN.

摘要：本文探讨了视觉和语言导航 (VLN) 任务中精细粒度对齐的挑战，在该任务中，机器人根据自然语言指令在逼真的 3D 环境中导航。当前的方法使用对比学习将语言与视觉轨迹序列对齐。然而，它们遇到了精细粒度视觉否定的困难。为了增强跨模态嵌入，我们引入了一个新颖的基于贝叶斯优化的对抗优化框架，用于创建精细粒度的对比视觉样本。为了验证所提出的方法，我们进行了一系列实验来评估丰富嵌入在精细粒度视觉否定上的有效性。我们在两个常见的 VLN 基准 R2R 和 REVERIE 上进行实验，在它们上的实验表明这些嵌入有利于导航，并且可以带来有希望的性能提升。我们的源代码和训练好的模型可在以下位置获得：
https://anonymous.4open.science/r/FGVLN。

##### **High-Resolution Image Synthesis via Next-Token Prediction**
2411.14808v1 by Dengsheng Chen, Jie Hu, Tiezhu Yue, Xiaoming Wei

Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), an
autoregressive model, has demonstrated outstanding performance in
class-conditional image generation. However, the application of next-token
prediction in high-resolution text-to-image generation remains underexplored.
In this paper, we introduce D-JEPA$\cdot$T2I, an extension of D-JEPA
incorporating flow matching loss, designed to enable data-efficient continuous
resolution learning. D-JEPA$\cdot$T2I leverages a multimodal visual transformer
to effectively integrate textual and visual features and adopts Visual Rotary
Positional Embedding (VoPE) to facilitate continuous resolution learning.
Furthermore, we devise a data feedback mechanism that significantly enhances
data utilization efficiency. For the first time, we achieve state-of-the-art
\textbf{high-resolution} image synthesis via next-token prediction.
  The experimental code and pretrained models will be open-sourced at
\url{https://d-jepa.github.io/t2i}.

摘要：使用联合嵌入预测架构 (D-JEPA) 进行去噪，一种自回归模型，已在类别条件图像生成中展示了出色的性能。然而，在高分辨率文本到图像生成中使用下一个标记预测仍然未被充分探索。在本文中，我们介绍了 D-JEPA⋅T2I，它是 D-JEPA 的一个扩展，集成了流匹配损失，旨在实现数据高效的连续分辨率学习。D-JEPA⋅T2I 利用多模态视觉转换器有效地整合文本和视觉特征，并采用视觉旋转位置嵌入 (VoPE) 来促进连续分辨率学习。此外，我们设计了一种数据反馈机制，可以显著提高数据利用效率。我们首次通过下一个标记预测实现了最先进的\textbf{高分辨率}图像合成。实验代码和预训练模型将在 \url{https://d-jepa.github.io/t2i} 开源。

##### **Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension**
2411.14807v1 by Luca Parolari, Elena Izzo, Lamberto Ballan

Referring Expression Comprehension (REC) aims to identify a particular object
in a scene by a natural language expression, and is an important topic in
visual language understanding. State-of-the-art methods for this task are based
on deep learning, which generally requires expensive and manually labeled
annotations. Some works tackle the problem with limited-supervision learning or
relying on Large Vision and Language Models. However, the development of
techniques to synthesize labeled data is overlooked. In this paper, we propose
a novel framework that generates artificial data for the REC task, taking into
account both textual and visual modalities. At first, our pipeline processes
existing data to create variations in the annotations. Then, it generates an
image using altered annotations as guidance. The result of this pipeline is a
new dataset, called Harlequin, made by more than 1M queries. This approach
eliminates manual data collection and annotation, enabling scalability and
facilitating arbitrary complexity. We pre-train three REC models on Harlequin,
then fine-tuned and evaluated on human-annotated datasets. Our experiments show
that the pre-training on artificial data is beneficial for performance.

摘要：引用表達理解 (REC) 旨在透過自然語言表達方式來識別場景中的特定物件，是視覺語言理解中的重要課題。此任務的現有方法基於深度學習，通常需要昂貴且手動標記的註解。有些作品會使用有限監督學習或依賴大型視覺和語言模型來解決此問題。然而，合成標記資料的技術發展卻被忽略了。在本文中，我們提出一個新穎的架構，可為 REC 任務產生人工資料，同時考量文字和視覺模式。首先，我們的管線處理現有資料以在註解中產生變化。然後，它使用已變更的註解作為指引來產生影像。此管線的結果是一個名為 Harlequin 的新資料集，由超過 100 萬個查詢組成。此方法消除了手動資料收集和註解，實現了可擴充性，並簡化了任意複雜性。我們在 Harlequin 上預訓練了三個 REC 模型，然後在人工標記的資料集上進行微調和評估。我們的實驗顯示，在人工資料上進行預訓練有助於提升效能。

##### **Continual SFT Matches Multimodal RLHF with Negative Supervision**
2411.14797v1 by Ke Zhu, Yu Wang, Yanpeng Sun, Qiang Chen, Jiangjiang Liu, Gang Zhang, Jingdong Wang

Multimodal RLHF usually happens after supervised finetuning (SFT) stage to
continually improve vision-language models' (VLMs) comprehension. Conventional
wisdom holds its superiority over continual SFT during this preference
alignment stage. In this paper, we observe that the inherent value of
multimodal RLHF lies in its negative supervision, the logit of the rejected
responses. We thus propose a novel negative supervised finetuning (nSFT)
approach that fully excavates these information resided. Our nSFT disentangles
this negative supervision in RLHF paradigm, and continually aligns VLMs with a
simple SFT loss. This is more memory efficient than multimodal RLHF where 2
(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The
effectiveness of nSFT is rigorously proved by comparing it with various
multimodal RLHF approaches, across different dataset sources, base VLMs and
evaluation metrics. Besides, fruitful of ablations are provided to support our
hypothesis. We hope this paper will stimulate further research to properly
align large vision language models.

摘要：多模态 RLHF 通常在监督微调 (SFT) 阶段之后发生，以持续改进视觉语言模型 (VLM) 的理解力。传统智慧认为在这一偏好对齐阶段，它优于持续的 SFT。在本文中，我们观察到多模态 RLHF 的内在价值在于其负监督，即被拒绝的响应的对数几率。因此，我们提出了一种新颖的负监督微调 (nSFT) 方法，该方法充分挖掘了这些驻留的信息。我们的 nSFT 解开了 RLHF 范式中的负监督，并持续使用简单的 SFT 损失对齐 VLM。这比多模态 RLHF 更节省内存，在多模态 RLHF 中严格需要 2（例如，DPO）或 4（例如，PPO）个大型 VLM。通过在不同的数据集源、基本 VLM 和评估指标上将 nSFT 与各种多模态 RLHF 方法进行比较，严格证明了其有效性。此外，还提供了丰富的消融实验来支持我们的假设。我们希望本文将激发进一步的研究，以正确对齐大型视觉语言模型。

##### **De-biased Multimodal Electrocardiogram Analysis**
2411.14795v1 by Haitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, Zhengxing Huang

Multimodal large language models (MLLMs) are increasingly being applied in
the medical field, particularly in medical imaging. However, developing MLLMs
for ECG signals, which are crucial in clinical settings, has been a significant
challenge beyond medical imaging. Previous studies have attempted to address
this by converting ECGs into several text tags using an external classifier in
a training-free manner. However, this approach significantly compresses the
information in ECGs and underutilizes the reasoning capabilities of LLMs. In
this work, we directly feed the embeddings of ECGs into the LLM through a
projection layer, retaining more information about ECGs and better leveraging
the reasoning abilities of LLMs. Our method can also effectively handle a
common situation in clinical practice where it is necessary to compare two ECGs
taken at different times. Recent studies found that MLLMs may rely solely on
text input to provide answers, ignoring inputs from other modalities. We
analyzed this phenomenon from a causal perspective in the context of ECG MLLMs
and discovered that the confounder, severity of illness, introduces a spurious
correlation between the question and answer, leading the model to rely on this
spurious correlation and ignore the ECG input. Such models do not comprehend
the ECG input and perform poorly in adversarial tests where different
expressions of the same question are used in the training and testing sets. We
designed a de-biased pre-training method to eliminate the confounder's effect
according to the theory of backdoor adjustment. Our model performed well on the
ECG-QA task under adversarial testing and demonstrated zero-shot capabilities.
An interesting random ECG test further validated that our model effectively
understands and utilizes the input ECG signal.

摘要：多模态大型语言模型 (MLLM) 正越来越多地应用于医学领域，尤其是在医学影像中。然而，为在临床环境中至关重要的 ECG 信号开发 MLLM 一直是医学影像之外的一项重大挑战。先前的研究尝试通过以无训练的方式使用外部分类器将 ECG 转换为多个文本标签来解决此问题。然而，这种方法极大地压缩了 ECG 中的信息，并且低估了 LLM 的推理能力。在这项工作中，我们通过投影层直接将 ECG 的嵌入馈送到 LLM 中，保留了更多有关 ECG 的信息，并更好地利用了 LLM 的推理能力。我们的方法还可以有效地处理临床实践中常见的需要比较在不同时间采集的两个 ECG 的情况。最近的研究发现，MLLM 可能仅依靠文本输入来提供答案，而忽略来自其他模式的输入。我们从因果角度分析了 ECG MLLM 背景下的这种现象，发现混杂因素（疾病严重程度）引入了问题和答案之间的虚假相关性，导致模型依赖于这种虚假相关性并忽略 ECG 输入。此类模型不理解 ECG 输入，并且在对抗性测试中表现不佳，其中在训练和测试集中使用了同一问题的不同表达方式。我们设计了一种去偏差预训练方法，以根据后门调整理论消除混杂因素的影响。我们的模型在对抗性测试中在 ECG-QA 任务上表现良好，并展示了零样本学习能力。一个有趣的随机 ECG 测试进一步验证了我们的模型有效地理解并利用了输入 ECG 信号。

##### **VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection**
2411.14794v1 by Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, Si Liu

The advancement of Large Vision Language Models (LVLMs) has significantly
improved multimodal understanding, yet challenges remain in video reasoning
tasks due to the scarcity of high-quality, large-scale datasets. Existing video
question-answering (VideoQA) datasets often rely on costly manual annotations
with insufficient granularity or automatic construction methods with redundant
frame-by-frame analysis, limiting their scalability and effectiveness for
complex reasoning. To address these challenges, we introduce VideoEspresso, a
novel dataset that features VideoQA pairs preserving essential spatial details
and temporal coherence, along with multimodal annotations of intermediate
reasoning steps. Our construction pipeline employs a semantic-aware method to
reduce redundancy, followed by generating QA pairs using GPT-4o. We further
develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,
guiding GPT-4o in extracting logical relationships from QA pairs and video
content. To exploit the potential of high-quality VideoQA pairs, we propose a
Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a
two-stage instruction fine-tuned reasoning LVLM. This framework adaptively
selects core frames and performs CoT reasoning using multimodal evidence.
Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our
method outperforms existing baselines on most tasks, demonstrating superior
video reasoning capabilities. Our code and dataset will be released at:
https://github.com/hshjerry/VideoEspresso

摘要：大型視覺語言模型 (LVLMs) 的進步顯著改善了多模態理解，但由於缺乏高品質、大規模的資料集，影片推理任務仍面臨挑戰。現有的影片問答 (VideoQA) 資料集通常依賴於昂貴的手動標註，但顆粒度不足，或使用逐幀分析的自動建構方法，限制了它們在複雜推理中的可擴充性和有效性。為了應對這些挑戰，我們引入了 VideoEspresso，這是一個新穎的資料集，其特點是 VideoQA 配對保留了必要的空間細節和時間連貫性，以及對中間推理步驟的多模態標註。我們的建構管道採用語義感知方法來減少冗餘，然後使用 GPT-4o 產生 QA 配對。我們進一步開發了影片思考鏈 (CoT) 標註，以豐富推理過程，指導 GPT-4o 從 QA 配對和影片內容中提取邏輯關係。為了利用高品質 VideoQA 配對的潛力，我們提出了一個混合 LVLMs 協作框架，其特點是一個 Frame Selector 和一個兩階段指令微調推理 LVLM。此框架自適應地選擇核心幀，並使用多模態證據執行 CoT 推理。在我們提出的基準上，針對 9 個流行的 LVLMs 進行了 14 項任務的評估，我們的模型在大部分任務上都優於現有的基準，展現出優異的影片推理能力。我們的程式碼和資料集將在以下位置發布：https://github.com/hshjerry/VideoEspresso

##### **KBAda: Efficient Self Adaptation on Specific Knowledge Bases**
2411.14790v1 by Zheni Zeng, Yuxuan Chen, Shi Yu, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun

Humans can utilize techniques to quickly acquire knowledge from specific
materials in advance, such as creating self-assessment questions, enabling us
to achieving related tasks more efficiently. In contrast, large language models
(LLMs) usually relies on retrieval-augmented generation to exploit knowledge
materials in an instant manner, or requires external signals such as human
preference data and stronger LLM annotations to conduct knowledge adaptation.
To unleash the self-learning potential of LLMs, we propose KBAda, an approach
designed for efficient adaptation to downstream tasks involving knowledge
bases. Our method utilizes iterative training with self-annotated data such as
Q&A pairs and revision suggestions, enabling the model to grasp the knowledge
content efficiently. Experimental results on multiple datasets demonstrate the
effectiveness of our approach, significantly boosting model performance in
downstream tasks that require specific knowledge at a low cost. Notably, our
approach achieves over 90% of the performance improvement that can be obtained
by using GPT-4-turbo annotation, while relying entirely on self-supervision. We
release our experimental data, models, and process analyses to the community
for further exploration (https://github.com/thunlp/KBAda).

摘要：人類可以利用技術，提前從特定材料中快速獲取知識，例如建立自我評量問題，讓我們能夠更有效地達成相關任務。相比之下，大型語言模型 (LLM) 通常依賴於檢索增強生成，來即時利用知識材料，或者需要外部訊號，例如人類偏好數據和更強大的 LLM 注解，來進行知識適應。為了釋放 LLM 的自學潛力，我們提出了 KBAda，一種專門設計用於有效適應涉及知識庫的下游任務的方法。我們的模型利用自我註解數據（例如問答配對和修訂建議）進行反覆訓練，讓模型能夠有效掌握知識內容。在多個數據集上的實驗結果證明了我們方法的有效性，顯著提升了模型在需要特定知識的下游任務中的表現，而且成本很低。值得注意的是，我們的模型僅依賴於自我監督，就能達到使用 GPT-4-turbo 注解所能獲得的 90% 以上的效能提升。我們將實驗數據、模型和程序分析發布到社群中，以供進一步探索 (https://github.com/thunlp/KBAda)。

##### **Resolution-Agnostic Transformer-based Climate Downscaling**
2411.14774v1 by Declan Curran, Hira Saleem, Flora Salim, Sanaa Hobeichi

Understanding future weather changes at regional and local scales is crucial
for planning and decision-making, particularly in the context of extreme
weather events, as well as for broader applications in agriculture, insurance,
and infrastructure development. However, the computational cost of downscaling
Global Climate Models (GCMs) to the fine resolutions needed for such
applications presents a significant barrier. Drawing on advancements in weather
forecasting models, this study introduces a cost-efficient downscaling method
using a pretrained Earth Vision Transformer (Earth ViT) model. Initially
trained on ERA5 data to downscale from 50 km to 25 km resolution, the model is
then tested on the higher resolution BARRA-SY dataset at a 3 km resolution.
Remarkably, it performs well without additional training, demonstrating its
ability to generalize across different resolutions. This approach holds promise
for generating large ensembles of regional climate simulations by downscaling
GCMs with varying input resolutions without incurring additional training
costs. Ultimately, this method could provide more comprehensive estimates of
potential future changes in key climate variables, aiding in effective planning
for extreme weather events and climate change adaptation strategies.

摘要：了解区域和地方尺度的未来天气变化对于规划和决策至关重要，尤其是在极端天气事件的背景下，以及在农业、保险和基础设施开发等更广泛的应用中。然而，将全球气候模型 (GCM) 降尺度到此类应用所需的精细分辨率的计算成本构成了重大障碍。本研究借鉴天气预报模型的进步，引入了一种使用预训练地球视觉转换器 (Earth ViT) 模型的经济高效的降尺度方法。该模型最初在 ERA5 数据上进行训练，以将分辨率从 50 公里降至 25 公里，然后在分辨率为 3 公里的更高分辨率 BARRA-SY 数据集上进行测试。值得注意的是，它在没有额外训练的情况下表现良好，展示了其跨不同分辨率进行泛化的能力。这种方法有望通过降尺度具有不同输入分辨率的 GCM 来生成大量的区域气候模拟集合，而无需产生额外的训练成本。最终，这种方法可以提供对关键气候变量未来潜在变化更全面的估计，从而有助于对极端天气事件和气候变化适应策略进行有效的规划。

##### **Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology**
2411.14773v1 by Qian Liang, Yi Zeng, Menghaoran Tang

Musical mode is one of the most critical element that establishes the
framework of pitch organization and determines the harmonic relationships.
Previous works often use the simplistic and rigid alignment method, and
overlook the diversity of modes. However, in contrast to AI models, humans
possess cognitive mechanisms for perceiving the various modes and keys. In this
paper, we propose a spiking neural network inspired by brain mechanisms and
psychological theories to represent musical modes and keys, ultimately
generating musical pieces that incorporate tonality features. Specifically, the
contributions are detailed as follows: 1) The model is designed with multiple
collaborated subsystems inspired by the structures and functions of
corresponding brain regions; 2)We incorporate mechanisms for neural circuit
evolutionary learning that enable the network to learn and generate
mode-related features in music, reflecting the cognitive processes involved in
human music perception. 3)The results demonstrate that the proposed model shows
a connection framework closely similar to the Krumhansl-Schmuckler model, which
is one of the most significant key perception models in the music psychology
domain. 4) Experiments show that the model can generate music pieces with
characteristics of the given modes and keys. Additionally, the quantitative
assessments of generated pieces reveals that the generating music pieces have
both tonality characteristics and the melodic adaptability needed to generate
diverse and musical content. By combining insights from neuroscience,
psychology, and music theory with advanced neural network architectures, our
research aims to create a system that not only learns and generates music but
also bridges the gap between human cognition and artificial intelligence.

摘要：音樂模式是建立音高組織架構和決定和聲關係的最重要元素之一。
先前的作品通常使用簡化且僵化的對齊方法，並忽視模式的多樣性。然而，與 AI 模型相反，人類擁有認知機制來感知各種模式和音階。在本文中，我們提出了一個受大腦機制和心理理論啟發的尖峰神經網路，以表示音樂模式和音階，最終生成包含調性特徵的音樂作品。具體來說，貢獻詳述如下：1) 該模型採用多個協作子系統設計，其靈感來自對應腦區的結構和功能；2) 我們結合神經迴路進化學習機制，使網路能夠學習和生成音樂中的模式相關特徵，反映人類音樂感知所涉及的認知過程。3) 結果表明，所提出的模型顯示了一個與 Krumhansl-Schmuckler 模型非常相似的連接架構，而後者是音樂心理學領域中最重要的關鍵感知模型之一。4) 實驗表明，該模型可以生成具有給定模式和音階特徵的音樂作品。此外，對生成作品的定量評估表明，生成音樂作品既具有調性特徵，又具有生成多樣化音樂內容所需的旋律適應性。通過結合神經科學、心理學和音樂理論與先進的神經網路架構的見解，我們的研究旨在創建一個不僅學習和生成音樂，而且還能彌合理性認知和人工智慧之間差距的系統。

##### **Grid and Road Expressions Are Complementary for Trajectory Representation Learning**
2411.14768v1 by Silin Zhou, Shuo Shang, Lisi Chen, Peng Han, Christian S. Jensen

Trajectory representation learning (TRL) maps trajectories to vectors that
can be used for many downstream tasks. Existing TRL methods use either grid
trajectories, capturing movement in free space, or road trajectories, capturing
movement in a road network, as input. We observe that the two types of
trajectories are complementary, providing either region and location
information or providing road structure and movement regularity. Therefore, we
propose a novel multimodal TRL method, dubbed GREEN, to jointly utilize Grid
and Road trajectory Expressions for Effective representatioN learning. In
particular, we transform raw GPS trajectories into both grid and road
trajectories and tailor two encoders to capture their respective information.
To align the two encoders such that they complement each other, we adopt a
contrastive loss to encourage them to produce similar embeddings for the same
raw trajectory and design a mask language model (MLM) loss to use grid
trajectories to help reconstruct masked road trajectories. To learn the final
trajectory representation, a dual-modal interactor is used to fuse the outputs
of the two encoders via cross-attention. We compare GREEN with 7
state-of-the-art TRL methods for 3 downstream tasks, finding that GREEN
consistently outperforms all baselines and improves the accuracy of the
best-performing baseline by an average of 15.99\%.

摘要：軌跡表徵學習 (TRL) 將軌跡對應到向量，可供許多下游任務使用。現有的 TRL 方法使用網格軌跡（捕捉自由空間中的移動）或道路軌跡（捕捉道路網路中的移動）作為輸入。我們觀察到這兩種軌跡類型互補，提供區域和位置資訊或提供道路結構和移動規律。因此，我們提出了一種新穎的多模態 TRL 方法，稱為 GREEN，以聯合利用網格和道路軌跡表達式進行有效表徵學習。具體來說，我們將原始 GPS 軌跡轉換為網格和道路軌跡，並調整兩個編碼器以捕捉它們各自的資訊。為了對齊兩個編碼器以使其互相補充，我們採用對比損失來鼓勵它們為相同的原始軌跡產生類似的嵌入，並設計一個遮罩語言模型 (MLM) 損失來使用網格軌跡幫助重建遮罩道路軌跡。為了學習最終軌跡表徵，使用雙模態交互器透過交叉注意融合兩個編碼器的輸出。我們將 GREEN 與 7 種最先進的 TRL 方法進行比較，用於 3 個下游任務，發現 GREEN 一致地優於所有基準，並將效能最佳的基準的準確度平均提高了 15.99%。

##### **Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction**
2411.14762v1 by Huiwon Jang, Sihyun Yu, Jinwoo Shin, Pieter Abbeel, Younggyo Seo

Efficient tokenization of videos remains a challenge in training vision
models that can process long videos. One promising direction is to develop a
tokenizer that can encode long video clips, as it would enable the tokenizer to
leverage the temporal coherence of videos better for tokenization. However,
training existing tokenizers on long videos often incurs a huge training cost
as they are trained to reconstruct all the frames at once. In this paper, we
introduce CoordTok, a video tokenizer that learns a mapping from
coordinate-based representations to the corresponding patches of input videos,
inspired by recent advances in 3D generative models. In particular, CoordTok
encodes a video into factorized triplane representations and reconstructs
patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows
for training large tokenizer models directly on long videos without requiring
excessive training resources. Our experiments show that CoordTok can
drastically reduce the number of tokens for encoding long video clips. For
instance, CoordTok can encode a 128-frame video with 128$\times$128 resolution
into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar
reconstruction quality. We further show that this efficient video tokenization
enables memory-efficient training of a diffusion transformer that can generate
128 frames at once.

摘要：長影片的有效標記化在訓練能處理長影片的視覺模型時仍然是一項挑戰。一個有前景的方向是開發一個能編碼長影片片段的標記器，因為它能讓標記器更好地利用影片的時間連貫性進行標記化。然而，在長影片上訓練現有的標記器通常會產生巨大的訓練成本，因為它們被訓練一次重建所有影格。在本文中，我們介紹 CoordTok，一個影片標記器，它從基於座標的表示學習對應輸入影片的補丁的對應，靈感來自 3D 生成模型的最新進展。特別是，CoordTok 將影片編碼成分解的三平面表示，並重建對應於隨機取樣的 $(x,y,t)$ 座標的補丁。這允許在長影片上直接訓練大型標記器模型，而不需要過多的訓練資源。我們的實驗表明，CoordTok 可以大幅減少編碼長影片片段的標記數量。例如，CoordTok 可以將 128 幀、解析度為 128$\times$128 的影片編碼成 1280 個標記，而基線需要 6144 或 8192 個標記才能達到類似的重建品質。我們進一步表明，這種有效的影片標記化能讓擴散轉換器的記憶體有效訓練，一次可以產生 128 幀。

##### **TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior**
2411.14751v1 by Sen Yang, Minyue Jiang, Ziwei Fan, Xiaolu Xie, Xiao Tan, Yingying Li, Errui Ding, Liang Wang, Jingdong Wang

Recent advances in autonomous driving systems have shifted towards reducing
reliance on high-definition maps (HDMaps) due to the huge costs of annotation
and maintenance. Instead, researchers are focusing on online vectorized HDMap
construction using on-board sensors. However, sensor-only approaches still face
challenges in long-range perception due to the restricted views imposed by the
mounting angles of onboard cameras, just as human drivers also rely on
bird's-eye-view navigation maps for a comprehensive understanding of road
structures. To address these issues, we propose to train the perception model
to "see" standard definition maps (SDMaps). We encode SDMap elements into
neural spatial map representations and instance tokens, and then incorporate
such complementary features as prior information to improve the bird's eye view
(BEV) feature for lane geometry and topology decoding. Based on the lane
segment representation framework, the model simultaneously predicts lanes,
centrelines and their topology. To further enhance the ability of geometry
prediction and topology reasoning, we also use a topology-guided decoder to
refine the predictions by exploiting the mutual relationships between
topological and geometric features. We perform extensive experiments on
OpenLane-V2 datasets to validate the proposed method. The results show that our
model outperforms state-of-the-art methods by a large margin, with gains of
+6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that
models trained with SDMap noise augmentation exhibit enhanced robustness.

摘要：<paragraph>自動駕駛系統的最新進展已轉向減少對高解析度地圖 (HDMaps) 的依賴，因為註解和維護成本龐大。取而代之的是，研究人員專注於使用車載感測器進行線上向量化 HDMap 建構。然而，僅使用感測器的方法仍面臨長距離感知的挑戰，因為車載相機的安裝角度限制了視野，就像人類駕駛也依賴鳥瞰導航地圖全面了解道路結構一樣。為了解決這些問題，我們建議訓練感知模型「查看」標準定義地圖 (SDMaps)。我們將 SDMap 元素編碼成神經空間地圖表示和實例標記，然後將這些互補特徵作為先驗資訊納入，以改善車道幾何形狀和拓撲解碼的鳥瞰圖 (BEV) 特徵。基於車道區段表示架構，該模型同時預測車道、中心線及其拓撲。為了進一步增強幾何預測和拓撲推理的能力，我們還使用拓撲引導的解碼器，透過利用拓撲和幾何特徵之間的相互關係來改善預測。我們在 OpenLane-V2 資料集上進行廣泛的實驗，以驗證所提出的方法。結果顯示，我們的模型在 mAP 和拓撲指標上分別提升了 +6.7 和 +9.1，大幅優於最先進的方法。我們的分析還顯示，使用 SDMap 雜訊擴充訓練的模型表現出增強的穩健性。</paragraph>

##### **Point Cloud Understanding via Attention-Driven Contrastive Learning**
2411.14744v1 by Yi Wang, Jiaze Wang, Ziyu Guo, Renrui Zhang, Donghao Zhou, Guangyong Chen, Anfeng Liu, Pheng-Ann Heng

Recently Transformer-based models have advanced point cloud understanding by
leveraging self-attention mechanisms, however, these methods often overlook
latent information in less prominent regions, leading to increased sensitivity
to perturbations and limited global comprehension. To solve this issue, we
introduce PointACL, an attention-driven contrastive learning framework designed
to address these limitations. Our method employs an attention-driven dynamic
masking strategy that guides the model to focus on under-attended regions,
enhancing the understanding of global structures within the point cloud. Then
we combine the original pre-training loss with a contrastive learning loss,
improving feature discrimination and generalization. Extensive experiments
validate the effectiveness of PointACL, as it achieves state-of-the-art
performance across a variety of 3D understanding tasks, including object
classification, part segmentation, and few-shot learning. Specifically, when
integrated with different Transformer backbones like Point-MAE and PointGPT,
PointACL demonstrates improved performance on datasets such as ScanObjectNN,
ModelNet40, and ShapeNetPart. This highlights its superior capability in
capturing both global and local features, as well as its enhanced robustness
against perturbations and incomplete data.

摘要：近期基于 Transformer 的模型通过利用自注意力机制提升了点云理解，然而，这些方法经常忽略不显着区域中的潜在信息，导致对扰动的敏感性增加且全局理解受限。为了解决这个问题，我们引入了 PointACL，一个注意力驱动的对比学习框架，旨在解决这些限制。我们的方法采用了一个注意力驱动的动态掩蔽策略，引导模型专注于未被充分关注的区域，增强对点云中全局结构的理解。然后，我们将原始的预训练损失与对比学习损失相结合，提高特征辨别和泛化能力。广泛的实验验证了 PointACL 的有效性，因为它在各种 3D 理解任务中实现了最先进的性能，包括对象分类、部分分割和少量学习。具体来说，当与不同的 Transformer 主干（如 Point-MAE 和 PointGPT）集成时，PointACL 在 ScanObjectNN、ModelNet40 和 ShapeNetPart 等数据集上展示了改进的性能。这突出了其在捕捉全局和局部特征方面的卓越能力，以及其对扰动和不完整数据的增强鲁棒性。

##### **FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification**
2411.14743v1 by Zhengrui Guo, Conghao Xiong, Jiabo Ma, Qichen Sun, Lishuang Feng, Jinzhuo Wang, Hao Chen

Few-shot learning presents a critical solution for cancer diagnosis in
computational pathology (CPath), addressing fundamental limitations in data
availability, particularly the scarcity of expert annotations and patient
privacy constraints. A key challenge in this paradigm stems from the inherent
disparity between the limited training set of whole slide images (WSIs) and the
enormous number of contained patches, where a significant portion of these
patches lacks diagnostically relevant information, potentially diluting the
model's ability to learn and focus on critical diagnostic features. While
recent works attempt to address this by incorporating additional knowledge,
several crucial gaps hinder further progress: (1) despite the emergence of
powerful pathology foundation models (FMs), their potential remains largely
untapped, with most approaches limiting their use to basic feature extraction;
(2) current language guidance mechanisms attempt to align text prompts with
vast numbers of WSI patches all at once, struggling to leverage rich
pathological semantic information. To this end, we introduce the
knowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which
uniquely combines pathology FMs with language prior knowledge to enable a
focused analysis of diagnostically relevant regions by prioritizing
discriminative WSI patches. Our approach implements a progressive three-stage
compression strategy: we first leverage FMs for global visual redundancy
elimination, and integrate compressed features with language prompts for
semantic relevance assessment, then perform neighbor-aware visual token
filtering while preserving spatial coherence. Extensive experiments on
pathological datasets spanning breast, lung, and ovarian cancers demonstrate
its superior performance in few-shot pathology diagnosis. Code will be made
available at https://github.com/dddavid4real/FOCUS.

摘要：<paragraph>小樣本學習為計算病理學 (CPath) 中的癌症診斷提供了一個關鍵的解決方案，解決了數據可用性中的基本限制，特別是專家註解的稀缺性和患者隱私限制。這種範例中的主要挑戰源於全玻片影像 (WSI) 的有限訓練集與包含的龐大影像塊之間的固有差異，其中很大一部分影像塊缺乏診斷相關資訊，可能會稀釋模型學習和關注關鍵診斷特徵的能力。雖然最近的研究嘗試透過納入額外知識來解決這個問題，但有幾個關鍵的差距阻礙了進一步的進展：(1) 儘管強大的病理基礎模型 (FM) 出現，但它們的潛力在很大程度上仍未開發，大多數方法將它們的使用限制在基本特徵提取；(2) 目前的語言引導機制嘗試一次將文字提示與大量的 WSI 影像塊對齊，難以利用豐富的病理語義資訊。為此，我們引入了基於知識增強的自適應視覺壓縮框架，稱為 FOCUS，它獨特地將病理 FM 與語言先驗知識相結合，透過優先考慮區分性的 WSI 影像塊，對診斷相關區域進行重點分析。我們的做法實施了一個漸進的三階段壓縮策略：我們首先利用 FM 消除全局視覺冗餘，並將壓縮特徵與語言提示整合進行語義相關性評估，然後在保留空間一致性的同時執行鄰近感知視覺符號過濾。在涵蓋乳癌、肺癌和卵巢癌的病理資料集上進行的廣泛實驗證明了其在小樣本病理診斷中的優異效能。程式碼將在 https://github.com/dddavid4real/FOCUS 上提供。</paragraph>

##### **TEXGen: a Generative Diffusion Model for Mesh Textures**
2411.14740v1 by Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, JianHui Liu, Yangguang Li, Yan-Pei Cao, Ding Liang, Xiaojuan Qi

While high-quality texture maps are essential for realistic 3D asset
rendering, few studies have explored learning directly in the texture space,
especially on large-scale datasets. In this work, we depart from the
conventional approach of relying on pre-trained 2D diffusion models for
test-time optimization of 3D textures. Instead, we focus on the fundamental
problem of learning in the UV texture space itself. For the first time, we
train a large diffusion model capable of directly generating high-resolution
texture maps in a feed-forward manner. To facilitate efficient learning in
high-resolution UV spaces, we propose a scalable network architecture that
interleaves convolutions on UV maps with attention layers on point clouds.
Leveraging this architectural design, we train a 700 million parameter
diffusion model that can generate UV texture maps guided by text prompts and
single-view images. Once trained, our model naturally supports various extended
applications, including text-guided texture inpainting, sparse-view texture
completion, and text-driven texture synthesis. Project page is at
http://cvmi-lab.github.io/TEXGen/.

摘要：儘管高品質紋理貼圖對於逼真的 3D 資產渲染至關重要，但鮮少有研究探討直接在紋理空間中進行學習，特別是在大型資料集上。在本文中，我們跳脫依賴預先訓練的 2D 擴散模型來對 3D 紋理進行測試時間最佳化的傳統方法。取而代之的是，我們專注於在 UV 紋理空間本身中學習的基本問題。我們首次訓練了一個大型擴散模型，能夠以饋送前向的方式直接生成高解析度紋理貼圖。為了促進在高解析度 UV 空間中進行有效率的學習，我們提出了一種可擴充的網路架構，將 UV 貼圖上的卷積與點雲上的注意力層交錯。利用這種架構設計，我們訓練了一個擁有 7 億個參數的擴散模型，該模型能夠生成受文字提示和單視圖影像引導的 UV 紋理貼圖。訓練完成後，我們的模型自然而然地支援各種延伸應用，包括文字引導紋理修復、稀疏視圖紋理完成以及文字驅動紋理合成。專案頁面位於 http://cvmi-lab.github.io/TEXGen/。

##### **IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search**
2411.14739v1 by Simon Lupart, Zahra Abbasiantaeb, Mohammad Aliannejadi

The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing
conversational assistants, able to adapt their interaction and responses from
personalized user knowledge. The track incorporates a Personal Textual
Knowledge Base (PTKB) alongside Conversational AI tasks, such as passage
ranking and response generation. Query Rewrite being an effective approach for
resolving conversational context, we explore Large Language Models (LLMs), as
query rewriters. Specifically, our submitted runs explore multi-aspect query
generation using the MQ4CS framework, which we further enhance with Learned
Sparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder
models. We also propose an alternative to the previous interleaving strategy,
aggregating multiple aspects during the reranking phase. Our findings indicate
that multi-aspect query generation is effective in enhancing performance when
integrated with advanced retrieval and reranking models. Our results also lead
the way for better personalization in Conversational Search, relying on LLMs to
integrate personalization within query rewrite, and outperforming human rewrite
performance.

摘要：2024 年互動知識助理軌道 (iKAT) 專注於推進對話助理，能夠根據個人化使用者知識調整互動和回應。該軌道結合了個人文本知識庫 (PTKB) 和對話式 AI 任務，例如段落排名和回應產生。查詢改寫是一種解決對話內容的有效方法，我們探索大型語言模型 (LLM) 作為查詢改寫器。具體來說，我們提交的執行探索使用 MQ4CS 框架的多方面查詢產生，我們進一步透過 SPLADE 架構的學習稀疏檢索來增強它，並結合穩健的交叉編碼模型。我們還提出了一個替代先前的交錯策略，在重新排名階段彙總多個方面。我們的發現表明，多方面查詢產生在與先進的檢索和重新排名模型整合時，能夠有效提升效能。我們的結果也為對話式搜尋中更好的個人化鋪路，依賴 LLM 在查詢改寫中整合個人化，並且優於人類改寫效能。

##### **Universal and Context-Independent Triggers for Precise Control of LLM Outputs**
2411.14738v1 by Jiashuo Liang, Guancheng Li, Yang Yu

Large language models (LLMs) have been widely adopted in applications such as
automated content generation and even critical decision-making systems.
However, the risk of prompt injection allows for potential manipulation of LLM
outputs. While numerous attack methods have been documented, achieving full
control over these outputs remains challenging, often requiring experienced
attackers to make multiple attempts and depending heavily on the prompt
context. Recent advancements in gradient-based white-box attack techniques have
shown promise in tasks like jailbreaks and system prompt leaks. Our research
generalizes gradient-based attacks to find a trigger that is (1) Universal:
effective irrespective of the target output; (2) Context-Independent: robust
across diverse prompt contexts; and (3) Precise Output: capable of manipulating
LLM inputs to yield any specified output with high accuracy. We propose a novel
method to efficiently discover such triggers and assess the effectiveness of
the proposed attack. Furthermore, we discuss the substantial threats posed by
such attacks to LLM-based applications, highlighting the potential for
adversaries to taking over the decisions and actions made by AI agents.

摘要：大型語言模型 (LLM) 已廣泛應用於自動化內容生成甚至關鍵決策系統等應用中。
然而，提示注入的風險允許潛在操縱 LLM 輸出。雖然已記錄了許多攻擊方法，但要完全控制這些輸出仍然具有挑戰性，通常需要經驗豐富的攻擊者進行多次嘗試，並且嚴重依賴提示上下文。基於梯度的白盒攻擊技術的最新進展已在越獄和系統提示洩漏等任務中顯示出前景。我們的研究將基於梯度的攻擊概括為找到一個觸發器，該觸發器是 (1) 通用：無論目標輸出如何，都有效；(2) 與上下文無關：在不同的提示上下文中都很穩健；以及 (3) 精確輸出：能夠操縱 LLM 輸入，以高精度產生任何指定的輸出。我們提出了一種新穎的方法來有效發現此類觸發器並評估所提出的攻擊的有效性。此外，我們討論了此類攻擊對基於 LLM 的應用構成的重大威脅，強調了對手接管 AI 代理做出的決策和行動的潛力。

##### **Evaluating and Advancing Multimodal Large Language Models in Ability Lens**
2411.14725v1 by Feng Chen, Chenhui Gou, Jing Liu, Yang Yang, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu

As multimodal large language models (MLLMs) advance rapidly, rigorous
evaluation has become essential, providing further guidance for their
development. In this work, we focus on a unified and robust evaluation of
\textbf{vision perception} abilities, the foundational skill of MLLMs. We find
that existing perception benchmarks, each focusing on different question types,
domains, and evaluation metrics, introduce significant evaluation variance,
complicating comprehensive assessments of perception abilities when relying on
any single benchmark. To address this, we introduce \textbf{AbilityLens}, a
unified benchmark designed to evaluate MLLMs across six key perception
abilities, focusing on both accuracy and stability, with each ability
encompassing diverse question types, domains, and metrics. With the assistance
of AbilityLens, we: (1) identify the strengths and weaknesses of current
models, highlighting stability patterns and revealing a notable performance gap
between open-source and closed-source models; (2) introduce an online
evaluation mode, which uncovers interesting ability conflict and early
convergence phenomena during MLLM training; and (3) design a simple
ability-specific model merging method that combines the best ability checkpoint
from early training stages, effectively mitigating performance decline due to
ability conflict. The benchmark and online leaderboard will be released soon.

摘要：<paragraph>隨著多模態大型語言模型 (MLLM) 快速進步，嚴謹的評估已變得至關重要，為其開發提供進一步的指導。在這項工作中，我們專注於統一且穩健的評估，即 MLLM 的基礎技能「視覺感知」能力。我們發現現有的感知基準，每個都專注於不同的問題類型、領域和評估指標，引入了顯著的評估差異，在依賴任何單一基準時，會使感知能力的全面評估複雜化。為了解決這個問題，我們引入了 AbilityLens，一個統一的基準，旨在評估 MLLM 的六項關鍵感知能力，重點關注準確性和穩定性，每項能力都包含不同的問題類型、領域和指標。在 AbilityLens 的協助下，我們：(1) 找出當前模型的優缺點，強調穩定性模式並揭示開源模型和閉源模型之間顯著的效能差距；(2) 介紹一種線上評估模式，它揭示了 MLLM 訓練期間有趣的能力衝突和早期收斂現象；(3) 設計了一個簡單的能力特定模型合併方法，它結合了早期訓練階段中最佳的能力檢查點，有效減輕了因能力衝突導致的效能下降。基準和線上排行榜將很快發布。</paragraph>

##### **MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts**
2411.14721v1 by Jiatong Li, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, Qing Li

Molecule discovery is a pivotal research field, impacting everything from the
medicines we take to the materials we use. Recently, Large Language Models
(LLMs) have been widely adopted in molecule understanding and generation, yet
the alignments between molecules and their corresponding captions remain a
significant challenge. Previous endeavours often treat the molecule as a
general SMILES string or molecular graph, neglecting the fine-grained
alignments between the molecular sub-structures and the descriptive textual
phrases, which are crucial for accurate and explainable predictions. In this
case, we introduce MolReFlect, a novel teacher-student framework designed to
contextually perform the molecule-caption alignments in a fine-grained way. Our
approach initially leverages a larger teacher LLM to label the detailed
alignments by directly extracting critical phrases from molecule captions or
SMILES strings and implying them to corresponding sub-structures or
characteristics. To refine these alignments, we propose In-Context Selective
Reflection, which retrieves previous extraction results as context examples for
teacher LLM to reflect and lets a smaller student LLM select from in-context
reflection and previous extraction results. Finally, we enhance the learning
process of the student LLM through Chain-of-Thought In-Context Molecule Tuning,
integrating the fine-grained alignments and the reasoning processes within the
Chain-of-Thought format. Our experimental results demonstrate that MolReFlect
enables LLMs like Mistral-7B to significantly outperform the previous
baselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement
not only enhances the generative capabilities of LLMs in the molecule-caption
translation task, but also contributes to a more explainable framework.

摘要：分子發現是一個關鍵的研究領域，從我們服用的藥物到我們使用的材料，影響著一切。最近，大型語言模型 (LLM) 已廣泛應用於分子理解和生成中，但分子及其對應標題之間的對齊仍然是一項重大挑戰。先前的努力通常將分子視為一般的 SMILES 字符串或分子圖，忽略了分子子結構和描述性文本短語之間的細粒度對齊，這對於準確且可解釋的預測至關重要。在這種情況下，我們引入了 MolReFlect，這是一個新穎的師生框架，旨在以細粒度的方式對分子標題對齊進行上下文執行。我們的做法最初利用一個更大的教師 LLM 來標記詳細對齊，方法是直接從分子標題或 SMILES 字符串中提取關鍵短語，並將它們暗示為對應的子結構或特徵。為了優化這些對齊，我們提出了上下文選擇性反射，它將以前的提取結果作為上下文範例，供教師 LLM 進行反射，並讓一個較小的學生 LLM 從上下文反射和以前的提取結果中進行選擇。最後，我們通過思想鏈上下文分子調整增強了學生 LLM 的學習過程，將細粒度對齊和推理過程整合到思想鏈格式中。我們的實驗結果表明，MolReFlect 使像 Mistral-7B 這樣的 LLM 能夠顯著優於先前的基準，在 ChEBI-20 數據集上實現了 SOTA 性能。這一進步不僅增強了 LLM 在分子標題翻譯任務中的生成能力，而且還有助於建立一個更具可解釋性的框架。

##### **Optimizing Social Media Annotation of HPV Vaccine Skepticism and Misinformation Using Large Language Models: An Experimental Evaluation of In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models**
2411.14720v1 by Luhang Sun, Varsha Pendyala, Yun-Shiuan Chuang, Shanglin Yang, Jonathan Feldman, Andrew Zhao, Munmun De Choudhury, Sijia Yang, Dhavan Shah

This paper leverages large-language models (LLMs) to experimentally determine
optimal strategies for scaling up social media content annotation for stance
detection on HPV vaccine-related tweets. We examine both conventional
fine-tuning and emergent in-context learning methods, systematically varying
strategies of prompt engineering across widely used LLMs and their variants
(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt
template design, shot sampling methods, and shot quantity to detect stance on
HPV vaccination. Our findings reveal that 1) in general, in-context learning
outperforms fine-tuning in stance detection for HPV vaccine social media
content; 2) increasing shot quantity does not necessarily enhance performance
across models; and 3) different LLMs and their variants present differing
sensitivity to in-context learning conditions. We uncovered that the optimal
in-context learning configuration for stance detection on HPV vaccine tweets
involves six stratified shots paired with detailed contextual prompts. This
study highlights the potential and provides an applicable approach for applying
LLMs to research on social media stance and skepticism detection.

摘要：本文利用大型語言模型 (LLM) 實驗性地確定擴充社群媒體內容標註的最佳策略，以偵測與 HPV 疫苗相關推文的立場。我們檢視傳統的微調和新興的情境學習方法，系統性地改變廣泛使用的 LLM 和其變體（例如 GPT4、Mistral 和 Llama3 等）的提示工程策略。具體來說，我們改變了提示範本設計、範例抽樣方法和範例數量，以偵測對 HPV 疫苗的立場。我們的研究結果顯示，1) 一般而言，情境學習在 HPV 疫苗社群媒體內容的立場偵測上優於微調；2) 增加範例數量不一定會提升所有模型的效能；3) 不同的 LLM 和其變體對情境學習條件有不同的敏感度。我們發現，針對 HPV 疫苗推文的立場偵測，最佳的情境學習組態涉及六個分層範例，搭配詳細的情境提示。本研究突顯了 LLM 的潛力，並提供了一個可行的途徑，將 LLM 應用於社群媒體立場和懷疑偵測的研究中。

##### **FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data**
2411.14717v1 by Binqian Xu, Xiangbo Shu, Haiyang Mei, Guosen Xie, Basura Fernando, Mike Zheng Shou, Jinhui Tang

Multimodal Large Language Models (MLLMs) have made significant advancements,
demonstrating powerful capabilities in processing and understanding multimodal
data. Fine-tuning MLLMs with Federated Learning (FL) allows for expanding the
training data scope by including private data sources, thereby enhancing their
practical applicability in privacy-sensitive domains. However, current research
remains in the early stage, particularly in addressing the \textbf{multimodal
heterogeneities} in real-world applications. In this paper, we introduce a
benchmark for evaluating various downstream tasks in the federated fine-tuning
of MLLMs within multimodal heterogeneous scenarios, laying the groundwork for
the research in the field. Our benchmark encompasses two datasets, five
comparison baselines, and four multimodal scenarios, incorporating over ten
types of modal heterogeneities. To address the challenges posed by modal
heterogeneity, we develop a general FedMLLM framework that integrates four
representative FL methods alongside two modality-agnostic strategies. Extensive
experimental results show that our proposed FL paradigm improves the
performance of MLLMs by broadening the range of training data and mitigating
multimodal heterogeneity. Code is available at https://github.com/1xbq1/FedMLLM

摘要：多模態大型語言模型 (MLLM) 已取得重大進展，在處理和理解多模態數據方面展現強大的能力。透過聯合學習 (FL) 微調 MLLM，可以透過納入私人資料來源來擴展訓練資料範圍，進而提升其在注重隱私的領域中的實用性。然而，目前的研​​究仍處於早期階段，特別是在處理實際應用中的「多模態異質性」方面。在本文中，我們介紹了一個基準，用於評估 MLLM 在多模態異質場景中進行聯合微調的各種下游任務，為該領域的研究奠定基礎。我們的基準包含兩個資料集、五個比較基準線和四個多模態場景，結合了十多種類型的模態異質性。為了應對模態異質性帶來的挑戰，我們開發了一個通用的 FedMLLM 框架，該框架整合了四種代表性的 FL 方法以及兩種與模態無關的策略。廣泛的實驗結果表明，我們提出的 FL 典範透過擴展訓練資料範圍和減輕多模態異質性來提升 MLLM 的效能。程式碼可在 https://github.com/1xbq1/FedMLLM 取得

##### **LIBER: Lifelong User Behavior Modeling Based on Large Language Models**
2411.14713v1 by Chenxu Zhu, Shigang Quan, Bo Chen, Jianghao Lin, Xiaoling Cai, Hong Zhu, Xiangyang Li, Yunjia Xi, Weinan Zhang, Ruiming Tang

CTR prediction plays a vital role in recommender systems. Recently, large
language models (LLMs) have been applied in recommender systems due to their
emergence abilities. While leveraging semantic information from LLMs has shown
some improvements in the performance of recommender systems, two notable
limitations persist in these studies. First, LLM-enhanced recommender systems
encounter challenges in extracting valuable information from lifelong user
behavior sequences within textual contexts for recommendation tasks. Second,
the inherent variability in human behaviors leads to a constant stream of new
behaviors and irregularly fluctuating user interests. This characteristic
imposes two significant challenges on existing models. On the one hand, it
presents difficulties for LLMs in effectively capturing the dynamic shifts in
user interests within these sequences, and on the other hand, there exists the
issue of substantial computational overhead if the LLMs necessitate recurrent
calls upon each update to the user sequences. In this work, we propose Lifelong
User Behavior Modeling (LIBER) based on large language models, which includes
three modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest
Learning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed
to condense lengthy user behavior sequences into shorter partitions in an
incremental paradigm, facilitating more efficient processing. Subsequently, UIL
leverages LLMs in a cascading way to infer insights from these partitions.
Finally, UIF integrates the textual outputs generated by the aforementioned
processes to construct a comprehensive representation, which can be
incorporated by any recommendation model to enhance performance. LIBER has been
deployed on Huawei's music recommendation service and achieved substantial
improvements in users' play count and play time by 3.01% and 7.69%.

摘要：<paragraph>CTR 預測在推薦系統中扮演著至關重要的角色。最近，由於其新興能力，大型語言模型 (LLM) 已應用於推薦系統中。雖然利用 LLM 的語義資訊已在推薦系統的效能上展現出一些進步，但這些研究仍持續存在兩個顯著的限制。首先，LLM 增強的推薦系統在從文字脈絡中提取有價值的資訊以進行推薦任務時，會遇到挑戰。其次，人類行為的內在變異性導致持續出現新的行為和不規則波動的使用者興趣。此特性對現有模型造成兩個重大的挑戰。一方面，它讓 LLM 難以在這些序列中有效捕捉使用者興趣的動態轉變，另一方面，如果 LLM 需要在每次更新使用者序列時不斷呼叫，則會產生大量的運算負擔。在這項工作中，我們提出基於大型語言模型的終身使用者行為建模 (LIBER)，其中包含三個模組：(1) 使用者行為串流分割 (UBSP)、(2) 使用者興趣學習 (UIL) 和 (3) 使用者興趣融合 (UIF)。最初，UBSP 用於在遞增範例中將冗長的使用者行為序列濃縮成較短的分割，以利更有效率的處理。隨後，UIL 以串聯的方式利用 LLM 推論這些分割的見解。最後，UIF 整合前述流程產生的文字輸出，以建構一個全面的表示，推薦模型可以納入此表示以提升效能。LIBER 已部署在華為的音樂推薦服務中，並在使用者的播放次數和播放時間方面取得顯著的進步，分別為 3.01% 和 7.69%。</paragraph>

##### **Understanding LLM Embeddings for Regression**
2411.14708v1 by Eric Tang, Bangding Yang, Xingyou Song

With the rise of large language models (LLMs) for flexibly processing
information as strings, a natural application is regression, specifically by
preprocessing string representations into LLM embeddings as downstream features
for metric prediction. In this paper, we provide one of the first comprehensive
investigations into embedding-based regression and demonstrate that LLM
embeddings as features can be better for high-dimensional regression tasks than
using traditional feature engineering. This regression performance can be
explained in part due to LLM embeddings over numeric data inherently preserving
Lipschitz continuity over the feature space. Furthermore, we quantify the
contribution of different model effects, most notably model size and language
understanding, which we find surprisingly do not always improve regression
performance.

摘要：隨著大型語言模型 (LLM) 靈活處理資訊作為字串的興起，自然應用是回歸，特別是透過預處理字串表示成 LLM 嵌入作為下游特徵以進行度量預測。在本文中，我們提供了基於嵌入的回歸的首批全面調查之一，並證明 LLM 嵌入作為特徵可以比使用傳統特徵工程更適合高維度回歸任務。這種回歸效能的部分原因可以解釋為 LLM 嵌入在數字資料上固有地保持了特徵空間上的 Lipschitz 連續性。此外，我們量化了不同模型效應的貢獻，最顯著的是模型大小和語言理解，我們驚訝地發現它們並非總是會改善回歸效能。

##### **Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation**
2411.14698v1 by Xunyu Zhu, Jian Li, Can Ma, Weiping Wang

Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,
often achieving state-of-the-art performance in various tasks. However, their
substantial computational and memory demands, due to billions of parameters,
hinder deployment in resource-constrained environments. A promising solution is
knowledge distillation, where LLMs transfer reasoning capabilities to Small
Language Models (SLMs, $\le$ 1B parameters), enabling wider deployment on
low-resource devices. Existing methods primarily focus on generating
high-quality reasoning rationales for distillation datasets but often neglect
the critical role of data quantity and quality. To address these challenges, we
propose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'
mathematical reasoning capabilities. In the initialization stage, a
distillation dataset is constructed by prompting LLMs to pair mathematical
problems with corresponding reasoning rationales. We classify problems into
easy and hard categories based on SLM performance. For easy problems, LLMs
generate more complex variations, while for hard problems, new questions of
similar complexity are synthesized. In addition, we propose a multi-round
distillation paradigm to iteratively enrich the distillation datasets, thereby
progressively improving the mathematical reasoning abilities of SLMs.
Experimental results demonstrate that our method can make SLMs achieve SOTA
mathematical reasoning performance.

摘要：大型語言模型 (LLM) 展現出色的推理能力，在各項任務中經常達到最先進的表現。然而，由於數十億個參數，它們龐大的運算和記憶體需求阻礙了在資源受限環境中的部署。一個有前途的解決方案是知識蒸餾，其中 LLM 將推理能力轉移到小型語言模型 (SLM，$\le$ 1B 參數)，從而能夠在低資源裝置上更廣泛地部署。現有方法主要專注於為蒸餾資料集產生高品質的推理依據，但常常忽略資料數量和品質的關鍵作用。為了應對這些挑戰，我們提出了一個回饋驅動蒸餾 (FDD) 架構，以增強 SLM 的數學推理能力。在初始化階段，通過提示 LLM 將數學問題與對應的推理依據配對來建構一個蒸餾資料集。我們根據 SLM 的表現將問題分類為容易和困難的類別。對於容易的問題，LLM 會產生更複雜的變化，而對於困難的問題，則會合成類似複雜度的新問題。此外，我們提出了一個多輪蒸餾範例，以反覆豐富蒸餾資料集，從而逐步提升 SLM 的數學推理能力。實驗結果證明，我們的模型可以讓 SLM 達到數學推理表現的 SOTA 水準。

##### **Whats in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning**
2411.14688v1 by AJ Piergiovanni, Dahun Kim, Michael S. Ryoo, Isaac Noble, Anelia Angelova

Generating automatic dense captions for videos that accurately describe their
contents remains a challenging area of research. Most current models require
processing the entire video at once. Instead, we propose an efficient, online
approach which outputs frequent, detailed and temporally aligned captions,
without access to future frames. Our model uses a novel autoregressive
factorized decoding architecture, which models the sequence of visual features
for each time segment, outputting localized descriptions and efficiently
leverages the context from the previous video segments. This allows the model
to output frequent, detailed captions to more comprehensively describe the
video, according to its actual local content, rather than mimic the training
data. Second, we propose an optimization for efficient training and inference,
which enables scaling to longer videos. Our approach shows excellent
performance compared to both offline and online methods, and uses 20\% less
compute. The annotations produced are much more comprehensive and frequent, and
can further be utilized in automatic video tagging and in large-scale video
data harvesting.

摘要：生成自動且詳細的影片字幕，並準確描述影片內容，一直是研究領域中的一大挑戰。目前大多數模型都需要一次處理完整影片。我們提出了一種高效的線上方法，可以在不存取未來影格的情況下，輸出頻繁、詳細且時間對齊的字幕。我們的模型使用一種新穎的自迴歸分解解碼架構，它會為每個時間區段建模視覺特徵序列，輸出局部描述，並有效利用前一個影片區段的內容。這讓模型可以輸出頻繁且詳細的字幕，根據實際的局部內容更全面地描述影片，而不是模仿訓練資料。其次，我們提出了一種用於高效訓練和推論的最佳化，這讓我們能夠擴充到更長的影片。我們的做法與離線和線上方法相比，展現了卓越的效能，並減少了 20% 的運算。產生的註解更全面且頻繁，可用於自動影片標記和大型影片資料收集。

##### **Cross Group Attention and Group-wise Rolling for Multimodal Medical Image Synthesis**
2411.14684v1 by Tao Song, Yicheng Wu, Minhao Hu, Xiangde Luo, Linda Wei, Guotai Wang, Yi Guo, Feng Xu, Shaoting Zhang

Multimodal MR image synthesis aims to generate missing modality image by
fusing and mapping a few available MRI data. Most existing approaches typically
adopt an image-to-image translation scheme. However, these methods often suffer
from sub-optimal performance due to the spatial misalignment between different
modalities while they are typically treated as input channels. Therefore, in
this paper, we propose an Adaptive Group-wise Interaction Network (AGI-Net)
that explores both inter-modality and intra-modality relationships for
multimodal MR image synthesis. Specifically, groups are first pre-defined along
the channel dimension and then we perform an adaptive rolling for the standard
convolutional kernel to capture inter-modality spatial correspondences. At the
same time, a cross-group attention module is introduced to fuse information
across different channel groups, leading to better feature representation. We
evaluated the effectiveness of our model on the publicly available IXI and
BraTS2023 datasets, where the AGI-Net achieved state-of-the-art performance for
multimodal MR image synthesis. Code will be released.

摘要：多模态磁共振图像合成旨在通过融合和映射一些可用的 MRI 数据来生成缺失模态图像。大多数现有方法通常采用图像到图像的转换方案。然而，由于不同模态之间的空间错位，这些方法通常会出现次优性能，而它们通常被视为输入通道。因此，在本文中，我们提出了一种自适应组交互网络 (AGI-Net)，它探索了用于多模态磁共振图像合成的模态间和模态内关系。具体而言，首先沿着通道维度预定义组，然后我们对标准卷积核执行自适应滚动以捕获模态间空间对应关系。同时，引入跨组注意力模块以融合不同通道组之间的信息，从而获得更好的特征表示。我们在公开可用的 IXI 和 BraTS2023 数据集上评估了我们模型的有效性，其中 AGI-Net 在多模态磁共振图像合成方面取得了最先进的性能。代码将被释放。

##### **Multiverse of Greatness: Generating Story Branches with LLMs**
2411.14672v1 by Pittawat Taveekitworachai, Chollakorn Nimpattanavong, Mustafa Can Gursesli, Antonio Lanata, Andrea Guazzini, Ruck Thawonmas

This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel
framework for interacting with LLMs to generate graph-based content with a
dynamic context window history. While there is an existing study utilizing LLMs
to generate a visual novel game, the previous study involved a manual process
of output extraction and did not provide flexibility in generating a longer,
coherent story. We evaluate DCP/P against our baseline, which does not provide
context history to an LLM and only relies on the initial story data. Through
objective evaluation, we show that simply providing the LLM with a summary
leads to a subpar story compared to additionally providing the LLM with the
proper context of the story. We also provide an extensive qualitative analysis
and discussion. We qualitatively examine the quality of the objectively
best-performing generated game from each approach. In addition, we examine
biases in word choices and word sentiment of the generated content. We find a
consistent observation with previous studies that LLMs are biased towards
certain words, even with a different LLM family. Finally, we provide a
comprehensive discussion on opportunities for future studies.

摘要：本文提出動態內容提示/程式設計 (DCP/P)，這是一個互動式 LLM 新架構，可產生具有動態內容視窗歷程記錄的圖形化內容。儘管現有研究利用 LLM 產生視覺小說遊戲，但先前的研究涉及手動輸出擷取過程，且無法靈活產生較長、連貫的故事。我們評估 DCP/P 與我們的基準，基準未提供 LLM 內容歷程記錄，且僅依賴於初始故事資料。透過客觀評估，我們顯示僅提供 LLM 摘要，會產生次佳故事，相較於額外提供 LLM 適當的故事內容。我們也提供廣泛的定性分析與討論。我們定性地檢視每種方法產生的遊戲中，客觀最佳表現的品質。此外，我們檢視產生內容中字詞選擇和字詞情緒的偏誤。我們發現一項與先前研究一致的觀察，即 LLM 偏向特定字詞，即使使用不同的 LLM 家族。最後，我們提供有關未來研究機會的全面討論。

##### **Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective**
2411.14654v1 by Jinming Xing, Ruilin Xing, Yan Sun

Large Language Models (LLMs) have revolutionized natural language processing
(NLP) by delivering state-of-the-art performance across a variety of tasks.
Among these, Transformer-based models like BERT and GPT rely on pooling layers
to aggregate token-level embeddings into sentence-level representations. Common
pooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in
this aggregation process. Despite their widespread use, the comparative
performance of these strategies on different LLM architectures remains
underexplored. To address this gap, this paper investigates the effects of
these pooling mechanisms on two prominent LLM families -- BERT and GPT, in the
context of sentence-level sentiment analysis. Comprehensive experiments reveal
that each pooling mechanism exhibits unique strengths and weaknesses depending
on the task's specific requirements. Our findings underline the importance of
selecting pooling methods tailored to the demands of particular applications,
prompting a re-evaluation of common assumptions regarding pooling operations.
By offering actionable insights, this study contributes to the optimization of
LLM-based models for downstream tasks.

摘要：大型語言模型 (LLM) 透過在各種任務中提供最先進的效能，徹底改變了自然語言處理 (NLP)。在這些模型中，像 BERT 和 GPT 等基於 Transformer 的模型依賴於彙總層，將標記層級的嵌入彙總到句子層級的表示中。平均值、最大值和加權總和等常見的彙總機制在此彙總過程中扮演著關鍵角色。儘管這些策略廣泛使用，但這些策略在不同 LLM 架構上的比較效能仍未得到充分探討。為了解決這個差距，本文探討了這些彙總機制對兩個著名的 LLM 家族（BERT 和 GPT）在句子層級情緒分析中的影響。全面的實驗顯示，每種彙總機制都展現出獨特的優勢和劣勢，具體取決於任務的特定需求。我們的研究結果強調了根據特定應用需求選擇彙總方法的重要性，促使重新評估有關彙總運算的常見假設。透過提供可行的見解，這項研究有助於針對下游任務最佳化基於 LLM 的模型。

##### **Social Media Algorithms Can Shape Affective Polarization via Exposure to Antidemocratic Attitudes and Partisan Animosity**
2411.14652v1 by Tiziano Piccardi, Martin Saveski, Chenyan Jia, Jeffrey T. Hancock, Jeanne L. Tsai, Michael Bernstein

There is widespread concern about the negative impacts of social media feed
ranking algorithms on political polarization. Leveraging advancements in large
language models (LLMs), we develop an approach to re-rank feeds in real-time to
test the effects of content that is likely to polarize: expressions of
antidemocratic attitudes and partisan animosity (AAPA). In a preregistered
10-day field experiment on X/Twitter with 1,256 consented participants, we
increase or decrease participants' exposure to AAPA in their algorithmically
curated feeds. We observe more positive outparty feelings when AAPA exposure is
decreased and more negative outparty feelings when AAPA exposure is increased.
Exposure to AAPA content also results in an immediate increase in negative
emotions, such as sadness and anger. The interventions do not significantly
impact traditional engagement metrics such as re-post and favorite rates. These
findings highlight a potential pathway for developing feed algorithms that
mitigate affective polarization by addressing content that undermines the
shared values required for a healthy democracy.

摘要：對於社群媒體動態排序演算法對政治兩極化的負面影響，存在廣泛的擔憂。利用大型語言模型 (LLM) 的進展，我們開發了一種方法來重新排列動態，以測試可能會導致兩極化的內容的效果：反民主態度和黨派敵意的表達 (AAPA)。在 X/Twitter 上進行為期 10 天的預先註冊實地實驗，有 1,256 名參與者同意，我們增加或減少參與者在演算法策展的動態中接觸 AAPA 的次數。當 AAPA 曝光量減少時，我們觀察到更多正面的黨外感受，而當 AAPA 曝光量增加時，我們觀察到更多負面的黨外感受。接觸 AAPA 內容也會導致負面情緒立即增加，例如悲傷和憤怒。這些干預措施並未顯著影響傳統的參與指標，例如轉發和按讚率。這些發現突顯了一條潛在的途徑，可以透過處理破壞健康民主所需的共同價值觀的內容，來開發動態演算法，以減輕情感兩極化。

##### **Benchmarking Multimodal Models for Ukrainian Language Understanding Across Academic and Cultural Domains**
2411.14647v1 by Yurii Paniv, Artur Kiulian, Dmytro Chaplynskyi, Mykola Khandoga, Anton Polishko, Tetiana Bas, Guillermo Gabrielli

While the evaluation of multimodal English-centric models is an active area
of research with numerous benchmarks, there is a profound lack of benchmarks or
evaluation suites for low- and mid-resource languages. We introduce ZNO-Vision,
a comprehensive multimodal Ukrainian-centric benchmark derived from
standardized university entrance examination (ZNO). The benchmark consists of
over 4,300 expert-crafted questions spanning 12 academic disciplines, including
mathematics, physics, chemistry, and humanities. We evaluated the performance
of both open-source models and API providers, finding that only a handful of
models performed above baseline. Alongside the new benchmark, we performed the
first evaluation study of multimodal text generation for the Ukrainian
language: we measured caption generation quality on the Multi30K-UK dataset,
translated the VQA benchmark into Ukrainian, and measured performance
degradation relative to original English versions. Lastly, we tested a few
models from a cultural perspective on knowledge of national cuisine. We believe
our work will advance multimodal generation capabilities for the Ukrainian
language and our approach could be useful for other low-resource languages.

摘要：雖然多模態以英語為中心的模型評估是研究領域中的活躍區域，有許多基準，但低資源和中等資源語言的基準或評估套件卻嚴重不足。我們介紹了 ZNO-Vision，這是一個全面的以烏克蘭為中心的模態基準，源自標準化的大學入學考試 (ZNO)。該基準包含超過 4,300 個由專家設計的問題，涵蓋 12 個學術領域，包括數學、物理、化學和人文學科。我們評估了開源模型和 API 提供者的效能，發現只有少數模型的效能高於基線。除了新的基準，我們還對烏克蘭語的多模態文字產生進行了首次評估研究：我們在 Multi30K-UK 資料集上測量了字幕產生品質，將 VQA 基準翻譯成烏克蘭語，並測量相對於原始英文版本的效能下降。最後，我們從文化觀點測試了幾個模型對國家料理的知識。我們相信我們的作品將提升烏克蘭語的多模態產生能力，而且我們的做法對其他低資源語言可能也有用。

##### **Evaluating Representational Similarity Measures from the Lens of Functional Correspondence**
2411.14633v1 by Yiqing Bo, Ansh Soni, Sudhanshu Srivastava, Meenakshi Khosla

Neuroscience and artificial intelligence (AI) both face the challenge of
interpreting high-dimensional neural data, where the comparative analysis of
such data is crucial for revealing shared mechanisms and differences between
these complex systems. Despite the widespread use of representational
comparisons and the abundance classes of comparison methods, a critical
question remains: which metrics are most suitable for these comparisons? While
some studies evaluate metrics based on their ability to differentiate models of
different origins or constructions (e.g., various architectures), another
approach is to assess how well they distinguish models that exhibit distinct
behaviors. To investigate this, we examine the degree of alignment between
various representational similarity measures and behavioral outcomes, employing
group statistics and a comprehensive suite of behavioral metrics for
comparison. In our evaluation of eight commonly used representational
similarity metrics in the visual domain -- spanning alignment-based, Canonical
Correlation Analysis (CCA)-based, inner product kernel-based, and
nearest-neighbor methods -- we found that metrics like linear Centered Kernel
Alignment (CKA) and Procrustes distance, which emphasize the overall geometric
structure or shape of representations, excelled in differentiating trained from
untrained models and aligning with behavioral measures, whereas metrics such as
linear predictivity, commonly used in neuroscience, demonstrated only moderate
alignment with behavior. These insights are crucial for selecting metrics that
emphasize behaviorally meaningful comparisons in NeuroAI research.

摘要：<paragraph>神經科學和人工智慧 (AI) 都面臨著詮釋高維度神經數據的挑戰，其中此類數據的比較分析對於揭示這些複雜系統之間的共同機制和差異至關重要。儘管表示比較已廣泛使用，且比較方法的分類豐富，但仍有一個關鍵問題：哪些指標最適合這些比較？雖然有些研究根據指標區分不同來源或結構的模型（例如，各種架構）的能力來評估指標，但另一種方法是評估它們區分表現出不同行為的模型的程度。為了研究這一點，我們檢視了各種表示相似性測量與行為結果之間的一致性程度，採用群體統計和一套全面的行為指標進行比較。在我們對視覺領域中八種常用的表示相似性指標的評估中——涵蓋基於對齊的、正則相關分析 (CCA) 基於的、內積核基於的和最近鄰方法——我們發現像線性中心核對齊 (CKA) 和 Procrustes 距離這樣的指標強調表示的整體幾何結構或形狀，在區分訓練過的模型和未訓練過的模型以及與行為測量保持一致方面表現出色，而線性預測性等指標在神經科學中通常使用，僅表現出與行為的適度一致性。這些見解對於選擇強調神經 AI 研究中行為有意義比較的指標至關重要。</paragraph>

##### **Unveiling the Hidden: A Comprehensive Evaluation of Underwater Image Enhancement and Its Impact on Object Detection**
2411.14626v1 by Ali Awad, Ashraf Saleem, Sidike Paheding, Evan Lucas, Serein Al-Ratrout, Timothy C. Havens

Underwater imagery often suffers from severe degradation that results in low
visual quality and object detection performance. This work aims to evaluate
state-of-the-art image enhancement models, investigate their impact on
underwater object detection, and explore their potential to improve detection
performance. To this end, we selected representative underwater image
enhancement models covering major enhancement categories and applied them
separately to two recent datasets: 1) the Real-World Underwater Object
Detection Dataset (RUOD), and 2) the Challenging Underwater Plant Detection
Dataset (CUPDD). Following this, we conducted qualitative and quantitative
analyses on the enhanced images and developed a quality index (Q-index) to
compare the quality distribution of the original and enhanced images.
Subsequently, we compared the performance of several YOLO-NAS detection models
that are separately trained and tested on the original and enhanced image sets.
Then, we performed a correlation study to examine the relationship between
enhancement metrics and detection performance. We also analyzed the inference
results from the trained detectors presenting cases where enhancement increased
the detection performance as well as cases where enhancement revealed missed
objects by human annotators. This study suggests that although enhancement
generally deteriorates the detection performance, it can still be harnessed in
some cases for increased detection performance and more accurate human
annotation.

摘要：水下影像经常遭受严重劣化，导致视觉品质低落，且物体侦测效能不佳。这项研究旨在评估最先进的影像增强模型，调查它们对水下物体侦测的影响，并探索它们改善侦测效能的可能性。为此，我们选择了涵盖主要增强类别的代表性水下影像增强模型，并将它们分别应用于两个最近的数据集：1) 真实世界水下物体侦测数据集 (RUOD)，以及 2) 具有挑战性的水下植物侦测数据集 (CUPDD)。在此之后，我们对增强后的影像进行了定性和定量分析，并开发了一个质量指标 (Q 指标) 来比较原始影像和增强影像的质量分布。随后，我们比较了几个 YOLO-NAS 侦测模型的效能，这些模型分别在原始影像和增强影像集上进行训练和测试。然后，我们进行了一项相关性研究，以检验增强指标和侦测效能之间的关系。我们还分析了训练过的侦测器的推论结果，呈现出增强提升侦测效能的案例，以及增强揭露了人类注释员遗漏物体的案例。这项研究表明，虽然增强通常会降低侦测效能，但在某些情况下，它仍然可以被利用来提升侦测效能和更准确的人类注释。

##### **Predictive Analytics of Air Alerts in the Russian-Ukrainian War**
2411.14625v1 by Demian Pavlyshenko, Bohdan Pavlyshenko

The paper considers exploratory data analysis and approaches in predictive
analytics for air alerts during the Russian-Ukrainian war which broke out on
Feb 24, 2022. The results illustrate that alerts in regions correlate with one
another and have geospatial patterns which make it feasible to build a
predictive model which predicts alerts that are expected to take place in a
certain region within a specified time period. The obtained results show that
the alert status in a particular region is highly dependable on the features of
its adjacent regions. Seasonality features like hours, days of a week and
months are also crucial in predicting the target variable. Some regions highly
rely on the time feature which equals to a number of days from the initial date
of the dataset. From this, we can deduce that the air alert pattern changes
throughout the time.

摘要：本文探討了在俄烏戰爭期間（2022 年 2 月 24 日爆發）進行空襲警報的探索性資料分析和預測分析方法。結果表明，各區域的警報相互關聯，並具有地理空間模式，這使得建立一個預測模型成為可能，該模型可以預測在特定時間段內預計在特定區域發生的警報。獲得的結果表明，特定區域的警報狀態高度依賴於其鄰近區域的特徵。季節性特徵（例如小時、星期和月份）在預測目標變量時也至關重要。有些區域高度依賴於時間特徵，該特徵等於從資料集初始日期開始的天數。由此，我們可以推斷出空襲警報模式會隨著時間而改變。

##### **Exploiting Boosting in Hyperdimensional Computing for Enhanced Reliability in Healthcare**
2411.14612v1 by SungHeon Jeong, Hamza Errahmouni Barkam, Sanggeon Yun, Yeseong Kim, Shaahin Angizi, Mohsen Imani

Hyperdimensional computing (HDC) enables efficient data encoding and
processing in high-dimensional space, benefiting machine learning and data
analysis. However, underutilization of these spaces can lead to overfitting and
reduced model reliability, especially in data-limited systems a critical issue
in sectors like healthcare that demand robustness and consistent performance.
We introduce BoostHD, an approach that applies boosting algorithms to partition
the hyperdimensional space into subspaces, creating an ensemble of weak
learners. By integrating boosting with HDC, BoostHD enhances performance and
reliability beyond existing HDC methods. Our analysis highlights the importance
of efficient utilization of hyperdimensional spaces for improved model
performance. Experiments on healthcare datasets show that BoostHD outperforms
state-of-the-art methods. On the WESAD dataset, it achieved an accuracy of
98.37%, surpassing Random Forest, XGBoost, and OnlineHD. BoostHD also
demonstrated superior inference efficiency and stability, maintaining high
accuracy under data imbalance and noise. In person-specific evaluations, it
achieved an average accuracy of 96.19%, outperforming other models. By
addressing the limitations of both boosting and HDC, BoostHD expands the
applicability of HDC in critical domains where reliability and precision are
paramount.

摘要：超維度運算 (HDC) 能有效率地對高維度空間中的資料進行編碼和處理，這對機器學習和資料分析很有好處。然而，如果這些空間沒有充分利用，可能會導致過度擬合和降低模型可靠度，特別是在資料有限的系統中，這在醫療保健等領域是一個關鍵問題，因為這些領域需要強健性和一致的效能。我們介紹 BoostHD，這是一種將提升演算法應用於將超維度空間分割成子空間的方法，創造出一個由弱學習器組成的集合。透過將提升與 HDC 整合，BoostHD 能夠提升效能和可靠度，超越現有的 HDC 方法。我們的分析強調了有效利用超維度空間以改善模型效能的重要性。在醫療保健資料集上的實驗顯示，BoostHD 優於最先進的方法。在 WESAD 資料集上，它的準確度達到 98.37%，超越了 Random Forest、XGBoost 和 OnlineHD。BoostHD 也展現出優異的推論效率和穩定性，在資料不平衡和雜訊的情況下仍能維持高準確度。在特定個人的評估中，它的平均準確度達到 96.19%，優於其他模型。透過解決提升和 HDC 的限制，BoostHD 擴展了 HDC 在可靠度和精確度至關重要的關鍵領域的適用性。

##### **G-RAG: Knowledge Expansion in Material Science**
2411.14592v1 by Radeen Mostafa, Mirza Nihal Baig, Mashaekh Tausif Ehsan, Jakir Hasan

In the field of Material Science, effective information retrieval systems are
essential for facilitating research. Traditional Retrieval-Augmented Generation
(RAG) approaches in Large Language Models (LLMs) often encounter challenges
such as outdated information, hallucinations, limited interpretability due to
context constraints, and inaccurate retrieval. To address these issues, Graph
RAG integrates graph databases to enhance the retrieval process. Our proposed
method processes Material Science documents by extracting key entities
(referred to as MatIDs) from sentences, which are then utilized to query
external Wikipedia knowledge bases (KBs) for additional relevant information.
We implement an agent-based parsing technique to achieve a more detailed
representation of the documents. Our improved version of Graph RAG called G-RAG
further leverages a graph database to capture relationships between these
entities, improving both retrieval accuracy and contextual understanding. This
enhanced approach demonstrates significant improvements in performance for
domains that require precise information retrieval, such as Material Science.

摘要：在材料科學領域中，有效的資訊檢索系統對於促進研究至關重要。大型語言模型 (LLM) 中的傳統檢索增強生成 (RAG) 方法經常會遇到諸如資訊過時、幻覺、由於脈絡限制而導致的可解釋性有限以及檢索不準確等挑戰。為了解決這些問題，Graph RAG 整合圖形資料庫以增強檢索過程。我們提出的方法透過從句子中萃取關鍵實體（稱為 MatID）來處理材料科學文件，然後利用這些實體查詢外部的維基百科知識庫 (KB) 以取得其他相關資訊。我們實作了一種基於代理的解析技術，以達成更詳細的文件表示。我們改良的 Graph RAG 版本稱為 G-RAG，進一步利用圖形資料庫來擷取這些實體之間的關係，進而改善檢索準確度和脈絡理解。此增強的方法對於需要精確資訊檢索的領域（例如材料科學）展現出顯著的效能提升。

##### **SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world Human-Machine Interactions**
2411.14574v1 by Yaqi Wang, Haipei Xu

Recently, as Large Language Models (LLMs) have shown impressive emerging
capabilities and gained widespread popularity, research on LLM-based search
agents has proliferated. In real-world situations, users often input contextual
and highly personalized queries to chatbots, challenging LLMs to capture
context and generate appropriate answers. However, much of the prior research
has not focused specifically on authentic human-machine dialogue scenarios. It
also ignores the important balance between response quality and computational
cost by forcing all queries to follow the same agent process. To address these
gaps, we propose a Strategy-Router Search Agent (SRSA), routing different
queries to appropriate search strategies and enabling fine-grained serial
searches to obtain high-quality results at a relatively low cost. To evaluate
our work, we introduce a new dataset, Contextual Query Enhancement Dataset
(CQED), comprising contextual queries to simulate authentic and daily
interactions between humans and chatbots. Using LLM-based automatic evaluation
metrics, we assessed SRSA's performance in terms of informativeness,
completeness, novelty, and actionability. To conclude, SRSA provides an
approach that resolves the issue of simple serial searches leading to
degenerate answers for lengthy and contextual queries, effectively and
efficiently parses complex user queries, and generates more comprehensive and
informative responses without fine-tuning an LLM.

摘要：近来，随着大型语言模型 (LLM) 展现出令人印象深刻的新兴能力并获得广泛欢迎，基于 LLM 的搜索代理的研究也随之激增。在现实世界中，用户常常会向聊天机器人输入情境化且高度个性化的查询，这对于 LLM 捕捉语境并生成适当的答案提出了挑战。然而，许多先前的研究并未特别关注真实的人机对话场景。而且，通过强制所有查询遵循相同的代理流程，这些研究还忽略了响应质量与计算成本之间的重要平衡。为了解决这些差距，我们提出了一种策略路由搜索代理 (SRSA)，将不同的查询路由到适当的搜索策略，并启用细粒度的串行搜索，以相对较低的成本获得高质量的结果。为了评估我们的工作，我们引入了一个新的数据集，即情境查询增强数据集 (CQED)，其中包含情境查询，以模拟人与聊天机器人之间的真实且日常的互动。使用基于 LLM 的自动评估指标，我们评估了 SRSA 在信息性、完整性、新颖性和可操作性方面的性能。总而言之，SRSA 提供了一种方法来解决简单的串行搜索导致冗余答案的问题，以针对冗长且有情境的查询，有效且高效地解析复杂的使用者查询，并在不微调 LLM 的情况下生成更全面且信息丰富的响应。

##### **Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective**
2411.14572v1 by Shenglai Zeng, Jiankun Zhang, Bingheng Li, Yuping Lin, Tianqi Zheng, Dante Everaert, Hanqing Lu, Hui Liu, Hui Liu, Yue Xing, Monica Xiao Cheng, Jiliang Tang

Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing
the performance of Large Language Models (LLMs). However, these systems face
challenges in effectively integrating external knowledge with the LLM's
internal knowledge, often leading to issues with misleading or unhelpful
information. This work aims to provide a systematic study on knowledge checking
in RAG systems. We conduct a comprehensive analysis of LLM representation
behaviors and demonstrate the significance of using representations in
knowledge checking. Motivated by the findings, we further develop
representation-based classifiers for knowledge filtering. We show substantial
improvements in RAG performance, even when dealing with noisy knowledge
databases. Our study provides new insights into leveraging LLM representations
for enhancing the reliability and effectiveness of RAG systems.

摘要：檢索增強生成 (RAG) 系統已展現出增強大型語言模型 (LLM) 效能的潛力。然而，這些系統在有效整合外部知識與 LLM 的內部知識方面面臨挑戰，這通常會導致誤導或無益的資訊問題。這項工作旨在對 RAG 系統中的知識檢查進行系統性研究。我們進行了 LLM 表徵行為的全面分析，並展示了在知識檢查中使用表徵的重要性。受到這些發現的啟發，我們進一步開發了用於知識過濾的基於表徵的分類器。我們展示了 RAG 效能的顯著提升，即使在處理雜訊知識資料庫時也是如此。我們的研究為利用 LLM 表徵來增強 RAG 系統的可靠性和有效性提供了新的見解。

##### **Assessment of LLM Responses to End-user Security Questions**
2411.14571v1 by Vijay Prakash, Kevin Lee, Arkaprabha Bhattacharya, Danny Yuxing Huang, Jessica Staddon

Answering end user security questions is challenging. While large language
models (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have
shown promise in answering a variety of questions outside of security. We
studied LLM performance in the area of end user security by qualitatively
evaluating 3 popular LLMs on 900 systematically collected end user security
questions.
  While LLMs demonstrate broad generalist ``knowledge'' of end user security
information, there are patterns of errors and limitations across LLMs
consisting of stale and inaccurate answers, and indirect or unresponsive
communication styles, all of which impacts the quality of information received.
Based on these patterns, we suggest directions for model improvement and
recommend user strategies for interacting with LLMs when seeking assistance
with security.

摘要：回答最終用戶的安全問題是一項挑戰。雖然大型語言模型 (LLM)，例如 GPT、LLAMA 和 Gemini 遠非沒有錯誤，但它們在回答各種安全問題方面已展現出前景。我們透過定性評估 3 個流行的 LLM，針對 900 個系統收集的最終用戶安全問題，研究了 LLM 在最終用戶安全領域的表現。

雖然 LLM 證明對最終用戶安全資訊有廣泛的通才「知識」，但 LLM 中存在著錯誤和限制的模式，包括陳舊和不準確的答案，以及間接或無回應的溝通方式，所有這些都會影響所接收資訊的品質。基於這些模式，我們建議模型改進的方向，並建議用戶在尋求安全協助時與 LLM 互動的策略。

##### **An Experimental Study on Data Augmentation Techniques for Named Entity Recognition on Low-Resource Domains**
2411.14551v1 by Arthur Elwing Torres, Edleno Silva de Moura, Altigran Soares da Silva, Mario A. Nascimento, Filipe Mesquita

Named Entity Recognition (NER) is a machine learning task that traditionally
relies on supervised learning and annotated data. Acquiring such data is often
a challenge, particularly in specialized fields like medical, legal, and
financial sectors. Those are commonly referred to as low-resource domains,
which comprise long-tail entities, due to the scarcity of available data. To
address this, data augmentation techniques are increasingly being employed to
generate additional training instances from the original dataset. In this
study, we evaluate the effectiveness of two prominent text augmentation
techniques, Mention Replacement and Contextual Word Replacement, on two
widely-used NER models, Bi-LSTM+CRF and BERT. We conduct experiments on four
datasets from low-resource domains, and we explore the impact of various
combinations of training subset sizes and number of augmented examples. We not
only confirm that data augmentation is particularly beneficial for smaller
datasets, but we also demonstrate that there is no universally optimal number
of augmented examples, i.e., NER practitioners must experiment with different
quantities in order to fine-tune their projects.

摘要：命名實體辨識 (NER) 是一種機器學習任務，傳統上依賴監督式學習和標註資料。取得此類資料通常具有挑戰性，特別是在醫療、法律和金融等專業領域。這些領域通常稱為低資源網域，由於可用資料稀少，因此包含長尾實體。為了解決這個問題，資料擴充技術正日益被用於從原始資料集產生額外的訓練實例。在本研究中，我們評估了兩種著名的文字擴充技術，即提及替換和語境字詞替換，在兩個廣泛使用的 NER 模型，Bi-LSTM+CRF 和 BERT 上的有效性。我們對來自低資源網域的四個資料集進行實驗，並探討各種訓練子集大小和擴充範例數量組合的影響。我們不僅確認資料擴充對於較小的資料集特別有益，而且還證明沒有普遍最佳的擴充範例數量，即 NER 從業者必須嘗試不同的數量才能微調他們的專案。

##### **The importance of the clustering model to detect new types of intrusion in data traffic**
2411.14550v1 by Noor Saud Abd, Kamel Karoui

In the current digital age, the volume of data generated by various cyber
activities has become enormous and is constantly increasing. The data may
contain valuable insights that can be harnessed to improve cyber security
measures. However, much of this data is unclassified and qualitative, which
poses significant challenges to traditional analysis methods. Clustering
facilitates the identification of hidden patterns and structures in data
through grouping similar data points, which makes it simpler to identify and
address threats. Clustering can be defined as a data mining (DM) approach,
which uses similarity calculations for dividing a data set into several
categories. Hierarchical, density-based, along with partitioning clustering
algorithms are typical. The presented work use K-means algorithm, which is a
popular clustering technique. Utilizing K-means algorithm, we worked with two
different types of data: first, we gathered data with the use of XG-boost
algorithm following completing the aggregation with K-means algorithm. Data was
gathered utilizing Kali Linux environment, cicflowmeter traffic, and Putty
Software tools with the use of diverse and simple attacks. The concept could
assist in identifying new attack types, which are distinct from the known
attacks, and labeling them based on the characteristics they will exhibit, as
the dynamic nature regarding cyber threats means that new attack types often
emerge, for which labeled data might not yet exist. The model counted the
attacks and assigned numbers to each one of them. Secondly, We tried the same
work on the ready data inside the Kaggle repository called (Intrusion Detection
in Internet of Things Network), and the clustering model worked well and
detected the number of attacks correctly as shown in the results section.

摘要：<paragraph>在當今數位時代，各種網路活動所產生的資料量已變得龐大且持續增加。資料可能包含有價值的見解，可加以利用來改善網路安全措施。然而，這些資料大多未分類且為質性資料，這對傳統分析方法構成重大挑戰。叢集透過將相似資料點分組，進而促進識別資料中隱藏的模式和結構，這使得識別和處理威脅變得更簡單。叢集可定義為一資料探勘 (DM) 方法，該方法使用相似度計算來將資料集劃分為數個類別。階層式、基於密度的分群演算法與分割式分群演算法是常見的。所提出的工作使用 K 平均演算法，這是一種流行的群集技術。利用 K 平均演算法，我們處理了兩種不同類型的資料：首先，我們使用 XG-boost 演算法收集資料，然後使用 K 平均演算法完成匯總。資料是利用 Kali Linux 環境、cicflowmeter 流量和 Putty 軟體工具收集的，並使用多樣且簡單的攻擊。這個概念有助於識別新的攻擊類型，這些攻擊類型不同於已知的攻擊，並根據其所表現出的特徵對其進行標記，因為網路威脅的動態特性表示新的攻擊類型經常出現，而標記資料可能尚未存在。該模型計算了攻擊次數，並為每個攻擊次數指定編號。其次，我們嘗試對 Kaggle 儲存庫中稱為 (物聯網網路中的入侵偵測) 的現成資料進行相同的工作，而群集模型運作良好，並正確偵測到攻擊次數，如結果部分所示。</paragraph>

##### **Whack-a-Chip: The Futility of Hardware-Centric Export Controls**
2411.14425v1 by Ritwik Gupta, Leah Walker, Andrew W. Reddie

U.S. export controls on semiconductors are widely known to be permeable, with
the People's Republic of China (PRC) steadily creating state-of-the-art
artificial intelligence (AI) models with exfiltrated chips. This paper presents
the first concrete, public evidence of how leading PRC AI labs evade and
circumvent U.S. export controls. We examine how Chinese companies, notably
Tencent, are not only using chips that are restricted under U.S. export
controls but are also finding ways to circumvent these regulations by using
software and modeling techniques that maximize less capable hardware.
Specifically, we argue that Tencent's ability to power its Hunyuan-Large model
with non-export controlled NVIDIA H20s exemplifies broader gains in efficiency
in machine learning that have eroded the moat that the United States initially
built via its existing export controls. Finally, we examine the implications of
this finding for the future of the United States' export control strategy.

摘要：美國對半導體的出口管制措施眾所周知是具有滲透性的，而中華人民共和國（PRC）則持續透過外洩的晶片打造最先進的人工智慧（AI）模型。本文提出第一個具體的公開證據，說明中國領先的 AI 實驗室如何規避和繞過美國的出口管制。我們探討中國公司，尤其是騰訊，如何不僅使用美國出口管制限制的晶片，還透過使用軟體和建模技術，最大化功能較弱的硬體，來找出規避這些法規的方法。具體來說，我們認為騰訊能夠使用非出口管制的 NVIDIA H20s 來為其 Hunyuan-Large 模型供電，這說明了機器學習效率的更廣泛提升，而這已侵蝕了美國最初透過其現有出口管制措施所建立的護城河。最後，我們探討這一發現對美國未來出口管制策略的影響。

##### **Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions**
2411.14405v1 by Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang

Currently OpenAI o1 has sparked a surge of interest in the study of large
reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on
disciplines with standard answers, such as mathematics, physics, and coding --
which are well-suited for reinforcement learning (RL) -- but also places
greater emphasis on open-ended resolutions. We aim to address the question:
"Can the o1 model effectively generalize to broader domains where clear
standards are absent and rewards are challenging to quantify?" Marco-o1 is
powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),
reflection mechanisms, and innovative reasoning strategies -- optimized for
complex real-world problem-solving tasks.

摘要：目前，OpenAI o1 在大型推理模型 (LRM) 研究領域引起了極大的興趣。馬可 o1 不僅專注於具有標準答案的學科，例如數學、物理和編碼，這些學科非常適合強化學習 (RL)，還更加強調開放式的解決方案。我們的目標是解決這個問題：「o1 模型是否可以有效地概括到缺乏明確標準且獎勵難以量化的更廣泛領域？」馬可 o1 由思想鏈 (CoT) 微調、蒙地卡羅樹搜尋 (MCTS)、反思機制和創新推理策略提供支援，這些策略經過優化，可用於解決複雜的現實世界問題。

##### **Resolving Multiple-Dynamic Model Uncertainty in Hypothesis-Driven Belief-MDPs**
2411.14404v1 by Ofer Dagan, Tyler Becker, Zachary N. Sunberg

When human operators of cyber-physical systems encounter surprising behavior,
they often consider multiple hypotheses that might explain it. In some cases,
taking information-gathering actions such as additional measurements or control
inputs given to the system can help resolve uncertainty and determine the most
accurate hypothesis. The task of optimizing these actions can be formulated as
a belief-space Markov decision process that we call a hypothesis-driven belief
MDP. Unfortunately, this problem suffers from the curse of history similar to a
partially observable Markov decision process (POMDP). To plan in continuous
domains, an agent needs to reason over countlessly many possible
action-observation histories, each resulting in a different belief over the
unknown state. The problem is exacerbated in the hypothesis-driven context
because each action-observation pair spawns a different belief for each
hypothesis, leading to additional branching. This paper considers the case in
which each hypothesis corresponds to a different dynamic model in an underlying
POMDP. We present a new belief MDP formulation that: (i) enables reasoning over
multiple hypotheses, (ii) balances the goals of determining the (most likely)
correct hypothesis and performing well in the underlying POMDP, and (iii) can
be solved with sparse tree search.

摘要：當網路物理系統的人類操作員遇到令人驚訝的行為時，他們通常會考慮多種假設來解釋它。在某些情況下，採取資訊收集動作（例如額外的量測或給予系統的控制輸入）有助於解決不確定性並確定最準確的假設。最佳化這些動作的任務可以表述為一個信念空間馬可夫決策過程，我們稱之為假設驅動信念 MDP。不幸的是，這個問題與部分可觀察馬可夫決策過程 (POMDP) 類似，會受到歷史詛咒的影響。要在連續領域中規劃，代理需要對無數可能的動作觀察歷史進行推理，每個歷史都會導致對未知狀態的不同信念。這個問題在假設驅動的背景下會更加惡化，因為每對動作觀察會為每個假設產生不同的信念，導致額外的分支。本文考慮了每個假設在基礎 POMDP 中對應於不同動態模型的情況。我們提出一個新的信念 MDP 公式，它：(i) 能夠對多個假設進行推理，(ii) 平衡確定（最可能）正確假設和在基礎 POMDP 中表現良好的目標，以及 (iii) 可以透過稀疏樹搜尋來解決。

##### **Landing Trajectory Prediction for UAS Based on Generative Adversarial Network**
2411.14403v1 by Jun Xiang, Drake Essick, Luiz Gonzalez Bautista, Junfei Xie, Jun Chen

Models for trajectory prediction are an essential component of many advanced
air mobility studies. These models help aircraft detect conflict and plan
avoidance maneuvers, which is especially important in Unmanned Aircraft systems
(UAS) landing management due to the congested airspace near vertiports. In this
paper, we propose a landing trajectory prediction model for UAS based on
Generative Adversarial Network (GAN). The GAN is a prestigious neural network
that has been developed for many years. In previous research, GAN has achieved
many state-of-the-art results in many generation tasks. The GAN consists of one
neural network generator and a neural network discriminator. Because of the
learning capacity of the neural networks, the generator is capable to
understand the features of the sample trajectory. The generator takes the
previous trajectory as input and outputs some random status of a flight.
According to the results of the experiences, the proposed model can output more
accurate predictions than the baseline method(GMR) in various datasets. To
evaluate the proposed model, we also create a real UAV landing dataset that
includes more than 2600 trajectories of drone control manually by real pilots.

摘要：航跡預測模型是許多先進空中流動性研究中不可或缺的組成部分。這些模型有助於飛機偵測衝突並規劃避讓動作，這在無人機系統（UAS）著陸管理中尤其重要，這是因為垂直起降機場附近的空域壅塞。在本文中，我們提出一個基於生成對抗網路（GAN）的 UAS 著陸航跡預測模型。GAN 是一個久負盛名的神經網路，已經發展多年。在先前的研究中，GAN 在許多生成任務中取得許多最先進的成果。GAN 包含一個神經網路產生器和一個神經網路判別器。由於神經網路的學習能力，產生器能夠了解樣本航跡的特徵。產生器將先前的航跡作為輸入，並輸出飛行的某些隨機狀態。根據經驗結果，所提出的模型可以在各種資料集中輸出比基線方法（GMR）更準確的預測。為了評估所提出的模型，我們還建立了一個真實的無人機著陸資料集，其中包含超過 2600 條由真實飛行員手動控制的無人機航跡。

##### **Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings**
2411.14398v1 by Aaron Zheng, Mansi Rana, Andreas Stolcke

With the recent proliferation of large language models (LLMs), enterprises
have been able to rapidly develop proof-of-concepts and prototypes. As a
result, there is a growing need to implement robust guardrails that monitor,
quantize and control an LLM's behavior, ensuring that the use is reliable,
safe, accurate and also aligned with the users' expectations. Previous
approaches for filtering out inappropriate user prompts or system outputs, such
as LlamaGuard and OpenAI's MOD API, have achieved significant success by
fine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails
introduces increased latency and higher maintenance costs, which may not be
practical or scalable for cost-efficient deployments. We take a different
approach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.
This method reduces the model size from LlamaGuard's 7 billion parameters to
approximately 67 million, while maintaining comparable performance on the AEGIS
safety benchmark.

摘要：隨著大型語言模型 (LLM) 的快速擴散，企業已能快速開發概念驗證和原型。因此，越來越需要實施強固的防護措施，以監控、量化和控制 LLM 的行為，確保使用可靠、安全、準確，且符合使用者的預期。先前用於過濾不適當使用者提示或系統輸出的方法，例如 LlamaGuard 和 OpenAI 的 MOD API，已透過微調現有的 LLM 而獲得顯著的成功。然而，使用微調的 LLM 作為防護措施會增加延遲和更高的維護成本，這對於經濟高效的部署而言可能不切實際或無法擴展。我們採用不同的方法，專注於微調輕量級架構：Sentence-BERT。此方法將模型大小從 LlamaGuard 的 70 億個參數減少到約 6,700 萬個，同時在 AEGIS 安全基準上維持相當的效能。

##### **POS-tagging to highlight the skeletal structure of sentences**
2411.14393v1 by Grigorii Churakov

This study presents the development of a part-of-speech (POS) tagging model
to extract the skeletal structure of sentences using transfer learning with the
BERT architecture for token classification. The model, fine-tuned on Russian
text, demonstrating its effectiveness. The approach offers potential
applications in enhancing natural language processing tasks, such as improving
machine translation.
  Keywords: part of speech tagging, morphological analysis, natural language
processing, BERT.

摘要：本研究提出了一個詞性標記模型的開發
使用 BERT 架構進行標記分類，以轉移學習來提取句子的骨架結構。該模型針對俄語文本進行微調，證明了其有效性。此方法提供了增強自然語言處理任務的潛在應用，例如改進機器翻譯。
關鍵字：詞性標記、形態分析、自然語言處理、BERT。

##### **Using Formal Models, Safety Shields and Certified Control to Validate AI-Based Train Systems**
2411.14374v1 by Jan Gruteser, Jan Roßbach, Fabian Vu, Michael Leuschel

The certification of autonomous systems is an important concern in science
and industry. The KI-LOK project explores new methods for certifying and safely
integrating AI components into autonomous trains. We pursued a two-layered
approach: (1) ensuring the safety of the steering system by formal analysis
using the B method, and (2) improving the reliability of the perception system
with a runtime certificate checker. This work links both strategies within a
demonstrator that runs simulations on the formal model, controlled by the real
AI output and the real certificate checker. The demonstrator is integrated into
the validation tool ProB. This enables runtime monitoring, runtime
verification, and statistical validation of formal safety properties using a
formal B model. Consequently, one can detect and analyse potential
vulnerabilities and weaknesses of the AI and the certificate checker. We apply
these techniques to a signal detection case study and present our findings.

摘要：自主系統的認證是科學和產業中的重要課題。KI-LOK 計畫探索用於認證和安全地將 AI 元件整合到自動駕駛火車中的新方法。我們採用了雙層方法：(1) 使用 B 方法透過形式化分析確保轉向系統的安全，以及 (2) 使用執行時期證書檢查器來提升感知系統的可靠性。此項工作在一個示範程式中連結了這兩種策略，在形式化模型上執行模擬，由實際的 AI 輸出和實際的證書檢查器控制。此示範程式整合到驗證工具 ProB 中。這能使用形式化 B 模型進行執行時期監控、執行時期驗證和形式化安全屬性的統計驗證。因此，可以偵測和分析 AI 和證書檢查器的潛在漏洞和弱點。我們將這些技術應用於訊號偵測案例研究，並提出我們的發現。

##### **Synthesising Robust Controllers for Robot Collectives with Recurrent Tasks: A Case Study**
2411.14371v1 by Till Schnittka, Mario Gleirscher

When designing correct-by-construction controllers for autonomous
collectives, three key challenges are the task specification, the modelling,
and its use at practical scale. In this paper, we focus on a simple yet useful
abstraction for high-level controller synthesis for robot collectives with
optimisation goals (e.g., maximum cleanliness, minimum energy consumption) and
recurrence (e.g., re-establish contamination and charge thresholds) and safety
(e.g., avoid full discharge, mutually exclusive room occupation) constraints.
Due to technical limitations (related to scalability and using constraints in
the synthesis), we simplify our graph-based setting from a stochastic
two-player game into a single-player game on a partially observable Markov
decision process (POMDP). Robustness against environmental uncertainty is
encoded via partial observability. Linear-time correctness properties are
verified separately after synthesising the POMDP strategy. We contribute
at-scale guidance on POMDP modelling and controller synthesis for tasked robot
collectives exemplified by the scenario of battery-driven robots responsible
for cleaning public buildings with utilisation constraints.

摘要：在為自主集體設計正確的建構控制器時，有三個主要的挑戰：任務規範、建模，以及在實際規模下的使用。在本文中，我們專注於一個簡單但有用的抽象化，用於具有最佳化目標（例如，最大清潔度、最低能源消耗）和遞迴（例如，重新建立污染和充電閾值）以及安全（例如，避免完全放電、相互排斥的房間佔用）約束的機器人集體的高階控制器合成。由於技術限制（與擴充性和在合成中使用約束有關），我們將基於圖表的設定從隨機雙人遊戲簡化為部分可觀察馬可夫決策過程 (POMDP) 中的單人遊戲。對環境不確定性的魯棒性通過部分可觀察性編碼。線性時間正確性屬性在合成 POMDP 策略後單獨驗證。我們為任務機器人集體的 POMDP 建模和控制器合成提供了大規模指導，這些集體以負責在利用約束下清潔公共建築的電池驅動機器人為例。

##### **Contrasting local and global modeling with machine learning and satellite data: A case study estimating tree canopy height in African savannas**
2411.14354v1 by Esther Rolf, Lucia Gordon, Milind Tambe, Andrew Davies

While advances in machine learning with satellite imagery (SatML) are
facilitating environmental monitoring at a global scale, developing SatML
models that are accurate and useful for local regions remains critical to
understanding and acting on an ever-changing planet. As increasing attention
and resources are being devoted to training SatML models with global data, it
is important to understand when improvements in global models will make it
easier to train or fine-tune models that are accurate in specific regions. To
explore this question, we contrast local and global training paradigms for
SatML through a case study of tree canopy height (TCH) mapping in the Karingani
Game Reserve, Mozambique. We find that recent advances in global TCH mapping do
not necessarily translate to better local modeling abilities in our study
region. Specifically, small models trained only with locally-collected data
outperform published global TCH maps, and even outperform globally pretrained
models that we fine-tune using local data. Analyzing these results further, we
identify specific points of conflict and synergy between local and global
modeling paradigms that can inform future research toward aligning local and
global performance objectives in geospatial machine learning.

摘要：雖然機器學習與衛星影像（SatML）的進展促進了全球環境監測，但開發出對局部地區準確且有用的 SatML 模型對於理解和應對不斷變化的地球仍然至關重要。隨著越來越多的關注和資源投入到使用全球數據訓練 SatML 模型，了解何時全球模型的改進將使訓練或微調特定區域準確的模型變得更容易，這一點非常重要。為了探討這個問題，我們通過莫三比克卡林加尼野生動物保護區的樹冠高度（TCH）製圖案例研究，對比了 SatML 的局部和全球訓練範例。我們發現，全球 TCH 製圖的最新進展並非一定能轉化為我們研究區域中更好的局部建模能力。具體來說，僅使用當地收集的數據訓練的小模型優於已發布的全球 TCH 地圖，甚至優於我們使用當地數據微調的全球預訓練模型。進一步分析這些結果，我們找出了局部和全球建模範例之間衝突和協同作用的具體點，這些點可以為未來的研究提供信息，以調整地理空間機器學習中的局部和全球性能目標。

