{"2412.13594": {"publish_time": "2024-12-18", "title": "Generalizable Sensor-Based Activity Recognition via Categorical Concept Invariant Learning", "paper_summary": "Human Activity Recognition (HAR) aims to recognize activities by training\nmodels on massive sensor data. In real-world deployment, a crucial aspect of\nHAR that has been largely overlooked is that the test sets may have different\ndistributions from training sets due to inter-subject variability including\nage, gender, behavioral habits, etc., which leads to poor generalization\nperformance. One promising solution is to learn domain-invariant\nrepresentations to enable a model to generalize on an unseen distribution.\nHowever, most existing methods only consider the feature-invariance of the\npenultimate layer for domain-invariant learning, which leads to suboptimal\nresults. In this paper, we propose a Categorical Concept Invariant Learning\n(CCIL) framework for generalizable activity recognition, which introduces a\nconcept matrix to regularize the model in the training stage by simultaneously\nconcentrating on feature-invariance and logit-invariance. Our key idea is that\nthe concept matrix for samples belonging to the same activity category should\nbe similar. Extensive experiments on four public HAR benchmarks demonstrate\nthat our CCIL substantially outperforms the state-of-the-art approaches under\ncross-person, cross-dataset, cross-position, and one-person-to-another\nsettings.", "paper_summary_zh": "\u4eba\u985e\u6d3b\u52d5\u8b58\u5225 (HAR) \u65e8\u5728\u900f\u904e\u5728\u5927\u91cf\u611f\u6e2c\u5668\u8cc7\u6599\u4e0a\u8a13\u7df4\u6a21\u578b\u4f86\u8fa8\u8b58\u6d3b\u52d5\u3002\u5728\u5be6\u969b\u90e8\u7f72\u4e2d\uff0cHAR \u7684\u4e00\u500b\u95dc\u9375\u9762\u5411\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u7565\u4e86\uff0c\u90a3\u5c31\u662f\u6e2c\u8a66\u96c6\u53ef\u80fd\u8207\u8a13\u7df4\u96c6\u6709\u4e0d\u540c\u7684\u5206\u4f48\uff0c\u9019\u662f\u7531\u65bc\u5305\u62ec\u5e74\u9f61\u3001\u6027\u5225\u3001\u884c\u70ba\u7fd2\u6163\u7b49\u5728\u5167\u7684\u500b\u9ad4\u9593\u8b8a\u7570\u6240\u5c0e\u81f4\u7684\uff0c\u9019\u6703\u9020\u6210\u4e0d\u4f73\u7684\u6982\u5316\u6548\u80fd\u3002\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u662f\u5b78\u7fd2\u9818\u57df\u4e0d\u8b8a\u8868\u793a\uff0c\u4ee5\u4f7f\u6a21\u578b\u80fd\u5920\u5728\u672a\u898b\u904e\u7684\u5206\u4f48\u4e0a\u9032\u884c\u6982\u5316\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u5927\u90e8\u5206\u65b9\u6cd5\u53ea\u8003\u616e\u5012\u6578\u7b2c\u4e8c\u5c64\u7684\u7279\u6027\u4e0d\u8b8a\u6027\u4f86\u9032\u884c\u9818\u57df\u4e0d\u8b8a\u5b78\u7fd2\uff0c\u9019\u6703\u5c0e\u81f4\u6b21\u4f73\u7d50\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u985e\u5225\u6982\u5ff5\u4e0d\u8b8a\u5b78\u7fd2 (CCIL) \u67b6\u69cb\uff0c\u7528\u65bc\u53ef\u6982\u5316\u7684\u6d3b\u52d5\u8b58\u5225\uff0c\u5b83\u5f15\u5165\u4e00\u500b\u6982\u5ff5\u77e9\u9663\uff0c\u4ee5\u5728\u8a13\u7df4\u968e\u6bb5\u900f\u904e\u540c\u6642\u5c08\u6ce8\u65bc\u7279\u6027\u4e0d\u8b8a\u6027\u548c logit \u4e0d\u8b8a\u6027\u4f86\u898f\u7bc4\u6a21\u578b\u3002\u6211\u5011\u7684\u95dc\u9375\u60f3\u6cd5\u662f\uff0c\u5c6c\u65bc\u76f8\u540c\u6d3b\u52d5\u985e\u5225\u7684\u6a23\u672c\u7684\u6982\u5ff5\u77e9\u9663\u61c9\u8a72\u662f\u76f8\u4f3c\u7684\u3002\u5728\u56db\u500b\u516c\u958b HAR \u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684 CCIL \u5728\u8de8\u4eba\u3001\u8de8\u8cc7\u6599\u96c6\u3001\u8de8\u4f4d\u7f6e\u548c\u4e00\u4eba\u5c0d\u53e6\u4e00\u4eba\u8a2d\u5b9a\u4e0b\uff0c\u5927\u5e45\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002", "author": "Di Xiong et.al.", "authors": "Di Xiong, Shuoyuan Wang, Lei Zhang, Wenbo Huang, Chaolei Han", "id": "2412.13594v1", "paper_url": "http://arxiv.org/abs/2412.13594v1", "repo": "null"}}