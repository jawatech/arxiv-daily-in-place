{"2405.14333": {"publish_time": "2024-05-23", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "paper_summary": "Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.", "paper_summary_zh": "<paragraph>\u50cf Lean \u9019\u6a23\u7684\u8b49\u660e\u52a9\u7406\u5df2\u7d93\u5fb9\u5e95\u6539\u8b8a\u6578\u5b78\u8b49\u660e\u9a57\u8b49\uff0c\u78ba\u4fdd\u9ad8\u5ea6\u6e96\u78ba\u6027\u548c\u53ef\u9760\u6027\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6578\u5b78\u63a8\u7406\u65b9\u9762\u8868\u73fe\u51fa\u6f5b\u529b\uff0c\u4f46\u5b83\u5011\u5728\u5f62\u5f0f\u5316\u5b9a\u7406\u8b49\u660e\u65b9\u9762\u7684\u9032\u5c55\u53d7\u5230\u8a13\u7df4\u8cc7\u6599\u4e0d\u8db3\u7684\u963b\u7919\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\u4f86\u7522\u751f\u5927\u91cf\u7684 Lean 4 \u8b49\u660e\u8cc7\u6599\uff0c\u9019\u4e9b\u8cc7\u6599\u4f86\u81ea\u9ad8\u4e2d\u548c\u5927\u5b78\u7a0b\u5ea6\u7684\u6578\u5b78\u7af6\u8cfd\u984c\u76ee\u3002\u6b64\u65b9\u6cd5\u5305\u62ec\u5c07\u81ea\u7136\u8a9e\u8a00\u984c\u76ee\u8f49\u63db\u70ba\u5f62\u5f0f\u5316\u9673\u8ff0\u3001\u7be9\u9078\u51fa\u4f4e\u54c1\u8cea\u9673\u8ff0\uff0c\u4ee5\u53ca\u7522\u751f\u8b49\u660e\u4ee5\u5efa\u7acb\u5408\u6210\u8cc7\u6599\u3002\u5728\u9019\u500b\u5305\u542b 800 \u842c\u500b\u5e36\u6709\u8b49\u660e\u4e4b\u5f62\u5f0f\u5316\u9673\u8ff0\u7684\u5408\u6210\u8cc7\u6599\u96c6\u4e0a\u5fae\u8abf DeepSeekMath 7B \u6a21\u578b\u5f8c\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 Lean 4 miniF2F \u6e2c\u8a66\u4e2d\u4ee5 64 \u500b\u6a23\u672c\u7372\u5f97 46.3% \u7684\u5b8c\u6574\u8b49\u660e\u7522\u751f\u6e96\u78ba\u5ea6\uff0c\u7d2f\u8a08 52%\uff0c\u8d85\u8d8a\u4e86\u5728 64 \u500b\u6a23\u672c\u4e2d\u6e96\u78ba\u5ea6\u70ba 23.0% \u7684\u57fa\u6e96 GPT-4\uff0c\u4ee5\u53ca\u6e96\u78ba\u5ea6\u70ba 41.0% \u7684\u6a39\u72c0\u641c\u5c0b\u5f37\u5316\u5b78\u7fd2\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 Lean 4 Formalized International Mathematical Olympiad (FIMO) \u57fa\u6e96\u6e2c\u8a66\u4e2d\u6210\u529f\u8b49\u660e\u4e86 148 \u500b\u984c\u76ee\u4e2d\u7684 5 \u500b\uff0c\u800c GPT-4 \u5247\u672a\u80fd\u8b49\u660e\u4efb\u4f55\u984c\u76ee\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86\u5229\u7528\u5927\u898f\u6a21\u5408\u6210\u8cc7\u6599\u4f86\u589e\u5f37 LLM \u4e2d\u5b9a\u7406\u8b49\u660e\u80fd\u529b\u7684\u6f5b\u529b\u3002\u5408\u6210\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u90fd\u5c07\u63d0\u4f9b\u51fa\u4f86\uff0c\u4ee5\u5229\u65bc\u9019\u500b\u6709\u524d\u9014\u7684\u9818\u57df\u7684\u9032\u4e00\u6b65\u7814\u7a76\u3002</paragraph>", "author": "Huajian Xin et.al.", "authors": "Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang", "id": "2405.14333v1", "paper_url": "http://arxiv.org/abs/2405.14333v1", "repo": "null"}}