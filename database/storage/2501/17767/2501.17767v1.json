{"2501.17767": {"publish_time": "2025-01-29", "title": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs", "paper_summary": "Answering questions that require reasoning and aggregation across both\nstructured (tables) and unstructured (raw text) data sources presents\nsignificant challenges. Current methods rely on fine-tuning and high-quality,\nhuman-curated data, which is difficult to obtain. Recent advances in Large\nLanguage Models (LLMs) have shown promising results for multi-hop question\nanswering (QA) over single-source text data in a zero-shot setting, yet\nexploration into multi-source Table-Text QA remains limited. In this paper, we\npresent a novel Hybrid Graph-based approach for Table-Text QA that leverages\nLLMs without fine-tuning. Our method constructs a unified Hybrid Graph from\ntextual and tabular data, pruning information based on the input question to\nprovide the LLM with relevant context concisely. We evaluate our approach on\nthe challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,\nincluding GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot\nperformance on both datasets, improving Exact Match scores by up to 10% on\nHybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up\nto 53% compared to the original context.", "paper_summary_zh": "\u56de\u7b54\u9700\u8981\u5c0d\u7d50\u69cb\u5316\uff08\u8868\u683c\uff09\u548c\u975e\u7d50\u69cb\u5316\uff08\u539f\u59cb\u6587\u5b57\uff09\u8cc7\u6599\u4f86\u6e90\u9032\u884c\u63a8\u7406\u548c\u5f59\u7e3d\u7684\u554f\u984c\u6703\u5e36\u4f86\u91cd\u5927\u6311\u6230\u3002\u76ee\u524d\u7684\u8fa6\u6cd5\u4ef0\u8cf4\u5fae\u8abf\u548c\u9ad8\u54c1\u8cea\u3001\u4eba\u5de5\u6574\u7406\u7684\u8cc7\u6599\uff0c\u800c\u9019\u5f88\u96e3\u53d6\u5f97\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u9032\u5c55\u5df2\u91dd\u5c0d\u96f6\u6b21\u5b78\u7fd2\u8a2d\u5b9a\u7684\u55ae\u4e00\u4f86\u6e90\u6587\u5b57\u8cc7\u6599\u591a\u8df3\u554f\u984c\u56de\u7b54\uff08QA\uff09\u5c55\u73fe\u51fa\u6709\u5e0c\u671b\u7684\u7d50\u679c\uff0c\u4f46\u5c0d\u591a\u4f86\u6e90\u8868\u683c\u6587\u5b57 QA \u7684\u63a2\u8a0e\u4ecd\u7136\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u57fa\u65bc\u6df7\u5408\u5716\u8868\u7684\u8868\u683c\u6587\u5b57 QA \u65b9\u6cd5\uff0c\u5b83\u5229\u7528 LLM \u800c\u7121\u9700\u5fae\u8abf\u3002\u6211\u5011\u7684\u8fa6\u6cd5\u5f9e\u6587\u5b57\u548c\u8868\u683c\u8cc7\u6599\u5efa\u69cb\u4e00\u500b\u7d71\u4e00\u7684\u6df7\u5408\u5716\u8868\uff0c\u6839\u64da\u8f38\u5165\u554f\u984c\u4fee\u526a\u8cc7\u8a0a\uff0c\u4ee5\u7c21\u6f54\u5730\u70ba LLM \u63d0\u4f9b\u76f8\u95dc\u8108\u7d61\u3002\u6211\u5011\u4f7f\u7528\u6700\u5148\u9032\u7684 LLM\uff0c\u5305\u62ec GPT-3.5\u3001GPT-4 \u548c LLaMA-3\uff0c\u91dd\u5c0d\u5177\u6709\u6311\u6230\u6027\u7684 Hybrid-QA \u548c OTT-QA \u8cc7\u6599\u96c6\u8a55\u4f30\u6211\u5011\u7684\u8fa6\u6cd5\u3002\u6211\u5011\u7684\u8fa6\u6cd5\u5728\u5169\u500b\u8cc7\u6599\u96c6\u4e0a\u90fd\u9054\u5230\u4e86\u6700\u4f73\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\uff0c\u5728 Hybrid-QA \u4e0a\u5c07\u5b8c\u5168\u6bd4\u5c0d\u5206\u6578\u63d0\u9ad8\u4e86 10%\uff0c\u5728 OTT-QA \u4e0a\u5c07\u5b8c\u5168\u6bd4\u5c0d\u5206\u6578\u63d0\u9ad8\u4e86 5.4%\u3002\u6b64\u5916\uff0c\u8207\u539f\u59cb\u8108\u7d61\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u8fa6\u6cd5\u5c07\u7b26\u865f\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 53%\u3002", "author": "Ankush Agarwal et.al.", "authors": "Ankush Agarwal, Ganesh S, Chaitanya Devaguptapu", "id": "2501.17767v1", "paper_url": "http://arxiv.org/abs/2501.17767v1", "repo": "null"}}