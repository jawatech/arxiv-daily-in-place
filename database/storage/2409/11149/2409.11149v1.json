{"2409.11149": {"publish_time": "2024-09-17", "title": "SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration", "paper_summary": "The development of unbiased large language models is widely recognized as\ncrucial, yet existing benchmarks fall short in detecting biases due to limited\nscope, contamination, and lack of a fairness baseline. SAGED(-Bias) is the\nfirst holistic benchmarking pipeline to address these problems. The pipeline\nencompasses five core stages: scraping materials, assembling benchmarks,\ngenerating responses, extracting numeric features, and diagnosing with\ndisparity metrics. SAGED includes metrics for max disparity, such as impact\nratio, and bias concentration, such as Max Z-scores. Noticing that assessment\ntool bias and contextual bias in prompts can distort evaluation, SAGED\nimplements counterfactual branching and baseline calibration for mitigation.\nFor demonstration, we use SAGED on G20 Countries with popular 8b-level models\nincluding Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we\nfind that while Mistral and Qwen2 show lower max disparity and higher bias\nconcentration than Gemma2 and Llama3.1, all models are notably biased against\ncountries like Russia and (except for Qwen2) China. With further experiments to\nhave models role-playing U.S. (vice-/former-) presidents, we see bias amplifies\nand shifts in heterogeneous directions. Moreover, we see Qwen2 and Mistral not\nengage in role-playing, while Llama3.1 and Gemma2 role-play Trump notably more\nintensively than Biden and Harris, indicating role-playing performance bias in\nthese models.", "paper_summary_zh": "<paragraph>\u7121\u504f\u898b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u767c\u5c55\u88ab\u5ee3\u6cdb\u8a8d\u70ba\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u73fe\u6709\u7684\u57fa\u6e96\u5728\u6aa2\u6e2c\u504f\u898b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u539f\u56e0\u5728\u65bc\u7bc4\u570d\u6709\u9650\u3001\u6c59\u67d3\u4ee5\u53ca\u7f3a\u4e4f\u516c\u5e73\u57fa\u6e96\u3002SAGED(-Bias) \u662f\u7b2c\u4e00\u500b\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u7684\u6574\u9ad4\u57fa\u6e96\u7ba1\u9053\u3002\u8a72\u7ba1\u9053\u5305\u542b\u4e94\u500b\u6838\u5fc3\u968e\u6bb5\uff1a\u64f7\u53d6\u7d20\u6750\u3001\u7d44\u88dd\u57fa\u6e96\u3001\u7522\u751f\u56de\u61c9\u3001\u8403\u53d6\u6578\u503c\u7279\u5fb5\uff0c\u4ee5\u53ca\u4f7f\u7528\u5dee\u7570\u6307\u6a19\u8a3a\u65b7\u3002SAGED \u5305\u542b\u6700\u5927\u5dee\u7570\u6307\u6a19\uff0c\u4f8b\u5982\u5f71\u97ff\u6bd4\u7387\uff0c\u4ee5\u53ca\u504f\u898b\u96c6\u4e2d\u5ea6\u6307\u6a19\uff0c\u4f8b\u5982\u6700\u5927 Z \u5206\u6578\u3002SAGED \u89c0\u5bdf\u5230\u8a55\u91cf\u5de5\u5177\u504f\u898b\u548c\u63d0\u793a\u4e2d\u7684\u8108\u7d61\u504f\u898b\u6703\u626d\u66f2\u8a55\u91cf\uff0c\u56e0\u6b64\u5be6\u4f5c\u53cd\u4e8b\u5be6\u5206\u652f\u548c\u57fa\u6e96\u6821\u6b63\u4ee5\u9032\u884c\u7de9\u89e3\u3002\u70ba\u4e86\u793a\u7bc4\uff0c\u6211\u5011\u5728 G20 \u570b\u5bb6\u4f7f\u7528 SAGED\uff0c\u5176\u4e2d\u5305\u62ec\u71b1\u9580\u7684 8b \u7d1a\u5225\u6a21\u578b\uff0c\u4f8b\u5982 Gemma2\u3001Llama3.1\u3001Mistral \u548c Qwen2\u3002\u900f\u904e\u60c5\u7dd2\u5206\u6790\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5118\u7ba1 Mistral \u548c Qwen2 \u986f\u793a\u51fa\u6bd4 Gemma2 \u548c Llama3.1 \u66f4\u4f4e\u7684\u6700\u5927\u5dee\u7570\u548c\u66f4\u9ad8\u7684\u504f\u898b\u96c6\u4e2d\u5ea6\uff0c\u4f46\u6240\u6709\u6a21\u578b\u90fd\u660e\u986f\u504f\u5411\u65bc\u4fc4\u7f85\u65af\u7b49\u570b\u5bb6\uff0c\u800c Qwen2 \u9664\u5916\uff0c\u5247\u504f\u5411\u65bc\u4e2d\u570b\u3002\u900f\u904e\u9032\u4e00\u6b65\u7684\u5be6\u9a57\u8b93\u6a21\u578b\u626e\u6f14\u7f8e\u570b\uff08\u73fe\u4efb/\u524d\u4efb\uff09\u7e3d\u7d71\uff0c\u6211\u5011\u770b\u5230\u504f\u898b\u6703\u5728\u7570\u8cea\u65b9\u5411\u4e0a\u64f4\u5927\u548c\u8f49\u79fb\u3002\u6b64\u5916\uff0c\u6211\u5011\u770b\u5230 Qwen2 \u548c Mistral \u6c92\u6709\u53c3\u8207\u89d2\u8272\u626e\u6f14\uff0c\u800c Llama3.1 \u548c Gemma2 \u626e\u6f14\u5ddd\u666e\u7684\u89d2\u8272\u660e\u986f\u6bd4\u62dc\u767b\u548c\u8cc0\u9326\u9e97\u66f4\u70ba\u6df1\u5165\uff0c\u9019\u8868\u793a\u9019\u4e9b\u6a21\u578b\u5b58\u5728\u89d2\u8272\u626e\u6f14\u8868\u73fe\u504f\u898b\u3002</paragraph>", "author": "Xin Guan et.al.", "authors": "Xin Guan, Nathaniel Demchak, Saloni Gupta, Ze Wang, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu", "id": "2409.11149v1", "paper_url": "http://arxiv.org/abs/2409.11149v1", "repo": "null"}}