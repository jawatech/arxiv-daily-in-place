{"2406.02350": {"publish_time": "2024-06-04", "title": "LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing", "paper_summary": "Large language models (LLMs) have shown amazing capabilities in knowledge\nmemorization and present. However, when it comes to domain-specific knowledge\nand downstream tasks like medical, general LLMs are often unable to give\nprecise answers. In addition, when people want LLMs to answer classification\nquestions, they usually go through instruction tuning first, however, LLMs do\nnot always give a direct index of the categorization after instruction tuning.\nIn this paper, we proposed LlamaCare, a fine-tuned medical language model, and\nExtended Classification Integration(ECI), a module to handle classification\nproblems of LLMs. Our contributions are : (i) We fine-tuned a large language\nmodel of medical knowledge with very low carbon emissions and achieved similar\nperformance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant\ncategorical answers and improved the performance of LLMs by proposing a new\nmodule called Extended Classification Integration. (iii) We released our\nprocessed data for one-shot and few-shot training for some benchmarks such as\nPubMedQA and USMLE 1-3 step. Our method achieves a close effect with the\nstate-of-the-art model in benchmarks while costing lower GPU resources compared\nto LLMs with the same quantity of parameters. Our models, codes, and datasets\ncan be found in https://github.com/Stephen-SMJ/LLamaCare", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u77e5\u8b58\u8a18\u61b6\u548c\u5448\u73fe\u65b9\u9762\u5c55\u73fe\u51fa\u9a5a\u4eba\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7576\u6d89\u53ca\u5230\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\u548c\u4e0b\u6e38\u4efb\u52d9\uff08\u4f8b\u5982\u91ab\u7642\uff09\u6642\uff0c\u4e00\u822c\u7684 LLM \u901a\u5e38\u7121\u6cd5\u7d66\u51fa\u7cbe\u78ba\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u7576\u4eba\u5011\u5e0c\u671b LLM \u56de\u7b54\u5206\u985e\u554f\u984c\u6642\uff0c\u4ed6\u5011\u901a\u5e38\u6703\u5148\u9032\u884c\u6307\u4ee4\u8abf\u6574\uff0c\u7136\u800c\uff0cLLM \u5728\u6307\u4ee4\u8abf\u6574\u5f8c\u4e26\u4e0d\u7e3d\u662f\u7d66\u51fa\u5206\u985e\u7684\u76f4\u63a5\u7d22\u5f15\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LlamaCare\uff0c\u4e00\u7a2e\u5fae\u8abf\u5f8c\u7684\u91ab\u5b78\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u64f4\u5c55\u5206\u985e\u6574\u5408 (ECI)\uff0c\u4e00\u500b\u8655\u7406 LLM \u5206\u985e\u554f\u984c\u7684\u6a21\u7d44\u3002\u6211\u5011\u7684\u8ca2\u737b\u5305\u62ec\uff1a(i) \u6211\u5011\u5fae\u8abf\u4e86\u4e00\u500b\u5177\u6709\u6975\u4f4e\u78b3\u6392\u653e\u91cf\u7684\u91ab\u5b78\u77e5\u8b58\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u4f7f\u7528 24G GPU \u9054\u5230\u4e86\u8207 ChatGPT \u76f8\u4f3c\u7684\u6548\u80fd\u3002(ii) \u6211\u5011\u89e3\u6c7a\u4e86\u5197\u9918\u5206\u985e\u7b54\u6848\u7684\u554f\u984c\uff0c\u4e26\u900f\u904e\u63d0\u51fa\u4e00\u500b\u540d\u70ba\u64f4\u5c55\u5206\u985e\u6574\u5408\u7684\u65b0\u6a21\u7d44\u4f86\u6539\u5584 LLM \u7684\u6548\u80fd\u3002(iii) \u6211\u5011\u91cb\u51fa\u4e86\u6211\u5011\u8655\u7406\u904e\u7684\u8cc7\u6599\uff0c\u7528\u65bc PubMedQA \u548c USMLE 1-3 \u6b65\u9a5f\u7b49\u4e00\u4e9b\u57fa\u6e96\u7684\u55ae\u6b21\u548c\u5c11\u6b21\u8a13\u7df4\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u57fa\u6e96\u4e2d\u9054\u5230\u4e86\u8207\u6700\u5148\u9032\u6a21\u578b\u63a5\u8fd1\u7684\u6548\u679c\uff0c\u540c\u6642\u8207\u5177\u6709\u76f8\u540c\u53c3\u6578\u6578\u91cf\u7684 LLM \u76f8\u6bd4\uff0c\u6210\u672c\u66f4\u4f4e\u3002\u6211\u5011\u7684\u6a21\u578b\u3001\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u53ef\u4ee5\u5728 https://github.com/Stephen-SMJ/LLamaCare \u4e2d\u627e\u5230", "author": "Maojun Sun et.al.", "authors": "Maojun Sun", "id": "2406.02350v1", "paper_url": "http://arxiv.org/abs/2406.02350v1", "repo": "https://github.com/stephen-smj/llamacare"}}