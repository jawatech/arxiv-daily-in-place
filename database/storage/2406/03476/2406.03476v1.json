{"2406.03476": {"publish_time": "2024-06-05", "title": "Does your data spark joy? Performance gains from domain upsampling at the end of training", "paper_summary": "Pretraining datasets for large language models (LLMs) have grown to trillions\nof tokens composed of large amounts of CommonCrawl (CC) web scrape along with\nsmaller, domain-specific datasets. It is expensive to understand the impact of\nthese domain-specific datasets on model capabilities as training at large FLOP\nscales is required to reveal significant changes to difficult and emergent\nbenchmarks. Given the increasing cost of experimenting with pretraining data,\nhow does one determine the optimal balance between the diversity in general web\nscrapes and the information density of domain specific data? In this work, we\nshow how to leverage the smaller domain specific datasets by upsampling them\nrelative to CC at the end of training to drive performance improvements on\ndifficult benchmarks. This simple technique allows us to improve up to 6.90 pp\non MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data\nmix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2\n(7B)$\\unicode{x2014}$a model trained for twice as long. We experiment with\nablating the duration of domain upsampling from 5% to 30% of training and find\nthat 10% to 20% percent is optimal for navigating the tradeoff between general\nlanguage modeling capabilities and targeted benchmarks. We also use domain\nupsampling to characterize at scale the utility of individual datasets for\nimproving various benchmarks by removing them during this final phase of\ntraining. This tool opens up the ability to experiment with the impact of\ndifferent pretraining datasets at scale, but at an order of magnitude lower\ncost compared to full pretraining runs.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5df2\u589e\u957f\u5230\u6570\u4e07\u4ebf\u4e2a\u6807\u8bb0\uff0c\u5176\u4e2d\u5305\u542b\u5927\u91cf CommonCrawl (CC) \u7f51\u7edc\u6293\u53d6\uff0c\u4ee5\u53ca\u8f83\u5c0f\u7684\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u3002\u4e86\u89e3\u8fd9\u4e9b\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u529f\u80fd\u7684\u5f71\u54cd\u4ee3\u4ef7\u9ad8\u6602\uff0c\u56e0\u4e3a\u9700\u8981\u5728\u5927\u89c4\u6a21\u6d6e\u70b9\u8fd0\u7b97 (FLOP) \u8303\u56f4\u5185\u8fdb\u884c\u8bad\u7ec3\u624d\u80fd\u63ed\u793a\u5bf9\u56f0\u96be\u548c\u65b0\u5174\u57fa\u51c6\u7684\u91cd\u5927\u53d8\u5316\u3002\u9274\u4e8e\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u7684\u6210\u672c\u8d8a\u6765\u8d8a\u9ad8\uff0c\u5982\u4f55\u786e\u5b9a\u901a\u7528\u7f51\u7edc\u6293\u53d6\u7684\u591a\u6837\u6027\u4e0e\u7279\u5b9a\u9886\u57df\u6570\u636e\u7684\u7684\u4fe1\u606f\u5bc6\u5ea6\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\uff1f\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\u76f8\u5bf9\u4e8e CC \u5bf9\u8f83\u5c0f\u7684\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u8fdb\u884c\u4e0a\u91c7\u6837\u6765\u63d0\u9ad8\u5176\u5229\u7528\u7387\uff0c\u4ece\u800c\u63d0\u9ad8\u56f0\u96be\u57fa\u51c6\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u7b80\u5355\u7684\u6280\u672f\u4f7f\u6211\u4eec\u80fd\u591f\u5728\u9488\u5bf9 1 \u4e07\u4ebf (T) \u4e2a\u6807\u8bb0\u8bad\u7ec3\u7684 7B \u6a21\u578b\u4e0a\uff0c\u76f8\u5bf9\u4e8e\u57fa\u672c\u6570\u636e\u7ec4\u5408\uff0c\u5728 MMLU \u4e0a\u63d0\u9ad8\u9ad8\u8fbe 6.90 pp\uff0c\u5728 GSM8K \u4e0a\u63d0\u9ad8 8.26 pp\uff0c\u5728 HumanEval \u4e0a\u63d0\u9ad8 6.17 pp\uff0c\u4ece\u800c\u4e0e Llama-2 (7B) \u76f8\u5ab2\u7f8e\u2014\u2014\u4e00\u4e2a\u8bad\u7ec3\u65f6\u95f4\u957f\u8fbe\u4e24\u500d\u7684\u6a21\u578b\u3002\u6211\u4eec\u5c1d\u8bd5\u4ece\u8bad\u7ec3\u7684 5% \u5230 30% \u51cf\u5c11\u9886\u57df\u4e0a\u91c7\u6837\u7684\u6301\u7eed\u65f6\u95f4\uff0c\u5e76\u53d1\u73b0 10% \u5230 20% \u662f\u5728\u901a\u7528\u8bed\u8a00\u5efa\u6a21\u529f\u80fd\u548c\u76ee\u6807\u57fa\u51c6\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u6700\u4f73\u9009\u62e9\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u9886\u57df\u4e0a\u91c7\u6837\u6765\u5927\u89c4\u6a21\u8868\u5f81\u5404\u4e2a\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u7684\u6700\u540e\u9636\u6bb5\u901a\u8fc7\u5220\u9664\u5b83\u4eec\u6765\u6539\u5584\u5404\u79cd\u57fa\u51c6\u7684\u6548\u7528\u3002\u6b64\u5de5\u5177\u5f00\u542f\u4e86\u4ee5\u8f83\u4f4e\u6570\u91cf\u7ea7\u6210\u672c\uff08\u4e0e\u5b8c\u6574\u7684\u9884\u8bad\u7ec3\u8fd0\u884c\u76f8\u6bd4\uff09\u5927\u89c4\u6a21\u8bd5\u9a8c\u4e0d\u540c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5f71\u54cd\u7684\u80fd\u529b\u3002", "author": "Cody Blakeney et.al.", "authors": "Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, Jonathan Frankle", "id": "2406.03476v1", "paper_url": "http://arxiv.org/abs/2406.03476v1", "repo": "null"}}