{"2406.11640": {"publish_time": "2024-06-17", "title": "Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions", "paper_summary": "One of the most natural approaches to reinforcement learning (RL) with\nfunction approximation is value iteration, which inductively generates\napproximations to the optimal value function by solving a sequence of\nregression problems. To ensure the success of value iteration, it is typically\nassumed that Bellman completeness holds, which ensures that these regression\nproblems are well-specified. We study the problem of learning an optimal policy\nunder Bellman completeness in the online model of RL with linear function\napproximation. In the linear setting, while statistically efficient algorithms\nare known under Bellman completeness (e.g., Jiang et al. (2017); Zanette et al.\n(2020)), these algorithms all rely on the principle of global optimism which\nrequires solving a nonconvex optimization problem. In particular, it has\nremained open as to whether computationally efficient algorithms exist. In this\npaper we give the first polynomial-time algorithm for RL under linear Bellman\ncompleteness when the number of actions is any constant.", "paper_summary_zh": "\u5728\u4f7f\u7528\u51fd\u6578\u903c\u8fd1\u9032\u884c\u5f37\u5316\u5b78\u7fd2 (RL) \u6642\uff0c\u6700\u81ea\u7136\u7684\u5176\u4e2d\u4e00\u7a2e\u65b9\u6cd5\u662f\u50f9\u503c\u8fed\u4ee3\uff0c\u5b83\u900f\u904e\u89e3\u4e00\u7cfb\u5217\u56de\u6b78\u554f\u984c\uff0c\u4ee5\u6b78\u7d0d\u65b9\u5f0f\u7522\u751f\u5c0d\u6700\u4f73\u50f9\u503c\u51fd\u6578\u7684\u903c\u8fd1\u3002\u70ba\u4e86\u78ba\u4fdd\u50f9\u503c\u8fed\u4ee3\u7684\u6210\u529f\uff0c\u901a\u5e38\u5047\u8a2d\u8c9d\u723e\u66fc\u5b8c\u5099\u6027\u6210\u7acb\uff0c\u9019\u78ba\u4fdd\u4e86\u9019\u4e9b\u56de\u6b78\u554f\u984c\u6709\u660e\u78ba\u7684\u898f\u7bc4\u3002\u6211\u5011\u7814\u7a76\u5728\u7dda RL \u6a21\u578b\u4e2d\uff0c\u5728\u8c9d\u723e\u66fc\u5b8c\u5099\u6027\u4e0b\u5b78\u7fd2\u6700\u4f73\u7b56\u7565\u7684\u554f\u984c\uff0c\u4e26\u4f7f\u7528\u7dda\u6027\u51fd\u6578\u903c\u8fd1\u3002\u5728\u7dda\u6027\u8a2d\u5b9a\u4e2d\uff0c\u96d6\u7136\u5728\u8c9d\u723e\u66fc\u5b8c\u5099\u6027\u4e0b\u5df2\u77e5\u7d71\u8a08\u4e0a\u6709\u6548\u7387\u7684\u6f14\u7b97\u6cd5\uff08\u4f8b\u5982\uff0cJiang \u7b49\u4eba (2017)\uff1bZanette \u7b49\u4eba (2020)\uff09\uff0c\u4f46\u9019\u4e9b\u6f14\u7b97\u6cd5\u90fd\u4f9d\u8cf4\u65bc\u6574\u9ad4\u6a02\u89c0\u539f\u5247\uff0c\u9019\u9700\u8981\u89e3\u4e00\u500b\u975e\u51f8\u6700\u4f73\u5316\u554f\u984c\u3002\u7279\u5225\u662f\uff0c\u8a08\u7b97\u4e0a\u6709\u6548\u7387\u7684\u6f14\u7b97\u6cd5\u662f\u5426\u5b58\u5728\uff0c\u4ecd\u7136\u662f\u4e00\u500b\u672a\u89e3\u7684\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7d66\u51fa\u4e86\u5728\u7dda\u6027\u8c9d\u723e\u66fc\u5b8c\u5099\u6027\u4e0b RL \u7684\u7b2c\u4e00\u500b\u591a\u9805\u5f0f\u6642\u9593\u6f14\u7b97\u6cd5\uff0c\u5176\u4e2d\u52d5\u4f5c\u6578\u91cf\u70ba\u4efb\u610f\u5e38\u6578\u3002", "author": "Noah Golowich et.al.", "authors": "Noah Golowich, Ankur Moitra", "id": "2406.11640v1", "paper_url": "http://arxiv.org/abs/2406.11640v1", "repo": "null"}}