{"2501.02628": {"publish_time": "2025-01-05", "title": "Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets", "paper_summary": "A critical part of creating code suggestion systems is the pre-training of\nLarge Language Models on vast amounts of source code and natural language text,\noften of questionable origin or quality. This may contribute to the presence of\nbugs and vulnerabilities in code generated by LLMs. While efforts to identify\nbugs at or after code generation exist, it is preferable to pre-train or\nfine-tune LLMs on curated, high-quality, and compliant datasets. The need for\nvast amounts of training data necessitates that such curation be automated,\nminimizing human intervention.\n  We propose an automated source code autocuration technique that leverages the\ncomplete version history of open-source software projects to improve the\nquality of training data. This approach leverages the version history of all\nOSS projects to identify training data samples that have been modified or have\nundergone changes in at least one OSS project, and pinpoint a subset of samples\nthat include fixes for bugs or vulnerabilities. We evaluate this method using\nThe Stack v2 dataset, and find that 17% of the code versions in the dataset\nhave newer versions, with 17% of those representing bug fixes, including 2.36%\naddressing known CVEs. The deduplicated version of Stack v2 still includes\nblobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the\ndataset were never modified after creation, suggesting they likely represent\nsoftware with minimal or no use. Misidentified blob origins present an\nadditional challenge, as they lead to the inclusion of non-permissively\nlicensed code, raising serious compliance concerns.\n  By addressing these issues, the training of new models can avoid perpetuating\nbuggy code patterns or license violations. We expect our results to inspire\nprocess improvements for automated data curation, with the potential to enhance\nthe reliability of outputs generated by AI tools.", "paper_summary_zh": "\u5efa\u7acb\u7a0b\u5f0f\u78bc\u5efa\u8b70\u7cfb\u7d71\u7684\u95dc\u9375\u90e8\u5206\u662f\u91dd\u5c0d\u5927\u91cf\u4f86\u6e90\u7a0b\u5f0f\u78bc\u548c\u81ea\u7136\u8a9e\u8a00\u6587\u5b57\u9032\u884c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u9810\u8a13\u7df4\uff0c\u9019\u4e9b\u6587\u5b57\u7684\u4f86\u6e90\u6216\u54c1\u8cea\u901a\u5e38\u6709\u7591\u616e\u3002\u9019\u53ef\u80fd\u6703\u5c0e\u81f4 LLM \u6240\u7522\u751f\u7684\u7a0b\u5f0f\u78bc\u51fa\u73fe\u932f\u8aa4\u548c\u6f0f\u6d1e\u3002\u96d6\u7136\u5df2\u6709\u4eba\u52aa\u529b\u5728\u7a0b\u5f0f\u78bc\u7522\u751f\u671f\u9593\u6216\u7522\u751f\u5f8c\u627e\u51fa\u932f\u8aa4\uff0c\u4f46\u6700\u597d\u662f\u5728\u7d93\u904e\u6574\u7406\u3001\u54c1\u8cea\u9ad8\u4e14\u5408\u898f\u7684\u8cc7\u6599\u96c6\u4e0a\u9810\u5148\u8a13\u7df4\u6216\u5fae\u8abf LLM\u3002\u7531\u65bc\u8a13\u7df4\u8cc7\u6599\u91cf\u9f90\u5927\uff0c\u56e0\u6b64\u5fc5\u9808\u81ea\u52d5\u5316\u9019\u7a2e\u6574\u7406\uff0c\u5c07\u4eba\u70ba\u4ecb\u5165\u964d\u5230\u6700\u4f4e\u3002\n  \u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u81ea\u52d5\u5316\u4f86\u6e90\u7a0b\u5f0f\u78bc\u81ea\u52d5\u6574\u7406\u6280\u8853\uff0c\u5229\u7528\u958b\u6e90\u8edf\u9ad4\u5c08\u6848\u7684\u5b8c\u6574\u7248\u672c\u6b77\u7a0b\uff0c\u4ee5\u63d0\u5347\u8a13\u7df4\u8cc7\u6599\u7684\u54c1\u8cea\u3002\u9019\u7a2e\u65b9\u6cd5\u5229\u7528\u6240\u6709 OSS \u5c08\u6848\u7684\u7248\u672c\u6b77\u7a0b\uff0c\u627e\u51fa\u5df2\u4fee\u6539\u6216\u81f3\u5c11\u5728\u4e00\u500b OSS \u5c08\u6848\u4e2d\u6b77\u7d93\u8b8a\u66f4\u7684\u8a13\u7df4\u8cc7\u6599\u7bc4\u4f8b\uff0c\u4e26\u627e\u51fa\u5305\u542b\u932f\u8aa4\u6216\u6f0f\u6d1e\u4fee\u6b63\u7684\u7bc4\u4f8b\u5b50\u96c6\u3002\u6211\u5011\u4f7f\u7528 Stack v2 \u8cc7\u6599\u96c6\u8a55\u4f30\u6b64\u65b9\u6cd5\uff0c\u767c\u73fe\u8cc7\u6599\u96c6\u4e2d 17% \u7684\u7a0b\u5f0f\u78bc\u7248\u672c\u6709\u8f03\u65b0\u7684\u7248\u672c\uff0c\u5176\u4e2d 17% \u4ee3\u8868\u932f\u8aa4\u4fee\u6b63\uff0c\u5305\u62ec 2.36% \u89e3\u6c7a\u5df2\u77e5\u7684 CVE\u3002Stack v2 \u7684\u91cd\u8907\u8cc7\u6599\u522a\u9664\u7248\u672c\u4ecd\u5305\u542b\u5bb9\u6613\u53d7\u5230 6,947 \u500b\u5df2\u77e5 CVE \u5f71\u97ff\u7684 blob\u3002\u6b64\u5916\uff0c\u8cc7\u6599\u96c6\u4e2d 58% \u7684 blob \u5728\u5efa\u7acb\u5f8c\u5f9e\u672a\u4fee\u6539\u904e\uff0c\u9019\u8868\u793a\u5b83\u5011\u53ef\u80fd\u4ee3\u8868\u4f7f\u7528\u7a0b\u5ea6\u5f88\u4f4e\u6216\u5b8c\u5168\u6c92\u5728\u4f7f\u7528\u7684\u8edf\u9ad4\u3002\u932f\u8aa4\u8fa8\u8b58\u7684 blob \u4f86\u6e90\u6703\u5e36\u4f86\u984d\u5916\u6311\u6230\uff0c\u56e0\u70ba\u5b83\u5011\u6703\u5c0e\u81f4\u5305\u542b\u975e\u8a31\u53ef\u6388\u6b0a\u7684\u7a0b\u5f0f\u78bc\uff0c\u5f15\u767c\u56b4\u91cd\u7684\u5408\u898f\u554f\u984c\u3002\n  \u900f\u904e\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u65b0\u6a21\u578b\u7684\u8a13\u7df4\u53ef\u4ee5\u907f\u514d\u50b3\u905e\u932f\u8aa4\u7684\u7a0b\u5f0f\u78bc\u6a21\u5f0f\u6216\u6388\u6b0a\u9055\u898f\u3002\u6211\u5011\u9810\u671f\u6211\u5011\u7684\u7d50\u679c\u80fd\u6fc0\u52f5\u81ea\u52d5\u5316\u8cc7\u6599\u6574\u7406\u7684\u6d41\u7a0b\u6539\u5584\uff0c\u9032\u800c\u63d0\u5347 AI \u5de5\u5177\u7522\u751f\u8f38\u51fa\u7684\u53ef\u9760\u6027\u3002", "author": "Mahmoud Jahanshahi et.al.", "authors": "Mahmoud Jahanshahi, Audris Mockus", "id": "2501.02628v1", "paper_url": "http://arxiv.org/abs/2501.02628v1", "repo": "null"}}