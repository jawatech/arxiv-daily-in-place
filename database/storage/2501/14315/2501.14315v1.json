{"2501.14315": {"publish_time": "2025-01-24", "title": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of Token Perplexity", "paper_summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. In this paper, we present a systematic analysis\nrevealing that fine-tuning with LLM-generated data not only improves target\ntask performance but also reduces out-of-domain (OOD) degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhanced OOD robustness\nstems from a reduced prevalence of high perplexity tokens in LLM-generated\nsequences. Following this hypothesis we showed that masking high perplexity\ntokens in ground truth training data also achieves similar OOD preservation\ncomparable to using LLM-generated data. Extensive experiments across diverse\nmodel architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B,\ncorroborate the consistency of our findings. To the best of our knowledge, this\nwork provides the first mechanistic explanation for the superior OOD robustness\nconferred by LLM-generated training data, offering valuable insights for\ndeveloping more robust fine-tuning strategies.", "paper_summary_zh": "\u5728\u6a5f\u5668\u5b78\u7fd2\u4e2d\uff0c\u7dad\u6301\u6a21\u578b\u5728\u4e0d\u540c\u9818\u57df\u4e2d\u7684\u4e00\u81f4\u6548\u80fd\u662f\u4e00\u9805\u57fa\u672c\u7684\u6311\u6230\u3002\u96d6\u7136\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u8a0e\u4e86\u4f7f\u7528 LLM \u751f\u6210\u7684\u8cc7\u6599\u9032\u884c\u5fae\u8abf\uff0c\u4f46\u5b83\u5c0d\u8de8\u9818\u57df\u6cdb\u5316\u7684\u5f71\u97ff\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7cfb\u7d71\u6027\u7684\u5206\u6790\uff0c\u63ed\u793a\u4f7f\u7528 LLM \u751f\u6210\u7684\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u4e0d\u50c5\u53ef\u4ee5\u6539\u5584\u76ee\u6a19\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u800c\u4e14\u8207\u4f7f\u7528\u771f\u5be6\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u76f8\u6bd4\uff0c\u9084\u80fd\u6e1b\u5c11\u9818\u57df\u5916 (OOD) \u7684\u9000\u5316\u3002\u900f\u904e\u5206\u6790\u4e0d\u540c\u9818\u57df\u4efb\u52d9\u4e2d\u7684\u8cc7\u6599\u5e8f\u5217\uff0c\u6211\u5011\u8b49\u660e\u4e86\u9019\u7a2e\u589e\u5f37\u7684 OOD \u7a69\u5065\u6027\u6e90\u65bc LLM \u751f\u6210\u7684\u5e8f\u5217\u4e2d\u9ad8\u56f0\u60d1\u5ea6\u6a19\u8a18\u7684\u767c\u751f\u7387\u964d\u4f4e\u3002\u6839\u64da\u9019\u500b\u5047\u8a2d\uff0c\u6211\u5011\u8868\u660e\u5728\u771f\u5be6\u8a13\u7df4\u8cc7\u6599\u4e2d\u906e\u853d\u9ad8\u56f0\u60d1\u5ea6\u6a19\u8a18\u4e5f\u53ef\u4ee5\u5be6\u73fe\u8207\u4f7f\u7528 LLM \u751f\u6210\u7684\u8cc7\u6599\u76f8\u4f3c\u7684 OOD \u4fdd\u7559\u3002\u5728\u5305\u62ec Gemma2-2B\u3001Mistral-7B \u548c Llama3-8B \u5728\u5167\u7684\u5404\u7a2e\u6a21\u578b\u67b6\u69cb\u548c\u898f\u6a21\u4e2d\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\uff0c\u8b49\u5be6\u4e86\u6211\u5011\u767c\u73fe\u7684\u4e00\u81f4\u6027\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u9805\u5de5\u4f5c\u9996\u6b21\u63d0\u4f9b\u4e86 LLM \u751f\u6210\u7684\u8a13\u7df4\u8cc7\u6599\u8ce6\u4e88\u7684\u512a\u7570 OOD \u7a69\u5065\u6027\u7684\u6a5f\u5236\u89e3\u91cb\uff0c\u70ba\u958b\u767c\u66f4\u7a69\u5065\u7684\u5fae\u8abf\u7b56\u7565\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002", "author": "Chao-Chung Wu et.al.", "authors": "Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Hung-yi Lee, Yun-Nung Chen", "id": "2501.14315v1", "paper_url": "http://arxiv.org/abs/2501.14315v1", "repo": "null"}}