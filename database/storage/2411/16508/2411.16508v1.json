{"2411.16508": {"publish_time": "2024-11-25", "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages", "paper_summary": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available.", "paper_summary_zh": "\u73fe\u6709\u7684\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\uff08LMM\uff09\u901a\u5e38\u53ea\u5c08\u6ce8\u65bc\u5c11\u6578\u5340\u57df\u548c\u8a9e\u8a00\u3002\u96a8\u8457 LMM \u6301\u7e8c\u9032\u6b65\uff0c\u78ba\u4fdd\u5b83\u5011\u80fd\u7406\u89e3\u6587\u5316\u80cc\u666f\u3001\u5c0a\u91cd\u7576\u5730\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u652f\u63f4\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\uff0c\u540c\u6642\u9084\u8981\u6709\u6548\u6574\u5408\u5c0d\u61c9\u7684\u8996\u89ba\u63d0\u793a\u3002\u70ba\u4e86\u8ffd\u6c42\u6587\u5316\u591a\u5143\u7684\u5168\u7403\u591a\u6a21\u614b\u6a21\u578b\uff0c\u6211\u5011\u63d0\u51fa\u7684\u6240\u6709\u8a9e\u8a00\u91cd\u8981\u57fa\u6e96\uff08ALM-bench\uff09\u4ee3\u8868\u4e86\u8fc4\u4eca\u70ba\u6b62\u8a55\u4f30 100 \u7a2e\u8a9e\u8a00\u7684 LMM \u7684\u6700\u5927\u898f\u6a21\u4e14\u6700\u5168\u9762\u7684\u52aa\u529b\u3002ALM-bench \u900f\u904e\u6e2c\u8a66 LMM \u7406\u89e3\u548c\u63a8\u7406\u8207\u5404\u7a2e\u8a9e\u8a00\uff08\u5305\u62ec\u50b3\u7d71\u4e0a\u5728 LMM \u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8a31\u591a\u4f4e\u8cc7\u6e90\u8a9e\u8a00\uff09\u914d\u5c0d\u7684\u6587\u5316\u591a\u5143\u5716\u50cf\u7684\u80fd\u529b\uff0c\u5c0d\u73fe\u6709\u6a21\u578b\u63d0\u51fa\u6311\u6230\u3002\u6b64\u57fa\u6e96\u63d0\u4f9b\u4e86\u4e00\u500b\u5f37\u5065\u4e14\u7d30\u7dfb\u7684\u8a55\u4f30\u67b6\u69cb\uff0c\u5177\u6709\u5404\u7a2e\u554f\u984c\u683c\u5f0f\uff0c\u5305\u62ec\u662f\u975e\u984c\u3001\u591a\u9078\u984c\u548c\u958b\u653e\u5f0f\u554f\u984c\uff0c\u9032\u4e00\u6b65\u5206\u70ba\u7c21\u7b54\u548c\u9577\u7b54\u985e\u5225\u3002ALM-bench \u8a2d\u8a08\u78ba\u4fdd\u5168\u9762\u8a55\u4f30\u6a21\u578b\u5728\u8996\u89ba\u548c\u8a9e\u8a00\u63a8\u7406\u4e2d\u8655\u7406\u4e0d\u540c\u96e3\u5ea6\u7b49\u7d1a\u7684\u80fd\u529b\u3002\u70ba\u4e86\u6355\u6349\u5168\u7403\u6587\u5316\u7684\u8c50\u5bcc\u6a23\u8c8c\uff0cALM-bench \u7cbe\u5fc3\u7b56\u5283\u4e86\u4f86\u81ea 13 \u500b\u4e0d\u540c\u6587\u5316\u9762\u5411\u7684\u5167\u5bb9\uff0c\u5f9e\u50b3\u7d71\u548c\u5100\u5f0f\u5230\u540d\u4eba\u8207\u6176\u5178\u3002\u900f\u904e\u6b64\u65b9\u5f0f\uff0cALM-bench \u4e0d\u50c5\u70ba\u6700\u5148\u9032\u7684\u958b\u653e\u548c\u9589\u6e90 LMM \u63d0\u4f9b\u4e86\u56b4\u683c\u7684\u6e2c\u8a66\u5834\u57df\uff0c\u4e5f\u7a81\u986f\u4e86\u6587\u5316\u548c\u8a9e\u8a00\u5305\u5bb9\u6027\u7684\u91cd\u8981\u6027\uff0c\u9f13\u52f5\u958b\u767c\u80fd\u6709\u6548\u670d\u52d9\u5168\u7403\u591a\u5143\u4eba\u53e3\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u57fa\u6e96\u662f\u516c\u958b\u7684\u3002", "author": "Ashmal Vayani et.al.", "authors": "Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman M Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Fabian Farestam, Muztoba Rabbani, Sanoojan Baliah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Xavier, Amit Bhatkal, Hawau Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, Fahad Khan", "id": "2411.16508v1", "paper_url": "http://arxiv.org/abs/2411.16508v1", "repo": "null"}}