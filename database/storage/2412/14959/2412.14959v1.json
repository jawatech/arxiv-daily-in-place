{"2412.14959": {"publish_time": "2024-12-19", "title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction", "paper_summary": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.", "paper_summary_zh": "\u5167\u5728\u81ea\u6211\u4fee\u6b63\u88ab\u63d0\u51fa\u4f86\u900f\u904e\u56de\u994b\u63d0\u793a\u4f86\u6539\u5584 LLM \u7684\u56de\u61c9\uff0c\u9019\u4e9b\u63d0\u793a\u50c5\u57fa\u65bc\u5176\u56fa\u6709\u80fd\u529b\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0cLLM \u7684\u5167\u5728\u81ea\u6211\u4fee\u6b63\u6703\u5728\u6c92\u6709\u795e\u8aed\u6a19\u7c64\u4f5c\u70ba\u56de\u994b\u63d0\u793a\u7684\u60c5\u6cc1\u4e0b\u5931\u6557\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u89e3\u91cb LLM \u7684\u5167\u5728\u81ea\u6211\u4fee\u6b63\u5c0d\u65bc\u4e0d\u540c\u4efb\u52d9\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u5931\u6557\u6848\u4f8b\u3002\u900f\u904e\u5305\u542b\u4e00\u500b\u7c21\u55ae\u4efb\u52d9\u548c\u4e09\u500b\u8907\u96dc\u4efb\u52d9\uff0c\u4f7f\u7528\u6700\u5148\u9032 (SOTA) \u7684 LLM\uff0c\u4f8b\u5982 ChatGPT \u5bb6\u65cf (o1\u30014o\u30013.5-turbo) \u548c Llama \u5bb6\u65cf (2-7B\u30013-8B \u548c 3.1-8B)\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e09\u7a2e\u89e3\u91cb\u65b9\u6cd5\u4f86\u63ed\u793a LLM \u5167\u5728\u81ea\u6211\u4fee\u6b63\u7684\u9ed1\u6697\u9762\u3002\u6211\u5011\u767c\u73fe\u5167\u5728\u81ea\u6211\u4fee\u6b63\u53ef\u80fd (1) \u5c0e\u81f4 LLM \u5728\u4e2d\u9593\u7b54\u6848\u548c\u6700\u7d42\u7b54\u6848\u4e4b\u9593\u6416\u64fa\u4e0d\u5b9a\uff0c\u4e26\u5c0e\u81f4\u7c21\u55ae\u4e8b\u5be6\u554f\u984c\u7684\u63d0\u793a\u504f\u5dee\uff1b(2) \u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u5f15\u5165\u985e\u4f3c\u4eba\u985e\u7684\u8a8d\u77e5\u504f\u5dee\u3002\u6839\u64da\u6211\u5011\u7684\u767c\u73fe\uff0c\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u5169\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u7de9\u89e3\u7b56\u7565\uff1a\u554f\u984c\u91cd\u8907\u548c\u4f7f\u7528\u5c11\u6578\u6a23\u672c\u9032\u884c\u76e3\u7763\u5fae\u8abf\u3002\u6211\u5011\u5728 https://x-isc.info/ \u4e0a\u958b\u653e\u6211\u5011\u7684\u539f\u59cb\u78bc\u3002", "author": "Qingjie Zhang et.al.", "authors": "Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang", "id": "2412.14959v1", "paper_url": "http://arxiv.org/abs/2412.14959v1", "repo": "null"}}