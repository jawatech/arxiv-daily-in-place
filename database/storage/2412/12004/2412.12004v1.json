{"2412.12004": {"publish_time": "2024-12-16", "title": "The Open Source Advantage in Large Language Models (LLMs)", "paper_summary": "Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6a19\u8a8c\u8457\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7684\u95dc\u9375\u8f49\u8b8a\uff0c\u9032\u6b65\u4e86\u6587\u672c\u7522\u751f\u3001\u7ffb\u8b6f\u548c\u7279\u5b9a\u9818\u57df\u7684\u63a8\u7406\u3002\u7531\u5c08\u6709\u8cc7\u6599\u96c6\u548c\u5ee3\u6cdb\u8a08\u7b97\u8cc7\u6e90\u652f\u63f4\u7684\u5c01\u9589\u539f\u59cb\u78bc\u6a21\u578b\uff0c\u4f8b\u5982 GPT-4\uff0c\u4ee5\u5176\u6700\u5148\u9032\u7684\u6548\u80fd\u9818\u5148\u696d\u754c\u3002\u7136\u800c\uff0c\u5b83\u5011\u56e0\u5176\u300c\u9ed1\u76d2\u5b50\u300d\u6027\u8cea\u548c\u4ee5\u963b\u7919\u53ef\u8907\u88fd\u6027\u548c\u516c\u5e73\u7684\u4eba\u5de5\u667a\u6167\u767c\u5c55\u7684\u65b9\u5f0f\u9650\u5236\u53ef\u53ca\u6027\u800c\u53d7\u5230\u6279\u8a55\u3002\u76f8\u53cd\u5730\uff0cLLaMA \u548c BLOOM \u7b49\u958b\u653e\u539f\u59cb\u78bc\u8a08\u756b\u900f\u904e\u793e\u7fa4\u9a45\u52d5\u7684\u958b\u767c\u548c\u8a08\u7b97\u6548\u7387\u512a\u5148\u5be6\u73fe\u6c11\u4e3b\u5316\u3002\u9019\u4e9b\u6a21\u578b\u5df2\u5927\u5e45\u7e2e\u5c0f\u6548\u80fd\u5dee\u8ddd\uff0c\u7279\u5225\u662f\u5728\u8a9e\u8a00\u591a\u6a23\u6027\u548c\u7279\u5b9a\u9818\u57df\u61c9\u7528\u65b9\u9762\uff0c\u540c\u6642\u70ba\u5168\u7403\u7814\u7a76\u4eba\u54e1\u548c\u958b\u767c\u4eba\u54e1\u63d0\u4f9b\u4e86\u53ef\u53ca\u7684\u5de5\u5177\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u9019\u5169\u7a2e\u7bc4\u4f8b\u90fd\u4f9d\u8cf4\u65bc\u57fa\u790e\u67b6\u69cb\u5275\u65b0\uff0c\u4f8b\u5982 Vaswani \u7b49\u4eba (2017) \u7684 Transformer \u6846\u67b6\u3002\u5c01\u9589\u539f\u59cb\u78bc\u6a21\u578b\u900f\u904e\u6709\u6548\u64f4\u5145\u800c\u52dd\u51fa\uff0c\u800c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u5247\u9069\u61c9\u65bc\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8a9e\u8a00\u548c\u9818\u57df\u4e2d\u7684\u5be6\u969b\u61c9\u7528\u3002\u4f4e\u79e9\u9069\u61c9 (LoRA) \u548c\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u96c6\u7b49\u6280\u8853\u4f7f\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u80fd\u5920\u5728\u8cc7\u6e90\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\u53d6\u5f97\u5177\u7af6\u722d\u529b\u7684\u7d50\u679c\u3002\u53ef\u4ee5\u78ba\u5b9a\u7684\u662f\uff0c\u5c01\u9589\u539f\u59cb\u78bc\u548c\u958b\u653e\u539f\u59cb\u78bc\u65b9\u6cd5\u4e4b\u9593\u7684\u7dca\u5f35\u95dc\u4fc2\u7a81\u986f\u4e86\u4eba\u5de5\u667a\u6167\u4e2d\u900f\u660e\u5ea6\u8207\u5c08\u6709\u63a7\u5236\u4e4b\u9593\u66f4\u5ee3\u6cdb\u7684\u8faf\u8ad6\u3002\u502b\u7406\u8003\u91cf\u9032\u4e00\u6b65\u7a81\u986f\u4e86\u9019\u4e00\u5206\u6b67\u3002\u5c01\u9589\u539f\u59cb\u78bc\u7cfb\u7d71\u9650\u5236\u5916\u90e8\u5be9\u67e5\uff0c\u800c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u4fc3\u9032\u53ef\u8907\u88fd\u6027\u548c\u5354\u4f5c\uff0c\u4f46\u7f3a\u4e4f\u6a19\u6e96\u5316\u7684\u7a3d\u6838\u6587\u4ef6\u67b6\u69cb\u4f86\u6e1b\u8f15\u504f\u5dee\u3002\u5229\u7528\u9019\u5169\u7a2e\u7bc4\u4f8b\u512a\u52e2\u7684\u6df7\u5408\u65b9\u6cd5\u53ef\u80fd\u6703\u5f62\u5851 LLM \u5275\u65b0\u7684\u672a\u4f86\uff0c\u78ba\u4fdd\u53ef\u53ca\u6027\u3001\u5177\u6709\u7af6\u722d\u529b\u7684\u6280\u8853\u6548\u80fd\u548c\u5408\u4e4e\u9053\u5fb7\u7684\u90e8\u7f72\u3002</paragraph>", "author": "Jiya Manchanda et.al.", "authors": "Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser", "id": "2412.12004v1", "paper_url": "http://arxiv.org/abs/2412.12004v1", "repo": "null"}}