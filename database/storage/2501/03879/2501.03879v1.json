{"2501.03879": {"publish_time": "2025-01-07", "title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds", "paper_summary": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u4e0d\u4ec5\u9650\u4e8e\u7eaf\u6587\u672c\u4efb\u52a1\uff0c\u8fd8\u53ef\u4ee5\u8de8\u8d8a\u5404\u79cd\u6a21\u5f0f\uff08\u5305\u62ec\u97f3\u9891\u3001\u56fe\u50cf\u548c\u89c6\u9891\uff09\u4f5c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u53d1\u6325\u4f5c\u7528\u3002\u7279\u522b\u662f\uff0c3D \u5927\u591a\u6a21\u6001\u6a21\u578b (3D LMM) \u7684\u7814\u7a76\u6b63\u5728\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5904\u7406\u70b9\u4e91\u7b49\u9ad8\u7ef4\u6570\u636e\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u7ecf\u8fc7\u4ed4\u7ec6\u68c0\u67e5\uff0c\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6bcf\u4e2a\u6837\u672c\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u5185\u5bb9\u90fd\u7f3a\u4e4f\u9ad8\u4fe1\u606f\u7c92\u5ea6\u548c\u6e05\u6670\u5ea6\uff0c\u8fd9\u6210\u4e3a\u7cbe\u786e\u8de8\u6a21\u6001\u7406\u89e3\u7684\u74f6\u9888\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CL3DOR\uff0c\u5373\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u70b9\u4e91\u4e0a\u7684\u6bd4\u503c\u5bf9 3D \u5927\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u65e8\u5728\u786e\u4fdd\u89c6\u89c9\u548c\u6587\u672c\u5185\u5bb9\u5177\u6709\u66f4\u9ad8\u7684\u7279\u5f02\u6027\u548c\u6e05\u6670\u5ea6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u6bcf\u4e2a\u5bf9\u8c61\u7684\u70b9\u4e91\u5bc6\u5ea6\uff0c\u5e76\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u6784\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u786c\u8d1f\u9762\u54cd\u5e94\uff0c\u4ee5\u60e9\u7f5a\u4e0d\u9700\u8981\u7684\u54cd\u5e94\u3002\u4e3a\u4e86\u5229\u7528\u786c\u8d1f\u9762\u54cd\u5e94\uff0c\u6211\u4eec\u5c06\u6bd4\u503c\u4f5c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u7684\u8f85\u52a9\u9879\u7eb3\u5165\u4f20\u7edf\u7684\u8bed\u8a00\u5efa\u6a21\u635f\u5931\u4e2d\u3002CL3DOR \u5728 3D \u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86 CL3DOR \u5173\u952e\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "author": "Keonwoo Kim et.al.", "authors": "Keonwoo Kim, Yeongjae Cho, Taebaek Hwang, Minsoo Jo, Sangdo Han", "id": "2501.03879v1", "paper_url": "http://arxiv.org/abs/2501.03879v1", "repo": "null"}}