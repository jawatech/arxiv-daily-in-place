{"2409.12122": {"publish_time": "2024-09-18", "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement", "paper_summary": "In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems.", "paper_summary_zh": "<paragraph>\u5728\u9019\u4efd\u5831\u544a\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6578\u5b78\u5c08\u7528\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff1a\nQwen2.5-Math \u548c Qwen2.5-Math-Instruct-1.5B/7B/72B\u3002\nQwen2.5 \u7cfb\u5217\u7684\u6838\u5fc3\u5275\u65b0\u5728\u65bc\u5c07\u81ea\u6211\u63d0\u5347\u7684\u7406\u5ff5\u6574\u5408\u5230\u6574\u500b\u6d41\u7a0b\u4e2d\uff0c\u5f9e\u9810\u8a13\u7df4\u548c\u5f8c\u8a13\u7df4\u5230\u63a8\u7406\uff1a(1) \u5728\u9810\u8a13\u7df4\u968e\u6bb5\uff0cQwen2-Math-Instruct \u88ab\u7528\u65bc\u751f\u6210\u5927\u898f\u6a21\u3001\u9ad8\u54c1\u8cea\u7684\u6578\u5b78\u6578\u64da\u3002(2) \u5728\u5f8c\u8a13\u7df4\u968e\u6bb5\uff0c\u6211\u5011\u901a\u904e\u5f9e Qwen2-Math-Instruct \u9032\u884c\u5927\u91cf\u63a1\u6a23\u4f86\u958b\u767c\u4e00\u500b\u734e\u52f5\u6a21\u578b (RM)\u3002\u7136\u5f8c\u5c07\u6b64 RM \u61c9\u7528\u65bc\u76e3\u7763\u5fae\u8abf (SFT) \u4e2d\u6578\u64da\u7684\u8fed\u4ee3\u6f14\u5316\u3002\u4f7f\u7528\u66f4\u5f37\u5927\u7684 SFT \u6a21\u578b\uff0c\u53ef\u4ee5\u8fed\u4ee3\u8a13\u7df4\u548c\u66f4\u65b0 RM\uff0c\u800c RM \u53c8\u6307\u5c0e\u4e0b\u4e00\u8f2a SFT \u6578\u64da\u8fed\u4ee3\u3002\u5728\u6700\u7d42\u7684 SFT \u6a21\u578b\u4e2d\uff0c\u6211\u5011\u63a1\u7528\u6700\u7d42\u7684 RM \u9032\u884c\u5f37\u5316\u5b78\u7fd2\uff0c\u5f9e\u800c\u5f97\u5230 Qwen2.5-Math-Instruct\u3002(3) \u6b64\u5916\uff0c\u5728\u63a8\u7406\u968e\u6bb5\uff0cRM \u88ab\u7528\u65bc\u6307\u5c0e\u63a1\u6a23\uff0c\u512a\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002\nQwen2.5-Math-Instruct \u540c\u6642\u652f\u63f4\u4e2d\u6587\u548c\u82f1\u6587\uff0c\u4e26\u5177\u5099\u5148\u9032\u7684\u6578\u5b78\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u601d\u7dad\u93c8 (CoT) \u548c\u5de5\u5177\u6574\u5408\u63a8\u7406 (TIR)\u3002\u6211\u5011\u5728 10 \u500b\u82f1\u6587\u548c\u4e2d\u6587\u6578\u5b78\u6578\u64da\u96c6\u4e0a\u8a55\u4f30\u4e86\u6211\u5011\u7684\u6a21\u578b\uff0c\u4f8b\u5982 GSM8K\u3001MATH\u3001\u9ad8\u8003\u3001AMC23 \u548c AIME24\uff0c\u6db5\u84cb\u4e86\u5f9e\u5c0f\u5b78\u5230\u6578\u5b78\u7af6\u8cfd\u554f\u984c\u7684\u5404\u7a2e\u96e3\u5ea6\u3002</paragraph>", "author": "An Yang et.al.", "authors": "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang", "id": "2409.12122v1", "paper_url": "http://arxiv.org/abs/2409.12122v1", "repo": "null"}}