{"2406.18832": {"publish_time": "2024-06-27", "title": "OutlierTune: Efficient Channel-Wise Quantization for Large Language Models", "paper_summary": "Quantizing the activations of large language models (LLMs) has been a\nsignificant challenge due to the presence of structured outliers. Most existing\nmethods focus on the per-token or per-tensor quantization of activations,\nmaking it difficult to achieve both accuracy and hardware efficiency. To\naddress this problem, we propose OutlierTune, an efficient per-channel\npost-training quantization (PTQ) method for the activations of LLMs.\nOutlierTune consists of two components: pre-execution of dequantization and\nsymmetrization. The pre-execution of dequantization updates the model weights\nby the activation scaling factors, avoiding the internal scaling and costly\nadditional computational overheads brought by the per-channel activation\nquantization. The symmetrization further reduces the quantization differences\narising from the weight updates by ensuring the balanced numerical ranges\nacross different activation channels. OutlierTune is easy to implement and\nhardware-efficient, introducing almost no additional computational overheads\nduring the inference. Extensive experiments show that the proposed framework\noutperforms existing methods across multiple different tasks. Demonstrating\nbetter generalization, this framework improves the Int6 quantization of the\ninstruction-tuning LLMs, such as OPT-IML, to the same level as half-precision\n(FP16). Moreover, we have shown that the proposed framework is 1.48x faster\nthan the FP16 implementation while reducing approximately 2x memory usage.", "paper_summary_zh": "\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6fc0\u6d3b\u4e00\u76f4\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u539f\u56e0\u5728\u4e8e\u5b58\u5728\u7ed3\u6784\u5316\u5f02\u5e38\u503c\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u6fc0\u6d3b\u7684\u9010\u4ee4\u724c\u6216\u9010\u5f20\u91cf\u91cf\u5316\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u51c6\u786e\u6027\u548c\u786c\u4ef6\u6548\u7387\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 OutlierTune\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9 LLM \u6fc0\u6d3b\u7684\u9ad8\u6548\u9010\u901a\u9053\u540e\u8bad\u7ec3\u91cf\u5316 (PTQ) \u65b9\u6cd5\u3002OutlierTune \u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u53cd\u91cf\u5316\u7684\u9884\u6267\u884c\u548c\u5bf9\u79f0\u5316\u3002\u53cd\u91cf\u5316\u7684\u9884\u6267\u884c\u901a\u8fc7\u6fc0\u6d3b\u6bd4\u4f8b\u56e0\u5b50\u66f4\u65b0\u6a21\u578b\u6743\u91cd\uff0c\u907f\u514d\u4e86\u9010\u901a\u9053\u6fc0\u6d3b\u91cf\u5316\u5e26\u6765\u7684\u5185\u90e8\u6bd4\u4f8b\u548c\u6602\u8d35\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002\u5bf9\u79f0\u5316\u901a\u8fc7\u786e\u4fdd\u4e0d\u540c\u6fc0\u6d3b\u901a\u9053\u4e4b\u95f4\u7684\u6570\u503c\u8303\u56f4\u5e73\u8861\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u7531\u6743\u91cd\u66f4\u65b0\u5f15\u8d77\u7684\u91cf\u5316\u5dee\u5f02\u3002OutlierTune \u6613\u4e8e\u5b9e\u73b0\u4e14\u786c\u4ef6\u9ad8\u6548\uff0c\u5728\u63a8\u7406\u671f\u95f4\u51e0\u4e4e\u6ca1\u6709\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c06\u6307\u4ee4\u5fae\u8c03 LLM\uff08\u4f8b\u5982 OPT-IML\uff09\u7684 Int6 \u91cf\u5316\u63d0\u5347\u5230\u4e0e\u534a\u7cbe\u5ea6 (FP16) \u76f8\u540c\u7684\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5df2\u7ecf\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6bd4 FP16 \u5b9e\u73b0\u5feb 1.48 \u500d\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5927\u7ea6 2 \u500d\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u3002", "author": "Jinguang Wang et.al.", "authors": "Jinguang Wang, Yuexi Yin, Haifeng Sun, Qi Qi, Jingyu Wang, Zirui Zhuang, Tingting Yang, Jianxin Liao", "id": "2406.18832v1", "paper_url": "http://arxiv.org/abs/2406.18832v1", "repo": "null"}}