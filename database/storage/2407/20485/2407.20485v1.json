{"2407.20485": {"publish_time": "2024-07-30", "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder", "paper_summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u57fa\u4e8e Transformer \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\uff0c\u56e0 KV \u7f13\u5b58\u800c\u9762\u4e34\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002\n\u5148\u524d\u7684\u7814\u7a76\u63d0\u51fa\u4e86 KV \u7f13\u5b58\u538b\u7f29\u6280\u672f\uff0c\u8be5\u6280\u672f\u57fa\u4e8e\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522b\u4e0d\u91cd\u8981\u7684\u6807\u8bb0\uff0c\u5e76\u4ece KV \u7f13\u5b58\u4e2d\u5220\u9664\u5176\u9879\u76ee\uff0c\u56e0\u4e3a\u53ea\u6709\u5c11\u6570\u6807\u8bb0\u5728\u6ce8\u610f\u529b\u64cd\u4f5c\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u73b0\u6709\u7684\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570\u4e0d\u9002\u7528\u4e8e Transformer \u89e3\u7801\u5668\u7ed3\u6784\u3002\u5728\u89e3\u7801\u5668\u6a21\u578b\u4e2d\uff0c\u7531\u4e8e\u906e\u853d\u6548\u5e94\uff0c\u6ce8\u610f\u529b\u5206\u6570\u7d2f\u79ef\u7684\u6b21\u6570\u4f1a\u6839\u636e\u6807\u8bb0\u51fa\u73b0\u987a\u5e8f\u800c\u6709\u6240\u4e0d\u540c\uff0c\u4ece\u800c\u5bfc\u81f4\u6807\u8bb0\u4e4b\u95f4\u7684\u6bd4\u8f83\u4e0d\u5747\u5300\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5e26\u6709\u9057\u5fd8\u56e0\u5b50\u7684\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570 (A2SF) \u6280\u672f\uff0c\u8be5\u6280\u672f\u5728\u6ce8\u610f\u529b\u5206\u6570\u7d2f\u79ef\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u9057\u5fd8\u56e0\u5b50\u3002A2SF \u901a\u8fc7\u53cd\u590d\u5c06\u9057\u5fd8\u56e0\u5b50\u4e58\u4ee5\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5bf9\u65e7\u6807\u8bb0\u4ea7\u751f\u7684\u8fc7\u53bb\u6ce8\u610f\u529b\u5206\u6570\u65bd\u52a0\u60e9\u7f5a\u3002\u56e0\u6b64\uff0c\u8f83\u65e7\u7684\u6807\u8bb0\u4f1a\u53d7\u5230\u66f4\u5927\u7684\u60e9\u7f5a\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u5e74\u9f84\u7684\u6807\u8bb0\u4e4b\u95f4\u63d0\u4f9b\u516c\u5e73\u6027\u3002\u901a\u8fc7\u5728\u6807\u8bb0\u4e4b\u95f4\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u9009\u62e9\u91cd\u8981\u7684\u6807\u8bb0\u3002\u6211\u4eec\u901a\u8fc7 A2SF \u5728 OPT \u548c LLaMA \u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u51c6\u786e\u6027\u7684\u63d0\u9ad8\uff0cA2SF \u5c06 LLaMA 2 \u5728 1 \u6b21\u548c 0 \u6b21\u4e2d\u7684\u51c6\u786e\u6027\u5206\u522b\u63d0\u9ad8\u4e86 7.8% \u548c 5.1%\u3002", "author": "Hyun Rae Jo et.al.", "authors": "Hyun Rae Jo, Dong Kun Shin", "id": "2407.20485v1", "paper_url": "http://arxiv.org/abs/2407.20485v1", "repo": "null"}}