{"2410.02811": {"publish_time": "2024-09-22", "title": "SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graphs", "paper_summary": "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks\nacross specialized domains, where the acquisition of precise and dependable\nknowledge is crucial. However, existing KG construction methods heavily rely on\nhuman intervention to attain qualified KGs, which severely hinders the\npractical applicability in real-world scenarios. To address this challenge, we\npropose a general KG construction framework, named SAC-KG, to exploit large\nlanguage models (LLMs) as Skilled Automatic Constructors for domain Knowledge\nGraph. SAC-KG effectively involves LLMs as domain experts to generate\nspecialized and precise multi-level KGs. Specifically, SAC-KG consists of three\ncomponents: Generator, Verifier, and Pruner. For a given entity, Generator\nproduces its relations and tails from raw domain corpora, to construct a\nspecialized single-level KG. Verifier and Pruner then work together to ensure\nprecision by correcting generation errors and determining whether newly\nproduced tails require further iteration for the next-level KG.Experiments\ndemonstrate that SAC-KG automatically constructs a domain KG at the scale of\nover one million nodes and achieves a precision of 89.32%, leading to a\nsuperior performance with over 20% increase in precision rate compared to\nexisting state-of-the-art methods for the KG construction task.", "paper_summary_zh": "\u77e5\u8b58\u5716\u8b5c (KG) \u5728\u5c08\u696d\u9818\u57df\u4e2d\u626e\u6f14\u8457\u95dc\u9375\u89d2\u8272\uff0c\u5728\u9019\u4e9b\u9818\u57df\u4e2d\uff0c\u53d6\u5f97\u7cbe\u78ba\u4e14\u53ef\u9760\u7684\u77e5\u8b58\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 KG \u5efa\u69cb\u65b9\u6cd5\u56b4\u91cd\u4f9d\u8cf4\u4eba\u5de5\u4ecb\u5165\u624d\u80fd\u7372\u5f97\u5408\u683c\u7684 KG\uff0c\u9019\u56b4\u91cd\u963b\u7919\u4e86\u5728\u5be6\u969b\u5834\u666f\u4e2d\u7684\u5be6\u7528\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u9805\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u901a\u7528\u7684 KG \u5efa\u69cb\u67b6\u69cb\uff0c\u540d\u70ba SAC-KG\uff0c\u4ee5\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u5c08\u696d\u7684\u81ea\u52d5\u5efa\u69cb\u5668\uff0c\u7528\u65bc\u9818\u57df\u77e5\u8b58\u5716\u8b5c\u3002SAC-KG \u6709\u6548\u5730\u5c07 LLM \u8996\u70ba\u9818\u57df\u5c08\u5bb6\uff0c\u4ee5\u7522\u751f\u5c08\u696d\u4e14\u7cbe\u78ba\u7684\u591a\u5c64\u7d1a KG\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSAC-KG \u5305\u542b\u4e09\u500b\u7d44\u6210\u90e8\u5206\uff1a\u751f\u6210\u5668\u3001\u9a57\u8b49\u5668\u548c\u4fee\u526a\u5668\u3002\u5c0d\u65bc\u7d66\u5b9a\u7684\u5be6\u9ad4\uff0c\u751f\u6210\u5668\u6703\u5f9e\u539f\u59cb\u9818\u57df\u8a9e\u6599\u5eab\u4e2d\u7522\u751f\u5176\u95dc\u4fc2\u548c\u5c3e\u90e8\uff0c\u4ee5\u5efa\u69cb\u5c08\u696d\u7684\u55ae\u5c64\u7d1a KG\u3002\u7136\u5f8c\uff0c\u9a57\u8b49\u5668\u548c\u4fee\u526a\u5668\u6703\u5171\u540c\u78ba\u4fdd\u7cbe\u78ba\u6027\uff0c\u65b9\u6cd5\u662f\u66f4\u6b63\u7522\u751f\u932f\u8aa4\u4e26\u78ba\u5b9a\u65b0\u7522\u751f\u7684\u5c3e\u90e8\u662f\u5426\u9700\u8981\u9032\u4e00\u6b65\u8fed\u4ee3\u4ee5\u7528\u65bc\u4e0b\u4e00\u5c64\u7d1a\u7684 KG\u3002\u5be6\u9a57\u8b49\u660e\uff0cSAC-KG \u81ea\u52d5\u5efa\u69cb\u4e86\u4e00\u500b\u898f\u6a21\u8d85\u904e\u4e00\u767e\u842c\u500b\u7bc0\u9ede\u7684\u9818\u57df KG\uff0c\u4e26\u9054\u5230\u4e86 89.32% \u7684\u7cbe\u78ba\u5ea6\uff0c\u8207\u73fe\u6709\u7684 KG \u5efa\u69cb\u4efb\u52d9\u7684\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cbe\u78ba\u5ea6\u63d0\u9ad8\u4e86 20% \u4ee5\u4e0a\uff0c\u8868\u73fe\u512a\u7570\u3002", "author": "Hanzhu Chen et.al.", "authors": "Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, Jieping Ye", "id": "2410.02811v1", "paper_url": "http://arxiv.org/abs/2410.02811v1", "repo": "null"}}