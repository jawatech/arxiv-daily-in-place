{"2410.11437": {"publish_time": "2024-10-15", "title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "paper_summary": "Multimodal Large Language Models (MLLMs) demonstrate a strong understanding\nof the real world and can even handle complex tasks. However, they still fail\non some straightforward visual question-answering (VQA) problems. This paper\ndives deeper into this issue, revealing that models tend to err when answering\neasy questions (e.g. Yes/No questions) about an image, even though they can\ncorrectly describe it. We refer to this model behavior discrepancy between\ndifficult and simple questions as model laziness. To systematically investigate\nmodel laziness, we manually construct LazyBench, a benchmark that includes\nYes/No, multiple choice, short answer questions, and image description tasks\nthat are related to the same subjects in the images. Based on LazyBench, we\nobserve that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,\nGemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on\nstronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find\nthat about half of its failure cases are caused by model laziness, which\nfurther highlights the importance of ensuring that the model fully utilizes its\ncapability. To this end, we conduct preliminary exploration on how to mitigate\nlaziness and find that chain of thought (CoT) can effectively address this\nissue.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5c55\u793a\u4e86\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u6df1\u523b\u7406\u89e3\uff0c\u751a\u81f3\u53ef\u4ee5\u5904\u7406\u590d\u6742\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u4e00\u4e9b\u7b80\u5355\u7684\u89c6\u89c9\u95ee\u7b54 (VQA) \u95ee\u9898\u4e0a\u4ecd\u7136\u4f1a\u5931\u8d25\u3002\u672c\u6587\u5bf9\u6b64\u95ee\u9898\u8fdb\u884c\u4e86\u66f4\u6df1\u5165\u7684\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u56de\u7b54\u6709\u5173\u56fe\u50cf\u7684\u7b80\u5355\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u662f/\u5426\u95ee\u9898\uff09\u65f6\u5f80\u5f80\u4f1a\u51fa\u9519\uff0c\u5373\u4f7f\u5b83\u4eec\u53ef\u4ee5\u6b63\u786e\u63cf\u8ff0\u56fe\u50cf\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u6a21\u578b\u884c\u4e3a\u5dee\u5f02\u79f0\u4e3a\u6a21\u578b\u60f0\u6027\uff0c\u4ecb\u4e8e\u56f0\u96be\u548c\u7b80\u5355\u95ee\u9898\u4e4b\u95f4\u3002\u4e3a\u4e86\u7cfb\u7edf\u5730\u8c03\u67e5\u6a21\u578b\u60f0\u6027\uff0c\u6211\u4eec\u624b\u52a8\u6784\u5efa\u4e86 LazyBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u62ec\u662f/\u5426\u3001\u591a\u9009\u3001\u7b80\u7b54\u9898\u548c\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e0e\u56fe\u50cf\u4e2d\u7684\u76f8\u540c\u4e3b\u9898\u76f8\u5173\u3002\u57fa\u4e8e LazyBench\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u60f0\u6027\u5e7f\u6cdb\u5b58\u5728\u4e8e\u5f53\u524d\u5148\u8fdb\u7684 MLLM\uff08\u4f8b\u5982 GPT-4o\u3001Gemini-1.5-pro\u3001Claude 3 \u548c LLaVA-v1.5-13B\uff09\u4e2d\uff0c\u5e76\u4e14\u5728\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u4e2d\u66f4\u4e3a\u660e\u663e\u3002\u6211\u4eec\u8fd8\u5206\u6790\u4e86 VQA v2 (LLaVA-v1.5-13B) \u57fa\u51c6\uff0c\u53d1\u73b0\u5176\u5927\u7ea6\u4e00\u534a\u7684\u5931\u8d25\u6848\u4f8b\u662f\u7531\u6a21\u578b\u60f0\u6027\u9020\u6210\u7684\uff0c\u8fd9\u8fdb\u4e00\u6b65\u51f8\u663e\u4e86\u786e\u4fdd\u6a21\u578b\u5145\u5206\u5229\u7528\u5176\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5bf9\u5982\u4f55\u51cf\u8f7b\u60f0\u6027\u8fdb\u884c\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u5e76\u53d1\u73b0\u601d\u7ef4\u94fe (CoT) \u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "author": "Sihang Zhao et.al.", "authors": "Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He", "id": "2410.11437v1", "paper_url": "http://arxiv.org/abs/2410.11437v1", "repo": "null"}}