{"2407.02775": {"publish_time": "2024-07-03", "title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "paper_summary": "Knowledge distillation is an effective technique for pre-trained language\nmodel compression. Although existing knowledge distillation methods perform\nwell for the most typical model BERT, they could be further improved in two\naspects: the relation-level knowledge could be further explored to improve\nmodel performance; and the setting of student attention head number could be\nmore flexible to decrease inference time. Therefore, we are motivated to\npropose a novel knowledge distillation method MLKD-BERT to distill multi-level\nknowledge in teacher-student framework. Extensive experiments on GLUE benchmark\nand extractive question answering tasks demonstrate that our method outperforms\nstate-of-the-art knowledge distillation methods on BERT. In addition, MLKD-BERT\ncan flexibly set student attention head number, allowing for substantial\ninference time decrease with little performance drop.", "paper_summary_zh": "\u77e5\u8b58\u84b8\u993e\u662f\u4e00\u7a2e\u91dd\u5c0d\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u58d3\u7e2e\u7684\u6709\u6548\u6280\u8853\u3002\u5118\u7ba1\u73fe\u6709\u7684\u77e5\u8b58\u84b8\u993e\u65b9\u6cd5\u5c0d\u6700\u5178\u578b\u7684\u6a21\u578b BERT \u57f7\u884c\u826f\u597d\uff0c\u4f46\u5b83\u5011\u53ef\u4ee5\u5728\u5169\u500b\u65b9\u9762\u9032\u4e00\u6b65\u6539\u9032\uff1a\u53ef\u4ee5\u9032\u4e00\u6b65\u63a2\u7d22\u95dc\u4fc2\u5c64\u7d1a\u77e5\u8b58\u4ee5\u63d0\u9ad8\u6a21\u578b\u6548\u80fd\uff1b\u4e26\u4e14\u53ef\u4ee5\u66f4\u9748\u6d3b\u5730\u8a2d\u5b9a\u5b78\u751f\u6ce8\u610f\u529b\u982d\u90e8\u6578\u91cf\u4ee5\u6e1b\u5c11\u63a8\u8ad6\u6642\u9593\u3002\u56e0\u6b64\uff0c\u6211\u5011\u6709\u52d5\u529b\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u77e5\u8b58\u84b8\u993e\u65b9\u6cd5 MLKD-BERT\uff0c\u4ee5\u5728\u5e2b\u751f\u67b6\u69cb\u4e2d\u84b8\u993e\u591a\u5c64\u7d1a\u77e5\u8b58\u3002\u5728 GLUE \u57fa\u6e96\u548c\u8403\u53d6\u5f0f\u554f\u7b54\u4efb\u52d9\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u512a\u65bc BERT \u4e0a\u6700\u5148\u9032\u7684\u77e5\u8b58\u84b8\u993e\u65b9\u6cd5\u3002\u6b64\u5916\uff0cMLKD-BERT \u53ef\u4ee5\u9748\u6d3b\u5730\u8a2d\u5b9a\u5b78\u751f\u6ce8\u610f\u529b\u982d\u90e8\u6578\u91cf\uff0c\u5141\u8a31\u5728\u6548\u80fd\u4e0b\u964d\u4e0d\u591a\u7684\u60c5\u6cc1\u4e0b\u5927\u5e45\u6e1b\u5c11\u63a8\u8ad6\u6642\u9593\u3002", "author": "Ying Zhang et.al.", "authors": "Ying Zhang, Ziheng Yang, Shufan Ji", "id": "2407.02775v1", "paper_url": "http://arxiv.org/abs/2407.02775v1", "repo": "null"}}