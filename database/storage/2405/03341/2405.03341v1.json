{"2405.03341": {"publish_time": "2024-05-06", "title": "Enhancing Q-Learning with Large Language Model Heuristics", "paper_summary": "Q-learning excels in learning from feedback within sequential decision-making\ntasks but requires extensive sampling for significant improvements. Although\nreward shaping is a powerful technique for enhancing learning efficiency, it\ncan introduce biases that affect agent performance. Furthermore,\npotential-based reward shaping is constrained as it does not allow for reward\nmodifications based on actions or terminal states, potentially limiting its\neffectiveness in complex environments. Additionally, large language models\n(LLMs) can achieve zero-shot learning, but this is generally limited to simpler\ntasks. They also exhibit low inference speeds and occasionally produce\nhallucinations. To address these issues, we propose \\textbf{LLM-guided\nQ-learning} that employs LLMs as heuristic to aid in learning the Q-function\nfor reinforcement learning. It combines the advantages of both technologies\nwithout introducing performance bias. Our theoretical analysis demonstrates\nthat the LLM heuristic provides action-level guidance. Additionally, our\narchitecture has the capability to convert the impact of hallucinations into\nexploration costs. Moreover, the converged Q function corresponds to the MDP\noptimal Q function. Experiment results demonstrated that our algorithm enables\nagents to avoid ineffective exploration, enhances sampling efficiency, and is\nwell-suited for complex control tasks.", "paper_summary_zh": "", "author": "Xiefeng Wu et.al.", "authors": "Xiefeng Wu", "id": "2405.03341v1", "paper_url": "http://arxiv.org/abs/2405.03341v1", "repo": "null"}}