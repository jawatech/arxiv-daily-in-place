{"2406.09454": {"publish_time": "2024-06-12", "title": "Advancing High Resolution Vision-Language Models in Biomedicine", "paper_summary": "Multi-modal learning has significantly advanced generative AI, especially in\nvision-language modeling. Innovations like GPT-4V and open-source projects such\nas LLaVA have enabled robust conversational agents capable of zero-shot task\ncompletions. However, applying these technologies in the biomedical field\npresents unique challenges. Recent initiatives like LLaVA-Med have started to\nadapt instruction-tuning for biomedical contexts using large datasets such as\nPMC-15M. Our research offers three key contributions: (i) we present a new\ninstruct dataset enriched with medical image-text pairs from Claude3-Opus and\nLLaMA3 70B, (ii) we propose a novel image encoding strategy using hierarchical\nrepresentations to improve fine-grained biomedical visual comprehension, and\n(iii) we develop the Llama3-Med model, which achieves state-of-the-art\nzero-shot performance on biomedical visual question answering benchmarks, with\nan average performance improvement of over 10% compared to previous methods.\nThese advancements provide more accurate and reliable tools for medical\nprofessionals, bridging gaps in current multi-modal conversational assistants\nand promoting further innovations in medical AI.", "paper_summary_zh": "\u591a\u6a21\u6001\u5b66\u4e60\u5df2\u5927\u5e45\u63d0\u5347\u751f\u6210\u5f0f AI\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u65b9\u9762\u3002GPT-4V \u7b49\u521b\u65b0\u548c LLaVA \u7b49\u5f00\u6e90\u9879\u76ee\u5df2\u8ba9\u5065\u5168\u7684\u5bf9\u8bdd\u4ee3\u7406\u80fd\u591f\u5b8c\u6210\u96f6\u6b21\u5b66\u4e60\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002LLaVA-Med \u7b49\u8fd1\u671f\u8ba1\u5212\u5df2\u5f00\u59cb\u4f7f\u7528 PMC-15M \u7b49\u5927\u578b\u6570\u636e\u96c6\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u8bed\u5883\u8c03\u6574\u6307\u4ee4\u3002\u6211\u4eec\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a(i) \u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u65b0\u7684\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6765\u81ea Claude3-Opus \u548c LLaMA3 70B \u7684\u533b\u5b66\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c(ii) \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f16\u7801\u7b56\u7565\uff0c\u4f7f\u7528\u5206\u5c42\u8868\u793a\u6765\u6539\u5584\u7ec6\u7c92\u5ea6\u7684\u751f\u7269\u533b\u5b66\u89c6\u89c9\u7406\u89e3\uff0c\u4ee5\u53ca (iii) \u6211\u4eec\u5f00\u53d1\u4e86 Llama3-Med \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6b21\u5b66\u4e60\u6027\u80fd\uff0c\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e86 10% \u4ee5\u4e0a\u3002\u8fd9\u4e9b\u8fdb\u6b65\u4e3a\u533b\u5b66\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u5de5\u5177\uff0c\u5f25\u8865\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5bf9\u8bdd\u52a9\u7406\u4e2d\u7684\u5dee\u8ddd\uff0c\u5e76\u4fc3\u8fdb\u4e86\u533b\u5b66 AI \u7684\u8fdb\u4e00\u6b65\u521b\u65b0\u3002", "author": "Zekai Chen et.al.", "authors": "Zekai Chen, Arda Pekis, Kevin Brown", "id": "2406.09454v1", "paper_url": "http://arxiv.org/abs/2406.09454v1", "repo": "null"}}