{"2412.20891": {"publish_time": "2024-12-30", "title": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models", "paper_summary": "Low-rank adaptation (LoRA) reduces the computational and memory demands of\nfine-tuning large language models (LLMs) by approximating updates with low-rank\nmatrices. However, low-rank approximation in two-dimensional space fails to\ncapture high-dimensional structures within the target matrix. Recently, tensor\ndecomposition methods have been explored for fine-tuning LLMs, leveraging their\nability to extract structured information. Yet, these approaches primarily rely\non random initialization, and the impact of initialization on tensor adaptation\nremains underexplored. In this paper, we reveal that random initialization\nsignificantly diverges from the validation loss achieved by full fine-tuning.\nTo address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which\nleverages the Matrix Product Operator (MPO) decomposition of pre-trained\nweights for effective initialization in fine-tuning LLMs. Additionally, we\nintroduce QDoTA, a quantized version of DoTA designed for 4-bit quantization.\nExperiments on commonsense and arithmetic reasoning tasks show that DoTA\noutperforms random initialization methods with fewer parameters. QDoTA further\nreduces memory consumption and achieves comparable performance to DoTA on\ncommonsense reasoning tasks. We will release our code to support future\nresearch.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9 (LoRA) \u900f\u904e\u4f7f\u7528\u4f4e\u79e9\u77e9\u9663\u8fd1\u4f3c\u66f4\u65b0\uff0c\u4f86\u964d\u4f4e\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u3002\u7136\u800c\uff0c\u5728\u4e8c\u7dad\u7a7a\u9593\u4e2d\u7684\u4f4e\u79e9\u8fd1\u4f3c\u7121\u6cd5\u64f7\u53d6\u76ee\u6a19\u77e9\u9663\u4e2d\u7684\u9ad8\u7dad\u7d50\u69cb\u3002\u6700\u8fd1\uff0c\u5df2\u7d93\u63a2\u8a0e\u4f7f\u7528\u5f35\u91cf\u5206\u89e3\u65b9\u6cd5\u4f86\u5fae\u8abf LLM\uff0c\u5229\u7528\u5176\u63d0\u53d6\u7d50\u69cb\u5316\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u65bc\u96a8\u6a5f\u521d\u59cb\u5316\uff0c\u800c\u521d\u59cb\u5316\u5c0d\u5f35\u91cf\u9069\u61c9\u7684\u5f71\u97ff\u4ecd\u672a\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63ed\u793a\u96a8\u6a5f\u521d\u59cb\u5316\u8207\u5b8c\u5168\u5fae\u8abf\u6240\u9054\u6210\u7684\u9a57\u8b49\u640d\u5931\u6709\u986f\u8457\u5dee\u7570\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u91cd\u91cf\u5206\u89e3\u5f35\u91cf\u9069\u61c9 (DoTA)\uff0c\u5b83\u5229\u7528\u9810\u8a13\u7df4\u6b0a\u91cd\u7684\u77e9\u9663\u4e58\u7a4d\u7b97\u5b50 (MPO) \u5206\u89e3\uff0c\u5728\u5fae\u8abf LLM \u4e2d\u9032\u884c\u6709\u6548\u7684\u521d\u59cb\u5316\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86 QDoTA\uff0c\u9019\u662f\u4e00\u7a2e\u91dd\u5c0d 4 \u4f4d\u5143\u91cf\u5316\u8a2d\u8a08\u7684 DoTA \u91cf\u5316\u7248\u672c\u3002\u5728\u5e38\u8b58\u548c\u7b97\u8853\u63a8\u7406\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cDoTA \u4ee5\u8f03\u5c11\u7684\u53c3\u6578\u512a\u65bc\u96a8\u6a5f\u521d\u59cb\u5316\u65b9\u6cd5\u3002QDoTA \u9032\u4e00\u6b65\u964d\u4f4e\u4e86\u8a18\u61b6\u9ad4\u6d88\u8017\uff0c\u4e26\u5728\u5e38\u8b58\u63a8\u7406\u4efb\u52d9\u4e0a\u9054\u5230\u4e86\u8207 DoTA \u76f8\u7576\u7684\u6548\u80fd\u3002\u6211\u5011\u5c07\u91cb\u51fa\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u4ee5\u652f\u63f4\u672a\u4f86\u7684\u7814\u7a76\u3002", "author": "Xiaolin Hu et.al.", "authors": "Xiaolin Hu, Xiang Cheng, Peiyu Liu, Wei Liu, Jian Luan, Bin Wang, Yong Liu", "id": "2412.20891v1", "paper_url": "http://arxiv.org/abs/2412.20891v1", "repo": "null"}}