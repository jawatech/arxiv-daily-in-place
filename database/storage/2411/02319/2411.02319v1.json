{"2411.02319": {"publish_time": "2024-11-04", "title": "GenXD: Generating Any 3D and 4D Scenes", "paper_summary": "Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.", "paper_summary_zh": "\u8fd1\u4f86 2D \u8996\u89ba\u751f\u6210\u7684\u767c\u5c55\u975e\u5e38\u6210\u529f\u3002\n\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u5927\u898f\u6a21\u7684 4D \u8cc7\u6599\u548c\u6709\u6548\u7684\u6a21\u578b\u8a2d\u8a08\uff0c3D \u548c 4D \u751f\u6210\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5efa\u8b70\u901a\u904e\u5229\u7528\u65e5\u5e38\u751f\u6d3b\u4e2d\u5e38\u898b\u7684\u76f8\u6a5f\u548c\u7269\u9ad4\u52d5\u4f5c\u4f86\u5171\u540c\u7814\u7a76\u4e00\u822c\u7684 3D \u548c 4D \u751f\u6210\u3002\u7531\u65bc\u793e\u7fa4\u4e2d\u7f3a\u4e4f\u771f\u5be6\u4e16\u754c\u7684 4D \u8cc7\u6599\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u4e00\u500b\u8cc7\u6599\u7b56\u5c55\u7ba1\u9053\uff0c\u5f9e\u5f71\u7247\u4e2d\u53d6\u5f97\u76f8\u6a5f\u59ff\u52e2\u548c\u7269\u9ad4\u904b\u52d5\u5f37\u5ea6\u3002\u57fa\u65bc\u9019\u500b\u7ba1\u9053\uff0c\u6211\u5011\u5f15\u9032\u4e00\u500b\u5927\u898f\u6a21\u7684\u771f\u5be6\u4e16\u754c 4D \u5834\u666f\u8cc7\u6599\u96c6\uff1aCamVid-30K\u3002\n\u901a\u904e\u5229\u7528\u6240\u6709 3D \u548c 4D \u8cc7\u6599\uff0c\u6211\u5011\u958b\u767c\u4e86\u6211\u5011\u7684\u67b6\u69cb GenXD\uff0c\u5b83\u5141\u8a31\u6211\u5011\u7522\u751f\u4efb\u4f55 3D \u6216 4D \u5834\u666f\u3002\u6211\u5011\u63d0\u51fa\u4e86\u591a\u8996\u89d2\u6642\u9593\u6a21\u7d44\uff0c\u5b83\u53ef\u4ee5\u89e3\u958b\u76f8\u6a5f\u548c\u7269\u9ad4\u7684\u52d5\u4f5c\uff0c\u5f9e 3D \u548c 4D \u8cc7\u6599\u4e2d\u7121\u7e2b\u5b78\u7fd2\u3002\u6b64\u5916\uff0cGenXD \u4f7f\u7528\u906e\u7f69\u6f5b\u5728\u689d\u4ef6\u4f86\u652f\u63f4\u5404\u7a2e\u689d\u4ef6\u6aa2\u8996\u3002GenXD \u53ef\u4ee5\u7522\u751f\u5f71\u7247\uff0c\u9019\u4e9b\u5f71\u7247\u9075\u5faa\u76f8\u6a5f\u8ecc\u8de1\uff0c\u4ee5\u53ca\u53ef\u4ee5\u63d0\u5347\u5230 3D \u8868\u793a\u7684\u4e00\u81f4 3D \u8996\u5716\u3002\u6211\u5011\u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u548c\u5408\u6210\u8cc7\u6599\u96c6\u4e0a\u57f7\u884c\u5ee3\u6cdb\u7684\u8a55\u4f30\uff0c\u8b49\u660e\u4e86 GenXD \u5728 3D \u548c 4D \u751f\u6210\u4e2d\u8207\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\u7684\u6709\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "author": "Yuyang Zhao et.al.", "authors": "Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, Lijuan Wang", "id": "2411.02319v1", "paper_url": "http://arxiv.org/abs/2411.02319v1", "repo": "null"}}