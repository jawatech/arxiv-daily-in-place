{"2407.04181": {"publish_time": "2024-07-04", "title": "Orchestrating LLMs with Different Personalizations", "paper_summary": "This paper presents a novel approach to aligning large language models (LLMs)\nwith individual human preferences, sometimes referred to as Reinforcement\nLearning from \\textit{Personalized} Human Feedback (RLPHF). Given stated\npreferences along multiple dimensions, such as helpfulness, conciseness, or\nhumor, the goal is to create an LLM without re-training that best adheres to\nthis specification. Starting from specialized expert LLMs, each trained for one\nsuch particular preference dimension, we propose a black-box method that merges\ntheir outputs on a per-token level. We train a lightweight Preference Control\nModel (PCM) that dynamically translates the preference description and current\ncontext into next-token prediction weights. By combining the expert models'\noutputs at the token level, our approach dynamically generates text that\noptimizes the given preference. Empirical tests show that our method matches or\nsurpasses existing preference merging techniques, providing a scalable,\nefficient alternative to fine-tuning LLMs for individual personalization.", "paper_summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u7528\u65bc\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u500b\u4eba\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\uff0c\u6709\u6642\u7a31\u70ba\u5f9e\u300c\u500b\u4eba\u5316\u300d\u4eba\u985e\u56de\u994b\u4e2d\u9032\u884c\u5f37\u5316\u5b78\u7fd2 (RLPHF)\u3002\u7d66\u5b9a\u591a\u7dad\u5ea6\u7684\u65e2\u5b9a\u504f\u597d\uff0c\u4f8b\u5982\u6709\u5e6b\u52a9\u3001\u7c21\u6f54\u6216\u5e7d\u9ed8\uff0c\u76ee\u6a19\u662f\u5275\u5efa\u4e00\u500b\u7121\u9700\u91cd\u65b0\u8a13\u7df4\u7684 LLM\uff0c\u4ee5\u6700\u4f73\u65b9\u5f0f\u9075\u5b88\u6b64\u898f\u7bc4\u3002\u5f9e\u91dd\u5c0d\u4e00\u500b\u6b64\u985e\u7279\u5b9a\u504f\u597d\u7dad\u5ea6\u9032\u884c\u8a13\u7df4\u7684\u5c08\u696d\u5c08\u5bb6 LLM \u958b\u59cb\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u9ed1\u76d2\u65b9\u6cd5\uff0c\u5c07\u5176\u8f38\u51fa\u5728\u6bcf\u500b\u4ee3\u5e63\u5c64\u7d1a\u5408\u4f75\u3002\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b\u8f15\u91cf\u7d1a\u504f\u597d\u63a7\u5236\u6a21\u578b (PCM)\uff0c\u5b83\u6703\u52d5\u614b\u5730\u5c07\u504f\u597d\u63cf\u8ff0\u548c\u7576\u524d\u5167\u5bb9\u8f49\u63db\u70ba\u4e0b\u4e00\u500b\u4ee3\u5e63\u9810\u6e2c\u6b0a\u91cd\u3002\u900f\u904e\u5728\u4ee3\u5e63\u5c64\u7d1a\u7d50\u5408\u5c08\u5bb6\u6a21\u578b\u7684\u8f38\u51fa\uff0c\u6211\u5011\u7684\u505a\u6cd5\u6703\u52d5\u614b\u7522\u751f\u6700\u4f73\u5316\u65e2\u5b9a\u504f\u597d\u7684\u6587\u5b57\u3002\u5be6\u8b49\u6e2c\u8a66\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u7b26\u5408\u6216\u8d85\u8d8a\u73fe\u6709\u7684\u504f\u597d\u5408\u4f75\u6280\u8853\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u3001\u6709\u6548\u7387\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u65bc\u5fae\u8abf\u500b\u4eba\u5316 LLM\u3002", "author": "Jin Peng Zhou et.al.", "authors": "Jin Peng Zhou, Katie Z Luo, Jingwen Gu, Jason Yuan, Kilian Q. Weinberger, Wen Sun", "id": "2407.04181v1", "paper_url": "http://arxiv.org/abs/2407.04181v1", "repo": "null"}}