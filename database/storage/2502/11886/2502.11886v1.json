{"2502.11886": {"publish_time": "2025-02-17", "title": "LIMR: Less is More for RL Scaling", "paper_summary": "In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.", "paper_summary_zh": "<paragraph>\u5728\u9019\u7bc7\u8ad6\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u554f\u984c\uff1a\u7a76\u7adf\u662f\u4ec0\u9ebc\u6c7a\u5b9a\u4e86 RL \u8a13\u7df4\u8cc7\u6599\u589e\u5f37\u8a9e\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027\uff1f\u96d6\u7136\u6700\u8fd1\u7684\u9032\u5c55\uff0c\u4f8b\u5982 o1\u3001Deepseek R1 \u548c Kimi1.5\uff0c\u5c55\u793a\u4e86 RL \u7684\u6f5b\u529b\uff0c\u4f46\u7f3a\u4e4f\u95dc\u65bc\u8a13\u7df4\u8cc7\u6599\u9700\u6c42\u7684\u900f\u660e\u5ea6\u963b\u7919\u4e86\u7cfb\u7d71\u5316\u7684\u9032\u5c55\u3002\u5f9e\u6c92\u6709\u84b8\u993e\u7684\u57fa\u672c\u6a21\u578b\u76f4\u63a5\u958b\u59cb\uff0c\u6211\u5011\u6311\u6230\u4e86\u64f4\u5145 RL \u8a13\u7df4\u8cc7\u6599\u672c\u8cea\u4e0a\u5c31\u6703\u63d0\u5347\u6548\u80fd\u7684\u5047\u8a2d\u3002\u6211\u5011\u8b49\u660e\uff0c\u7b56\u7565\u6027\u5730\u9078\u51fa\u50c5 1,389 \u500b\u6a23\u672c\u7684\u5b50\u96c6\u5c31\u80fd\u52dd\u904e\u5b8c\u6574\u7684 8,523 \u500b\u6a23\u672c\u8cc7\u6599\u96c6\u3002\u6211\u5011\u5f15\u5165\u4e86\u5b78\u7fd2\u5f71\u97ff\u529b\u6e2c\u91cf (LIM)\uff0c\u9019\u662f\u4e00\u7a2e\u81ea\u52d5\u5316\u65b9\u6cd5\uff0c\u7528\u4f86\u8a55\u4f30\u548c\u512a\u5148\u8655\u7406\u8a13\u7df4\u6a23\u672c\uff0c\u6839\u64da\u5b83\u5011\u8207\u6a21\u578b\u5b78\u7fd2\u8ecc\u8de1\u7684\u4e00\u81f4\u6027\uff0c\u80fd\u6709\u6548\u5229\u7528\u8cc7\u6e90\u548c\u64f4\u5145\u5be6\u4f5c\u3002\u6211\u5011\u7684\u65b9\u6cd5\u4f7f\u7528\u50c5 1,389 \u500b\u6a23\u672c\u5c31\u80fd\u9054\u5230\u8207\u4f7f\u7528\u5b8c\u6574\u7684 8,523 \u500b\u6a23\u672c\u8cc7\u6599\u96c6\u76f8\u7576\u751a\u81f3\u66f4\u4f73\u7684\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u96d6\u7136\u6700\u8fd1\u8cc7\u6599\u6709\u6548\u7387\u7684\u65b9\u6cd5\uff08\u4f8b\u5982 LIMO \u548c s1\uff09\u5728 32B \u898f\u6a21\u7684\u6a21\u578b\u4e0a\u5c55\u73fe\u4e86\u524d\u666f\uff0c\u4f46\u6211\u5011\u767c\u73fe\u5b83\u5728 7B \u898f\u6a21\u4e0a\u900f\u904e\u76e3\u7763\u5fae\u8abf (SFT) \u7684\u8868\u73fe\u5927\u5e45\u843d\u5f8c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u5011\u57fa\u65bc RL \u7684 LIMR \u5728 AIME24 \u4e0a\u9054\u5230\u4e86\u9ad8\u51fa 16.7% \u7684\u6e96\u78ba\u5ea6\uff0c\u4e26\u5728 MATH500 \u4e0a\u6bd4 LIMO \u548c s1 \u5206\u5225\u9ad8\u51fa 13.0% \u548c 22.2%\u3002\u9019\u4e9b\u7d50\u679c\u5f9e\u6839\u672c\u4e0a\u6539\u8b8a\u4e86\u6211\u5011\u5c0d LLM \u4e2d RL \u64f4\u5145\u7684\u7406\u89e3\uff0c\u8b49\u660e\u7cbe\u78ba\u7684\u6a23\u672c\u9078\u53d6\uff0c\u800c\u975e\u8cc7\u6599\u898f\u6a21\uff0c\u53ef\u80fd\u662f\u89e3\u9396\u589e\u5f37\u63a8\u7406\u80fd\u529b\u7684\u95dc\u9375\u3002\u70ba\u4e86\u53ef\u91cd\u88fd\u7684\u7814\u7a76\u548c\u672a\u4f86\u7684\u5275\u65b0\uff0c\u6211\u5011\u958b\u653e\u539f\u59cb\u78bc LIMR\uff0c\u5305\u62ec LIM \u7684\u5be6\u4f5c\u3001\u8a13\u7df4\u548c\u8a55\u4f30\u7a0b\u5f0f\u78bc\u3001\u7b56\u5c55\u7684\u8cc7\u6599\u96c6\uff0c\u4ee5\u53ca\u5728 https://github.com/GAIR-NLP/LIMR \u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u3002</paragraph>", "author": "Xuefeng Li et.al.", "authors": "Xuefeng Li, Haoyang Zou, Pengfei Liu", "id": "2502.11886v1", "paper_url": "http://arxiv.org/abs/2502.11886v1", "repo": "null"}}