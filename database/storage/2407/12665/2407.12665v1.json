{"2407.12665": {"publish_time": "2024-07-17", "title": "Patch-Level Training for Large Language Models", "paper_summary": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5$\\times$, without compromising the\nmodel performance compared to token-level training. Source code:\n\\url{https://github.com/shaochenze/PatchTrain}.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u5176\u8a13\u7df4\u6548\u7387\u5df2\u6210\u70ba\u95dc\u9375\u554f\u984c\u3002\u50b3\u7d71\u4e0a\uff0cLLM \u88ab\u8a13\u7df4\u4f86\u9810\u6e2c\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u500b\u7b26\u865f\u3002\u5118\u7ba1\u7b26\u865f\u7d1a\u5225\u8a13\u7df4\u5f88\u6210\u529f\uff0c\u4f46\u7531\u65bc\u9700\u8981\u8655\u7406\u5927\u91cf\u7b26\u865f\uff0c\u56e0\u6b64\u6703\u7522\u751f\u76f8\u7576\u5927\u7684\u904b\u7b97\u6210\u672c\u3002\u70ba\u4e86\u7de9\u89e3\u9019\u500b\u554f\u984c\uff0c\u672c\u6587\u5f15\u5165\u4e86 LLM \u7684\u5340\u584a\u7d1a\u8a13\u7df4\uff0c\u5b83\u901a\u904e\u5c07\u591a\u500b\u7b26\u865f\u58d3\u7e2e\u6210\u55ae\u500b\u5340\u584a\u4f86\u7e2e\u77ed\u5e8f\u5217\u9577\u5ea6\u3002\u5728\u5340\u584a\u7d1a\u8a13\u7df4\u671f\u9593\uff0c\u6211\u5011\u5c07\u8f03\u77ed\u7684\u5340\u584a\u5e8f\u5217\u63d0\u4f9b\u7d66\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u8a13\u7df4\u5b83\u9810\u6e2c\u4e0b\u4e00\u500b\u5340\u584a\uff0c\u5f9e\u800c\u4ee5\u986f\u8457\u964d\u4f4e\u7684\u904b\u7b97\u6210\u672c\u8655\u7406\u5927\u90e8\u5206\u8a13\u7df4\u8cc7\u6599\u3002\u5728\u6b64\u4e4b\u5f8c\uff0c\u6a21\u578b\u7e7c\u7e8c\u5c0d\u5269\u9918\u7684\u8a13\u7df4\u8cc7\u6599\u9032\u884c\u7b26\u865f\u7d1a\u8a13\u7df4\uff0c\u4ee5\u8207\u63a8\u7406\u6a21\u5f0f\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u5404\u7a2e\u6a21\u578b\uff08370M-2.7B \u53c3\u6578\uff09\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5340\u584a\u7d1a\u8a13\u7df4\u53ef\u4ee5\u5c07\u6574\u9ad4\u904b\u7b97\u6210\u672c\u964d\u4f4e\u5230 0.5 \u500d\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u6a21\u578b\u6548\u80fd\uff0c\u9019\u8207\u7b26\u865f\u7d1a\u8a13\u7df4\u76f8\u6bd4\u3002\u539f\u59cb\u78bc\uff1a\\url{https://github.com/shaochenze/PatchTrain}\u3002", "author": "Chenze Shao et.al.", "authors": "Chenze Shao, Fandong Meng, Jie Zhou", "id": "2407.12665v1", "paper_url": "http://arxiv.org/abs/2407.12665v1", "repo": "https://github.com/shaochenze/patchtrain"}}