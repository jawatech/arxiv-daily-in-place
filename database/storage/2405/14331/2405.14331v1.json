{"2405.14331": {"publish_time": "2024-05-23", "title": "LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision", "paper_summary": "Prototypical parts networks combine the power of deep learning with the\nexplainability of case-based reasoning to make accurate, interpretable\ndecisions. They follow the this looks like that reasoning, representing each\nprototypical part with patches from training images. However, a single image\npatch comprises multiple visual features, such as color, shape, and texture,\nmaking it difficult for users to identify which feature is important to the\nmodel.\n  To reduce this ambiguity, we introduce the Lucid Prototypical Parts Network\n(LucidPPN), a novel prototypical parts network that separates color prototypes\nfrom other visual features. Our method employs two reasoning branches: one for\nnon-color visual features, processing grayscale images, and another focusing\nsolely on color information. This separation allows us to clarify whether the\nmodel's decisions are based on color, shape, or texture. Additionally, LucidPPN\nidentifies prototypical parts corresponding to semantic parts of classified\nobjects, making comparisons between data classes more intuitive, e.g., when two\nbird species might differ primarily in belly color.\n  Our experiments demonstrate that the two branches are complementary and\ntogether achieve results comparable to baseline methods. More importantly,\nLucidPPN generates less ambiguous prototypical parts, enhancing user\nunderstanding.", "paper_summary_zh": "\u539f\u578b\u96f6\u4ef6\u7db2\u8def\u7d50\u5408\u6df1\u5ea6\u5b78\u7fd2\u7684\u529b\u91cf\u8207\u6848\u4f8b\u63a8\u7406\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u505a\u51fa\u6e96\u78ba\u4e14\u53ef\u8a6e\u91cb\u7684\u6c7a\u7b56\u3002\u5b83\u5011\u9075\u5faa\u6b64\u770b\u8d77\u4f86\u50cf\u90a3\u6a23\u7684\u63a8\u7406\uff0c\u7528\u8a13\u7df4\u5f71\u50cf\u4e2d\u7684\u88dc\u4e01\u8868\u793a\u6bcf\u500b\u539f\u578b\u96f6\u4ef6\u3002\u7136\u800c\uff0c\u55ae\u4e00\u5f71\u50cf\u88dc\u4e01\u5305\u542b\u591a\u7a2e\u8996\u89ba\u7279\u5fb5\uff0c\u4f8b\u5982\u984f\u8272\u3001\u5f62\u72c0\u548c\u7d0b\u7406\uff0c\u4f7f\u7528\u6236\u96e3\u4ee5\u8fa8\u5225\u54ea\u500b\u7279\u5fb5\u5c0d\u6a21\u578b\u5f88\u91cd\u8981\u3002\n\u70ba\u4e86\u6e1b\u5c11\u9019\u7a2e\u6b67\u7fa9\uff0c\u6211\u5011\u5f15\u5165\u4e86 Lucid \u539f\u578b\u96f6\u4ef6\u7db2\u8def (LucidPPN)\uff0c\u4e00\u7a2e\u5c07\u984f\u8272\u539f\u578b\u8207\u5176\u4ed6\u8996\u89ba\u7279\u5fb5\u5206\u958b\u7684\u65b0\u578b\u539f\u578b\u96f6\u4ef6\u7db2\u8def\u3002\u6211\u5011\u7684\u6a21\u578b\u63a1\u7528\u5169\u500b\u63a8\u7406\u5206\u652f\uff1a\u4e00\u500b\u7528\u65bc\u975e\u8272\u5f69\u8996\u89ba\u7279\u5fb5\uff0c\u8655\u7406\u7070\u968e\u5f71\u50cf\uff0c\u53e6\u4e00\u500b\u5247\u5c08\u6ce8\u65bc\u8272\u5f69\u8cc7\u8a0a\u3002\u9019\u7a2e\u5206\u96e2\u8b93\u6211\u5011\u80fd\u5920\u91d0\u6e05\u6a21\u578b\u7684\u6c7a\u7b56\u662f\u57fa\u65bc\u984f\u8272\u3001\u5f62\u72c0\u9084\u662f\u7d0b\u7406\u3002\u6b64\u5916\uff0cLucidPPN \u8b58\u5225\u8207\u5206\u985e\u7269\u4ef6\u7684\u8a9e\u7fa9\u96f6\u4ef6\u76f8\u61c9\u7684\u539f\u578b\u96f6\u4ef6\uff0c\u8b93\u8cc7\u6599\u985e\u5225\u4e4b\u9593\u7684\u6bd4\u8f03\u66f4\u76f4\u89c0\uff0c\u4f8b\u5982\uff0c\u7576\u5169\u500b\u9ce5\u985e\u7269\u7a2e\u53ef\u80fd\u4e3b\u8981\u5728\u8179\u90e8\u984f\u8272\u4e0a\u6709\u6240\u4e0d\u540c\u6642\u3002\n\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\u9019\u5169\u500b\u5206\u652f\u662f\u4e92\u88dc\u7684\uff0c\u4e26\u4e14\u5171\u540c\u9054\u6210\u8207\u57fa\u6e96\u65b9\u6cd5\u76f8\u7576\u7684\u7d50\u679c\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0cLucidPPN \u7522\u751f\u7684\u539f\u578b\u96f6\u4ef6\u8f03\u4e0d\u6a21\u7a1c\u5169\u53ef\uff0c\u589e\u5f37\u4f7f\u7528\u8005\u7684\u7406\u89e3\u529b\u3002", "author": "Mateusz Pach et.al.", "authors": "Mateusz Pach, Dawid Rymarczyk, Koryna Lewandowska, Jacek Tabor, Bartosz Zieli\u0144ski", "id": "2405.14331v1", "paper_url": "http://arxiv.org/abs/2405.14331v1", "repo": "null"}}