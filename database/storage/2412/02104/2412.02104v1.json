{"2412.02104": {"publish_time": "2024-12-03", "title": "Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey", "paper_summary": "The rapid development of Artificial Intelligence (AI) has revolutionized\nnumerous fields, with large language models (LLMs) and computer vision (CV)\nsystems driving advancements in natural language understanding and visual\nprocessing, respectively. The convergence of these technologies has catalyzed\nthe rise of multimodal AI, enabling richer, cross-modal understanding that\nspans text, vision, audio, and video modalities. Multimodal large language\nmodels (MLLMs), in particular, have emerged as a powerful framework,\ndemonstrating impressive capabilities in tasks like image-text generation,\nvisual question answering, and cross-modal retrieval. Despite these\nadvancements, the complexity and scale of MLLMs introduce significant\nchallenges in interpretability and explainability, essential for establishing\ntransparency, trustworthiness, and reliability in high-stakes applications.\nThis paper provides a comprehensive survey on the interpretability and\nexplainability of MLLMs, proposing a novel framework that categorizes existing\nresearch across three perspectives: (I) Data, (II) Model, (III) Training \\&\nInference. We systematically analyze interpretability from token-level to\nembedding-level representations, assess approaches related to both architecture\nanalysis and design, and explore training and inference strategies that enhance\ntransparency. By comparing various methodologies, we identify their strengths\nand limitations and propose future research directions to address unresolved\nchallenges in multimodal explainability. This survey offers a foundational\nresource for advancing interpretability and transparency in MLLMs, guiding\nresearchers and practitioners toward developing more accountable and robust\nmultimodal AI systems.", "paper_summary_zh": "<paragraph>\u4eba\u5de5\u667a\u6167 (AI) \u7684\u5feb\u901f\u767c\u5c55\uff0c\u5df2\u7d93\u5fb9\u5e95\u6539\u8b8a\u4e86\u8a31\u591a\u9818\u57df\uff0c\u5176\u4e2d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u96fb\u8166\u8996\u89ba (CV) \u7cfb\u7d71\u5206\u5225\u63a8\u52d5\u4e86\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u8996\u89ba\u8655\u7406\u7684\u9032\u5c55\u3002\u9019\u4e9b\u6280\u8853\u7684\u878d\u5408\u50ac\u5316\u4e86\u591a\u6a21\u614b AI \u7684\u8208\u8d77\uff0c\u5be6\u73fe\u4e86\u8de8\u6a21\u614b\u7684\u8c50\u5bcc\u7406\u89e3\uff0c\u6db5\u84cb\u6587\u5b57\u3001\u8996\u89ba\u3001\u97f3\u8a0a\u548c\u8996\u8a0a\u6a21\u614b\u3002\u7279\u5225\u662f\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM)\uff0c\u5df2\u7d93\u6210\u70ba\u4e00\u500b\u5f37\u5927\u7684\u67b6\u69cb\uff0c\u5728\u5f71\u50cf\u6587\u5b57\u751f\u6210\u3001\u8996\u89ba\u554f\u984c\u89e3\u7b54\u548c\u8de8\u6a21\u614b\u6aa2\u7d22\u7b49\u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u9032\u5c55\uff0cMLLM \u7684\u8907\u96dc\u6027\u548c\u898f\u6a21\u5728\u53ef\u89e3\u91cb\u6027\u548c\u53ef\u8aaa\u660e\u6027\u65b9\u9762\u5e36\u4f86\u4e86\u91cd\u5927\u7684\u6311\u6230\uff0c\u800c\u9019\u5c0d\u65bc\u5728\u9ad8\u98a8\u96aa\u61c9\u7528\u4e2d\u5efa\u7acb\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u5c0d MLLM \u7684\u53ef\u89e3\u91cb\u6027\u548c\u53ef\u8aaa\u660e\u6027\u7684\u5168\u9762\u8abf\u67e5\uff0c\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u67b6\u69cb\uff0c\u5c07\u73fe\u6709\u7684\u7814\u7a76\u5206\u70ba\u4e09\u500b\u89c0\u9ede\uff1a(I) \u8cc7\u6599\u3001(II) \u6a21\u578b\u3001(III) \u8a13\u7df4\u548c\u63a8\u7406\u3002\u6211\u5011\u7cfb\u7d71\u5730\u5206\u6790\u4e86\u5f9e\u7b26\u865f\u5c64\u7d1a\u5230\u5d4c\u5165\u5c64\u7d1a\u7684\u8868\u793a\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u8a55\u4f30\u8207\u67b6\u69cb\u5206\u6790\u548c\u8a2d\u8a08\u76f8\u95dc\u7684\u65b9\u6cd5\uff0c\u4e26\u63a2\u8a0e\u4e86\u589e\u5f37\u900f\u660e\u5ea6\u7684\u8a13\u7df4\u548c\u63a8\u7406\u7b56\u7565\u3002\u900f\u904e\u6bd4\u8f03\u5404\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u627e\u51fa\u5b83\u5011\u7684\u512a\u9ede\u548c\u9650\u5236\uff0c\u4e26\u63d0\u51fa\u672a\u4f86\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u89e3\u6c7a\u591a\u6a21\u614b\u53ef\u8aaa\u660e\u6027\u4e2d\u5c1a\u672a\u89e3\u6c7a\u7684\u6311\u6230\u3002\u672c\u8abf\u67e5\u63d0\u4f9b\u4e86\u4e00\u500b\u57fa\u790e\u8cc7\u6e90\uff0c\u7528\u65bc\u63a8\u9032 MLLM \u7684\u53ef\u89e3\u91cb\u6027\u548c\u900f\u660e\u5ea6\uff0c\u5f15\u5c0e\u7814\u7a76\u4eba\u54e1\u548c\u5be6\u52d9\u4eba\u54e1\u958b\u767c\u66f4\u8ca0\u8cac\u4efb\u3001\u66f4\u5f37\u5927\u7684\u591a\u6a21\u614b AI \u7cfb\u7d71\u3002</paragraph>", "author": "Yunkai Dang et.al.", "authors": "Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, Yong Liu, Jing Shao, Hui Xiong, Xuming Hu", "id": "2412.02104v1", "paper_url": "http://arxiv.org/abs/2412.02104v1", "repo": "null"}}