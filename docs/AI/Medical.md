
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-17**|**The Disparate Benefits of Deep Ensembles**|Kajetan Schweighofer et.al.|[2410.13831v1](http://arxiv.org/abs/2410.13831v1)|null|
|**2024-10-17**|**Scaling Wearable Foundation Models**|Girish Narayanswamy et.al.|[2410.13638v1](http://arxiv.org/abs/2410.13638v1)|null|
|**2024-10-17**|**MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling**|Yakun Zhu et.al.|[2410.13610v1](http://arxiv.org/abs/2410.13610v1)|null|
|**2024-10-17**|**OAH-Net: A Deep Neural Network for Hologram Reconstruction of Off-axis Digital Holographic Microscope**|Wei Liu et.al.|[2410.13592v1](http://arxiv.org/abs/2410.13592v1)|null|
|**2024-10-17**|**RGB to Hyperspectral: Spectral Reconstruction for Enhanced Surgical Imaging**|Tobias Czempiel et.al.|[2410.13570v1](http://arxiv.org/abs/2410.13570v1)|null|
|**2024-10-17**|**Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?**|Che Liu et.al.|[2410.13523v1](http://arxiv.org/abs/2410.13523v1)|null|
|**2024-10-17**|**Representation Learning of Structured Data for Medical Foundation Models**|Vijay Prakash Dwivedi et.al.|[2410.13351v1](http://arxiv.org/abs/2410.13351v1)|null|
|**2024-10-17**|**Active inference and deep generative modeling for cognitive ultrasound**|Ruud JG van Sloun et.al.|[2410.13310v1](http://arxiv.org/abs/2410.13310v1)|null|
|**2024-10-17**|**Hiformer: Hybrid Frequency Feature Enhancement Inverted Transformer for Long-Term Wind Power Prediction**|Chongyang Wan et.al.|[2410.13303v1](http://arxiv.org/abs/2410.13303v1)|null|
|**2024-10-17**|**CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy**|Mian Zhang et.al.|[2410.13218v1](http://arxiv.org/abs/2410.13218v1)|null|
|**2024-10-17**|**MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records through Hierarchical Guided-Topic Modeling**|Ruohan Wang et.al.|[2410.13217v1](http://arxiv.org/abs/2410.13217v1)|null|
|**2024-10-17**|**LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch**|Caigao Jiang et.al.|[2410.13213v1](http://arxiv.org/abs/2410.13213v1)|[link](https://github.com/caigaojiang/llmopt)|
|**2024-10-17**|**MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback**|Zonghai Yao et.al.|[2410.13191v1](http://arxiv.org/abs/2410.13191v1)|null|
|**2024-10-16**|**Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information**|Yingya Li et.al.|[2410.12774v1](http://arxiv.org/abs/2410.12774v1)|null|
|**2024-10-16**|**FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**|Zhenheng Tang et.al.|[2410.12707v1](http://arxiv.org/abs/2410.12707v1)|null|
|**2024-10-16**|**Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2**|Mohamad Abdi et.al.|[2410.12686v2](http://arxiv.org/abs/2410.12686v2)|null|
|**2024-10-16**|**Cascade learning in multi-task encoder-decoder networks for concurrent bone segmentation and glenohumeral joint assessment in shoulder CT scans**|Luca Marsilio et.al.|[2410.12641v1](http://arxiv.org/abs/2410.12641v1)|null|
|**2024-10-16**|**NSSI-Net: Multi-Concept Generative Adversarial Network for Non-Suicidal Self-Injury Detection Using High-Dimensional EEG Signals in a Semi-Supervised Learning Framework**|Zhen Liang et.al.|[2410.12159v1](http://arxiv.org/abs/2410.12159v1)|null|
|**2024-10-15**|**SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding**|Ying Chen et.al.|[2410.11761v1](http://arxiv.org/abs/2410.11761v1)|null|
|**2024-10-15**|**RS-MOCO: A deep learning-based topology-preserving image registration method for cardiac T1 mapping**|Chiyi Huang et.al.|[2410.11651v1](http://arxiv.org/abs/2410.11651v1)|null|
|**2024-10-15**|**Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development**|Tengfei Ma et.al.|[2410.11550v1](http://arxiv.org/abs/2410.11550v1)|null|
|**2024-10-15**|**AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data**|Xinjie Zhao et.al.|[2410.11531v1](http://arxiv.org/abs/2410.11531v1)|null|
|**2024-10-15**|**Explainable AI Methods for Multi-Omics Analysis: A Survey**|Ahmad Hussein et.al.|[2410.11910v1](http://arxiv.org/abs/2410.11910v1)|null|
|**2024-10-15**|**HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications**|Weijie Xu et.al.|[2410.11239v1](http://arxiv.org/abs/2410.11239v1)|null|
|**2024-10-15**|**SplitSEE: A Splittable Self-supervised Framework for Single-Channel EEG Representation Learning**|Rikuto Kotoge et.al.|[2410.11200v1](http://arxiv.org/abs/2410.11200v1)|null|
|**2024-10-14**|**EchoApex: A General-Purpose Vision Foundation Model for Echocardiography**|Abdoul Aziz Amadou et.al.|[2410.11092v2](http://arxiv.org/abs/2410.11092v2)|null|
|**2024-10-14**|**Parsing altered brain connectivity in neurodevelopmental disorders by integrating graph-based normative modeling and deep generative networks**|Rui Sherry Shen et.al.|[2410.11064v1](http://arxiv.org/abs/2410.11064v1)|null|
|**2024-10-14**|**Thinking LLMs: General Instruction Following with Thought Generation**|Tianhao Wu et.al.|[2410.10630v1](http://arxiv.org/abs/2410.10630v1)|null|
|**2024-10-14**|**BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using Multi-parametric MRI**|Shaohao Rui et.al.|[2410.10604v1](http://arxiv.org/abs/2410.10604v1)|null|
|**2024-10-14**|**Reproducible Machine Learning-based Voice Pathology Detection: Introducing the Pitch Difference Feature**|Jan Vrba et.al.|[2410.10537v1](http://arxiv.org/abs/2410.10537v1)|[link](https://github.com/aailab-uct/automated-robust-and-reproducible-voice-pathology-detection)|
|**2024-10-14**|**Study on the Helpfulness of Explainable Artificial Intelligence**|Tobias Labarta et.al.|[2410.11896v1](http://arxiv.org/abs/2410.11896v1)|null|
|**2024-10-14**|**Advancing Newborn Care: Precise Birth Time Detection Using AI-Driven Thermal Imaging with Adaptive Normalization**|Jorge Garc√≠a-Torres et.al.|[2410.10483v1](http://arxiv.org/abs/2410.10483v1)|[link](https://github.com/jtorres258/image-based-tob)|
|**2024-10-14**|**Affinity-Graph-Guided Contractive Learning for Pretext-Free Medical Image Segmentation with Minimal Annotation**|Zehua Cheng et.al.|[2410.10366v1](http://arxiv.org/abs/2410.10366v1)|null|
|**2024-10-14**|**Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning**|Hongyi Yuan et.al.|[2410.10144v1](http://arxiv.org/abs/2410.10144v1)|null|
|**2024-10-14**|**REHRSeg: Unleashing the Power of Self-Supervised Super-Resolution for Resource-Efficient 3D MRI Segmentation**|Zhiyun Song et.al.|[2410.10097v1](http://arxiv.org/abs/2410.10097v1)|null|
|**2024-10-13**|**IMAS: A Comprehensive Agentic Approach to Rural Healthcare Delivery**|Agasthya Gangavarapu et.al.|[2410.12868v1](http://arxiv.org/abs/2410.12868v1)|null|
|**2024-10-13**|**Adaptive Reasoning and Acting in Medical Language Agents**|Abhishek Dutta et.al.|[2410.10020v1](http://arxiv.org/abs/2410.10020v1)|null|
|**2024-10-13**|**Improving 3D Few-Shot Segmentation with Inference-Time Pseudo-Labeling**|Mohammad Mozafari et.al.|[2410.09967v1](http://arxiv.org/abs/2410.09967v1)|null|
|**2024-10-13**|**Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble for Zero-shot Learning**|Pengfei Jin et.al.|[2410.09908v1](http://arxiv.org/abs/2410.09908v1)|null|
|**2024-10-13**|**Equitable Access to Justice: Logical LLMs Show Promise**|Manuj Kant et.al.|[2410.09904v1](http://arxiv.org/abs/2410.09904v1)|null|
|**2024-10-13**|**Large-Scale 3D Medical Image Pre-training with Geometric Context Priors**|Linshan Wu et.al.|[2410.09890v1](http://arxiv.org/abs/2410.09890v1)|[link](https://github.com/luffy03/large-scale-medical)|
|**2024-10-13**|**HypomimiaCoach: An AU-based Digital Therapy System for Hypomimia Detection & Rehabilitation with Parkinson's Disease**|Yingjing Xu et.al.|[2410.09772v1](http://arxiv.org/abs/2410.09772v1)|null|
|**2024-10-13**|**STA-Unet: Rethink the semantic redundant for Medical Imaging Segmentation**|Vamsi Krishna Vasa et.al.|[2410.11578v1](http://arxiv.org/abs/2410.11578v1)|null|
|**2024-10-13**|**MIRAGE: Multimodal Identification and Recognition of Annotations in Indian General Prescriptions**|Tavish Mankash et.al.|[2410.09729v1](http://arxiv.org/abs/2410.09729v1)|null|
|**2024-10-13**|**3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation**|Hongxin Ding et.al.|[2410.10901v1](http://arxiv.org/abs/2410.10901v1)|null|
|**2024-10-12**|**Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions**|Abdullah Mamun et.al.|[2410.09643v1](http://arxiv.org/abs/2410.09643v1)|[link](https://github.com/ab9mamun/movesense)|
|**2024-10-12**|**Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health**|Abdullah Mamun et.al.|[2410.09635v1](http://arxiv.org/abs/2410.09635v1)|[link](https://github.com/ab9mamun/aimen)|
|**2024-10-11**|**AuD-Former: A Hierarchical Transformer Network for Multimodal Audio-Based Disease Prediction**|Jinjin Cai et.al.|[2410.09289v1](http://arxiv.org/abs/2410.09289v1)|null|
|**2024-10-11**|**LLMD: A Large Language Model for Interpreting Longitudinal Medical Records**|Robert Porter et.al.|[2410.12860v1](http://arxiv.org/abs/2410.12860v1)|null|
|**2024-10-11**|**Large Language Models for Medical OSCE Assessment: A Novel Approach to Transcript Analysis**|Ameer Hamza Shakur et.al.|[2410.12858v1](http://arxiv.org/abs/2410.12858v1)|null|
|**2024-10-11**|**Optimized Biomedical Question-Answering Services with LLM and Multi-BERT Integration**|Cheng Qian et.al.|[2410.12856v1](http://arxiv.org/abs/2410.12856v1)|null|
|**2024-10-11**|**Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models**|Yeeun Kim et.al.|[2410.08731v1](http://arxiv.org/abs/2410.08731v1)|null|
|**2024-10-11**|**ViT3D Alignment of LLaMA3: 3D Medical Image Report Generation**|Siyou Li et.al.|[2410.08588v1](http://arxiv.org/abs/2410.08588v1)|null|
|**2024-10-11**|**oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness**|Yu He Ke et.al.|[2410.08431v1](http://arxiv.org/abs/2410.08431v1)|null|
|**2024-10-10**|**VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis**|Andrew Hoopes et.al.|[2410.08397v1](http://arxiv.org/abs/2410.08397v1)|null|
|**2024-10-10**|**Optimizing Vital Sign Monitoring in Resource-Constrained Maternal Care: An RL-Based Restless Bandit Approach**|Niclas Boehmer et.al.|[2410.08377v1](http://arxiv.org/abs/2410.08377v1)|null|
|**2024-10-10**|**ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation**|L√©o Machado et.al.|[2410.07908v2](http://arxiv.org/abs/2410.07908v2)|null|
|**2024-10-10**|**Exploring ASR-Based Wav2Vec2 for Automated Speech Disorder Assessment: Insights and Analysis**|Tuan Nguyen et.al.|[2410.08250v1](http://arxiv.org/abs/2410.08250v1)|null|
|**2024-10-10**|**Forecasting mortality associated emergency department crowding**|Jalmari Nevanlinna et.al.|[2410.08247v1](http://arxiv.org/abs/2410.08247v1)|null|
|**2024-10-10**|**Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions**|Per Niklas Waaler et.al.|[2410.12848v1](http://arxiv.org/abs/2410.12848v1)|null|
|**2024-10-10**|**Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts**|Sukwon Yun et.al.|[2410.08245v1](http://arxiv.org/abs/2410.08245v1)|[link](https://github.com/unites-lab/flex-moe)|
|**2024-10-10**|**Artificial intelligence techniques in inherited retinal diseases: A review**|Han Trinh et.al.|[2410.09105v1](http://arxiv.org/abs/2410.09105v1)|null|
|**2024-10-10**|**Toward Relieving Clinician Burden by Automatically Generating Progress Notes using Interim Hospital Data**|Sarvesh Soni et.al.|[2410.12845v1](http://arxiv.org/abs/2410.12845v1)|null|
|**2024-10-10**|**Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare**|Nan Fang et.al.|[2410.07525v2](http://arxiv.org/abs/2410.07525v2)|null|
|**2024-10-09**|**A Two-Model Approach for Humour Style Recognition**|Mary Ogbuka Kenneth et.al.|[2410.12842v1](http://arxiv.org/abs/2410.12842v1)|null|
|**2024-10-09**|**Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel Parallelism for FPGA-Accelerated Processing**|Ismail Erbas et.al.|[2410.07364v1](http://arxiv.org/abs/2410.07364v1)|null|
|**2024-10-09**|**Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy**|Vivian Nguyen et.al.|[2410.07147v1](http://arxiv.org/abs/2410.07147v1)|null|
|**2024-10-09**|**Mental Disorders Detection in the Era of Large Language Models**|Gleb Kuzmin et.al.|[2410.07129v2](http://arxiv.org/abs/2410.07129v2)|null|
|**2024-10-09**|**MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders**|Cheng Li et.al.|[2410.06845v1](http://arxiv.org/abs/2410.06845v1)|[link](https://github.com/scarelette/mentalarena)|
|**2024-10-09**|**An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion**|Narjes Benameur et.al.|[2410.06818v1](http://arxiv.org/abs/2410.06818v1)|null|
|**2024-10-09**|**Deep Learning for Surgical Instrument Recognition and Segmentation in Robotic-Assisted Surgeries: A Systematic Review**|Fatimaelzahraa Ali Ahmed et.al.|[2410.07269v1](http://arxiv.org/abs/2410.07269v1)|null|
|**2024-10-08**|**Multimodal Representation Learning using Adaptive Graph Construction**|Weichen Huang et.al.|[2410.06395v1](http://arxiv.org/abs/2410.06395v1)|null|
|**2024-10-08**|**Skin Cancer Machine Learning Model Tone Bias**|James Pope et.al.|[2410.06385v1](http://arxiv.org/abs/2410.06385v1)|null|
|**2024-10-08**|**HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid**|Hemank Lamba et.al.|[2410.06370v2](http://arxiv.org/abs/2410.06370v2)|[link](https://github.com/dataminr-ai/humvi-dataset)|
|**2024-10-08**|**A Comparative Study of Hybrid Models in Health Misinformation Text Classification**|Mkululi Sikosana et.al.|[2410.06311v1](http://arxiv.org/abs/2410.06311v1)|null|
|**2024-10-08**|**Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging**|Ryota Tozuka et.al.|[2410.10869v1](http://arxiv.org/abs/2410.10869v1)|null|
|**2024-10-08**|**CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept**|YuXuan Wu et.al.|[2410.10866v1](http://arxiv.org/abs/2410.10866v1)|null|
|**2024-10-08**|**KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server**|Wenhao Wang et.al.|[2410.05725v2](http://arxiv.org/abs/2410.05725v2)|[link](https://github.com/wwh0411/knowledgesg)|
|**2024-10-08**|**Copiloting Diagnosis of Autism in Real Clinical Scenarios via LLMs**|Yi Jiang et.al.|[2410.05684v2](http://arxiv.org/abs/2410.05684v2)|null|
|**2024-10-08**|**NegMerge: Consensual Weight Negation for Strong Machine Unlearning**|Hyoseo Kim et.al.|[2410.05583v1](http://arxiv.org/abs/2410.05583v1)|[link](https://github.com/naver-ai/negmerge)|
|**2024-10-07**|**AI-Driven Early Mental Health Screening with Limited Data: Analyzing Selfies of Pregnant Women**|Gustavo A. Bas√≠lio et.al.|[2410.05450v1](http://arxiv.org/abs/2410.05450v1)|null|
|**2024-10-07**|**Improving Predictor Reliability with Selective Recalibration**|Thomas P. Zollo et.al.|[2410.05407v1](http://arxiv.org/abs/2410.05407v1)|null|
|**2024-10-07**|**CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures**|Ekaterina Sviridova et.al.|[2410.05235v2](http://arxiv.org/abs/2410.05235v2)|null|
|**2024-10-07**|**RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction**|Yuwei Zhang et.al.|[2410.05361v1](http://arxiv.org/abs/2410.05361v1)|null|
|**2024-10-07**|**Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization**|Rohan Reddy Mekala et.al.|[2410.05114v1](http://arxiv.org/abs/2410.05114v1)|null|
|**2024-10-07**|**Named Clinical Entity Recognition Benchmark**|Wadood M Abdul et.al.|[2410.05046v1](http://arxiv.org/abs/2410.05046v1)|[link](https://github.com/wadoodabdul/clinical_ner_benchmark)|
|**2024-10-07**|**Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data**|Manuel Brenner et.al.|[2410.04814v1](http://arxiv.org/abs/2410.04814v1)|null|
|**2024-10-07**|**$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization**|Dylan Zhang et.al.|[2410.04717v2](http://arxiv.org/abs/2410.04717v2)|null|
|**2024-10-07**|**Rule-based Data Selection for Large Language Models**|Xiaomin Li et.al.|[2410.04715v1](http://arxiv.org/abs/2410.04715v1)|null|
|**2024-10-07**|**Knowledge Graph Based Agent for Complex, Knowledge-Intensive QA in Medicine**|Xiaorui Su et.al.|[2410.04660v1](http://arxiv.org/abs/2410.04660v1)|null|
|**2024-10-06**|**Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task**|Chengyuan Xu et.al.|[2410.11860v1](http://arxiv.org/abs/2410.11860v1)|null|
|**2024-10-06**|**Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry (MWR) Breast Cancer Detection**|Christoforos Galazis et.al.|[2410.04636v1](http://arxiv.org/abs/2410.04636v1)|[link](https://github.com/cgalaz01/self_contrastive_mwr)|
|**2024-10-06**|**Semi-Markovian Planning to Coordinate Aerial and Maritime Medical Evacuation Platforms**|Mahdi Al-Husseini et.al.|[2410.04523v1](http://arxiv.org/abs/2410.04523v1)|null|
|**2024-10-06**|**Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support**|Abdul Muqtadir et.al.|[2410.10853v1](http://arxiv.org/abs/2410.10853v1)|null|
|**2024-10-06**|**On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts**|Toluwani Aremu et.al.|[2410.10850v2](http://arxiv.org/abs/2410.10850v2)|null|
|**2024-10-06**|**RespDiff: An End-to-End Multi-scale RNN Diffusion Model for Respiratory Waveform Estimation from PPG Signals**|Yuyang Miao et.al.|[2410.04366v1](http://arxiv.org/abs/2410.04366v1)|null|
|**2024-10-05**|**Applying Quantum Autoencoders for Time Series Anomaly Detection**|Robin Frehner et.al.|[2410.04154v2](http://arxiv.org/abs/2410.04154v2)|null|
|**2024-10-05**|**DAMMI:Daily Activities in a Psychologically Annotated Multi-Modal IoT dataset**|Mohsen Falah Rad et.al.|[2410.04152v1](http://arxiv.org/abs/2410.04152v1)|null|
|**2024-10-05**|**From Hospital to Portables: A Universal ECG Foundation Model Built on 10+ Million Diverse Recordings**|Jun Li et.al.|[2410.04133v1](http://arxiv.org/abs/2410.04133v1)|null|
|**2024-10-05**|**Taming the Tail: Leveraging Asymmetric Loss and Pade Approximation to Overcome Medical Image Long-Tailed Class Imbalance**|Pankhi Kashyap et.al.|[2410.04084v1](http://arxiv.org/abs/2410.04084v1)|null|

#### Abstracts
##### **The Disparate Benefits of Deep Ensembles**
2410.13831v1 by Kajetan Schweighofer, Adrian Arnaiz-Rodriguez, Sepp Hochreiter, Nuria Oliver

Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a
simple way to boost predictive performance. However, their impact on
algorithmic fairness is not well understood yet. Algorithmic fairness
investigates how a model's performance varies across different groups,
typically defined by protected attributes such as age, gender, or race. In this
work, we investigate the interplay between the performance gains from Deep
Ensembles and fairness. Our analysis reveals that they unevenly favor different
groups in what we refer to as a disparate benefits effect. We empirically
investigate this effect with Deep Ensembles applied to popular facial analysis
and medical imaging datasets, where protected group attributes are given and
find that it occurs for multiple established group fairness metrics, including
statistical parity and equal opportunity. Furthermore, we identify the
per-group difference in predictive diversity of ensemble members as the
potential cause of the disparate benefits effect. Finally, we evaluate
different approaches to reduce unfairness due to the disparate benefits effect.
Our findings show that post-processing is an effective method to mitigate this
unfairness while preserving the improved performance of Deep Ensembles.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Á•ûÁªèÁ∂≤Ë∑ØÁöÑÈõÜÂêàÔºåÊ∑±Â∫¶ÈõÜÂêàÔºåË¢´Âª£Ê≥õÁî®‰ΩúÊèêÂçáÈ†êÊ∏¨ÊïàËÉΩÁöÑÁ∞°ÂñÆÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂ∞çÊºîÁÆóÊ≥ïÂÖ¨Âπ≥ÊÄßÁöÑÂΩ±ÈüøÂ∞öÊú™Ë¢´ÂÖÖÂàÜÁêÜËß£„ÄÇÊºîÁÆóÊ≥ïÂÖ¨Âπ≥ÊÄßÊé¢Ë®éÊ®°ÂûãÁöÑÊïàËÉΩÂ¶Ç‰ΩïÂõ†‰∏çÂêåÁæ§ÁµÑËÄåÁï∞ÔºåÈÄô‰∫õÁæ§ÁµÑÈÄöÂ∏∏Áî±Âèó‰øùË≠∑ÁöÑÂ±¨ÊÄßÔºà‰æãÂ¶ÇÂπ¥ÈΩ°„ÄÅÊÄßÂà•ÊàñÁ®ÆÊóèÔºâÂÆöÁæ©„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÊ∑±Â∫¶ÈõÜÂêàÁöÑÊïàËÉΩÊèêÂçáËàáÂÖ¨Âπ≥ÊÄß‰πãÈñìÁöÑ‰∫§‰∫í‰ΩúÁî®„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂÆÉÂÄë‰∏çÂùáÂãªÂú∞ÂÅèÂ•Ω‰∏çÂêåÁæ§ÁµÑÔºåÊàëÂÄëÁ®±‰πãÁÇ∫‰∏çÂêåÁöÑÂ•ΩËôïÊïàÊáâ„ÄÇÊàëÂÄë‰ª•ÊáâÁî®ÊñºÊµÅË°åÈù¢ÈÉ®ÂàÜÊûêÂíåÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÁöÑÊ∑±Â∫¶ÈõÜÂêàÔºåÂ∞çÊ≠§ÊïàÊáâÈÄ≤Ë°åÂØ¶Ë≠âÊé¢Ë®éÔºåÂÖ∂‰∏≠Êèê‰æõ‰∫ÜÂèó‰øùË≠∑Áæ§ÁµÑÂ±¨ÊÄßÔºå‰∏¶ÁôºÁèæÂÆÉÁôºÁîüÂú®Â§öÂÄãÂ∑≤Âª∫Á´ãÁöÑÁæ§ÁµÑÂÖ¨Âπ≥ÊÄßÊåáÊ®ô‰∏≠ÔºåÂåÖÊã¨Áµ±Ë®àÂêåÁ≠âÊÄßÂíåÊ©üÊúÉÂùáÁ≠â„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÈ†êÊ∏¨Â§öÊ®£ÊÄßÂú®ÈõÜÂêàÊàêÂì°‰∏≠ÁöÑÁæ§ÁµÑÂ∑ÆÁï∞ÔºåË¶ñÁÇ∫‰∏çÂêåÁöÑÂ•ΩËôïÊïàÊáâÁöÑÊΩõÂú®ÂéüÂõ†„ÄÇÊúÄÂæåÔºåÊàëÂÄëË©ï‰º∞‰∫Ü‰∏çÂêåÁöÑÊñπÊ≥ïÔºå‰ª•Ê∏õÂ∞ëÁî±Êñº‰∏çÂêåÁöÑÂ•ΩËôïÊïàÊáâËÄåÂ∞éËá¥ÁöÑ‰∏çÂÖ¨Âπ≥ÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂæåËôïÁêÜÊòØ‰∏ÄÁ®ÆÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Ê∏õËºïÈÄôÁ®Æ‰∏çÂÖ¨Âπ≥ÊÄßÔºåÂêåÊôÇ‰øùÁïôÊ∑±Â∫¶ÈõÜÂêàÁöÑÊïàËÉΩÊèêÂçá„ÄÇ

##### **Scaling Wearable Foundation Models**
2410.13638v1 by Girish Narayanswamy, Xin Liu, Kumar Ayush, Yuzhe Yang, Xuhai Xu, Shun Liao, Jake Garrison, Shyam Tailor, Jake Sunshine, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Samy Abdel-Ghaffar, Daniel McDuff

Wearable sensors have become ubiquitous thanks to a variety of health
tracking features. The resulting continuous and longitudinal measurements from
everyday life generate large volumes of data; however, making sense of these
observations for scientific and actionable insights is non-trivial. Inspired by
the empirical success of generative modeling, where large neural networks learn
powerful representations from vast amounts of text, image, video, or audio
data, we investigate the scaling properties of sensor foundation models across
compute, data, and model size. Using a dataset of up to 40 million hours of
in-situ heart rate, heart rate variability, electrodermal activity,
accelerometer, skin temperature, and altimeter per-minute data from over
165,000 people, we create LSM, a multimodal foundation model built on the
largest wearable-signals dataset with the most extensive range of sensor
modalities to date. Our results establish the scaling laws of LSM for tasks
such as imputation, interpolation and extrapolation, both across time and
sensor modalities. Moreover, we highlight how LSM enables sample-efficient
downstream learning for tasks like exercise and activity recognition.

ÊëòË¶ÅÔºö<paragraph>Á©øÊà¥ÂºèÊÑüÊ∏¨Âô®Â∑≤ËÆäÂæóÁÑ°ÊâÄ‰∏çÂú®ÔºåÈÄôË¶ÅÊ≠∏ÂäüÊñºÂêÑÁ®ÆÂÅ•Â∫∑ËøΩËπ§ÂäüËÉΩ„ÄÇÂæûÊó•Â∏∏ÁîüÊ¥ª‰∏≠Áî¢ÁîüÁöÑÈÄ£Á∫å‰∏îÈï∑ÊúüÁöÑÊ∏¨ÈáèÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑË≥áÊñôÔºõÁÑ∂ËÄåÔºåË¶ÅËÆìÈÄô‰∫õËßÄÂØüÁµêÊûúÁî¢ÁîüÁßëÂ≠∏‰∏îÂèØË°åÁöÑË¶ãËß£‰∏¶ÈùûÊòì‰∫ã„ÄÇÂèóÂà∞ÁîüÊàêÂºèÂª∫Ê®°ÁöÑÁ∂ìÈ©óÊàêÂäüÂïüÁôºÔºåÂÖ∂‰∏≠Â§ßÂûãÁ•ûÁ∂ìÁ∂≤Ë∑ØÂæûÂ§ßÈáèÁöÑÊñáÂ≠ó„ÄÅÂΩ±ÂÉè„ÄÅÂΩ±ÁâáÊàñÈü≥Ë®äË≥áÊñô‰∏≠Â≠∏ÁøíÂº∑Â§ßÁöÑË°®ÂæµÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÊÑüÊ∏¨Âô®Âü∫Á§éÊ®°ÂûãÂú®ÈÅãÁÆó„ÄÅË≥áÊñôÂíåÊ®°ÂûãÂ§ßÂ∞èÊñπÈù¢ÁöÑË¶èÊ®°ÂåñÂ±¨ÊÄß„ÄÇÊàëÂÄë‰ΩøÁî®‰∏ÄÂÄãË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™Ë∂ÖÈÅé 165,000 ‰∫∫ÁöÑÈï∑ÈÅî 4,000 Ëê¨Â∞èÊôÇÁöÑÁèæÂ†¥ÂøÉÁéá„ÄÅÂøÉÁéáËÆäÁï∞ÊÄß„ÄÅÁöÆËÜöÈõªÊ¥ªÂãï„ÄÅÂä†ÈÄüÂ∫¶Ë®à„ÄÅÁöÆËÜöÊ∫´Â∫¶ÂíåÊØèÂàÜÈêòÈ´òÂ∫¶Ë®àË≥áÊñôÔºåÂª∫Á´ã‰∫Ü LSMÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÊ®°ÊÖãÂü∫Á§éÊ®°ÂûãÔºåÂª∫ÊßãÂú®ËøÑ‰ªäÁÇ∫Ê≠¢ÂÖ∑ÊúâÊúÄÂª£Ê≥õÊÑüÊ∏¨Âô®Ê®°ÂºèÁöÑÊúÄÂ§ßÁ©øÊà¥ÂºèË®äËôüË≥áÊñôÈõÜ‰∏ä„ÄÇÊàëÂÄëÁöÑÁµêÊûúÂª∫Á´ã‰∫Ü LSM ÁöÑË¶èÊ®°ÂåñÂÆöÂæãÔºåÈÅ©Áî®ÊñºÊôÇÈñìÂíåÊÑüÊ∏¨Âô®Ê®°ÂºèÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÂ°´Ë£ú„ÄÅÂÖßÊèíÂíåÂ§ñÊèí„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂº∑Ë™ø‰∫Ü LSM Â¶Ç‰ΩïÁÇ∫ÈÅãÂãïÂíåÊ¥ªÂãïËæ®Ë≠òÁ≠â‰ªªÂãôÂïüÁî®Ê®£Êú¨ÊúâÊïàÁéáÁöÑ‰∏ãÊ∏∏Â≠∏Áøí„ÄÇ</paragraph>

##### **MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling**
2410.13610v1 by Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, Shaoting Zhang

Integrating tools into Large Language Models (LLMs) has facilitated the
widespread application. Despite this, in specialized downstream task contexts,
reliance solely on tools is insufficient to fully address the complexities of
the real world. This particularly restricts the effective deployment of LLMs in
fields such as medicine. In this paper, we focus on the downstream tasks of
medical calculators, which use standardized tests to assess an individual's
health status. We introduce MeNTi, a universal agent architecture for LLMs.
MeNTi integrates a specialized medical toolkit and employs meta-tool and nested
calling mechanisms to enhance LLM tool utilization. Specifically, it achieves
flexible tool selection and nested tool calling to address practical issues
faced in intricate medical scenarios, including calculator selection, slot
filling, and unit conversion. To assess the capabilities of LLMs for
quantitative assessment throughout the clinical process of calculator
scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical
calculators to perform calculations and assess patient health status. CalcQA is
constructed by professional physicians and includes 100 case-calculator pairs,
complemented by a toolkit of 281 medical tools. The experimental results
demonstrate significant performance improvements with our framework. This
research paves new directions for applying LLMs in demanding scenarios of
medicine.

ÊëòË¶ÅÔºöÂ∞áÂ∑•ÂÖ∑Êï¥ÂêàÂà∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠‰øÉËøõ‰∫ÜÂª£Ê≥õÁöÑÊáâÁî®„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂú®Â∞àÊ•≠ÁöÑ‰∏ãÊ∏∏‰ªªÂãôÊÉÖÂ¢É‰∏≠ÔºåÂñÆÁç®‰æùË≥¥Â∑•ÂÖ∑‰∏çË∂≥‰ª•ÂÖÖÂàÜËß£Ê±∫ÁèæÂØ¶‰∏ñÁïåÁöÑË§áÈõúÊÄß„ÄÇÈÄôÁâπÂà•ÈôêÂà∂‰∫Ü LLM Âú®ÈÜ´Â≠∏Á≠âÈ†òÂüüÁöÑÊúâÊïàÈÉ®ÁΩ≤„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÈÜ´ÁôÇË®àÁÆóÂô®ÁöÑ‰∏ãÊ∏∏‰ªªÂãôÔºåÂÆÉ‰ΩøÁî®Ê®ôÊ∫ñÂåñÊ∏¨Ë©¶‰æÜË©ï‰º∞ÂÄã‰∫∫ÁöÑÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÇÊàëÂÄë‰ªãÁ¥π MeNTiÔºå‰∏ÄÁ®ÆÈÅ©Áî®Êñº LLM ÁöÑÈÄöÁî®‰ª£ÁêÜÊû∂Êßã„ÄÇMeNTi Êï¥Âêà‰∫Ü‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÈÜ´ÁôÇÂ∑•ÂÖ∑ÂåÖÔºå‰∏¶Êé°Áî®ÂÖÉÂ∑•ÂÖ∑ÂíåÂµåÂ•óÂëºÂè´Ê©üÂà∂‰æÜÂ¢ûÂº∑ LLM Â∑•ÂÖ∑ÁöÑÂà©Áî®Áéá„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂÆÉÂØ¶Áèæ‰∫ÜÈùàÊ¥ªÁöÑÂ∑•ÂÖ∑ÈÅ∏ÊìáÂíåÂµåÂ•óÂ∑•ÂÖ∑ÂëºÂè´Ôºå‰ª•Ëß£Ê±∫Ë§áÈõúÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠Èù¢Ëá®ÁöÑÂØ¶ÈöõÂïèÈ°åÔºåÂåÖÊã¨Ë®àÁÆóÂô®ÈÅ∏Êìá„ÄÅÊèíÊßΩÂ°´ÂÖÖÂíåÂñÆ‰ΩçËΩâÊèõ„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ LLM Âú®Ë®àÁÆóÂô®Â†¥ÊôØÁöÑÊï¥ÂÄãËá®Â∫äÈÅéÁ®ã‰∏≠ÈÄ≤Ë°åÈáèÂåñË©ï‰º∞ÁöÑËÉΩÂäõÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü CalcQA„ÄÇÊ≠§Âü∫Ê∫ñË¶ÅÊ±Ç LLM ‰ΩøÁî®ÈÜ´ÁôÇË®àÁÆóÂô®ÈÄ≤Ë°åË®àÁÆó‰∏¶Ë©ï‰º∞ÊÇ£ËÄÖÁöÑÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÇCalcQA Áî±Â∞àÊ•≠ÈÜ´ÁîüÁ∑®Âà∂ÔºåÂåÖÊã¨ 100 ÂÄãÊ°à‰æãË®àÁÆóÂô®Â∞çÔºå‰∏¶Ëºî‰ª• 281 ÂÄãÈÜ´ÁôÇÂ∑•ÂÖ∑ÁöÑÂ∑•ÂÖ∑ÂåÖ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊ°ÜÊû∂ÁöÑÈ°ØËëóÊïàËÉΩÊèêÂçá„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Âú®Ë¶ÅÊ±ÇÂö¥Ê†ºÁöÑÈÜ´Â≠∏Â†¥ÊôØ‰∏≠ÊáâÁî® LLM Èã™Âπ≥‰∫ÜÊñ∞ÁöÑÈÅìË∑Ø„ÄÇ

##### **OAH-Net: A Deep Neural Network for Hologram Reconstruction of Off-axis Digital Holographic Microscope**
2410.13592v1 by Wei Liu, Kerem Delikoyun, Qianyu Chen, Alperen Yildiz, Si Ko Myo, Win Sen Kuan, John Tshon Yit Soong, Matthew Edward Cove, Oliver Hayden, Hweekuan Lee

Off-axis digital holographic microscopy is a high-throughput, label-free
imaging technology that provides three-dimensional, high-resolution information
about samples, particularly useful in large-scale cellular imaging. However,
the hologram reconstruction process poses a significant bottleneck for timely
data analysis. To address this challenge, we propose a novel reconstruction
approach that integrates deep learning with the physical principles of off-axis
holography. We initialized part of the network weights based on the physical
principle and then fine-tuned them via weakly supersized learning. Our off-axis
hologram network (OAH-Net) retrieves phase and amplitude images with errors
that fall within the measurement error range attributable to hardware, and its
reconstruction speed significantly surpasses the microscope's acquisition rate.
Crucially, OAH-Net demonstrates remarkable external generalization capabilities
on unseen samples with distinct patterns and can be seamlessly integrated with
other models for downstream tasks to achieve end-to-end real-time hologram
analysis. This capability further expands off-axis holography's applications in
both biological and medical studies.

ÊëòË¶ÅÔºöÈõ¢Ëª∏Êï∏‰ΩçÂÖ®ÂÉèÈ°ØÂæÆÈè°ÊòØ‰∏ÄÁ®ÆÈ´òÈÄöÈáè„ÄÅÁÑ°Ê®ôÁ±§ÁöÑÂΩ±ÂÉèÊäÄË°ìÔºåÂèØÊèê‰æõÁ´ãÈ´î„ÄÅÈ´òËß£ÊûêÂ∫¶ÁöÑÊ®£ÂìÅË≥áË®äÔºåÁâπÂà•ÈÅ©Áî®ÊñºÂ§ßË¶èÊ®°Á¥∞ËÉûÂΩ±ÂÉè„ÄÇÁÑ∂ËÄåÔºåÂÖ®ÂÉèÈáçÂª∫ÈÅéÁ®ãÂ∞çÂèäÊôÇÁöÑË≥áÊñôÂàÜÊûêÊßãÊàêÈáçÂ§ßÁöÑÁì∂È†∏„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÈáçÂª∫ÊñπÊ≥ïÔºåÂ∞áÊ∑±Â∫¶Â≠∏ÁøíËàáÈõ¢Ëª∏ÂÖ®ÂÉèÁöÑÁâ©ÁêÜÂéüÁêÜÊï¥ÂêàÂú®‰∏ÄËµ∑„ÄÇÊàëÂÄëÊ†πÊìöÁâ©ÁêÜÂéüÁêÜÂàùÂßãÂåñÈÉ®ÂàÜÁ∂≤Ë∑ØÊ¨äÈáçÔºåÁÑ∂ÂæåÈÄèÈÅéÂº±Áõ£Áù£Â≠∏ÁøíÂæÆË™øÂÆÉÂÄë„ÄÇÊàëÂÄëÁöÑÈõ¢Ëª∏ÂÖ®ÂÉèÁ∂≤Ë∑Ø (OAH-Net) Êì∑ÂèñÁõ∏‰ΩçÂíåÊåØÂπÖÂΩ±ÂÉèÔºåÂÖ∂Ë™§Â∑ÆËêΩÂú®Ê≠∏Âõ†ÊñºÁ°¨È´îÁöÑÈáèÊ∏¨Ë™§Â∑ÆÁØÑÂúçÂÖßÔºåËÄå‰∏îÂÖ∂ÈáçÂª∫ÈÄüÂ∫¶È°ØËëóË∂ÖË∂äÈ°ØÂæÆÈè°ÁöÑÊì∑ÂèñÁéá„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåOAH-Net Âú®ÂÖ∑Êúâ‰∏çÂêåÊ®°ÂºèÁöÑÊú™Ë¶ãÊ®£Êú¨‰∏äÂ±ïÁèæÂá∫ÂçìË∂äÁöÑÂ§ñÂú®Ê≥õÂåñËÉΩÂäõÔºåËÄå‰∏îÂèØ‰ª•ËàáÂÖ∂‰ªñÊ®°ÂûãÁÑ°Á∏´Êï¥ÂêàÔºå‰ª•Âü∑Ë°å‰∏ãÊ∏∏‰ªªÂãôÔºå‰ª•ÈÅîÊàêÁ´ØÂ∞çÁ´ØÁöÑÂç≥ÊôÇÂÖ®ÂÉèÂàÜÊûê„ÄÇÊ≠§ËÉΩÂäõÈÄ≤‰∏ÄÊ≠•Êì¥Â±ï‰∫ÜÈõ¢Ëª∏ÂÖ®ÂÉèÂú®ÁîüÁâ©ÂíåÈÜ´Â≠∏Á†îÁ©∂‰∏≠ÁöÑÊáâÁî®„ÄÇ

##### **RGB to Hyperspectral: Spectral Reconstruction for Enhanced Surgical Imaging**
2410.13570v1 by Tobias Czempiel, Alfie Roddan, Maria Leiloglou, Zepeng Hu, Kevin O'Neill, Giulio Anichini, Danail Stoyanov, Daniel Elson

This study investigates the reconstruction of hyperspectral signatures from
RGB data to enhance surgical imaging, utilizing the publicly available
HeiPorSPECTRAL dataset from porcine surgery and an in-house neurosurgery
dataset. Various architectures based on convolutional neural networks (CNNs)
and transformer models are evaluated using comprehensive metrics. Transformer
models exhibit superior performance in terms of RMSE, SAM, PSNR and SSIM by
effectively integrating spatial information to predict accurate spectral
profiles, encompassing both visible and extended spectral ranges. Qualitative
assessments demonstrate the capability to predict spectral profiles critical
for informed surgical decision-making during procedures. Challenges associated
with capturing both the visible and extended hyperspectral ranges are
highlighted using the MAE, emphasizing the complexities involved. The findings
open up the new research direction of hyperspectral reconstruction for surgical
applications and clinical use cases in real-time surgical environments.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂæû RGB Ë≥áÊñôÈáçÂª∫È´òÂÖâË≠úÁâπÂæµÔºå‰ª•Â¢ûÂº∑ÊâãË°ìÂΩ±ÂÉèÔºå‰∏¶Âà©Áî®‰æÜËá™Ë±¨ÈöªÊâãË°ìÁöÑÂÖ¨Èñã HeiPorSPECTRAL Ë≥áÊñôÈõÜÂíåÈô¢ÂÖßÁ•ûÁ∂ìÂ§ñÁßëË≥áÊñôÈõÜ„ÄÇ‰ΩøÁî®Á∂úÂêàË©ïÈáèÊåáÊ®ôË©ï‰º∞‰∫ÜÂü∫ÊñºÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Âíå Transformer Ê®°ÂûãÁöÑÂêÑÁ®ÆÊû∂Êßã„ÄÇTransformer Ê®°ÂûãÂú® RMSE„ÄÅSAM„ÄÅPSNR Âíå SSIM ÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåÂõ†ÁÇ∫ÂÆÉÊúâÊïàÂú∞Êï¥Âêà‰∫ÜÁ©∫ÈñìË≥áË®ä‰ª•È†êÊ∏¨Ê∫ñÁ¢∫ÁöÑÂÖâË≠úËº™ÂªìÔºåÊ∂µËìã‰∫ÜÂèØË¶ãÂÖâÂíåÂª∂‰º∏ÂÖâË≠úÁØÑÂúç„ÄÇÂÆöÊÄßË©ï‰º∞Ë≠âÊòé‰∫ÜÂú®ÊâãË°ìÈÅéÁ®ã‰∏≠È†êÊ∏¨ÂÖâË≠úËº™ÂªìÁöÑËÉΩÂäõÔºåÂ∞çÊñºÊòéÊô∫ÁöÑÊâãË°ìÊ±∫Á≠ñÂà∂ÂÆöËá≥ÈóúÈáçË¶Å„ÄÇ‰ΩøÁî® MAE Âº∑Ë™ø‰∫ÜËàáÊì∑ÂèñÂèØË¶ãÂÖâÂíåÂª∂‰º∏È´òÂÖâË≠úÁØÑÂúçÁõ∏ÈóúÁöÑÊåëÊà∞ÔºåÂº∑Ë™ø‰∫ÜÊâÄÊ∂âÂèäÁöÑË§áÈõúÊÄß„ÄÇÈÄô‰∫õÁôºÁèæÈñãÂïü‰∫ÜÈ´òÂÖâË≠úÈáçÂª∫Âú®ÊâãË°ìÊáâÁî®ÂíåÂØ¶ÈöõÊâãË°ìÁí∞Â¢É‰∏≠Ëá®Â∫ä‰ΩøÁî®Ê°à‰æãÁöÑÊñ∞Á†îÁ©∂ÊñπÂêë„ÄÇ

##### **Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?**
2410.13523v1 by Che Liu, Zhongwei Wan, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, Rossella Arcucci

Medical Vision-Language Pre-training (MedVLP) has made significant progress
in enabling zero-shot tasks for medical image understanding. However, training
MedVLP models typically requires large-scale datasets with paired, high-quality
image-text data, which are scarce in the medical domain. Recent advancements in
Large Language Models (LLMs) and diffusion models have made it possible to
generate large-scale synthetic image-text pairs. This raises the question: *Can
MedVLP succeed using purely synthetic data?* To address this, we use
off-the-shelf generative models to create synthetic radiology reports and
paired Chest X-ray (CXR) images, and propose an automated pipeline to build a
diverse, high-quality synthetic dataset, enabling a rigorous study that
isolates model and training settings, focusing entirely from the data
perspective. Our results show that MedVLP models trained *exclusively on
synthetic data* outperform those trained on real data by **3.8%** in averaged
AUC on zero-shot classification. Moreover, using a combination of synthetic and
real data leads to a further improvement of **9.07%**. Additionally, MedVLP
models trained on synthetic or mixed data consistently outperform those trained
on real data in zero-shot grounding, as well as in fine-tuned classification
and segmentation tasks. Our analysis suggests MedVLP trained on well-designed
synthetic data can outperform models trained on real datasets, which may be
limited by low-quality samples and long-tailed distributions.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´ÁôÇË¶ñË¶∫Ë™ûË®ÄÈ†êË®ìÁ∑¥ (MedVLP) Âú®ÊîØÊè¥ÈÜ´Â≠∏ÂΩ±ÂÉèÁêÜËß£ÁöÑÈõ∂Ê¨°Â≠∏Áøí‰ªªÂãôÊñπÈù¢ÂèñÂæóÈáçÂ§ßÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåË®ìÁ∑¥ MedVLP Ê®°ÂûãÈÄöÂ∏∏ÈúÄË¶ÅÂÖ∑ÂÇôÈÖçÂ∞ç„ÄÅÈ´òÂìÅË≥™ÂΩ±ÂÉèÊñáÂ≠óË≥áÊñôÁöÑÂ§ßË¶èÊ®°Ë≥áÊñôÈõÜÔºåËÄåÈÄôÂú®ÈÜ´ÁôÇÈ†òÂüü‰∏≠ÂçÅÂàÜÁ®ÄÂ∞ë„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊì¥Êï£Ê®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰ΩøÂæóÁî¢ÁîüÂ§ßË¶èÊ®°ÁöÑÂêàÊàêÂΩ±ÂÉèÊñáÂ≠óÈÖçÂ∞çÊàêÁÇ∫ÂèØËÉΩ„ÄÇÈÄôÂºïÁôº‰∫Ü‰∏ÄÂÄãÂïèÈ°åÔºö*MedVLP ËÉΩÂÉÖ‰ΩøÁî®ÂêàÊàêË≥áÊñôÊàêÂäüÂóéÔºü*ÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄë‰ΩøÁî®ÁèæÊàêÁöÑÁîüÊàêÊ®°Âûã‰æÜÂª∫Á´ãÂêàÊàêÊîæÂ∞ÑÂ†±ÂëäÂíåÈÖçÂ∞çÁöÑËÉ∏ÈÉ® X ÂÖâ (CXR) ÂΩ±ÂÉèÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãËá™ÂãïÂåñÁöÑÊµÅÁ®ã‰æÜÂª∫Êßã‰∏ÄÂÄãÂ§öÂÖÉ„ÄÅÈ´òÂìÅË≥™ÁöÑÂêàÊàêË≥áÊñôÈõÜÔºåÈÄô‰ΩøÂæó‰∏ÄÈ†ÖÂö¥Ë¨πÁöÑÁ†îÁ©∂Âæó‰ª•Â∞àÊ≥®ÊñºË≥áÊñôËßÄÈªûÔºå‰∏¶ÂÆåÂÖ®ÈöîÈõ¢Ê®°ÂûãÂíåË®ìÁ∑¥Ë®≠ÂÆö„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫Ôºå*ÂÉÖ‰ΩøÁî®ÂêàÊàêË≥áÊñôË®ìÁ∑¥ÁöÑ* MedVLP Ê®°ÂûãÂú®Èõ∂Ê¨°ÂàÜÈ°ûÁöÑÂπ≥Âùá AUC ‰∏äÔºåÊØîÂú®ÁúüÂØ¶Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÈ´òÂá∫ **3.8%**„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®ÂêàÊàêË≥áÊñôÂíåÁúüÂØ¶Ë≥áÊñôÁöÑÁµÑÂêàÔºåÂèØÈÄ≤‰∏ÄÊ≠•ÊèêÂçá **9.07%**„ÄÇÊ≠§Â§ñÔºåÂú®ÂêàÊàêÊàñÊ∑∑ÂêàË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑ MedVLP Ê®°ÂûãÔºåÂú®Èõ∂Ê¨°ÂÆö‰Ωç„ÄÅÂæÆË™øÂàÜÈ°ûÂíåÂàÜÂâ≤‰ªªÂãô‰∏≠ÔºåÂßãÁµÇÂÑ™ÊñºÂú®ÁúüÂØ¶Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂú®Ë®≠Ë®àËâØÂ•ΩÁöÑÂêàÊàêË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑ MedVLP ÂèØ‰ª•ÂÑ™ÊñºÂú®ÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÔºåËÄåÁúüÂØ¶Ë≥áÊñôÈõÜÂèØËÉΩÂèóÂà∞‰ΩéÂìÅË≥™Ê®£Êú¨ÂíåÈï∑Â∞æÂàÜ‰ΩàÁöÑÈôêÂà∂„ÄÇ</paragraph>

##### **Representation Learning of Structured Data for Medical Foundation Models**
2410.13351v1 by Vijay Prakash Dwivedi, Viktor Schlegel, Andy T. Liu, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Jeng Wei, Wei-Hsian Yin, Stefan Winkler, Robby T. Tan

Large Language Models (LLMs) have demonstrated remarkable performance across
various domains, including healthcare. However, their ability to effectively
represent structured non-textual data, such as the alphanumeric medical codes
used in records like ICD-10 or SNOMED-CT, is limited and has been particularly
exposed in recent research. This paper examines the challenges LLMs face in
processing medical codes due to the shortcomings of current tokenization
methods. As a result, we introduce the UniStruct architecture to design a
multimodal medical foundation model of unstructured text and structured data,
which addresses these challenges by adapting subword tokenization techniques
specifically for the structured medical codes. Our approach is validated
through model pre-training on both an extensive internal medical database and a
public repository of structured medical records. Trained on over 1 billion
tokens on the internal medical database, the proposed model achieves up to a
23% improvement in evaluation metrics, with around 2% gain attributed to our
proposed tokenization. Additionally, when evaluated on the EHRSHOT public
benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model
improves performance on over 42% of the downstream tasks. Our approach not only
enhances the representation and generalization capabilities of patient-centric
models but also bridges a critical gap in representation learning models'
ability to handle complex structured medical data, alongside unstructured text.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®ÆÈ†òÂüüÂ±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩÔºåÂåÖÊã¨ÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÊúâÊïàË°®Á§∫ÁµêÊßãÂåñÈùûÊñáÂ≠óË≥áÊñôÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÁóÖÊ≠∑‰∏≠‰ΩøÁî®ÁöÑÂ≠óÊØçÊï∏Â≠óÈÜ´ÁôÇÁ¢ºÔºå‰æãÂ¶Ç ICD-10 Êàñ SNOMED-CTÔºåÂèóÂà∞ÈôêÂà∂Ôºå‰∏¶‰∏îÂú®ÊúÄËøëÁöÑÁ†îÁ©∂‰∏≠ÁâπÂà•ÊòéÈ°Ø„ÄÇÊú¨ÊñáÊé¢Ë®éÁî±ÊñºÁï∂ÂâçÊ®ôË®òÂåñÊñπÊ≥ïÁöÑÁº∫ÈªûÔºåLLM Âú®ËôïÁêÜÈÜ´ÁôÇÁ¢ºÊôÇÈù¢Ëá®ÁöÑÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü UniStruct Êû∂Êßã‰æÜË®≠Ë®àÈùûÁµêÊßãÂåñÊñáÂ≠óÂíåÁµêÊßãÂåñË≥áÊñôÁöÑÂ§öÊ®°ÊÖãÈÜ´ÁôÇÂü∫Á§éÊ®°ÂûãÔºåÂÆÉÈÄèÈÅéÁâπÂà•ÈáùÂ∞çÁµêÊßãÂåñÈÜ´ÁôÇÁ¢ºË™øÊï¥Ê¨°Â≠óÊ®ôË®òÂåñÊäÄË°ì‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÄèÈÅéÂú®Âª£Ê≥õÁöÑÂÖßÈÉ®ÈÜ´ÁôÇË≥áÊñôÂ∫´ÂíåÁµêÊßãÂåñÈÜ´ÁôÇË®òÈåÑÁöÑÂÖ¨ÈñãÂÑ≤Â≠òÂ∫´‰∏äÈÄ≤Ë°åÊ®°ÂûãÈ†êË®ìÁ∑¥‰æÜÈ©óË≠â„ÄÇÂú®ÂÖßÈÉ®ÈÜ´ÁôÇË≥áÊñôÂ∫´‰∏äË®ìÁ∑¥Ë∂ÖÈÅé 10 ÂÑÑÂÄãÊ®ôË®òÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú®Ë©ï‰º∞ÊåáÊ®ô‰∏≠Áç≤ÂæóÈ´òÈÅî 23% ÁöÑÊîπÈÄ≤ÔºåÂÖ∂‰∏≠Á¥Ñ 2% ÁöÑÊî∂ÁõäÊ≠∏ÂäüÊñºÊàëÂÄëÊèêÂá∫ÁöÑÊ®ôË®òÂåñ„ÄÇÊ≠§Â§ñÔºåÂú®‰ΩøÁî® 1/1000 ÁöÑÈ†êË®ìÁ∑¥Ë≥áÊñôÂ∞ç EHRSHOT ÂÖ¨ÈñãÂü∫Ê∫ñÈÄ≤Ë°åË©ï‰º∞ÊôÇÔºåUniStruct Ê®°ÂûãÂú®Ë∂ÖÈÅé 42% ÁöÑ‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÂ¢ûÂº∑‰∫Ü‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÊ®°ÂûãÁöÑË°®Á§∫ÂíåÊ¶ÇÂåñËÉΩÂäõÔºåÈÇÑÂΩåË£ú‰∫ÜË°®Á§∫Â≠∏ÁøíÊ®°ÂûãËôïÁêÜË§áÈõúÁµêÊßãÂåñÈÜ´ÁôÇË≥áÊñôÁöÑËÉΩÂäõËàáÈùûÁµêÊßãÂåñÊñáÂ≠ó‰πãÈñìÁöÑÈóúÈçµÂ∑ÆË∑ù„ÄÇ

##### **Active inference and deep generative modeling for cognitive ultrasound**
2410.13310v1 by Ruud JG van Sloun

Ultrasound (US) has the unique potential to offer access to medical imaging
to anyone, everywhere. Devices have become ultra-portable and cost-effective,
akin to the stethoscope. Nevertheless US image quality and diagnostic efficacy
are still highly operator- and patient-dependent. In difficult-to-image
patients, image quality is often insufficient for reliable diagnosis. In this
paper, we put forth that US imaging systems can be recast as
information-seeking agents that engage in reciprocal interactions with their
anatomical environment. Such agents autonomously adapt their transmit-receive
sequences to fully personalize imaging and actively maximize information gain
in-situ. To that end, we will show that the sequence of pulse-echo experiments
that a US system performs can be interpreted as a perception-action loop: the
action is the data acquisition, probing tissue with acoustic waves and
recording reflections at the detection array, and perception is the inference
of the anatomical and or functional state, potentially including associated
diagnostic quantities. We then equip systems with a mechanism to actively
reduce uncertainty and maximize diagnostic value across a sequence of
experiments, treating action and perception jointly using Bayesian inference
given generative models of the environment and action-conditional pulse-echo
observations. Since the representation capacity of the generative models
dictates both the quality of inferred anatomical states and the effectiveness
of inferred sequences of future imaging actions, we will be greatly leveraging
the enormous advances in deep generative modelling that are currently
disrupting many fields and society at large. Finally, we show some examples of
cognitive, closed-loop, US systems that perform active beamsteering and
adaptive scanline selection, based on deep generative models that track
anatomical belief states.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ (US) ÂÖ∑ÊúâÊèê‰æõÈÜ´ÁôÇÂΩ±ÂÉèÁöÑÁç®ÁâπÊΩõÂäõÔºåÂèØ‰æõ‰ªª‰Ωï‰∫∫Âú®‰ªª‰ΩïÂú∞Êñπ‰ΩøÁî®„ÄÇË£ùÁΩÆÂ∑≤ËÆäÂæóÊ•µÁÇ∫‰æøÊîú‰∏îÁ∂ìÊøüÂØ¶ÊÉ†ÔºåÈ°û‰ººÊñºËÅΩË®∫Âô®„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂìÅË≥™ÂíåË®∫Êñ∑ÊïàËÉΩ‰ªçÁÑ∂È´òÂ∫¶‰æùË≥¥Êìç‰ΩúËÄÖÂíåÊÇ£ËÄÖ„ÄÇÂú®Èõ£‰ª•ÊàêÂÉèÁöÑÊÇ£ËÄÖ‰∏≠ÔºåÂΩ±ÂÉèÂìÅË≥™ÈÄöÂ∏∏‰∏çË∂≥‰ª•ÈÄ≤Ë°åÂèØÈù†ÁöÑË®∫Êñ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÁ≥ªÁµ±ÂèØ‰ª•ÈáçÊñ∞ÂÆöÁæ©ÁÇ∫Ë≥áË®äÂ∞ãÊ±Ç‰ª£ÁêÜÔºåËàáÂÖ∂Ëß£ÂâñÁí∞Â¢ÉÈÄ≤Ë°å‰∫§‰∫í‰ΩúÁî®„ÄÇÊ≠§È°û‰ª£ÁêÜÊúÉËá™‰∏ªË™øÊï¥ÂÖ∂ÂÇ≥Ëº∏Êé•Êî∂Â∫èÂàóÔºå‰ª•ÂÆåÂÖ®ÂÄã‰∫∫ÂåñÂΩ±ÂÉè‰∏¶Á©çÊ•µÊúÄÂ§ßÂåñÁèæÂ†¥Ë≥áË®äÁç≤Âèñ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂ∞áÂ±ïÁ§∫Ë∂ÖÈü≥Ê≥¢Á≥ªÁµ±Âü∑Ë°åÁöÑËÑàË°ùÂõûÊ≥¢ÂØ¶È©óÂ∫èÂàóÂèØ‰ª•Ëß£ÈáãÁÇ∫ÊÑüÁü•Âãï‰ΩúËø¥ÂúàÔºöÂãï‰ΩúÊòØË≥áÊñôÊì∑ÂèñÔºå‰ΩøÁî®ËÅ≤Ê≥¢Êé¢Ê∏¨ÁµÑÁπî‰∏¶Ë®òÈåÑÂÅµÊ∏¨Èô£ÂàóÁöÑÂèçÂ∞ÑÔºåËÄåÊÑüÁü•ÊòØËß£ÂâñÂíåÂäüËÉΩÁãÄÊÖãÁöÑÊé®Ë´ñÔºåÂèØËÉΩÂåÖÊã¨Áõ∏ÈóúÁöÑË®∫Êñ∑Èáè„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÁÇ∫Á≥ªÁµ±ÈÖçÂÇô‰∏ÄÁ®ÆÊ©üÂà∂Ôºå‰ª•Á©çÊ•µÊ∏õÂ∞ë‰∏çÁ¢∫ÂÆöÊÄß‰∏¶Âú®ÂØ¶È©óÂ∫èÂàó‰∏≠ÊúÄÂ§ßÂåñË®∫Êñ∑ÂÉπÂÄºÔºå‰ΩøÁî®Ë≤ùÊ∞èÊé®Ë´ñÂÖ±ÂêåËôïÁêÜÂãï‰ΩúÂíåÊÑüÁü•Ôºå‰∏¶Êèê‰æõÁí∞Â¢ÉÁöÑÁîüÊàêÊ®°ÂûãÂíåÂãï‰ΩúÊ¢ù‰ª∂ËÑàË°ùÂõûÊ≥¢ËßÄÊ∏¨„ÄÇÁî±ÊñºÁîüÊàêÊ®°ÂûãÁöÑË°®Á§∫ËÉΩÂäõÊ±∫ÂÆö‰∫ÜÊé®Ë´ñËß£ÂâñÁãÄÊÖãÁöÑÂìÅË≥™ÂíåÊé®Ë´ñÊú™‰æÜÂΩ±ÂÉèÂãï‰ΩúÂ∫èÂàóÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂ∞áÂ§ßÂäõÂà©Áî®Ê∑±Â∫¶ÁîüÊàêÊ®°ÂûãÁöÑÂ∑®Â§ßÈÄ≤Â±ïÔºåÈÄô‰∫õÈÄ≤Â±ïÁõÆÂâçÊ≠£Âú®È°õË¶ÜË®±Â§öÈ†òÂüüÂíåÊï¥ÂÄãÁ§æÊúÉ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏Ä‰∫õË™çÁü•„ÄÅÈñâËø¥Ë∑ØË∂ÖÈü≥Ê≥¢Á≥ªÁµ±ÁöÑÁØÑ‰æãÔºåÈÄô‰∫õÁ≥ªÁµ±Âü∑Ë°å‰∏ªÂãïÊ≥¢ÊùüÂ∞éÂºïÂíåËá™ÈÅ©ÊáâÊéÉÊèèÁ∑öÈÅ∏ÂèñÔºåÂü∫ÊñºËøΩËπ§Ëß£Ââñ‰ø°ÂøµÁãÄÊÖãÁöÑÊ∑±Â∫¶ÁîüÊàêÊ®°Âûã„ÄÇ

##### **Hiformer: Hybrid Frequency Feature Enhancement Inverted Transformer for Long-Term Wind Power Prediction**
2410.13303v1 by Chongyang Wan, Shunbo Lei, Yuan Luo

The increasing severity of climate change necessitates an urgent transition
to renewable energy sources, making the large-scale adoption of wind energy
crucial for mitigating environmental impact. However, the inherent uncertainty
of wind power poses challenges for grid stability, underscoring the need for
accurate wind energy prediction models to enable effective power system
planning and operation. While many existing studies on wind power prediction
focus on short-term forecasting, they often overlook the importance of
long-term predictions. Long-term wind power forecasting is essential for
effective power grid dispatch and market transactions, as it requires careful
consideration of weather features such as wind speed and direction, which
directly influence power output. Consequently, methods designed for short-term
predictions may lead to inaccurate results and high computational costs in
long-term settings. To adress these limitations, we propose a novel approach
called Hybrid Frequency Feature Enhancement Inverted Transformer (Hiformer).
Hiformer introduces a unique structure that integrates signal decomposition
technology with weather feature extraction technique to enhance the modeling of
correlations between meteorological conditions and wind power generation.
Additionally, Hiformer employs an encoder-only architecture, which reduces the
computational complexity associated with long-term wind power forecasting.
Compared to the state-of-the-art methods, Hiformer: (i) can improve the
prediction accuracy by up to 52.5\%; and (ii) can reduce computational time by
up to 68.5\%.

ÊëòË¶ÅÔºöÊ∞£ÂÄôËÆäÈÅ∑Êó•ÁõäÂö¥ÈáçÔºåËø´ÂàáÈúÄË¶ÅËΩâÂêëÂÜçÁîüËÉΩÊ∫êÔºåÂõ†Ê≠§Â§ßË¶èÊ®°Êé°Áî®È¢®ËÉΩÂ∞çÊñºÊ∏õÁ∑©Áí∞Â¢ÉÂΩ±ÈüøËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈ¢®ËÉΩÁöÑÂÖßÂú®‰∏çÁ¢∫ÂÆöÊÄßÂ∞çÈõªÁ∂≤Á©©ÂÆöÊÄßÊßãÊàêÊåëÊà∞ÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÂ∞çÊ∫ñÁ¢∫È¢®ËÉΩÈ†êÊ∏¨Ê®°ÂûãÁöÑÈúÄÊ±ÇÔºå‰ª•ÂØ¶ÁèæÊúâÊïàÁöÑÈõªÂäõÁ≥ªÁµ±Ë¶èÂäÉÂíåÈÅã‰Ωú„ÄÇÈõñÁÑ∂Ë®±Â§öÁèæÊúâÁöÑÈ¢®ËÉΩÈ†êÊ∏¨Á†îÁ©∂ÈÉΩÂ∞àÊ≥®ÊñºÁü≠ÊúüÈ†êÊ∏¨Ôºå‰ΩÜÂÆÉÂÄëÂ∏∏Â∏∏ÂøΩÁï•Èï∑ÊúüÈ†êÊ∏¨ÁöÑÈáçË¶ÅÊÄß„ÄÇÈï∑ÊúüÈ¢®ËÉΩÈ†êÊ∏¨Â∞çÊñºÊúâÊïàÁöÑÈõªÁ∂≤Ë™øÂ∫¶ÂíåÂ∏ÇÂ†¥‰∫§ÊòìËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÈúÄË¶Å‰ªîÁ¥∞ËÄÉÊÖÆÂ§©Ê∞£ÁâπÂæµÔºå‰æãÂ¶ÇÈ¢®ÈÄüÂíåÈ¢®ÂêëÔºåÈÄô‰∫õÁâπÂæµÊúÉÁõ¥Êé•ÂΩ±ÈüøÈõªÂäõËº∏Âá∫„ÄÇÂõ†Ê≠§ÔºåÂ∞àÁÇ∫Áü≠ÊúüÈ†êÊ∏¨ËÄåË®≠Ë®àÁöÑÊñπÊ≥ïÂú®Èï∑ÊúüË®≠ÁΩÆ‰∏≠ÂèØËÉΩÊúÉÂ∞éËá¥‰∏çÊ∫ñÁ¢∫ÁöÑÁµêÊûúÂíåÈ´òË®àÁÆóÊàêÊú¨„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Ê∑∑ÂêàÈ†ªÁéáÁâπÂæµÂ¢ûÂº∑ÂèçËΩâTransformer (Hiformer) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇHiformer ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁç®ÁâπÁöÑÁµêÊßãÔºåÂ∞á‰ø°ËôüÂàÜËß£ÊäÄË°ìËàáÂ§©Ê∞£ÁâπÂæµÊèêÂèñÊäÄË°ìÁõ∏ÁµêÂêàÔºå‰ª•Â¢ûÂº∑Ê∞£Ë±°Ê¢ù‰ª∂ËàáÈ¢®ÂäõÁôºÈõª‰πãÈñìÁõ∏ÈóúÊÄßÁöÑÂª∫Ê®°„ÄÇÊ≠§Â§ñÔºåHiformer Êé°Áî®ÂÉÖÁ∑®Á¢ºÂô®Êû∂ÊßãÔºåÈÄôÈôç‰Ωé‰∫ÜËàáÈï∑ÊúüÈ¢®ËÉΩÈ†êÊ∏¨Áõ∏ÈóúÁöÑË®àÁÆóË§áÈõúÂ∫¶„ÄÇËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåHiformerÔºö(i) ÂèØ‰ª•Â∞áÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÊèêÈ´òÂ§öÈÅî 52.5%Ôºõ(ii) ÂèØ‰ª•Â∞áË®àÁÆóÊôÇÈñìÊ∏õÂ∞ëÂ§öÈÅî 68.5%„ÄÇ

##### **CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy**
2410.13218v1 by Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang, William Yang Wang, Zhiyu Zoey Chen

There is a significant gap between patient needs and available mental health
support today. In this paper, we aim to thoroughly examine the potential of
using Large Language Models (LLMs) to assist professional psychotherapy. To
this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation
of cognitive behavioral therapy (CBT) assistance. We include three levels of
tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of
multiple-choice questions; II: Cognitive model understanding, with the tasks of
cognitive distortion classification, primary core belief classification, and
fine-grained core belief classification; III: Therapeutic response generation,
with the task of generating responses to patient speech in CBT therapy
sessions. These tasks encompass key aspects of CBT that could potentially be
enhanced through AI assistance, while also outlining a hierarchy of capability
requirements, ranging from basic knowledge recitation to engaging in real
therapeutic conversations. We evaluated representative LLMs on our benchmark.
Experimental results indicate that while LLMs perform well in reciting CBT
knowledge, they fall short in complex real-world scenarios requiring deep
analysis of patients' cognitive structures and generating effective responses,
suggesting potential future work.

ÊëòË¶ÅÔºöÁèæ‰ªäÁóÖÊÇ£ÈúÄÊ±ÇËàáÂèØÂèñÂæóÁöÑÂøÉÁêÜÂÅ•Â∫∑ÊîØÊè¥‰πãÈñìÂ≠òÂú®ËëóÈ°ØËëóÁöÑÂ∑ÆË∑ù„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÂæπÂ∫ïÊé¢Ë®é‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂçîÂä©Â∞àÊ•≠ÂøÉÁêÜÊ≤ªÁôÇÁöÑÂèØËÉΩÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü CBT-BENCHÔºå‰∏ÄÂÄãÁî®ÊñºÁ≥ªÁµ±ÊÄßË©ï‰º∞Ë™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÂçîÂä©ÁöÑÊñ∞Âü∫Ê∫ñ„ÄÇÊàëÂÄëÂú® CBT-BENCH ‰∏≠ÂåÖÂê´‰∏âÁ¥ö‰ªªÂãôÔºöIÔºöÂü∫Êú¨ CBT Áü•Ë≠òÁøíÂæóÔºå‰ªªÂãôÁÇ∫Â§öÈÅ∏È°åÔºõIIÔºöË™çÁü•Ê®°ÂûãÁêÜËß£Ôºå‰ªªÂãôÁÇ∫Ë™çÁü•Êâ≠Êõ≤ÂàÜÈ°û„ÄÅ‰∏ªË¶ÅÊ†∏ÂøÉ‰ø°ÂøµÂàÜÈ°ûÂíåÁ¥∞Á≤íÂ∫¶Ê†∏ÂøÉ‰ø°ÂøµÂàÜÈ°ûÔºõIIIÔºöÊ≤ªÁôÇÂèçÊáâÁîüÊàêÔºå‰ªªÂãôÁÇ∫Âú® CBT Ê≤ªÁôÇÊúÉË´á‰∏≠Â∞çÁóÖÊÇ£ÁöÑË®ÄË™ûÁî¢ÁîüÂèçÊáâ„ÄÇÈÄô‰∫õ‰ªªÂãôÊ∂µËìã‰∫Ü CBT ÁöÑÈóúÈçµÈù¢ÂêëÔºåÈÄô‰∫õÈù¢ÂêëÊúâÂèØËÉΩÈÄèÈÅé AI ÂçîÂä©ËÄåÂæóÂà∞Âº∑ÂåñÔºåÂêåÊôÇ‰πüÊ¶ÇËø∞‰∫ÜËÉΩÂäõÈúÄÊ±ÇÁöÑÂ±§Á¥öÔºåÂæûÂü∫Êú¨ÁöÑÁü•Ë≠òËÉåË™¶Âà∞ÂèÉËàáÁúüÊ≠£ÁöÑÊ≤ªÁôÇÂ∞çË©±„ÄÇÊàëÂÄëÂú®Âü∫Ê∫ñ‰∏äË©ï‰º∞‰∫ÜÂÖ∑‰ª£Ë°®ÊÄßÁöÑ LLM„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ° LLM Âú®ËÉåË™¶ CBT Áü•Ë≠òÊñπÈù¢Ë°®ÁèæËâØÂ•ΩÔºå‰ΩÜÂú®ÈúÄË¶ÅÊ∑±ÂÖ•ÂàÜÊûêÁóÖÊÇ£Ë™çÁü•ÁµêÊßãÂíåÁî¢ÁîüÊúâÊïàÂèçÊáâÁöÑË§áÈõúÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠Ë°®Áèæ‰∏ç‰Ω≥ÔºåÈÄôË°®Á§∫Êú™‰æÜÊúâÊΩõÂú®ÁöÑÂ∑•‰ΩúÊ©üÊúÉ„ÄÇ

##### **MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records through Hierarchical Guided-Topic Modeling**
2410.13217v1 by Ruohan Wang, Zilong Wang, Ziyang Song, David Buckeridge, Yue Li

Automatic subphenotyping from electronic health records (EHRs)provides
numerous opportunities to understand diseases with unique subgroups and enhance
personalized medicine for patients. However, existing machine learning
algorithms either focus on specific diseases for better interpretability or
produce coarse-grained phenotype topics without considering nuanced disease
patterns. In this study, we propose a guided topic model, MixEHR-Nest, to infer
sub-phenotype topics from thousands of disease using multi-modal EHR data.
Specifically, MixEHR-Nest detects multiple subtopics from each phenotype topic,
whose prior is guided by the expert-curated phenotype concepts such as
Phenotype Codes (PheCodes) or Clinical Classification Software (CCS) codes. We
evaluated MixEHR-Nest on two EHR datasets: (1) the MIMIC-III dataset consisting
of over 38 thousand patients from intensive care unit (ICU) from Beth Israel
Deaconess Medical Center (BIDMC) in Boston, USA; (2) the healthcare
administrative database PopHR, comprising 1.3 million patients from Montreal,
Canada. Experimental results demonstrate that MixEHR-Nest can identify
subphenotypes with distinct patterns within each phenotype, which are
predictive for disease progression and severity. Consequently, MixEHR-Nest
distinguishes between type 1 and type 2 diabetes by inferring subphenotypes
using CCS codes, which do not differentiate these two subtype concepts.
Additionally, MixEHR-Nest not only improved the prediction accuracy of
short-term mortality of ICU patients and initial insulin treatment in diabetic
patients but also revealed the contributions of subphenotypes. For longitudinal
analysis, MixEHR-Nest identified subphenotypes of distinct age prevalence under
the same phenotypes, such as asthma, leukemia, epilepsy, and depression. The
MixEHR-Nest software is available at GitHub:
https://github.com/li-lab-mcgill/MixEHR-Nest.

ÊëòË¶ÅÔºö<paragraph>ÂæûÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) Ëá™ÂãïÈÄ≤Ë°å‰∫ûÂàÜÂûãÔºåÊèê‰æõ‰∫ÜË®±Â§ö‰∫ÜËß£ÂÖ∑ÊúâÁç®Áâπ‰∫ûÁæ§ÁöÑÁñæÁóÖ‰∏¶Â¢ûÂº∑ÊÇ£ËÄÖÂÄã‰∫∫ÂåñÈÜ´ÁôÇÁöÑÊ©üÊúÉ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ï‰∏çÊòØÂ∞àÊ≥®ÊñºÁâπÂÆöÁñæÁóÖ‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑÂèØËß£ÈáãÊÄßÔºåÂ∞±ÊòØÁî¢ÁîüÁ≤óÁï•ÁöÑÂàÜÂûã‰∏ªÈ°åÔºåËÄå‰∏çËÄÉÊÖÆÁ¥∞ÂæÆÁöÑÁñæÁóÖÊ®°Âºè„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂºïÂ∞éÂºè‰∏ªÈ°åÊ®°Âûã MixEHR-NestÔºå‰ª•‰ΩøÁî®Â§öÊ®°Âºè EHR Ë≥áÊñôÂæûÊï∏ÂçÉÁ®ÆÁñæÁóÖ‰∏≠Êé®Ë´ñÂá∫‰∫ûÂàÜÂûã‰∏ªÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåMixEHR-Nest ÂæûÊØèÂÄãÂàÜÂûã‰∏ªÈ°å‰∏≠ÂÅµÊ∏¨Âá∫Â§öÂÄãÂ≠ê‰∏ªÈ°åÔºåÂÖ∂‰∫ãÂâçÊ©üÁéáÁî±Â∞àÂÆ∂Á≠ñÂäÉÁöÑÂàÜÂûãÊ¶ÇÂøµÔºà‰æãÂ¶ÇÂàÜÂûã‰ª£Á¢º (PheCodes) ÊàñËá®Â∫äÂàÜÈ°ûËªüÈ´î (CCS) ‰ª£Á¢ºÔºâÂºïÂ∞é„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄã EHR Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫Ü MixEHR-NestÔºö(1) MIMIC-III Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ÁæéÂúãÊ≥¢Â£´È†ìË≤ùÊñØ‰ª•Ëâ≤ÂàóÂ•≥Âü∑‰∫ãÈÜ´ÁôÇ‰∏≠ÂøÉ (BIDMC) ÈáçÁóáÁõ£Ë≠∑ÁóÖÊàø (ICU) ÁöÑ 38,000 Â§öÂêçÊÇ£ËÄÖÔºõ(2) ÈÜ´ÁôÇË°åÊîøË≥áÊñôÂ∫´ PopHRÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™Âä†ÊãøÂ§ßËíôÁâπÂ©ÅÁöÑ 130 Ëê¨ÂêçÊÇ£ËÄÖ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåMixEHR-Nest ÂèØ‰ª•Ë≠òÂà•ÊØèÂÄãÂàÜÂûã‰∏≠ÂÖ∑Êúâ‰∏çÂêåÊ®°ÂºèÁöÑ‰∫ûÂàÜÂûãÔºåÈÄô‰∫õÊ®°ÂºèÂ∞çÊñºÁñæÁóÖÈÄ≤Â±ïÂíåÂö¥ÈáçÁ®ãÂ∫¶ÂÖ∑ÊúâÈ†êÊ∏¨ÊÄß„ÄÇÂõ†Ê≠§ÔºåMixEHR-Nest ÈÄöÈÅé‰ΩøÁî® CCS ‰ª£Á¢ºÊé®Ë´ñ‰∫ûÂàÜÂûã‰æÜÂçÄÂàÜ 1 ÂûãÂíå 2 ÂûãÁ≥ñÂ∞øÁóÖÔºåËÄå CCS ‰ª£Á¢º‰∏¶Êú™ÂçÄÂàÜÈÄôÂÖ©ÂÄã‰∫ûÂûãÊ¶ÇÂøµ„ÄÇÊ≠§Â§ñÔºåMixEHR-Nest ‰∏çÂÉÖÊèêÈ´ò‰∫Ü ICU ÊÇ£ËÄÖÁü≠ÊúüÊ≠ª‰∫°ÁéáÂíåÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÂàùÂßãËÉ∞Â≥∂Á¥†Ê≤ªÁôÇÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÈÇÑÊè≠Á§∫‰∫Ü‰∫ûÂàÜÂûãÁöÑË≤¢Áçª„ÄÇÂ∞çÊñºÁ∏±ÂêëÂàÜÊûêÔºåMixEHR-Nest Ë≠òÂà•Âá∫Âú®Áõ∏ÂêåÂàÜÂûã‰∏ãÁöÑ‰∏çÂêåÂπ¥ÈΩ°ÊÇ£ÁóÖÁéáÁöÑ‰∫ûÂàÜÂûãÔºå‰æãÂ¶ÇÂìÆÂñò„ÄÅÁôΩË°ÄÁóÖ„ÄÅÁô≤ÁôáÂíåÊÜÇÈ¨±Áóá„ÄÇMixEHR-Nest ËªüÈ´îÂèØÂú® GitHub ‰∏äÂèñÂæóÔºöhttps://github.com/li-lab-mcgill/MixEHR-Nest„ÄÇ</paragraph>

##### **LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch**
2410.13213v1 by Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, Yang Yu

Optimization problems are prevalent across various scenarios. Formulating and
then solving optimization problems described by natural language often requires
highly specialized human expertise, which could block the widespread
application of optimization-based decision making. To make problem formulating
and solving automated, leveraging large language models (LLMs) has emerged as a
potential way. However, this kind of way suffers from the issue of optimization
generalization. Namely, the accuracy of most current LLM-based methods and the
generality of optimization problem types that they can model are still limited.
In this paper, we propose a unified learning-based framework called LLMOPT to
boost optimization generalization. Starting from the natural language
descriptions of optimization problems and a pre-trained LLM, LLMOPT constructs
the introduced five-element formulation as a universal model for learning to
define diverse optimization problem types. Then, LLMOPT employs the
multi-instruction tuning to enhance both problem formalization and solver code
generation accuracy and generality. After that, to prevent hallucinations in
LLMs, such as sacrificing solving accuracy to avoid execution errors, model
alignment and self-correction mechanism are adopted in LLMOPT. We evaluate the
optimization generalization ability of LLMOPT and compared methods across six
real-world datasets covering roughly 20 fields such as health, environment,
energy and manufacturing, etc. Extensive experiment results show that LLMOPT is
able to model various optimization problem types such as linear/nonlinear
programming, mixed integer programming and combinatorial optimization, and
achieves a notable 11.08% average solving accuracy improvement compared with
the state-of-the-art methods. The code is available at
https://github.com/caigaojiang/LLMOPT.

ÊëòË¶ÅÔºö<paragraph>ÂÑ™ÂåñÂïèÈ°åÊôÆÈÅçÂ≠òÂú®ÊñºÂêÑÁ®ÆÂ†¥ÊôØ‰∏≠„ÄÇÂà∂ÂÆö‰∏¶Ëß£Ê±∫Ëá™ÁÑ∂Ë™ûË®ÄÊèèËø∞ÁöÑÂÑ™ÂåñÂïèÈ°åÈÄöÂ∏∏ÈúÄË¶ÅÈ´òÂ∫¶Â∞àÊ•≠ÁöÑ‰∫∫È°ûÂ∞àÊ•≠Áü•Ë≠òÔºåÈÄôÂèØËÉΩÊúÉÈòªÁ§ôÂü∫ÊñºÂÑ™ÂåñÁöÑÊ±∫Á≠ñÂà∂ÂÆöÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÁÇ∫‰∫Ü‰ΩøÂïèÈ°åÂà∂ÂÆöÂíåÊ±ÇËß£Ëá™ÂãïÂåñÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊΩõÂú®ÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÂºèÂ≠òÂú®ÂÑ™ÂåñÊ≥õÂåñÂïèÈ°å„ÄÇ‰πüÂ∞±ÊòØË™™ÔºåÁï∂ÂâçÂ§ßÂ§öÊï∏Âü∫Êñº LLM ÁöÑÊñπÊ≥ïÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂÆÉÂÄëÂèØ‰ª•Âª∫Ê®°ÁöÑÂÑ™ÂåñÂïèÈ°åÈ°ûÂûãÁöÑÊôÆÈÅçÊÄß‰ªçÁÑ∂ÊúâÈôê„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ LLMOPT ÁöÑÁµ±‰∏ÄÂü∫ÊñºÂ≠∏ÁøíÁöÑÊ°ÜÊû∂Ôºå‰ª•ÊèêÈ´òÂÑ™ÂåñÊ≥õÂåñËÉΩÂäõ„ÄÇÂæûÂÑ™ÂåñÂïèÈ°åÁöÑËá™ÁÑ∂Ë™ûË®ÄÊèèËø∞ÂíåÈ†êË®ìÁ∑¥ÁöÑ LLM ÈñãÂßãÔºåLLMOPT Â∞áÂºïÂÖ•ÁöÑ‰∫îË¶ÅÁ¥†Ë°®Ëø∞ÊßãÂª∫ÁÇ∫Â≠∏ÁøíÂÆöÁæ©ÂêÑÁ®ÆÂÑ™ÂåñÂïèÈ°åÈ°ûÂûãÁöÑÈÄöÁî®Ê®°Âûã„ÄÇÁÑ∂ÂæåÔºåLLMOPT Êé°Áî®Â§öÊåá‰ª§Ë™øÊï¥‰æÜÂ¢ûÂº∑ÂïèÈ°åÂΩ¢ÂºèÂåñÂíåÊ±ÇËß£Âô®‰ª£Á¢ºÁîüÊàêÊ∫ñÁ¢∫ÊÄßÂíåÊôÆÈÅçÊÄß„ÄÇÂú®ÈÇ£‰πãÂæåÔºåÁÇ∫‰∫ÜÈò≤Ê≠¢ LLM ‰∏≠ÁöÑÂπªË¶∫Ôºå‰æãÂ¶ÇÁäßÁâ≤Ê±ÇËß£Ê∫ñÁ¢∫ÊÄß‰ª•ÈÅøÂÖçÂü∑Ë°åÈåØË™§ÔºåÂú® LLMOPT ‰∏≠Êé°Áî®‰∫ÜÊ®°ÂûãÂ∞çÈΩäÂíåËá™Ê†°Ê≠£Ê©üÂà∂„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü LLMOPT ÁöÑÂÑ™ÂåñÊ≥õÂåñËÉΩÂäõÔºå‰∏¶ÊØîËºÉ‰∫ÜÂÖ≠ÂÄãÊ∂µËìãÂÅ•Â∫∑„ÄÅÁí∞Â¢É„ÄÅËÉΩÊ∫êÂíåË£ΩÈÄ†Á≠âÁ¥Ñ 20 ÂÄãÈ†òÂüüÁöÑÁúüÂØ¶‰∏ñÁïåÊï∏ÊìöÈõÜ‰∏≠ÁöÑÊñπÊ≥ï„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLMOPT ËÉΩÂ§†Â∞çÂêÑÁ®ÆÂÑ™ÂåñÂïèÈ°åÈ°ûÂûãÂª∫Ê®°Ôºå‰æãÂ¶ÇÁ∑öÊÄß/ÈùûÁ∑öÊÄßË¶èÂäÉ„ÄÅÊ∑∑ÂêàÊï¥Êï∏Ë¶èÂäÉÂíåÁµÑÂêàÂÑ™ÂåñÔºå‰∏¶ËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÂèñÂæó‰∫ÜÈ°ØËëóÁöÑ 11.08% Âπ≥ÂùáÊ±ÇËß£Ê∫ñÁ¢∫Â∫¶ÊèêÂçá„ÄÇ‰ª£Á¢ºÂèØÂú® https://github.com/caigaojiang/LLMOPT Áç≤Âæó„ÄÇ</paragraph>

##### **MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback**
2410.13191v1 by Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu

Automatic question generation (QG) is essential for AI and NLP, particularly
in intelligent tutoring, dialogue systems, and fact verification. Generating
multiple-choice questions (MCQG) for professional exams, like the United States
Medical Licensing Examination (USMLE), is particularly challenging, requiring
domain expertise and complex multi-hop reasoning for high-quality questions.
However, current large language models (LLMs) like GPT-4 struggle with
professional MCQG due to outdated knowledge, hallucination issues, and prompt
sensitivity, resulting in unsatisfactory quality and difficulty. To address
these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique
and Correction) framework for converting medical cases into high-quality
USMLE-style questions. By integrating expert-driven prompt engineering with
iterative self-critique and self-correction feedback, MCQG-SRefine
significantly enhances human expert satisfaction regarding both the quality and
difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based
automatic metric to replace the complex and costly expert evaluation process,
ensuring reliable and expert-aligned assessments.

ÊëòË¶ÅÔºöËá™ÂãïÂåñÂïèÈ°åÁîüÊàê (QG) Â∞çÊñº AI Âíå NLP Ëá≥ÈóúÈáçË¶ÅÔºåÁâπÂà•ÊòØÂú®Êô∫ÊÖßÊïôÂ≠∏„ÄÅÂ∞çË©±Á≥ªÁµ±Âíå‰∫ãÂØ¶È©óË≠â‰∏≠„ÄÇÁÇ∫Â∞àÊ•≠ËÄÉË©¶Ôºà‰æãÂ¶ÇÁæéÂúãÈÜ´Â∏´Âü∑ÁÖßËÄÉË©¶ (USMLE)ÔºâÁîüÊàêÂ§öÈÅ∏È°å (MCQG) ÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÈúÄË¶ÅÂ∞àÊ•≠Áü•Ë≠òÂíåË§áÈõúÁöÑÂ§öË∑≥Êé®ÁêÜÊâçËÉΩÁî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ GPT-4 Á≠âÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî±ÊñºÁü•Ë≠òÈÅéÊôÇ„ÄÅÂπªË¶∫ÂïèÈ°åÂíåÊèêÁ§∫ÊïèÊÑüÊÄßÔºåËÄåÈõ£‰ª•ËôïÁêÜÂ∞àÊ•≠ MCQGÔºåÂ∞éËá¥ÂìÅË≥™ÂíåÈõ£Â∫¶‰∏çÁõ°‰∫∫ÊÑè„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü MCQG-SRefineÔºå‰∏ÄÁ®ÆÂü∫Êñº LLM Ëá™ÊàëÁ≤æÈÄ≤ÔºàÊâπË©ïÂíå‰øÆÊ≠£ÔºâÊû∂ÊßãÔºåÁî®ÊñºÂ∞áÈÜ´ÁôÇÊ°à‰æãËΩâÊèõÁÇ∫È´òÂìÅË≥™ÁöÑ USMLE È¢®Ê†ºÂïèÈ°å„ÄÇÈÄèÈÅéÊï¥ÂêàÂ∞àÂÆ∂È©ÖÂãïÁöÑÊèêÁ§∫Â∑•Á®ãËàáÂèçË¶ÜËá™ÊàëÊâπË©ïÂíåËá™Êàë‰øÆÊ≠£ÁöÑÂõûÈ•ãÔºåMCQG-SRefine Â§ßÂπÖÊèêÂçá‰∫Ü‰∫∫È°ûÂ∞àÂÆ∂Â∞çÊñºÂïèÈ°åÂìÅË≥™ÂíåÈõ£Â∫¶ÁöÑÊªøÊÑèÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÈÄ≤‰∏ÄÂÄãÂü∫Êñº LLM ‰ΩúÁÇ∫Ë©ïÂØ©ÁöÑËá™ÂãïÂåñÊåáÊ®ôÔºå‰ª•Âèñ‰ª£Ë§áÈõú‰∏îÊòÇË≤¥ÁöÑÂ∞àÂÆ∂Ë©ï‰º∞Á®ãÂ∫èÔºåÁ¢∫‰øùÂèØÈù†‰∏îËàáÂ∞àÂÆ∂‰∏ÄËá¥ÁöÑË©ïÈáè„ÄÇ

##### **Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information**
2410.12774v1 by Yingya Li, Timothy Miller, Steven Bethard, Guergana Savova

The success of multi-task learning can depend heavily on which tasks are
grouped together. Naively grouping all tasks or a random set of tasks can
result in negative transfer, with the multi-task models performing worse than
single-task models. Though many efforts have been made to identify task
groupings and to measure the relatedness among different tasks, it remains a
challenging research topic to define a metric to identify the best task
grouping out of a pool of many potential task combinations. We propose a metric
of task relatedness based on task difficulty measured by pointwise V-usable
information (PVI). PVI is a recently proposed metric to estimate how much
usable information a dataset contains given a model. We hypothesize that tasks
with not statistically different PVI estimates are similar enough to benefit
from the joint learning process. We conduct comprehensive experiments to
evaluate the feasibility of this metric for task grouping on 15 NLP datasets in
the general, biomedical, and clinical domains. We compare the results of the
joint learners against single learners, existing baseline methods, and recent
large language models, including Llama 2 and GPT-4. The results show that by
grouping tasks with similar PVI estimates, the joint learners yielded
competitive results with fewer total parameters, with consistent performance
across domains.

ÊëòË¶ÅÔºöÂ§ö‰ªªÂãôÂ≠∏ÁøíÁöÑÊàêÂäüÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÂ∞áÂì™‰∫õ‰ªªÂãôÂàÜÁµÑÂú®‰∏ÄËµ∑„ÄÇÂ§©ÁúüÂú∞Â∞áÊâÄÊúâ‰ªªÂãôÊàñ‰∏ÄÁµÑÈö®Ê©ü‰ªªÂãôÂàÜÁµÑÂèØËÉΩÊúÉÂ∞éËá¥Ë≤†ÂêëÈÅ∑ÁßªÔºåÂ§ö‰ªªÂãôÊ®°ÂûãÁöÑË°®ÁèæÊúÉÊØîÂñÆ‰ªªÂãôÊ®°ÂûãÂ∑Æ„ÄÇÂÑòÁÆ°Â∑≤ÂÅöÂá∫Ë®±Â§öÂä™Âäõ‰æÜË≠òÂà•‰ªªÂãôÂàÜÁµÑ‰∏¶Ë°°Èáè‰∏çÂêå‰ªªÂãô‰πãÈñìÁöÑÈóúËÅØÊÄßÔºå‰ΩÜÂÆöÁæ©‰∏ÄÂÄãÊåáÊ®ô‰ª•ÂæûË®±Â§öÊΩõÂú®‰ªªÂãôÁµÑÂêà‰∏≠Ë≠òÂà•Âá∫ÊúÄ‰Ω≥‰ªªÂãôÂàÜÁµÑ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÁ†îÁ©∂Ë™≤È°å„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÈªûÂºè V ÂèØÁî®Ë≥áË®ä (PVI) Ë°°ÈáèÁöÑ‰ªªÂãôÈõ£Â∫¶‰æÜË°°Èáè‰ªªÂãôÁõ∏ÈóúÊÄßÁöÑÊåáÊ®ô„ÄÇPVI ÊòØ‰∏ÄÂÄãÊúÄËøëÊèêÂá∫ÁöÑÊåáÊ®ôÔºåÁî®Êñº‰º∞Ë®àÁµ¶ÂÆöÊ®°ÂûãË≥áÊñôÈõÜÂåÖÂê´Â§öÂ∞ëÂèØÁî®Ë≥áË®ä„ÄÇÊàëÂÄëÂÅáË®≠ PVI ‰º∞Ë®àÂú®Áµ±Ë®à‰∏äÊ≤íÊúâÂ∑ÆÁï∞ÁöÑ‰ªªÂãôË∂≥Â§†Áõ∏‰ººÔºåÂèØ‰ª•ÂæûËÅØÂêàÂ≠∏ÁøíÈÅéÁ®ã‰∏≠ÂèóÁõä„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©óÔºå‰ª•Ë©ï‰º∞Ê≠§ÊåáÊ®ôÂú® 15 ÂÄã‰∏ÄËà¨„ÄÅÁîüÁâ©ÈÜ´Â≠∏ÂíåËá®Â∫äÈ†òÂüüÁöÑ NLP Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰ªªÂãôÂàÜÁµÑÁöÑÂèØË°åÊÄß„ÄÇÊàëÂÄëÂ∞áËÅØÂêàÂ≠∏ÁøíËÄÖÁöÑÁµêÊûúËàáÂñÆ‰∏ÄÂ≠∏ÁøíËÄÖ„ÄÅÁèæÊúâÁöÑÂü∫Ê∫ñÊñπÊ≥ïÂíåÊúÄËøëÁöÑÂ§ßË™ûË®ÄÊ®°ÂûãÔºàÂåÖÊã¨ Llama 2 Âíå GPT-4ÔºâÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúË°®ÊòéÔºåÈÄöÈÅéÂ∞áÂÖ∑ÊúâÁõ∏‰ºº PVI ‰º∞Ë®àÂÄºÁöÑ‰ªªÂãôÂàÜÁµÑÔºåËÅØÂêàÂ≠∏ÁøíËÄÖ‰ª•ËºÉÂ∞ëÁöÑÁ∏ΩÂèÉÊï∏Áî¢Áîü‰∫ÜÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑÁµêÊûúÔºå‰∏¶‰∏îÂú®ÂêÑÂÄãÈ†òÂüü‰∏≠Ë°®Áèæ‰∏ÄËá¥„ÄÇ

##### **FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**
2410.12707v1 by Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, Xiaowen Chu

To alleviate hardware scarcity in training large deep neural networks (DNNs),
particularly large language models (LLMs), we present FusionLLM, a
decentralized training system designed and implemented for training DNNs using
geo-distributed GPUs across different computing clusters or individual devices.
Decentralized training faces significant challenges regarding system design and
efficiency, including: 1) the need for remote automatic differentiation (RAD),
2) support for flexible model definitions and heterogeneous software, 3)
heterogeneous hardware leading to low resource utilization or the straggler
problem, and 4) slow network communication. To address these challenges, in the
system design, we represent the model as a directed acyclic graph of operators
(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the
edge represents the data dependency between operators. Based on this design, 1)
users are allowed to customize any DNN without caring low-level operator
implementation; 2) we enable the task scheduling with the more fine-grained
sub-tasks, offering more optimization space; 3) a DAG runtime executor can
implement RAD withour requiring the consistent low-level ML framework versions.
  To enhance system efficiency, we implement a workload estimator and design an
OP-Fence scheduler to cluster devices with similar bandwidths together and
partition the DAG to increase throughput. Additionally, we propose an AdaTopK
compressor to adaptively compress intermediate activations and gradients at the
slowest communication links. To evaluate the convergence and efficiency of our
system and algorithms, we train ResNet-101 and GPT-2 on three real-world
testbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental
results demonstrate that our system and method can achieve 1.45 - 9.39x speedup
compared to baseline methods while ensuring convergence.

ÊëòË¶ÅÔºö<paragraph>ÁÇ∫‰∫ÜÊ∏õËºïË®ìÁ∑¥Â§ßÂûãÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN) ÁöÑÁ°¨È´îÁü≠Áº∫ÂïèÈ°åÔºåÂ∞§ÂÖ∂ÊòØÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü FusionLLMÔºå‰∏ÄÂÄãÂàÜÊï£ÂºèË®ìÁ∑¥Á≥ªÁµ±ÔºåÂÖ∂Ë®≠Ë®àÂíåÂØ¶‰ΩúÊòØÁî®ÊñºË®ìÁ∑¥Ë∑®‰∏çÂêåÈÅãÁÆóÂè¢ÈõÜÊàñÂÄãÂà•Ë£ùÁΩÆÁöÑÂú∞ÁêÜÂàÜÊï£Âºè GPU ÁöÑ DNN„ÄÇÂàÜÊï£ÂºèË®ìÁ∑¥Âú®Á≥ªÁµ±Ë®≠Ë®àÂíåÊïàÁéáÊñπÈù¢Èù¢Ëá®ÈáçÂ§ßÊåëÊà∞ÔºåÂåÖÊã¨Ôºö1) ÈúÄË¶ÅÈÅ†Á´ØËá™ÂãïÂæÆÂàÜ (RAD)Ôºå2) ÊîØÊè¥ÂΩàÊÄßÁöÑÊ®°ÂûãÂÆöÁæ©ÂíåÁï∞Ë≥™ËªüÈ´îÔºå3) Áï∞Ë≥™Á°¨È´îÂ∞éËá¥Ë≥áÊ∫êÂà©Áî®Áéá‰ΩéÊàñËêΩÂæåÂïèÈ°åÔºå‰ª•Âèä 4) Á∂≤Ë∑ØÈÄöË®äÈÄüÂ∫¶ÊÖ¢„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÂú®Á≥ªÁµ±Ë®≠Ë®à‰∏≠ÔºåÊàëÂÄëÂ∞áÊ®°ÂûãË°®Á§∫ÁÇ∫‰∏ÄÂÄãÊúâÂêëÈùûÂæ™Áí∞Âúñ (OP-DAG) ÁöÑÈÅãÁÆóÂ≠ê„ÄÇDAG ‰∏≠ÁöÑÊØèÂÄãÁØÄÈªû‰ª£Ë°® DNN ‰∏≠ÁöÑÈÅãÁÆóÂ≠êÔºåËÄåÈÇäÁ∑£‰ª£Ë°®ÈÅãÁÆóÂ≠ê‰πãÈñìÁöÑË≥áÊñô‰æùË≥¥ÊÄß„ÄÇÂü∫ÊñºÊ≠§Ë®≠Ë®àÔºå1) ‰ΩøÁî®ËÄÖÂèØ‰ª•Ëá™Ë®Ç‰ªª‰Ωï DNNÔºåËÄå‰∏çÁî®ËÄÉÊÖÆ‰ΩéÈöéÈÅãÁÆóÂ≠êÂØ¶‰ΩúÔºõ2) ÊàëÂÄëÂïüÁî®‰ªªÂãôÊéíÁ®ãÔºå‰∏¶‰ΩøÁî®Êõ¥Á¥∞Á∑ªÁöÑÂ≠ê‰ªªÂãôÔºåÊèê‰æõÊõ¥Â§öÊúÄ‰Ω≥ÂåñÁ©∫ÈñìÔºõ3) DAG Âü∑Ë°åÊôÇÈñìÂü∑Ë°åÂô®ÂèØ‰ª•ÂØ¶‰Ωú RADÔºåËÄå‰∏çÈúÄË¶Å‰∏ÄËá¥ÁöÑ‰ΩéÈöé ML Êû∂ÊßãÁâàÊú¨„ÄÇÁÇ∫‰∫ÜÊèêÂçáÁ≥ªÁµ±ÊïàÁéáÔºåÊàëÂÄëÂØ¶‰Ωú‰∏ÄÂÄãÂ∑•‰ΩúË≤†Ëºâ‰º∞Ë®àÂô®Ôºå‰∏¶Ë®≠Ë®à‰∏ÄÂÄã OP-Fence ÊéíÁ®ãÂô®ÔºåÂ∞áÈ†ªÂØ¨È°û‰ººÁöÑË£ùÁΩÆÂàÜÁµÑÂú®‰∏ÄËµ∑Ôºå‰∏¶ÂàÜÂâ≤ DAG ‰ª•Â¢ûÂä†ËôïÁêÜÈáè„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã AdaTopK Â£ìÁ∏ÆÂô®Ôºå‰ª•Ëá™ÈÅ©ÊáâÊñπÂºèÂ£ìÁ∏ÆÊúÄÊÖ¢ÈÄöË®äÈÄ£Áµê‰∏äÁöÑ‰∏≠ÈñìÂïüÂãïÂíåÊ¢ØÂ∫¶„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÁ≥ªÁµ±ÂíåÊºîÁÆóÊ≥ïÁöÑÊî∂ÊñÇÊÄßÂíåÊïàÁéáÔºåÊàëÂÄëÂú®‰∏âÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑÊ∏¨Ë©¶Âπ≥Âè∞‰∏äË®ìÁ∑¥ ResNet-101 Âíå GPT-2Ôºå‰ΩøÁî® 48 ÂÄã GPU ÈÄ£Êé•Âà∞ 8 Mbps~10 Gbps Á∂≤Ë∑Ø„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÁ≥ªÁµ±ÂíåÊñπÊ≥ïÂèØ‰ª•ÊØîÂü∫Ê∫ñÊñπÊ≥ïÂø´ 1.45 - 9.39 ÂÄçÔºåÂêåÊôÇÁ¢∫‰øùÊî∂ÊñÇ„ÄÇ</paragraph>

##### **Automatic Mapping of Anatomical Landmarks from Free-Text Using Large Language Models: Insights from Llama-2**
2410.12686v2 by Mohamad Abdi, Gerardo Hermosillo Valadez, Halid Ziya Yerebakan

Anatomical landmarks are vital in medical imaging for navigation and anomaly
detection. Modern large language models (LLMs), like Llama-2, offer promise for
automating the mapping of these landmarks in free-text radiology reports to
corresponding positions in image data. Recent studies propose LLMs may develop
coherent representations of generative processes. Motivated by these insights,
we investigated whether LLMs accurately represent the spatial positions of
anatomical landmarks. Through experiments with Llama-2 models, we found that
they can linearly represent anatomical landmarks in space with considerable
robustness to different prompts. These results underscore the potential of LLMs
to enhance the efficiency and accuracy of medical imaging workflows.

ÊëòË¶ÅÔºöËß£ÂâñÂú∞Ê®ôÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Â∞çÊñºÂ∞éËà™ÂíåÁï∞Â∏∏ÂÅµÊ∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇÂÉè Llama-2 ÈÄôÊ®£ÁöÑÁèæ‰ª£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúâÊúõËá™ÂãïÂ∞áÈÄô‰∫õÂú∞Ê®ôÂ∞çÊáâÂà∞ÂΩ±ÂÉèË≥áÊñô‰∏≠ÁöÑ‰ΩçÁΩÆÔºå‰∏¶Áπ™Ë£ΩÂú®Ëá™Áî±Ê†ºÂºèÁöÑÊîæÂ∞ÑÁßëÂ†±Âëä‰∏≠„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂ÊèêÂá∫ LLM ÂèØËÉΩÈñãÁôºÂá∫ÁîüÊàêÂºèÈÅéÁ®ãÁöÑÁõ∏Âπ≤Ë°®Âæµ„ÄÇÂèóÂà∞ÈÄô‰∫õË¶ãËß£ÁöÑÂïüÁôºÔºåÊàëÂÄëË™øÊü•‰∫Ü LLM ÊòØÂê¶ËÉΩÊ∫ñÁ¢∫Ë°®Á§∫Ëß£ÂâñÂú∞Ê®ôÁöÑÁ©∫Èñì‰ΩçÁΩÆ„ÄÇÈÄèÈÅé‰ΩøÁî® Llama-2 Ê®°ÂûãÈÄ≤Ë°åÂØ¶È©óÔºåÊàëÂÄëÁôºÁèæÂÆÉÂÄëÂèØ‰ª•Á∑öÊÄßË°®Á§∫Á©∫Èñì‰∏≠ÁöÑËß£ÂâñÂú∞Ê®ôÔºå‰∏¶‰∏îÂ∞çÊñº‰∏çÂêåÁöÑÊèêÁ§∫ÂÖ∑ÊúâÁõ∏Áï∂ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÈÄô‰∫õÁµêÊûúÂº∑Ë™ø‰∫Ü LLM ÊèêÂçáÈÜ´Â≠∏ÂΩ±ÂÉèÂ∑•‰ΩúÊµÅÁ®ãÊïàÁéáÂíåÊ∫ñÁ¢∫ÊÄßÁöÑÊΩõÂäõ„ÄÇ

##### **Cascade learning in multi-task encoder-decoder networks for concurrent bone segmentation and glenohumeral joint assessment in shoulder CT scans**
2410.12641v1 by Luca Marsilio, Davide Marzorati, Matteo Rossi, Andrea Moglia, Luca Mainardi, Alfonso Manzotti, Pietro Cerveri

Osteoarthritis is a degenerative condition affecting bones and cartilage,
often leading to osteophyte formation, bone density loss, and joint space
narrowing. Treatment options to restore normal joint function vary depending on
the severity of the condition. This work introduces an innovative deep-learning
framework processing shoulder CT scans. It features the semantic segmentation
of the proximal humerus and scapula, the 3D reconstruction of bone surfaces,
the identification of the glenohumeral (GH) joint region, and the staging of
three common osteoarthritic-related pathologies: osteophyte formation (OS), GH
space reduction (JS), and humeroscapular alignment (HSA). The pipeline
comprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D
Arthro-Net for threefold classification. A retrospective dataset of 571 CT
scans featuring patients with various degrees of GH osteoarthritic-related
pathologies was used to train, validate, and test the pipeline. Root mean
squared error and Hausdorff distance median values for 3D reconstruction were
0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,
outperforming state-of-the-art architectures and making it potentially suitable
for a PSI-based shoulder arthroplasty preoperative plan context. The
classification accuracy for OS, JS, and HSA consistently reached around 90%
across all three categories. The computational time for the inference pipeline
was less than 15s, showcasing the framework's efficiency and compatibility with
orthopedic radiology practice. The outcomes represent a promising advancement
toward the medical translation of artificial intelligence tools. This progress
aims to streamline the preoperative planning pipeline delivering high-quality
bone surfaces and supporting surgeons in selecting the most suitable surgical
approach according to the unique patient joint conditions.

ÊëòË¶ÅÔºöÈ™®ÈóúÁØÄÁÇéÊòØ‰∏ÄÁ®ÆÈÄÄÂåñÊÄßÁñæÁóÖÔºåÊúÉÂΩ±ÈüøÈ™®È™ºÂíåËªüÈ™®Ôºå
ÈÄöÂ∏∏ÊúÉÂ∞éËá¥È™®Ë¥ÖÂΩ¢Êàê„ÄÅÈ™®ÂØÜÂ∫¶ÊµÅÂ§±ÂíåÈóúÁØÄÈñìÈöôËÆäÁ™Ñ„ÄÇÊ≤ªÁôÇÈÅ∏È†ÖÊúÉÊ†πÊìöÁóÖÊÉÖÁöÑÂö¥ÈáçÁ®ãÂ∫¶ËÄåÊúâÊâÄ‰∏çÂêåÔºå‰ª•ÊÅ¢Âæ©Ê≠£Â∏∏ÁöÑÈóúÁØÄÂäüËÉΩ„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÁî®ÊñºËôïÁêÜËÇ©ÈÉ® CT ÊéÉÊèè„ÄÇÂÆÉÁöÑÁâπÈªûÊòØËøëÁ´ØËÇ±È™®ÂíåËÇ©ËÉõÈ™®ÁöÑË™ûÁæ©ÂàÜÂâ≤„ÄÅÈ™®Ë°®Èù¢ÁöÑ 3D ÈáçÂª∫„ÄÅÁõÇËÇ± (GH) ÈóúÁØÄÂçÄÂüüÁöÑË≠òÂà•Ôºå‰ª•Âèä‰∏âÁ®ÆÂ∏∏Ë¶ãÈ™®ÈóúÁØÄÁÇéÁõ∏ÈóúÁóÖÁêÜÁöÑÂàÜÈ°ûÔºöÈ™®Ë¥ÖÂΩ¢Êàê (OS)„ÄÅGH ÈñìÈöôÁ∏ÆÂ∞è (JS) ÂíåËÇ±È™®ËÇ©ËÉõÈ™®Â∞çÈΩä (HSA)„ÄÇË©≤ÁÆ°ÈÅìÂåÖÂê´ÂÖ©ÂÄã‰∏≤ËÅØÁöÑ CNN Êû∂ÊßãÔºöÁî®ÊñºÂàÜÂâ≤ÁöÑ 3D CEL-UNet ÂíåÁî®Êñº‰∏âÂàÜÈ°ûÁöÑ 3D Arthro-Net„ÄÇ‰∏ÄÂÄãÂåÖÂê´ 571 ‰æã CT ÊéÉÊèèÁöÑÂõûÈ°ßÊÄßÊï∏ÊìöÈõÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊÇ£Êúâ‰∏çÂêåÁ®ãÂ∫¶ GH È™®ÈóúÁØÄÁÇéÁõ∏ÈóúÁóÖÁêÜÁöÑÊÇ£ËÄÖÔºåÁî®ÊñºË®ìÁ∑¥„ÄÅÈ©óË≠âÂíåÊ∏¨Ë©¶Ë©≤ÁÆ°ÈÅì„ÄÇËÇ±È™®ÁöÑ 3D ÈáçÂª∫ÁöÑÂùáÊñπÊ†πË™§Â∑ÆÂíå Hausdorff Ë∑ùÈõ¢‰∏≠ÂÄºÁÇ∫ 0.22mm Âíå 1.48mmÔºåËÇ©ËÉõÈ™®ÁöÑÂùáÊñπÊ†πË™§Â∑ÆÂíå Hausdorff Ë∑ùÈõ¢‰∏≠ÂÄºÁÇ∫ 0.24mm Âíå 1.48mmÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊû∂ÊßãÔºå‰ΩøÂÖ∂ÊΩõÂú®Âú∞ÈÅ©Áî®ÊñºÂü∫Êñº PSI ÁöÑËÇ©ÈÉ®ÈóúÁØÄÁΩÆÊèõË°ìË°ìÂâçË®àÂäÉËÉåÊôØ„ÄÇOS„ÄÅJS Âíå HSA ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫ÁéáÂú®ÊâÄÊúâ‰∏âÈ°û‰∏≠ÂßãÁµÇÈÅîÂà∞Á¥Ñ 90%„ÄÇÊé®ÁêÜÁÆ°ÈÅìÁöÑË®àÁÆóÊôÇÈñì‰∏çÂà∞ 15 ÁßíÔºåÂ±ïÁ§∫‰∫ÜË©≤Ê°ÜÊû∂ÁöÑÊïàÁéáÂíåËàáÈ™®ÁßëÊîæÂ∞ÑÂ≠∏ÂØ¶Ë∏êÁöÑÁõ∏ÂÆπÊÄß„ÄÇÁµêÊûú‰ª£Ë°®‰∫Ü‰∫∫Â∑•Êô∫ÊÖßÂ∑•ÂÖ∑ÈÜ´Â≠∏ËΩâÂåñÁöÑÊúâÂ∏åÊúõÁöÑÈÄ≤Â±ï„ÄÇÈÄôÈ†ÖÈÄ≤Â±ïÊó®Âú®Á∞°ÂåñË°ìÂâçË®àÂäÉÁÆ°ÈÅìÔºåÊèê‰æõÈ´òÂìÅË≥™ÁöÑÈ™®Ë°®Èù¢Ôºå‰∏¶ÊîØÊåÅÂ§ñÁßëÈÜ´ÁîüÊ†πÊìöÊÇ£ËÄÖÁç®ÁâπÁöÑÈóúÁØÄÁãÄÊ≥ÅÈÅ∏ÊìáÊúÄÂêàÈÅ©ÁöÑÊâãË°ìÊñπÊ≥ï„ÄÇ

##### **NSSI-Net: Multi-Concept Generative Adversarial Network for Non-Suicidal Self-Injury Detection Using High-Dimensional EEG Signals in a Semi-Supervised Learning Framework**
2410.12159v1 by Zhen Liang, Weishan Ye, Qile Liu, Li Zhang, Gan Huang, Yongjie Zhou

Non-suicidal self-injury (NSSI) is a serious threat to the physical and
mental health of adolescents, significantly increasing the risk of suicide and
attracting widespread public concern. Electroencephalography (EEG), as an
objective tool for identifying brain disorders, holds great promise. However,
extracting meaningful and reliable features from high-dimensional EEG data,
especially by integrating spatiotemporal brain dynamics into informative
representations, remains a major challenge. In this study, we introduce an
advanced semi-supervised adversarial network, NSSI-Net, to effectively model
EEG features related to NSSI. NSSI-Net consists of two key modules: a
spatial-temporal feature extraction module and a multi-concept discriminator.
In the spatial-temporal feature extraction module, an integrated 2D
convolutional neural network (2D-CNN) and a bi-directional Gated Recurrent Unit
(BiGRU) are used to capture both spatial and temporal dynamics in EEG data. In
the multi-concept discriminator, signal, gender, domain, and disease levels are
fully explored to extract meaningful EEG features, considering individual,
demographic, disease variations across a diverse population. Based on
self-collected NSSI data (n=114), the model's effectiveness and reliability are
demonstrated, with a 7.44% improvement in performance compared to existing
machine learning and deep learning methods. This study advances the
understanding and early diagnosis of NSSI in adolescents with depression,
enabling timely intervention. The source code is available at
https://github.com/Vesan-yws/NSSINet.

ÊëòË¶ÅÔºöÈùûËá™ÊùÄÊÄßËá™‰º§ (NSSI) ÂØπÈùíÂ∞ëÂπ¥ÁöÑË∫´ÂøÉÂÅ•Â∫∑ÊûÑÊàê‰∏•ÈáçÂ®ÅËÉÅÔºåÊòæËëóÂ¢ûÂä†‰∫ÜËá™ÊùÄÈ£éÈô©ÔºåÂπ∂ÂºïËµ∑‰∫ÜÂπøÊ≥õÁöÑÂÖ¨‰ºóÂÖ≥Ê≥®„ÄÇËÑëÁîµÂõæ (EEG) ‰Ωú‰∏∫‰∏ÄÁßçËØÜÂà´ËÑëÈÉ®ÁñæÁóÖÁöÑÂÆ¢ËßÇÂ∑•ÂÖ∑ÔºåÂÖ∑ÊúâÂπøÈòîÁöÑÂâçÊôØ„ÄÇÁÑ∂ËÄåÔºå‰ªéÈ´òÁª¥ EEG Êï∞ÊçÆ‰∏≠ÊèêÂèñÊúâÊÑè‰πâ‰∏îÂèØÈù†ÁöÑÁâπÂæÅÔºåÁâπÂà´ÊòØÈÄöËøáÂ∞ÜÊó∂Á©∫ËÑëÂä®ÊÄÅÊï¥ÂêàÂà∞‰ø°ÊÅØË°®Á§∫‰∏≠Ôºå‰ªçÁÑ∂ÊòØ‰∏ÄÈ°πÈáçÂ§ßÊåëÊàò„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨‰ªãÁªç‰∫Ü‰∏Ä‰∏™ÂÖàËøõÁöÑÂçäÁõëÁù£ÂØπÊäóÁΩëÁªú NSSI-NetÔºå‰ª•ÊúâÊïàÂª∫Ê®°‰∏é NSSI Áõ∏ÂÖ≥ÁöÑ EEG ÁâπÂæÅ„ÄÇNSSI-Net Áî±‰∏§‰∏™ÂÖ≥ÈîÆÊ®°ÂùóÁªÑÊàêÔºöÊó∂Á©∫ÁâπÂæÅÊèêÂèñÊ®°ÂùóÂíåÂ§öÊ¶ÇÂøµÂà§Âà´Âô®„ÄÇÂú®Êó∂Á©∫ÁâπÂæÅÊèêÂèñÊ®°Âùó‰∏≠ÔºåÈõÜÊàêÁöÑ‰∫åÁª¥Âç∑ÁßØÁ•ûÁªèÁΩëÁªú (2D-CNN) ÂíåÂèåÂêëÈó®ÊéßÂæ™ÁéØÂçïÂÖÉ (BiGRU) Áî®‰∫éÊçïÊçâ EEG Êï∞ÊçÆ‰∏≠ÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥Âä®ÊÄÅ„ÄÇÂú®Â§öÊ¶ÇÂøµÂà§Âà´Âô®‰∏≠ÔºåÂÖÖÂàÜÊé¢Á¥¢‰ø°Âè∑„ÄÅÊÄßÂà´„ÄÅÂüüÂíåÁñæÁóÖÊ∞¥Âπ≥Ôºå‰ª•ÊèêÂèñÊúâÊÑè‰πâÁöÑ EEG ÁâπÂæÅÔºåËÄÉËôë‰∏çÂêå‰∫∫Áæ§‰∏≠ÁöÑ‰∏™‰Ωì„ÄÅ‰∫∫Âè£ÁªüËÆ°Â≠¶„ÄÅÁñæÁóÖÂèòÂºÇ„ÄÇÂü∫‰∫éËá™Êî∂ÈõÜÁöÑ NSSI Êï∞ÊçÆ (n=114)ÔºåËØ•Ê®°ÂûãÁöÑÊúâÊïàÊÄßÂíåÂèØÈù†ÊÄßÂæóÂà∞ËØÅÂÆûÔºå‰∏éÁé∞ÊúâÁöÑÊú∫Âô®Â≠¶‰π†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåÊÄßËÉΩÊèêÈ´ò‰∫Ü 7.44%„ÄÇËøôÈ°πÁ†îÁ©∂‰øÉËøõ‰∫ÜÂØπÊÇ£ÊúâÊäëÈÉÅÁóáÁöÑÈùíÂ∞ëÂπ¥ NSSI ÁöÑÁêÜËß£ÂíåÊó©ÊúüËØäÊñ≠ÔºåÂÆûÁé∞‰∫ÜÂèäÊó∂ÁöÑÂπ≤È¢Ñ„ÄÇÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/Vesan-yws/NSSINet Ëé∑Âæó„ÄÇ

##### **SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding**
2410.11761v1 by Ying Chen, Guoan Wang, Yuanfeng Ji, Yanjun Li, Jin Ye, Tianbin Li, Bin Zhang, Nana Pei, Rongshan Yu, Yu Qiao, Junjun He

Despite the progress made by multimodal large language models (MLLMs) in
computational pathology, they remain limited by a predominant focus on
patch-level analysis, missing essential contextual information at the
whole-slide level. The lack of large-scale instruction datasets and the
gigapixel scale of whole slide images (WSIs) pose significant developmental
challenges. In this paper, we present SlideChat, the first vision-language
assistant capable of understanding gigapixel whole-slide images, exhibiting
excellent multimodal conversational capability and response complex instruction
across diverse pathology scenarios. To support its development, we created
SlideInstruction, the largest instruction-following dataset for WSIs consisting
of 4.2K WSI captions and 176K VQA pairs with multiple categories. Furthermore,
we propose SlideBench, a multimodal benchmark that incorporates captioning and
VQA tasks to assess SlideChat's capabilities in varied clinical settings such
as microscopy, diagnosis. Compared to both general and specialized MLLMs,
SlideChat exhibits exceptional capabilities achieving state-of-the-art
performance on 18 of 22 tasks. For example, it achieved an overall accuracy of
81.17% on SlideBench-VQA (TCGA), and 54.15% on SlideBench-VQA (BCNB). We will
fully release SlideChat, SlideInstruction and SlideBench as open-source
resources to facilitate research and development in computational pathology.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) Âú®Ë®àÁÆóÁóÖÁêÜÂ≠∏ÊñπÈù¢ÂèñÂæó‰∫ÜÈÄ≤Â±ïÔºå‰ΩÜÂÆÉÂÄë‰ªçÁÑ∂ÂèóÈôêÊñºÂ∞çÂçÄÂ°äÁ¥öÂàÜÊûêÁöÑÈóúÊ≥®ÔºåÈåØÂ§±‰∫ÜÂÖ®ÂπªÁáàÁâáÁ¥öÂà•ÁöÑÂøÖË¶ÅËÑàÁµ°Ë≥áË®ä„ÄÇÁº∫‰πèÂ§ßË¶èÊ®°ÁöÑÊåá‰ª§Ë≥áÊñôÈõÜÂíåÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉè (WSI) ÁöÑÂêâÂÉèÁ¥†Ë¶èÊ®°ÔºåÊßãÊàê‰∫ÜÈáçÂ§ßÁöÑÈñãÁôºÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SlideChatÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãËÉΩÂ§†ÁêÜËß£ÂêâÂÉèÁ¥†ÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉèÁöÑË¶ñË¶∫Ë™ûË®ÄÂä©ÁêÜÔºåÂ±ïÁèæÂá∫ÂÑ™ÁßÄÁöÑÂ§öÊ®°ÊÖãÂ∞çË©±ËÉΩÂäõÂíåÂ∞çÂêÑÁ®ÆÁóÖÁêÜÊÉÖÂ¢ÉÁöÑË§áÈõúÊåá‰ª§ÂõûÊáâ„ÄÇÁÇ∫‰∫ÜÊîØÊè¥ÂÖ∂ÈñãÁôºÔºåÊàëÂÄëÂª∫Á´ã‰∫Ü SlideInstructionÔºåÈÄôÊòØÊúÄÂ§ßÁöÑ WSI Êåá‰ª§ÈÅµÂæ™Ë≥áÊñôÈõÜÔºåÂåÖÂê´ 4.2K WSI Ê®ôÈ°åÂíå 176K ÂÄãÂÖ∑ÊúâÂ§öÂÄãÈ°ûÂà•ÁöÑ VQA ÈÖçÂ∞ç„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SlideBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÊ®°ÊÖãÂü∫Ê∫ñÔºåÁµêÂêà‰∫ÜÊ®ôÈ°åÂíå VQA ‰ªªÂãôÔºå‰ª•Ë©ï‰º∞ SlideChat Âú®È°ØÂæÆÈè°Ê™¢Êü•„ÄÅË®∫Êñ∑Á≠â‰∏çÂêåËá®Â∫äË®≠ÂÆö‰∏≠ÁöÑËÉΩÂäõ„ÄÇËàá‰∏ÄËà¨ÂíåÂ∞àÈñÄÁöÑ MLLM Áõ∏ÊØîÔºåSlideChat Â±ïÁèæ‰∫ÜÂçìË∂äÁöÑËÉΩÂäõÔºåÂú® 22 ÂÄã‰ªªÂãô‰∏≠ÁöÑ 18 ÂÄã‰ªªÂãô‰∏≠ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂÆÉÂú® SlideBench-VQA (TCGA) ‰∏äÈÅîÂà∞‰∫Ü 81.17% ÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶ÔºåÂú® SlideBench-VQA (BCNB) ‰∏äÈÅîÂà∞‰∫Ü 54.15%„ÄÇÊàëÂÄëÂ∞áÂÖ®Èù¢ÈáãÂá∫ SlideChat„ÄÅSlideInstruction Âíå SlideBench ‰ΩúÁÇ∫ÈñãÊîæÂéüÂßãÁ¢ºË≥áÊ∫êÔºå‰ª•‰øÉÈÄ≤Ë®àÁÆóÁóÖÁêÜÂ≠∏ÁöÑÁ†îÁ©∂ÂíåÈñãÁôº„ÄÇ

##### **RS-MOCO: A deep learning-based topology-preserving image registration method for cardiac T1 mapping**
2410.11651v1 by Chiyi Huang, Longwei Sun, Dong Liang, Haifeng Liang, Hongwu Zeng, Yanjie Zhu

Cardiac T1 mapping can evaluate various clinical symptoms of myocardial
tissue. However, there is currently a lack of effective, robust, and efficient
methods for motion correction in cardiac T1 mapping. In this paper, we propose
a deep learning-based and topology-preserving image registration framework for
motion correction in cardiac T1 mapping. Notably, our proposed implicit
consistency constraint dubbed BLOC, to some extent preserves the image topology
in registration by bidirectional consistency constraint and local anti-folding
constraint. To address the contrast variation issue, we introduce a weighted
image similarity metric for multimodal registration of cardiac T1-weighted
images. Besides, a semi-supervised myocardium segmentation network and a
dual-domain attention module are integrated into the framework to further
improve the performance of the registration. Numerous comparative experiments,
as well as ablation studies, demonstrated the effectiveness and high robustness
of our method. The results also indicate that the proposed weighted image
similarity metric, specifically crafted for our network, contributes a lot to
the enhancement of the motion correction efficacy, while the bidirectional
consistency constraint combined with the local anti-folding constraint ensures
a more desirable topology-preserving registration mapping.

ÊëòË¶ÅÔºöÂøÉËÇå T1 Â∞çÊØîÂ∫¶ÊàêÂÉèÂèØË©ï‰º∞ÂøÉËÇåÁµÑÁπîÁöÑÂêÑÁ®ÆËá®Â∫äË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÂú®ÂøÉËÇå T1 Â∞çÊØîÂ∫¶ÊàêÂÉè‰∏≠ÔºåÁº∫‰πèÊúâÊïà„ÄÅÁ©©ÂÅ•‰∏îÈ´òÊïàÁöÑÈÅãÂãïÊ†°Ê≠£ÊñπÊ≥ï„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÊ∑±Â∫¶Â≠∏Áøí‰∏î‰øùÁïôÊãìÊí≤ÁöÑÂΩ±ÂÉèÈÖçÊ∫ñÊû∂ÊßãÔºåÁî®ÊñºÂøÉËÇå T1 Â∞çÊØîÂ∫¶ÊàêÂÉè‰∏≠ÁöÑÈÅãÂãïÊ†°Ê≠£„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÊèêÂá∫ÁöÑÈö±Âºè‰∏ÄËá¥ÊÄßÁ¥ÑÊùüÁ®±ÁÇ∫ BLOCÔºåÂú®ÊüêÁ®ÆÁ®ãÂ∫¶‰∏äÈÄèÈÅéÈõôÂêë‰∏ÄËá¥ÊÄßÁ¥ÑÊùüÂíåÂ±ÄÈÉ®ÊäóÊë∫ÁñäÁ¥ÑÊùüÔºåÂú®ÈÖçÊ∫ñ‰∏≠‰øùÁïôÂΩ±ÂÉèÊãìÊí≤„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Â∞çÊØîÂ∫¶ËÆäÂåñÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•Âä†Ê¨äÂΩ±ÂÉèÁõ∏‰ººÊÄßÂ∫¶ÈáèÔºåÁî®ÊñºÂøÉËÇå T1 Âä†Ê¨äÂΩ±ÂÉèÁöÑÂ§öÊ®°ÂºèÈÖçÊ∫ñ„ÄÇÊ≠§Â§ñÔºå‰∏ÄÂÄãÂçäÁõ£Áù£ÂøÉËÇåÂàÜÂâ≤Á∂≤Ë∑ØÂíå‰∏ÄÂÄãÈõôÂüüÊ≥®ÊÑèÂäõÊ®°ÁµÑË¢´Êï¥ÂêàÂà∞Êû∂Êßã‰∏≠Ôºå‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÂçáÈÖçÊ∫ñÊïàËÉΩ„ÄÇÂ§ßÈáèÁöÑÊØîËºÉÂØ¶È©óÂíåÊ∂àËûçÁ†îÁ©∂Ë≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÈ´òÁ©©ÂÅ•ÊÄß„ÄÇÁµêÊûú‰πüË°®ÊòéÔºåÂ∞àÈñÄÁÇ∫ÊàëÂÄëÁöÑÁ∂≤Ë∑ØË®≠Ë®àÁöÑÊèêÂá∫ÁöÑÂä†Ê¨äÂΩ±ÂÉèÁõ∏‰ººÊÄßÂ∫¶ÈáèÔºåÂ∞çÈÅãÂãïÊ†°Ê≠£ÊïàËÉΩÁöÑÊèêÂçáÊúâÂæàÂ§ßË≤¢ÁçªÔºåËÄåÈõôÂêë‰∏ÄËá¥ÊÄßÁ¥ÑÊùüÁµêÂêàÂ±ÄÈÉ®ÊäóÊë∫ÁñäÁ¥ÑÊùüÔºåÂèØÁ¢∫‰øùÊõ¥ÁêÜÊÉ≥ÁöÑ‰øùÁïôÊãìÊí≤ÈÖçÊ∫ñÂ∞çÊáâ„ÄÇ

##### **Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development**
2410.11550v1 by Tengfei Ma, Xuan Lin, Tianle Li, Chaoyi Li, Long Chen, Peng Zhou, Xibao Cai, Xinyu Yang, Daojian Zeng, Dongsheng Cao, Xiangxiang Zeng

Large Language Models (LLMs) have recently demonstrated remarkable
performance in general tasks across various fields. However, their
effectiveness within specific domains such as drug development remains
challenges. To solve these challenges, we introduce \textbf{Y-Mol}, forming a
well-established LLM paradigm for the flow of drug development. Y-Mol is a
multiscale biomedical knowledge-guided LLM designed to accomplish tasks across
lead compound discovery, pre-clinic, and clinic prediction. By integrating
millions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,
Y-Mol augments the reasoning capability in the biomedical domain by learning
from a corpus of publications, knowledge graphs, and expert-designed synthetic
data. The capability is further enriched with three types of drug-oriented
instructions: description-based prompts from processed publications,
semantic-based prompts for extracting associations from knowledge graphs, and
template-based prompts for understanding expert knowledge from biomedical
tools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously
execute the downstream tasks across the entire process of drug development,
including virtual screening, drug design, pharmacological properties
prediction, and drug-related interaction prediction. Our extensive evaluations
of various biomedical sources demonstrate that Y-Mol significantly outperforms
general-purpose LLMs in discovering lead compounds, predicting molecular
properties, and identifying drug interaction events.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËøëÊúüÂú®ÂêÑÂÄãÈ†òÂüüÁöÑÈÄöÁî®‰ªªÂãô‰∏≠Â±ïÁ§∫Âá∫È°ØËëóÁöÑË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÁâπÂÆöÈ†òÂüüÔºà‰æãÂ¶ÇËó•Áâ©ÈñãÁôºÔºâ‰∏≠ÁöÑÊïàËÉΩ‰ªçÊúâÂæÖÂä†Âº∑„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü **Y-Mol**ÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÂÄãÂÆåÂñÑÁöÑ LLM ÂÖ∏ÁØÑÔºåÁî®ÊñºËó•Áâ©ÈñãÁôºÊµÅÁ®ã„ÄÇY-Mol ÊòØ‰∏ÄÂÄãÂ§öÂ∞∫Â∫¶ÁöÑÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂºïÂ∞é LLMÔºåÊó®Âú®ÂÆåÊàêÂÖàÂ∞éÂåñÂêàÁâ©ÁôºÁèæ„ÄÅËá®Â∫äÂâçÂíåËá®Â∫äÈ†êÊ∏¨Á≠â‰ªªÂãô„ÄÇÈÄèÈÅéÊï¥ÂêàÊï∏ÁôæËê¨ÂÄãÂ§öÂ∞∫Â∫¶ÁöÑÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÔºå‰∏¶‰ΩøÁî® LLaMA2 ‰ΩúÁÇ∫Âü∫Á§é LLMÔºåY-Mol ÂæûÂá∫ÁâàÁâ©„ÄÅÁü•Ë≠òÂúñË≠úÂíåÂ∞àÂÆ∂Ë®≠Ë®àÁöÑÂêàÊàêË≥áÊñô‰∏≠Â≠∏ÁøíÔºåÂ¢ûÂº∑‰∫ÜÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÖ∂ËÉΩÂäõÈÄ≤‰∏ÄÊ≠•ÈÄèÈÅé‰∏âÁ®ÆÈ°ûÂûãÁöÑËó•Áâ©Â∞éÂêëÊåá‰ª§ÂæóÂà∞Ë±êÂØåÔºöÂ∑≤ËôïÁêÜÂá∫ÁâàÁâ©ÁöÑÂü∫ÊñºÊèèËø∞ÁöÑÊèêÁ§∫„ÄÅÁî®ÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ÊèêÂèñÈóúËÅØÁöÑÂü∫ÊñºË™ûÁæ©ÁöÑÊèêÁ§∫Ôºå‰ª•ÂèäÁî®ÊñºÁêÜËß£ÁîüÁâ©ÈÜ´Â≠∏Â∑•ÂÖ∑‰∏≠Â∞àÂÆ∂Áü•Ë≠òÁöÑÂü∫ÊñºÁØÑÊú¨ÁöÑÊèêÁ§∫„ÄÇÊ≠§Â§ñÔºåY-Mol Êèê‰æõ‰∫Ü‰∏ÄÁµÑ LLM ÂÖ∏ÁØÑÔºåÂèØ‰ª•Âú®Êï¥ÂÄãËó•Áâ©ÈñãÁôºÈÅéÁ®ã‰∏≠Ëá™‰∏ªÂü∑Ë°å‰∏ãÊ∏∏‰ªªÂãôÔºåÂåÖÊã¨ËôõÊì¨ÁØ©ÈÅ∏„ÄÅËó•Áâ©Ë®≠Ë®à„ÄÅËó•ÁêÜÁâπÊÄßÈ†êÊ∏¨ÂíåËó•Áâ©Áõ∏Èóú‰∫§‰∫íÈ†êÊ∏¨„ÄÇÊàëÂÄëÂ∞çÂêÑÁ®ÆÁîüÁâ©ÈÜ´Â≠∏‰æÜÊ∫êÁöÑÂª£Ê≥õË©ï‰º∞Ë°®ÊòéÔºåY-Mol Âú®ÁôºÁèæÂÖàÂ∞éÂåñÂêàÁâ©„ÄÅÈ†êÊ∏¨ÂàÜÂ≠êÁâπÊÄßÂíåË≠òÂà•Ëó•Áâ©‰∫§‰∫í‰∫ã‰ª∂ÊñπÈù¢È°ØËëóÂÑ™ÊñºÈÄöÁî® LLM„ÄÇ

##### **AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data**
2410.11531v1 by Xinjie Zhao, Moritz Blum, Rui Yang, Boming Yang, Luis M√°rquez Carpintero, M√≥nica Pina-Navarro, Tony Wang, Xin Li, Huitao Li, Yanran Fu, Rongrong Wang, Juntao Zhang, Irene Li

Large Language Models~(LLMs) have demonstrated capabilities across various
applications but face challenges such as hallucination, limited reasoning
abilities, and factual inconsistencies, especially when tackling complex,
domain-specific tasks like question answering~(QA). While Knowledge
Graphs~(KGs) have been shown to help mitigate these issues, research on the
integration of LLMs with background KGs remains limited. In particular, user
accessibility and the flexibility of the underlying KG have not been thoroughly
explored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based
Interaction and Graphical Representation), a platform for knowledge management
through natural language interaction. It integrates knowledge extraction,
integration, and real-time visualization. AGENTiGraph employs a multi-agent
architecture to dynamically interpret user intents, manage tasks, and integrate
new knowledge, ensuring adaptability to evolving user requirements and data
contexts. Our approach demonstrates superior performance in knowledge graph
interactions, particularly for complex domain-specific tasks. Experimental
results on a dataset of 3,500 test cases show AGENTiGraph significantly
outperforms state-of-the-art zero-shot baselines, achieving 95.12\% accuracy in
task classification and 90.45\% success rate in task execution. User studies
corroborate its effectiveness in real-world scenarios. To showcase versatility,
we extended AGENTiGraph to legislation and healthcare domains, constructing
specialized KGs capable of answering complex queries in legal and medical
contexts.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®ÆÊáâÁî®‰∏≠Â±ïÁèæÂÖ∂ËÉΩÂäõÔºå‰ΩÜ‰ªçÈù¢Ëá®ÂπªË¶∫„ÄÅÊé®ÁêÜËÉΩÂäõÊúâÈôêÂíå‰∫ãÂØ¶‰∏ç‰∏ÄËá¥Á≠âÊåëÊà∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËôïÁêÜË§áÈõúÁöÑÁâπÂÆöÈ†òÂüü‰ªªÂãôÔºå‰æãÂ¶ÇÂïèÁ≠î (QA) ÊôÇ„ÄÇÈõñÁÑ∂Áü•Ë≠òÂúñË≠ú (KG) Â∑≤Ë¢´Ë≠âÊòéÊúâÂä©ÊñºÁ∑©Ëß£ÈÄô‰∫õÂïèÈ°åÔºå‰ΩÜ LLM ËàáËÉåÊôØ KG Êï¥ÂêàÁöÑÁ†îÁ©∂‰ªçÁÑ∂ÊúâÈôê„ÄÇÁâπÂà•ÊòØÔºå‰ΩøÁî®ËÄÖÁöÑÂèØÂèäÊÄßÂíåÂ∫ïÂ±§ KG ÁöÑÈùàÊ¥ªÊÄßÂ∞öÊú™ÂæóÂà∞ÂæπÂ∫ïÊé¢Ë®é„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü AGENTiGraphÔºàÁî®Êñº‰ªªÂãôÂûã‰∫íÂãïÂíåÂúñÂΩ¢Ë°®Á§∫ÁöÑËá™ÈÅ©ÊáâÁîüÊàêÂºïÊìéÔºâÔºå‰∏ÄÂÄãÈÄèÈÅéËá™ÁÑ∂Ë™ûË®Ä‰∫íÂãïÈÄ≤Ë°åÁü•Ë≠òÁÆ°ÁêÜÁöÑÂπ≥Âè∞„ÄÇÂÆÉÊï¥Âêà‰∫ÜÁü•Ë≠òËêÉÂèñ„ÄÅÊï¥ÂêàÂíåÂç≥ÊôÇË¶ñË¶∫Âåñ„ÄÇAGENTiGraph Êé°Áî®Â§ö‰ª£ÁêÜÊû∂ÊßãÔºå‰ª•ÂãïÊÖãËß£ËÆÄ‰ΩøÁî®ËÄÖÁöÑÊÑèÂúñ„ÄÅÁÆ°ÁêÜ‰ªªÂãô‰∏¶Êï¥ÂêàÊñ∞Áü•Ë≠òÔºåÁ¢∫‰øùÈÅ©Êáâ‰∏çÊñ∑ËÆäÂåñÁöÑ‰ΩøÁî®ËÄÖÈúÄÊ±ÇÂíåË≥áÊñôËÑàÁµ°„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Áü•Ë≠òÂúñË≠ú‰∫íÂãï‰∏≠Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂ∞çÊñºË§áÈõúÁöÑÁâπÂÆöÈ†òÂüü‰ªªÂãô„ÄÇÂú® 3,500 ÂÄãÊ∏¨Ë©¶Ê°à‰æãÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåAGENTiGraph ÊòéÈ°ØÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂü∫Ê∫ñÔºåÂú®‰ªªÂãôÂàÜÈ°û‰∏≠ÈÅîÂà∞ 95.12% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂú®‰ªªÂãôÂü∑Ë°å‰∏≠ÈÅîÂà∞ 90.45% ÁöÑÊàêÂäüÁéá„ÄÇ‰ΩøÁî®ËÄÖÁ†îÁ©∂Ë≠âÂØ¶‰∫ÜÂÆÉÂú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁÇ∫‰∫ÜÂ±ïÁ§∫ÂÖ∂Â§öÂäüËÉΩÊÄßÔºåÊàëÂÄëÂ∞á AGENTiGraph Âª∂‰º∏Âà∞Ê≥ïÂæãÂíåÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÂª∫Êßã‰∫ÜËÉΩÂ§†ÂõûÁ≠îÊ≥ïÂæãÂíåÈÜ´ÁôÇËÑàÁµ°‰∏≠Ë§áÈõúÊü•Ë©¢ÁöÑÂ∞àÊ•≠Áü•Ë≠òÂúñË≠ú„ÄÇ

##### **Explainable AI Methods for Multi-Omics Analysis: A Survey**
2410.11910v1 by Ahmad Hussein, Mukesh Prasad, Ali Braytee

Advancements in high-throughput technologies have led to a shift from
traditional hypothesis-driven methodologies to data-driven approaches.
Multi-omics refers to the integrative analysis of data derived from multiple
'omes', such as genomics, proteomics, transcriptomics, metabolomics, and
microbiomics. This approach enables a comprehensive understanding of biological
systems by capturing different layers of biological information. Deep learning
methods are increasingly utilized to integrate multi-omics data, offering
insights into molecular interactions and enhancing research into complex
diseases. However, these models, with their numerous interconnected layers and
nonlinear relationships, often function as black boxes, lacking transparency in
decision-making processes. To overcome this challenge, explainable artificial
intelligence (xAI) methods are crucial for creating transparent models that
allow clinicians to interpret and work with complex data more effectively. This
review explores how xAI can improve the interpretability of deep learning
models in multi-omics research, highlighting its potential to provide
clinicians with clear insights, thereby facilitating the effective application
of such models in clinical settings.

ÊëòË¶ÅÔºöÈ´òÈÄöÈáèÊäÄË°ìÁöÑÈÄ≤Ê≠•Â∞éËá¥ÂæûÂÇ≥Áµ±ÁöÑÂÅáË®≠È©ÖÂãïÊñπÊ≥ïËΩâËÆäÁÇ∫Ë≥áÊñôÈ©ÖÂãïÁöÑÊñπÊ≥ï„ÄÇÂ§öÁµÑÂ≠∏ÊòØÊåáÊï¥ÂêàÂàÜÊûê‰æÜËá™Â§öÂÄã„ÄåÁµÑÂ≠∏„ÄçÁöÑË≥áÊñôÔºå‰æãÂ¶ÇÂü∫Âõ†ÁµÑÂ≠∏„ÄÅËõãÁôΩË≥™ÁµÑÂ≠∏„ÄÅËΩâÈåÑÁµÑÂ≠∏„ÄÅ‰ª£Ë¨ùÁµÑÂ≠∏ÂíåÂæÆÁîüÁâ©ÁµÑÂ≠∏„ÄÇÊ≠§ÊñπÊ≥ïÈÄèÈÅéÊì∑ÂèñÁîüÁâ©Ë≥áË®äÁöÑ‰∏çÂêåÂ±§Èù¢ÔºåËÉΩÂÖ®Èù¢‰∫ÜËß£ÁîüÁâ©Á≥ªÁµ±„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÊÑà‰æÜÊÑàÂ∏∏Ë¢´Áî®ÊñºÊï¥ÂêàÂ§öÁµÑÂ≠∏Ë≥áÊñôÔºåÊèê‰æõÂàÜÂ≠ê‰∫§‰∫í‰ΩúÁî®ÁöÑÊ¥ûÂØüÂäõÔºå‰∏¶Âä†Âº∑Â∞çË§áÈõúÁñæÁóÖÁöÑÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°ÂûãÂÖ∑ÊúâË®±Â§öÁõ∏‰∫íÈÄ£Êé•ÁöÑÂ±§Á¥öÂíåÈùûÁ∑öÊÄßÈóú‰øÇÔºåÈÄöÂ∏∏ÊúÉÂÉèÈªëÁõíÂ≠ê‰∏ÄÊ®£ÈÅã‰ΩúÔºåÁº∫‰πèÊ±∫Á≠ñÈÅéÁ®ãÁöÑÈÄèÊòéÂ∫¶„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÊ≠§ÊåëÊà∞ÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (xAI) ÊñπÊ≥ïÂ∞çÊñºÂª∫Á´ãÈÄèÊòéÊ®°ÂûãËá≥ÈóúÈáçË¶ÅÔºåËÆìËá®Â∫äÈÜ´ÁîüÂèØ‰ª•Êõ¥ÊúâÊïàÂú∞Ëß£ÈáãÂíåËôïÁêÜË§áÈõúË≥áÊñô„ÄÇÊ≠§Ë©ïË´ñÊé¢Ë®é xAI Â¶Ç‰ΩïËÉΩÊîπÂñÑÂ§öÁµÑÂ≠∏Á†îÁ©∂‰∏≠Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºåÂº∑Ë™øÂÖ∂Êèê‰æõËá®Â∫äÈÜ´ÁîüÊòéÁ¢∫Ë¶ãËß£ÁöÑÊΩõÂäõÔºåÈÄ≤ËÄå‰øÉÈÄ≤Ê≠§È°ûÊ®°ÂûãÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊáâÁî®„ÄÇ

##### **HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications**
2410.11239v1 by Weijie Xu, Jay Desai, Fanyou Wu, Josef Valvoda, Srinivasan H. Sengamedu

Recent LLM (Large Language Models) advancements benefit many fields such as
education and finance, but HR has hundreds of repetitive processes, such as
access requests, medical claim filing and time-off submissions, which are
unaddressed. We relate these tasks to the LLM agent, which has addressed tasks
such as writing assisting and customer support. We present HR-Agent, an
efficient, confidential, and HR-specific LLM-based task-oriented dialogue
system tailored for automating repetitive HR processes such as medical claims
and access requests. Since conversation data is not sent to an LLM during
inference, it preserves confidentiality required in HR-related tasks.

ÊëòË¶ÅÔºöËøëÊúüÁöÑ LLMÔºàÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºâËøõÊ≠•ÊÉ†Âèä‰∫ÜËÆ∏Â§öÈ¢ÜÂüüÔºå‰æãÂ¶ÇÊïôËÇ≤ÂíåÈáëËûçÔºå‰ΩÜ‰∫∫ÂäõËµÑÊ∫êÊúâÊï∞Áôæ‰∏™ÈáçÂ§çÊÄßÁöÑÊµÅÁ®ãÔºå‰æãÂ¶ÇÂ≠òÂèñË¶ÅÊ±Ç„ÄÅÂåªÁñóÁ¥¢ËµîÁî≥Êä•Âíå‰ºëÂÅáÊèê‰∫§ÔºåËøô‰∫õÈóÆÈ¢òÂ∞öÊú™Ëß£ÂÜ≥„ÄÇÊàë‰ª¨Â∞ÜËøô‰∫õ‰ªªÂä°‰∏é LLM ‰ª£ÁêÜËÅîÁ≥ªËµ∑Êù•ÔºåËØ•‰ª£ÁêÜÂ∑≤Ëß£ÂÜ≥ËØ∏Â¶ÇÂÜô‰ΩúËæÖÂä©ÂíåÂÆ¢Êà∑ÊîØÊåÅ‰πãÁ±ªÁöÑ‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫ HR-AgentÔºåËøôÊòØ‰∏Ä‰∏™È´òÊïà„ÄÅ‰øùÂØÜ‰∏îÈíàÂØπ‰∫∫ÂäõËµÑÊ∫êÁöÑÁâπÂÆö LLM ‰∏∫Âü∫Á°ÄÁöÑ‰ªªÂä°ÂØºÂêëÂØπËØùÁ≥ªÁªüÔºå‰∏ì‰∏∫Ëá™Âä®ÂåñÈáçÂ§çÊÄß‰∫∫ÂäõËµÑÊ∫êÊµÅÁ®ãÔºà‰æãÂ¶ÇÂåªÁñóÁ¥¢ËµîÂíåÂ≠òÂèñËØ∑Ê±ÇÔºâËÄåËÆæËÆ°„ÄÇÁî±‰∫éÂØπËØùÊï∞ÊçÆÂú®Êé®ÁêÜËøáÁ®ã‰∏≠‰∏ç‰ºöÂèëÈÄÅÂà∞ LLMÔºåÂõ†Ê≠§ÂÆÉ‰øùÁïô‰∫Ü‰∫∫ÂäõËµÑÊ∫êÁõ∏ÂÖ≥‰ªªÂä°ÊâÄÈúÄÁöÑÊú∫ÂØÜÊÄß„ÄÇ

##### **SplitSEE: A Splittable Self-supervised Framework for Single-Channel EEG Representation Learning**
2410.11200v1 by Rikuto Kotoge, Zheng Chen, Tasuku Kimura, Yasuko Matsubara, Takufumi Yanagisawa, Haruhiko Kishima, Yasushi Sakurai

While end-to-end multi-channel electroencephalography (EEG) learning
approaches have shown significant promise, their applicability is often
constrained in neurological diagnostics, such as intracranial EEG resources.
When provided with a single-channel EEG, how can we learn representations that
are robust to multi-channels and scalable across varied tasks, such as seizure
prediction? In this paper, we present SplitSEE, a structurally splittable
framework designed for effective temporal-frequency representation learning in
single-channel EEG. The key concept of SplitSEE is a self-supervised framework
incorporating a deep clustering task. Given an EEG, we argue that the time and
frequency domains are two distinct perspectives, and hence, learned
representations should share the same cluster assignment. To this end, we first
propose two domain-specific modules that independently learn domain-specific
representation and address the temporal-frequency tradeoff issue in
conventional spectrogram-based methods. Then, we introduce a novel clustering
loss to measure the information similarity. This encourages representations
from both domains to coherently describe the same input by assigning them a
consistent cluster. SplitSEE leverages a pre-training-to-fine-tuning framework
within a splittable architecture and has following properties: (a)
Effectiveness: it learns representations solely from single-channel EEG but has
even outperformed multi-channel baselines. (b) Robustness: it shows the
capacity to adapt across different channels with low performance variance.
Superior performance is also achieved with our collected clinical dataset. (c)
Scalability: With just one fine-tuning epoch, SplitSEE achieves high and stable
performance using partial model layers.

ÊëòË¶ÅÔºö<paragraph>ÈõñÁÑ∂Á´ØÂà∞Á´ØÂ§öÈÄöÈÅìËÖ¶ÈõªÂúñ (EEG) Â≠∏ÁøíÊñπÊ≥ïÂ∑≤Â±ïÁèæÂá∫È°ØËëóÁöÑÂ∏åÊúõÔºå‰ΩÜÂÖ∂ÈÅ©Áî®ÊÄßÂú®Á•ûÁ∂ìË®∫Êñ∑Ôºå‰æãÂ¶ÇÈ°±ÂÖß EEG Ë≥áÊ∫ê‰∏≠ÔºåÈÄöÂ∏∏ÂèóÂà∞ÈôêÂà∂„ÄÇÁï∂Êèê‰æõÂñÆÈÄöÈÅì EEG ÊôÇÔºåÊàëÂÄëÂ¶Ç‰ΩïÂ≠∏ÁøíÂ∞çÂ§öÈÄöÈÅìÁ©©ÂÅ•‰∏îÂèØÊì¥Â±ïÂà∞ÂêÑÁ®Æ‰ªªÂãôÔºà‰æãÂ¶ÇÁô≤ÁôáÈ†êÊ∏¨ÔºâÁöÑË°®ÂæµÔºüÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ SplitSEEÔºå‰∏ÄÂÄãÁµêÊßãÂèØÂàÜÂâ≤ÁöÑÊ°ÜÊû∂ÔºåÂ∞àÁÇ∫Âú®ÂñÆÈÄöÈÅì EEG ‰∏≠ÈÄ≤Ë°åÊúâÊïàÁöÑÊôÇÈ†ªË°®ÂæµÂ≠∏ÁøíËÄåË®≠Ë®à„ÄÇSplitSEE ÁöÑÈóúÈçµÊ¶ÇÂøµÊòØ‰∏ÄÂÄãËá™Áõ£Áù£ÁöÑÊ°ÜÊû∂ÔºåÁµêÂêà‰∫Ü‰∏ÄÂÄãÊ∑±Â∫¶ËÅöÈ°û‰ªªÂãô„ÄÇÁµ¶ÂÆö‰∏ÄÂÄã EEGÔºåÊàëÂÄëË™çÁÇ∫ÊôÇÈñìÂíåÈ†ªÁéáÂüüÊòØÂÖ©ÂÄã‰∏çÂêåÁöÑËßÄÈªûÔºåÂõ†Ê≠§ÔºåÂ≠∏ÁøíÂà∞ÁöÑË°®ÂæµÊáâË©≤ÂÖ±‰∫´Áõ∏ÂêåÁöÑÂè¢ÈõÜÂàÜÈÖç„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫ÂÖ©ÂÄãÁâπÂÆöÊñºÈ†òÂüüÁöÑÊ®°ÁµÑÔºåÂÆÉÂÄëÁç®Á´ãÂ≠∏ÁøíÁâπÂÆöÊñºÈ†òÂüüÁöÑË°®ÂæµÔºå‰∏¶Ëß£Ê±∫ÂÇ≥Áµ±Âü∫ÊñºÊôÇË≠úÂúñÁöÑÊñπÊ≥ï‰∏≠ÁöÑÊôÇÈ†ªÊ¨äË°°ÂïèÈ°å„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÂÄãÊñ∞ÁöÑËÅöÈ°ûÊêçÂ§±‰æÜË°°ÈáèË≥áË®äÁõ∏‰ººÊÄß„ÄÇÈÄôÈºìÂãµ‰æÜËá™ÂÖ©ÂÄãÈ†òÂüüÁöÑË°®ÂæµÈÄöÈÅéÂ∞áÂÆÉÂÄëÂàÜÈÖçÂà∞‰∏ÄËá¥ÁöÑÂè¢ÈõÜ‰æÜ‰∏ÄËá¥Âú∞ÊèèËø∞Áõ∏ÂêåÁöÑËº∏ÂÖ•„ÄÇSplitSEE Âú®ÂèØÂàÜÂâ≤ÁöÑÊû∂Êßã‰∏≠Âà©Áî®È†êË®ìÁ∑¥Âà∞ÂæÆË™øÁöÑÊ°ÜÊû∂Ôºå‰∏¶ÂÖ∑Êúâ‰ª•‰∏ãÁâπÊÄßÔºö(a) ÊúâÊïàÊÄßÔºöÂÆÉÂÉÖÂæûÂñÆÈÄöÈÅì EEG Â≠∏ÁøíË°®ÂæµÔºå‰ΩÜÁîöËá≥ÂÑ™ÊñºÂ§öÈÄöÈÅìÂü∫Ê∫ñ„ÄÇ (b) Á©©ÂÅ•ÊÄßÔºöÂÆÉÈ°ØÁ§∫Âá∫‰ª•‰ΩéÊïàËÉΩËÆäÁï∞ÈÅ©Êáâ‰∏çÂêåÈÄöÈÅìÁöÑËÉΩÂäõ„ÄÇ‰ΩøÁî®ÊàëÂÄëÊî∂ÈõÜÁöÑËá®Â∫äË≥áÊñôÈõÜ‰πüÁç≤Âæó‰∫ÜÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇ (c) ÂèØÊì¥Â±ïÊÄßÔºöÂè™‰ΩøÁî®‰∏ÄÂÄãÂæÆË™øÊôÇÊúüÔºåSplitSEE ‰ΩøÁî®ÈÉ®ÂàÜÊ®°ÂûãÂ±§Âç≥ÂèØÈÅîÂà∞È´ò‰∏îÁ©©ÂÆöÁöÑÊïàËÉΩ„ÄÇ</paragraph>

##### **EchoApex: A General-Purpose Vision Foundation Model for Echocardiography**
2410.11092v2 by Abdoul Aziz Amadou, Yue Zhang, Sebastien Piat, Paul Klein, Ingo Schmuecking, Tiziano Passerini, Puneet Sharma

Quantitative evaluation of echocardiography is essential for precise
assessment of cardiac condition, monitoring disease progression, and guiding
treatment decisions. The diverse nature of echo images, including variations in
probe types, manufacturers, and pathologies, poses challenges for developing
artificial intelligent models that can generalize across different clinical
practice. We introduce EchoApex, the first general-purpose vision foundation
model echocardiography with applications on a variety of clinical practice.
Leveraging self-supervised learning, EchoApex is pretrained on over 20 million
echo images from 11 clinical centres. By incorporating task-specific decoders
and adapter modules, we demonstrate the effectiveness of EchoApex on 4
different kind of clinical applications with 28 sub-tasks, including view
classification, interactive structure segmentation, left ventricle hypertrophy
detection and automated ejection fraction estimation from view sequences.
Compared to state-of-the-art task-specific models, EchoApex attains improved
performance with a unified image encoding architecture, demonstrating the
benefits of model pretraining at scale with in-domain data. Furthermore,
EchoApex illustrates the potential for developing a general-purpose vision
foundation model tailored specifically for echocardiography, capable of
addressing a diverse range of clinical applications with high efficiency and
efficacy.

ÊëòË¶ÅÔºöÂÆöÈáèË©ï‰º∞Ë∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂ∞çÊñºÁ≤æÊ∫ñË©ï‰º∞ÂøÉËáüÁãÄÊ≥Å„ÄÅÁõ£ÊéßÁñæÁóÖÈÄ≤Á®ãÂíåÊåáÂ∞éÊ≤ªÁôÇÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÁöÑÂ§öÊ®£ÊÄßÔºåÂåÖÊã¨Êé¢È†≠È°ûÂûã„ÄÅË£ΩÈÄ†ÂïÜÂíåÁóÖÁêÜÁöÑËÆäÂåñÔºåÂ∞çÈñãÁôºËÉΩÂ§†Âú®‰∏çÂêåËá®Â∫äÂØ¶Âãô‰∏≠ÈÄöÁî®ÁöÑ AI Ê®°ÂûãÊßãÊàê‰∫ÜÊåëÊà∞„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü EchoApexÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈÄöÁî®Ë¶ñË¶∫Âü∫Á§éÊ®°ÂûãË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÔºåÂèØÊáâÁî®ÊñºÂêÑÁ®ÆËá®Â∫äÂØ¶Âãô„ÄÇÂà©Áî®Ëá™ÊàëÁõ£Áù£Â≠∏ÁøíÔºåEchoApex Âú®‰æÜËá™ 11 ÂÄãËá®Â∫ä‰∏≠ÂøÉÁöÑË∂ÖÈÅé 2000 Ëê¨ÂºµË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥„ÄÇÈÄèÈÅéÊï¥ÂêàÁâπÂÆöÊñº‰ªªÂãôÁöÑËß£Á¢ºÂô®ÂíåÈÅ©ÈÖçÂô®Ê®°ÁµÑÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü EchoApex Âú® 4 Á®Æ‰∏çÂêåÈ°ûÂûãÁöÑËá®Â∫äÊáâÁî®‰∏≠ÔºåÂåÖÂê´ 28 ÂÄãÂ≠ê‰ªªÂãôÁöÑÊúâÊïàÊÄßÔºåÂåÖÊã¨ÂΩ±ÂÉèÂàÜÈ°û„ÄÅ‰∫íÂãïÁµêÊßãÂàÜÂâ≤„ÄÅÂ∑¶ÂøÉÂÆ§ËÇ•ÂéöÂÅµÊ∏¨ÂíåÂæûÂΩ±ÂÉèÂ∫èÂàó‰∏≠Ëá™Âãï‰º∞Ë®àÂ∞ÑË°ÄÂàÜÊï∏„ÄÇËàáÊúÄÂÖàÈÄ≤ÁöÑÁâπÂÆöÊñº‰ªªÂãôÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåEchoApex ‰ª•Áµ±‰∏ÄÁöÑÂΩ±ÂÉèÁ∑®Á¢ºÊû∂ÊßãÁç≤Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩÔºåË≠âÊòé‰∫Ü‰ΩøÁî®È†òÂüüÂÖßË≥áÊñôÈÄ≤Ë°åÂ§ßË¶èÊ®°Ê®°ÂûãÈ†êË®ìÁ∑¥ÁöÑÂ•ΩËôï„ÄÇÊ≠§Â§ñÔºåEchoApex Ë™™Êòé‰∫ÜÈñãÁôºÂ∞àÈñÄÈáùÂ∞çË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÁöÑÈÄöÁî®Ë¶ñË¶∫Âü∫Á§éÊ®°ÂûãÁöÑÊΩõÂäõÔºåËÉΩÂ§†‰ª•È´òÊïàÁéáÂíåÊïàËÉΩËß£Ê±∫ÂêÑÁ®ÆËá®Â∫äÊáâÁî®„ÄÇ

##### **Parsing altered brain connectivity in neurodevelopmental disorders by integrating graph-based normative modeling and deep generative networks**
2410.11064v1 by Rui Sherry Shen, Yusuf Osmanlƒ±oƒülu, Drew Parker, Darien Aunapu, Benjamin E. Yerys, Birkan Tun√ß, Ragini Verma

Many neurodevelopmental disorders can be understood as divergent patterns of
neural interactions during brain development. Advances in neuroimaging have
illuminated these patterns by modeling the brain as a network structure using
diffution MRI tractography. However, characterizing and quantifying individual
heterogeneity in neurodevelopmental disorders within these highly complex brain
networks remains a significant challenge. In this paper, we present for the
first time, a framework that integrates deep generative models with graph-based
normative modeling to characterize brain network development in the
neurotypical population, which can then be used to quantify the
individual-level neurodivergence associated with disorders. Our deep generative
model incorporates bio-inspired wiring constraints to effectively capture the
developmental trajectories of neurotypical brain networks. Neurodivergence is
quantified by comparing individuals to this neurotypical trajectory, enabling
the creation of region-wise divergence maps that reveal latent developmental
differences at each brain regions, along with overall neurodivergence scores
based on predicted brain age gaps. We demonstrate the clinical utility of this
framework by applying it to a large sample of children with autism spectrum
disorders, showing that the individualized region-wise maps help parse the
heterogeneity in autism, and the neurodivergence scores correlate with clinical
assessments. Together, we provide powerful tools for quantifying
neurodevelopmental divergence in brain networks, paying the way for developing
imaging markers that will support disorder stratification, monitor progression,
and evaluate therapeutic effectiveness.

ÊëòË¶ÅÔºöË®±Â§öÁ•ûÁ∂ìÁôºËÇ≤ÈöúÁ§ôÂèØ‰ª•ÁêÜËß£ÁÇ∫Â§ßËÖ¶ÁôºËÇ≤ÈÅéÁ®ã‰∏≠Á•ûÁ∂ì‰∫§‰∫í‰ΩúÁî®Ê®°ÂºèÁöÑÂ∑ÆÁï∞„ÄÇÁ•ûÁ∂ìÂΩ±ÂÉèÂ≠∏ÁöÑÈÄ≤Ê≠•ÈÄöÈÅé‰ΩøÁî®Êì¥Êï£Á£ÅÊåØÈÄ†ÂΩ±Á∫ñÁ∂≠ÊùüÊîùÂΩ±Ë°ìÂ∞áÂ§ßËÖ¶Âª∫Ê®°ÁÇ∫Á∂≤Ë∑ØÁµêÊßãÔºåÈó°Êòé‰∫ÜÈÄô‰∫õÊ®°Âºè„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÄô‰∫õÈ´òÂ∫¶Ë§áÈõúÁöÑÂ§ßËÖ¶Á∂≤Ë∑Ø‰∏≠ÔºåÊèèËø∞ÂíåÈáèÂåñÁ•ûÁ∂ìÁôºËÇ≤ÈöúÁ§ôÁöÑÂÄãÈ´îÁï∞Ë≥™ÊÄß‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÊ¨°ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊ°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂Â∞áÊ∑±Â∫¶ÁîüÊàêÊ®°ÂûãËàáÂü∫ÊñºÂúñÂΩ¢ÁöÑË¶èÁØÑÊ®°ÂûãÁõ∏ÁµêÂêàÔºå‰ª•ÊèèËø∞Á•ûÁ∂ìÂÖ∏Âûã‰∫∫Áæ§‰∏≠ÁöÑÂ§ßËÖ¶Á∂≤Ë∑ØÁôºÂ±ïÔºåÁÑ∂ÂæåÂèØ‰ª•Áî®ÊñºÈáèÂåñËàáÈöúÁ§ôÁõ∏ÈóúÁöÑÂÄãÈ´îÁ•ûÁ∂ìÂàÜÊ≠ß„ÄÇÊàëÂÄëÁöÑÊ∑±Â∫¶ÁîüÊàêÊ®°ÂûãÁµêÂêà‰∫ÜÂèóÁîüÁâ©ÂïüÁôºÁöÑÊé•Á∑öÁ¥ÑÊùüÔºå‰ª•ÊúâÊïàÊçïÊçâÁ•ûÁ∂ìÂÖ∏ÂûãÂ§ßËÖ¶Á∂≤Ë∑ØÁöÑÁôºÂ±ïËªåË∑°„ÄÇÁ•ûÁ∂ìÂàÜÊ≠ßÈÄöÈÅéÂ∞áÂÄãÈ´îËàáÈÄôÁ®ÆÁ•ûÁ∂ìÂÖ∏ÂûãËªåË∑°ÈÄ≤Ë°åÊØîËºÉ‰æÜÈáèÂåñÔºåÂæûËÄåËÉΩÂ§†ÂâµÂª∫ÂçÄÂüüÂàÜÊ≠ßÂúñÔºåÊè≠Á§∫ÊØèÂÄãÂ§ßËÖ¶ÂçÄÂüüÁöÑÊΩõÂú®ÁôºËÇ≤Â∑ÆÁï∞Ôºå‰ª•ÂèäÂü∫ÊñºÈ†êÊ∏¨ËÖ¶ÈΩ°Â∑ÆË∑ùÁöÑÊï¥È´îÁ•ûÁ∂ìÂàÜÊ≠ßÂæóÂàÜ„ÄÇÊàëÂÄëÈÄöÈÅéÂ∞áÊ≠§Ê°ÜÊû∂ÊáâÁî®ÊñºÂ§ßÈáèËá™ÈñâÁóáË≠úÁ≥ªÈöúÁ§ôÂÖíÁ´•Ê®£Êú¨ÔºåÂ±ïÁ§∫‰∫ÜÈÄôÁ®ÆËá®Â∫äÊïàÁî®ÔºåË°®ÊòéÂÄãÊÄßÂåñÁöÑÂçÄÂüüÂàÜÊ≠ßÂúñÊúâÂä©ÊñºËß£ÊûêËá™ÈñâÁóáÁöÑÁï∞Ë≥™ÊÄßÔºå‰∏¶‰∏îÁ•ûÁ∂ìÂàÜÊ≠ßÂæóÂàÜËàáËá®Â∫äË©ï‰º∞Áõ∏Èóú„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂº∑Â§ßÁöÑÂ∑•ÂÖ∑‰æÜÈáèÂåñÂ§ßËÖ¶Á∂≤Ë∑Ø‰∏≠ÁöÑÁ•ûÁ∂ìÁôºËÇ≤ÂàÜÊ≠ßÔºåÁÇ∫ÈñãÁôºÊàêÂÉèÊ®ôË®òÈã™Âπ≥‰∫ÜÈÅìË∑ØÔºåÈÄô‰∫õÊ®ôË®òÂ∞áÊîØÊåÅÈöúÁ§ôÂàÜÂ±§„ÄÅÁõ£ÊéßÈÄ≤Â±ïÂíåË©ï‰º∞Ê≤ªÁôÇÊïàÊûú„ÄÇ

##### **Thinking LLMs: General Instruction Following with Thought Generation**
2410.10630v1 by Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar

LLMs are typically trained to answer user questions or follow instructions
similarly to how human experts respond. However, in the standard alignment
framework they lack the basic ability of explicit thinking before answering.
Thinking is important for complex questions that require reasoning and planning
-- but can be applied to any task. We propose a training method for equipping
existing LLMs with such thinking abilities for general instruction following
without use of additional human data. We achieve this by an iterative search
and optimization procedure that explores the space of possible thought
generations, allowing the model to learn how to think without direct
supervision. For each instruction, the thought candidates are scored using a
judge model to evaluate their responses only, and then optimized via preference
optimization. We show that this procedure leads to superior performance on
AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning
categories such as marketing, health and general knowledge, in addition to more
traditional reasoning & problem-solving tasks.

ÊëòË¶ÅÔºöLLM ÈÄöÂ∏∏Ë¢´ËÆ≠ÁªÉÊàêÂõûÁ≠îÁî®Êà∑ÁöÑÊèêÈóÆÊàñÈÅµÂæ™Êåá‰ª§ÔºåÁ±ª‰ºº‰∫é‰∫∫Á±ª‰∏ìÂÆ∂Â¶Ç‰ΩïÂõûÂ∫î„ÄÇÁÑ∂ËÄåÔºåÂú®Ê†áÂáÜÂØπÈΩêÊ°ÜÊû∂‰∏≠ÔºåÂÆÉ‰ª¨Áº∫‰πèÂú®ÂõûÁ≠î‰πãÂâçËøõË°åÊòéÁ°ÆÊÄùËÄÉÁöÑÂü∫Êú¨ËÉΩÂäõ„ÄÇÊÄùËÄÉÂØπ‰∫éÈúÄË¶ÅÊé®ÁêÜÂíåËßÑÂàíÁöÑÂ§çÊùÇÈóÆÈ¢òÈùûÂ∏∏ÈáçË¶ÅÔºå‰ΩÜÂÆÉÂèØ‰ª•Â∫îÁî®‰∫é‰ªª‰Ωï‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËÆ≠ÁªÉÊñπÊ≥ïÔºå‰∏∫Áé∞ÊúâÁöÑ LLM Êèê‰æõËøôÁßçÊÄùËÄÉËÉΩÂäõÔºå‰ª•‰æøÂú®Ê≤°Êúâ‰ΩøÁî®È¢ùÂ§ñ‰∫∫Á±ªÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ãÈÅµÂæ™‰∏ÄËà¨Êåá‰ª§„ÄÇÊàë‰ª¨ÈÄöËøá‰∏ÄÁßçËø≠‰ª£ÊêúÁ¥¢Âíå‰ºòÂåñÁ®ãÂ∫èÊù•ÂÆûÁé∞Ëøô‰∏ÄÁÇπÔºåËØ•Á®ãÂ∫èÊé¢Á¥¢ÂèØËÉΩÁöÑÊÄùÊÉ≥ÁîüÊàêÁ©∫Èó¥ÔºåÂÖÅËÆ∏Ê®°ÂûãÂ≠¶‰π†Â¶Ç‰ΩïÂú®Ê≤°ÊúâÁõ¥Êé•ÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÊÄùËÄÉ„ÄÇÂØπ‰∫éÊØèÊù°Êåá‰ª§ÔºåÊÄùÊÉ≥ÂÄôÈÄâËÄÖ‰ΩøÁî®ËØÑÂà§Ê®°ÂûãËøõË°åËØÑÂàÜÔºå‰ªÖËØÑ‰º∞ÂÖ∂ÂìçÂ∫îÔºåÁÑ∂ÂêéÈÄöËøáÂÅèÂ•Ω‰ºòÂåñËøõË°å‰ºòÂåñ„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÊ≠§Á®ãÂ∫èÂú® AlpacaEval Âíå Arena-Hard ‰∏äË°®Áé∞Âá∫ÂçìË∂äÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îÈô§‰∫ÜÊõ¥‰º†ÁªüÁöÑÊé®ÁêÜÂíåËß£ÂÜ≥ÈóÆÈ¢ò‰ªªÂä°‰πãÂ§ñÔºåËøòÂ±ïÁ§∫‰∫ÜÂú®ÈùûÊé®ÁêÜÁ±ªÂà´Ôºà‰æãÂ¶ÇËê•ÈîÄ„ÄÅÂÅ•Â∫∑Âíå‰∏ÄËà¨Áü•ËØÜÔºâ‰∏äÁöÑÊÄùËÄÉÊî∂Áõä„ÄÇ

##### **BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using Multi-parametric MRI**
2410.10604v1 by Shaohao Rui, Lingzhi Chen, Zhenyu Tang, Lilong Wang, Mianxin Liu, Shaoting Zhang, Xiaosong Wang

Accurate diagnosis of brain abnormalities is greatly enhanced by the
inclusion of complementary multi-parametric MRI imaging data. There is
significant potential to develop a universal pre-training model that can be
quickly adapted for image modalities and various clinical scenarios. However,
current models often rely on uni-modal image data, neglecting the cross-modal
correlations among different image modalities or struggling to scale up
pre-training in the presence of missing modality data. In this paper, we
propose BrainMVP, a multi-modal vision pre-training framework for brain image
analysis using multi-parametric MRI scans. First, we collect 16,022 brain MRI
scans (over 2.4 million images), encompassing eight MRI modalities sourced from
a diverse range of centers and devices. Then, a novel pre-training paradigm is
proposed for the multi-modal MRI data, addressing the issue of missing
modalities and achieving multi-modal information fusion. Cross-modal
reconstruction is explored to learn distinctive brain image embeddings and
efficient modality fusion capabilities. A modality-wise data distillation
module is proposed to extract the essence representation of each MR image
modality for both the pre-training and downstream application purposes.
Furthermore, we introduce a modality-aware contrastive learning module to
enhance the cross-modality association within a study. Extensive experiments on
downstream tasks demonstrate superior performance compared to state-of-the-art
pre-training methods in the medical domain, with Dice Score improvement of
0.28%-14.47% across six segmentation benchmarks and a consistent accuracy
improvement of 0.65%-18.07% in four individual classification tasks.

ÊëòË¶ÅÔºö<paragraph>Ê∫ñÁ¢∫Ë®∫Êñ∑ËÖ¶ÈÉ®Áï∞Â∏∏ÊúÉÈÄèÈÅéÂä†ÂÖ•‰∫íË£úÁöÑÂ§öÂèÉÊï∏ MRI ÂΩ±ÂÉèË≥áÊñôËÄåÂ§ßÂπÖÊèêÂçá„ÄÇÈñãÁôº‰∏ÄÂÄãÈÄöÁî®È†êË®ìÁ∑¥Ê®°ÂûãÂÖ∑ÊúâÁõ∏Áï∂Â§ßÁöÑÊΩõÂäõÔºåËÄåÊ≠§Ê®°ÂûãÂèØ‰ª•Âø´ÈÄüË™øÊï¥‰ª•Á¨¶ÂêàÂΩ±ÂÉèÂΩ¢ÂºèÂíåÂêÑÁ®ÆËá®Â∫äÂ†¥ÊôØ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊ®°ÂûãÈÄöÂ∏∏‰ª∞Ë≥¥ÂñÆ‰∏ÄÂΩ¢ÂºèÁöÑÂΩ±ÂÉèË≥áÊñôÔºåÂøΩÁï•‰∫Ü‰∏çÂêåÂΩ±ÂÉèÂΩ¢Âºè‰πãÈñìÁöÑË∑®ÂΩ¢ÂºèÈóúËÅØÊÄßÔºåÊàñÊòØÈõ£‰ª•Âú®Áº∫‰πèÂΩ¢ÂºèË≥áÊñôÁöÑÊÉÖÊ≥Å‰∏ãÊì¥Â±ïÈ†êË®ìÁ∑¥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ BrainMVPÔºå‰∏ÄÂÄãÁî®ÊñºËÖ¶ÈÉ®ÂΩ±ÂÉèÂàÜÊûêÁöÑÂ§öÂΩ¢ÂºèË¶ñË¶∫È†êË®ìÁ∑¥Êû∂ÊßãÔºå‰ΩøÁî®Â§öÂèÉÊï∏ MRI ÊéÉÊèè„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊî∂ÈõÜ‰∫Ü 16,022 ÂÄãËÖ¶ÈÉ® MRI ÊéÉÊèèÔºàË∂ÖÈÅé 240 Ëê¨ÂºµÂΩ±ÂÉèÔºâÔºåÊ∂µËìã‰∫ÜÂÖ´Á®Æ MRI ÂΩ¢ÂºèÔºåÈÄô‰∫õÂΩ¢Âºè‰æÜËá™ÊñºÂêÑÁ®Æ‰∏çÂêåÁöÑ‰∏≠ÂøÉÂíåË£ùÁΩÆ„ÄÇÊé•ËëóÔºåÈáùÂ∞çÂ§öÂΩ¢Âºè MRI Ë≥áÊñôÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÈ†êË®ìÁ∑¥ÁØÑ‰æãÔºåËß£Ê±∫‰∫ÜÁº∫‰πèÂΩ¢ÂºèÁöÑÂïèÈ°åÔºå‰∏¶ÈÅîÂà∞‰∫ÜÂ§öÂΩ¢ÂºèË≥áË®äËûçÂêà„ÄÇÊé¢Á¥¢‰∫ÜË∑®ÂΩ¢ÂºèÈáçÂª∫Ôºå‰ª•Â≠∏ÁøíÁç®ÁâπÁöÑËÖ¶ÈÉ®ÂΩ±ÂÉèÂµåÂÖ•ÂíåÊúâÊïàÁéáÁöÑÂΩ¢ÂºèËûçÂêàËÉΩÂäõ„ÄÇÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂΩ¢ÂºèÊòéÊô∫ÁöÑË≥áÊñôËêÉÂèñÊ®°ÁµÑÔºåÁî®ÊñºËêÉÂèñÊØèÂÄã MR ÂΩ±ÂÉèÂΩ¢ÂºèÁöÑÊú¨Ë≥™Ë°®ÂæµÔºå‰ª•Á¨¶ÂêàÈ†êË®ìÁ∑¥Âíå‰∏ãÊ∏∏ÊáâÁî®ÁõÆÁöÑ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂΩ¢ÂºèÊÑüÁü•Â∞çÊØîÂ≠∏ÁøíÊ®°ÁµÑÔºå‰ª•Âä†Âº∑Á†îÁ©∂‰∏≠ÁöÑË∑®ÂΩ¢ÂºèÈóúËÅØÊÄß„ÄÇÈáùÂ∞ç‰∏ãÊ∏∏‰ªªÂãôÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜËàáÈÜ´ÁôÇÈ†òÂüü‰∏≠ÁèæÊúâÊúÄÂÖàÈÄ≤ÁöÑÈ†êË®ìÁ∑¥ÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∂ÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåÂú®ÂÖ≠ÂÄãÂàÜÂâ≤Âü∫Ê∫ñ‰∏≠È™∞Â≠êÂàÜÊï∏ÊèêÂçá‰∫Ü 0.28%-14.47%ÔºåÂú®ÂõõÂÄãÂÄãÂà•ÂàÜÈ°û‰ªªÂãô‰∏≠Á≤æÁ¢∫Â∫¶‰∏ÄËá¥ÊèêÂçá‰∫Ü 0.65%-18.07%„ÄÇ</paragraph>

##### **Reproducible Machine Learning-based Voice Pathology Detection: Introducing the Pitch Difference Feature**
2410.10537v1 by Jan Vrba, Jakub Steinbach, Tom√°≈° Jirsa, Laura Verde, Roberta De Fazio, Noriyasu Homma, Yuwen Zeng, Key Ichiji, Luk√°≈° H√°jek, Zuzana Sedl√°kov√°, Jan Mare≈°

In this study, we propose a robust set of features derived from a thorough
research of contemporary practices in voice pathology detection. The feature
set is based on the combination of acoustic handcrafted features. Additionally,
we introduce pitch difference as a novel feature. We combine this feature set,
containing data from the publicly available Saarbr\"ucken Voice Database (SVD),
with preprocessing using the K-Means Synthetic Minority Over-Sampling Technique
algorithm to address class imbalance.
  Moreover, we applied multiple ML models as binary classifiers. We utilized
support vector machine, k-nearest neighbors, naive Bayes, decision tree, random
forest and AdaBoost classifiers. To determine the best classification approach,
we performed grid search on feasible hyperparameters of respective classifiers
and subsections of features.
  Our approach has achieved the state-of-the-art performance, measured by
unweighted average recall in voice pathology detection on SVD database. We
intentionally omit accuracy as it is highly biased metric in case of unbalanced
data compared to aforementioned metrics. The results are further enhanced by
eliminating the potential overestimation of the results with repeated
stratified cross-validation. This advancement demonstrates significant
potential for the clinical deployment of ML methods, offering a valuable tool
for an objective examination of voice pathologies. To support our claims, we
provide a publicly available GitHub repository with DOI
10.5281/zenodo.13771573. Finally, we provide REFORMS checklist.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁµÑÁ©©ÂÅ•ÁöÑÂäüËÉΩÔºåÈÄô‰∫õÂäüËÉΩÊ∫êËá™Â∞çÁï∂‰ª£Ë™ûÈü≥ÁóÖÁêÜÊ™¢Ê∏¨ÂØ¶ÂãôÁöÑÈÄèÂæπÁ†îÁ©∂„ÄÇÈÄôÁµÑÂäüËÉΩÂü∫ÊñºËÅ≤Â≠∏ÊâãÂ∑•ÁâπÂæµÁöÑÁµÑÂêà„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÈü≥È´òÂ∑ÆÂºïÂÖ•‰ΩúÁÇ∫‰∏ÄÈ†ÖÊñ∞Á©éÁöÑÂäüËÉΩ„ÄÇÊàëÂÄëÂ∞áÈÄôÁµÑÂäüËÉΩÔºàÂåÖÂê´‰æÜËá™ÂÖ¨ÈñãÁöÑËñ©ÁàæÂ∏ÉÂëÇËÇØË™ûÈü≥Ë≥áÊñôÂ∫´ (SVD) ÁöÑË≥áÊñôÔºâËàá‰ΩøÁî® K-Means ÂêàÊàêÂ∞ëÊï∏ÈÅéÊé°Ê®£ÊäÄË°ìÊºîÁÆóÊ≥ïÈÄ≤Ë°åÈ†êËôïÁêÜÁµêÂêàÔºå‰ª•Ëß£Ê±∫È°ûÂà•‰∏çÂπ≥Ë°°ÁöÑÂïèÈ°å„ÄÇ
  Ê≠§Â§ñÔºåÊàëÂÄëÂ∞áÂ§öÂÄã ML Ê®°ÂûãÊáâÁî®ÁÇ∫‰∫åÂÖÉÂàÜÈ°ûÂô®„ÄÇÊàëÂÄëÂà©Áî®ÊîØÊè¥ÂêëÈáèÊ©ü„ÄÅk-ÊúÄËøëÈÑ∞„ÄÅÊ®∏Á¥†Ë≤ùÊ∞è„ÄÅÊ±∫Á≠ñÊ®π„ÄÅÈö®Ê©üÊ£ÆÊûóÂíå AdaBoost ÂàÜÈ°ûÂô®„ÄÇÁÇ∫‰∫ÜÁ¢∫ÂÆöÊúÄ‰Ω≥ÂàÜÈ°ûÊñπÊ≥ïÔºåÊàëÂÄëÂ∞çÂêÑÂÄãÂàÜÈ°ûÂô®ÁöÑÂèØË°åË∂ÖÂèÉÊï∏ÂíåÂäüËÉΩÂ≠êÈõÜÂü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ã„ÄÇ
  ÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∑≤ÈÅîÊàêÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÁî± SVD Ë≥áÊñôÂ∫´‰∏≠Ë™ûÈü≥ÁóÖÁêÜÊ™¢Ê∏¨ÁöÑÊú™Âä†Ê¨äÂπ≥ÂùáÂè¨ÂõûÁéáÊ∏¨Èáè„ÄÇÊàëÂÄëÊïÖÊÑèÁúÅÁï•Ê∫ñÁ¢∫Â∫¶ÔºåÂõ†ÁÇ∫Ëàá‰∏äËø∞ÊåáÊ®ôÁõ∏ÊØîÔºåÂú®Ë≥áÊñô‰∏çÂπ≥Ë°°ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊ∫ñÁ¢∫Â∫¶ÊòØ‰∏ÄÂÄãÈ´òÂ∫¶ÂÅèÈ†óÁöÑÊåáÊ®ô„ÄÇÈÄèÈÅéÈáçË§áÂàÜÂ±§‰∫§ÂèâÈ©óË≠âÊ∂àÈô§ÁµêÊûúÁöÑÊΩõÂú®È´ò‰º∞ÔºåÈÄ≤‰∏ÄÊ≠•ÊîπÂñÑ‰∫ÜÁµêÊûú„ÄÇÈÄôÈ†ÖÈÄ≤Â±ïÂ±ïÁ§∫‰∫Ü ML ÊñπÊ≥ïÂú®Ëá®Â∫äÈÉ®ÁΩ≤‰∏äÁöÑÂ∑®Â§ßÊΩõÂäõÔºåÁÇ∫ÂÆ¢ËßÄÊ™¢Êü•Ë™ûÈü≥ÁóÖÁêÜÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇÁÇ∫‰∫ÜÊîØÊåÅÊàëÂÄëÁöÑË™™Ê≥ïÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ¨ÈñãÁöÑ GitHub ÂÑ≤Â≠òÂ∫´ÔºåÂÖ∂ DOI ÁÇ∫ 10.5281/zenodo.13771573„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèê‰æõ‰∫Ü REFORMS Ê†∏Â∞çÊ∏ÖÂñÆ„ÄÇ</paragraph>

##### **Study on the Helpfulness of Explainable Artificial Intelligence**
2410.11896v1 by Tobias Labarta, Elizaveta Kulicheva, Ronja Froelian, Christian Gei√üler, Xenia Melman, Julian von Klitzing

Explainable Artificial Intelligence (XAI) is essential for building advanced
machine learning-powered applications, especially in critical domains such as
medical diagnostics or autonomous driving. Legal, business, and ethical
requirements motivate using effective XAI, but the increasing number of
different methods makes it challenging to pick the right ones. Further, as
explanations are highly context-dependent, measuring the effectiveness of XAI
methods without users can only reveal a limited amount of information,
excluding human factors such as the ability to understand it. We propose to
evaluate XAI methods via the user's ability to successfully perform a proxy
task, designed such that a good performance is an indicator for the explanation
to provide helpful information. In other words, we address the helpfulness of
XAI for human decision-making. Further, a user study on state-of-the-art
methods was conducted, showing differences in their ability to generate trust
and skepticism and the ability to judge the rightfulness of an AI decision
correctly. Based on the results, we highly recommend using and extending this
approach for more objective-based human-centered user studies to measure XAI
performance in an end-to-end fashion.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∞çÊñºÂª∫ÊßãÂÖàÈÄ≤ÁöÑÊ©üÂô®Â≠∏ÁøíÈ©ÖÂãïÊáâÁî®Á®ãÂºèËá≥ÈóúÈáçË¶ÅÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇË®∫Êñ∑ÊàñËá™ÂãïÈßïÈßõÁ≠âÈóúÈçµÈ†òÂüü„ÄÇÊ≥ïÂæã„ÄÅÂïÜÊ•≠ÂíåÂÄ´ÁêÜË¶ÅÊ±Ç‰øÉ‰Ωø‰ΩøÁî®ÊúâÊïàÁöÑ XAIÔºå‰ΩÜÊï∏ÈáèÊó•ÁõäÂ¢ûÂä†ÁöÑ‰∏çÂêåÊñπÊ≥ï‰ΩøÂæóÊåëÈÅ∏Ê≠£Á¢∫ÁöÑÊñπÊ≥ïÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºËß£ÈáãÈ´òÂ∫¶‰æùË≥¥ÊñºËÉåÊôØÔºåÂú®Ê≤íÊúâ‰ΩøÁî®ËÄÖÁöÑÊÉÖÊ≥Å‰∏ãË°°Èáè XAI ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂè™ËÉΩÊè≠Á§∫ÊúâÈôêÁöÑË≥áË®äÔºåÊéíÈô§‰∫∫È°ûÂõ†Á¥†Ôºå‰æãÂ¶ÇÁêÜËß£ÂÆÉÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂª∫Ë≠∞ÈÄèÈÅé‰ΩøÁî®ËÄÖÊàêÂäüÂü∑Ë°å‰ª£ÁêÜ‰ªªÂãôÁöÑËÉΩÂäõ‰æÜË©ï‰º∞ XAI ÊñπÊ≥ïÔºåË®≠Ë®à‰ΩøÂæóËâØÂ•ΩÁöÑÂü∑Ë°åË°®ÁèæÊòØËß£ÈáãÊèê‰æõÊúâÁî®Ë≥áË®äÁöÑÊåáÊ®ô„ÄÇÊèõÂè•Ë©±Ë™™ÔºåÊàëÂÄëÊé¢Ë®é XAI Â∞ç‰∫∫È°ûÊ±∫Á≠ñÂà∂ÂÆöÁöÑÂπ´Âä©„ÄÇÊ≠§Â§ñÔºåÂ∞çÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÈÄ≤Ë°å‰ΩøÁî®ËÄÖÁ†îÁ©∂ÔºåÈ°ØÁ§∫Âá∫ÂÆÉÂÄëÂú®Áî¢Áîü‰ø°‰ªªÂíåÊá∑ÁñëÁöÑËÉΩÂäõ‰ª•ÂèäÊ≠£Á¢∫Âà§Êñ∑ AI Ê±∫Á≠ñÊòØÂê¶Ê≠£Á¢∫ÁöÑËÉΩÂäõÊñπÈù¢Â≠òÂú®Â∑ÆÁï∞„ÄÇÊ†πÊìöÁµêÊûúÔºåÊàëÂÄëÂº∑ÁÉàÂª∫Ë≠∞‰ΩøÁî®ÂíåÊì¥ÂÖÖÈÄôÁ®ÆÊñπÊ≥ïÔºå‰ª•ÈÄ≤Ë°åÊõ¥Â§ö‰ª•ÁõÆÊ®ôÁÇ∫Âü∫Á§éÁöÑ‰∫∫ÁÇ∫‰∏≠ÂøÉ‰ΩøÁî®ËÄÖÁ†îÁ©∂Ôºå‰ª•ÁµÇÁ´ØÂà∞ÁµÇÁ´ØÁöÑÊñπÂºèË°°Èáè XAI ÊïàËÉΩ„ÄÇ

##### **Advancing Newborn Care: Precise Birth Time Detection Using AI-Driven Thermal Imaging with Adaptive Normalization**
2410.10483v1 by Jorge Garc√≠a-Torres, √òyvind Meinich-Bache, Anders Johannessen, Siren Rettedal, Vilde Kolstad, Kjersti Engan

Around 5-10\% of newborns need assistance to start breathing. Currently,
there is a lack of evidence-based research, objective data collection, and
opportunities for learning from real newborn resuscitation emergency events.
Generating and evaluating automated newborn resuscitation algorithm activity
timelines relative to the Time of Birth (ToB) offers a promising opportunity to
enhance newborn care practices. Given the importance of prompt resuscitation
interventions within the "golden minute" after birth, having an accurate ToB
with second precision is essential for effective subsequent analysis of newborn
resuscitation episodes. Instead, ToB is generally registered manually, often
with minute precision, making the process inefficient and susceptible to error
and imprecision. In this work, we explore the fusion of Artificial Intelligence
(AI) and thermal imaging to develop the first AI-driven ToB detector. The use
of temperature information offers a promising alternative to detect the newborn
while respecting the privacy of healthcare providers and mothers. However, the
frequent inconsistencies in thermal measurements, especially in a multi-camera
setup, make normalization strategies critical. Our methodology involves a
three-step process: first, we propose an adaptive normalization method based on
Gaussian mixture models (GMM) to mitigate issues related to temperature
variations; second, we implement and deploy an AI model to detect the presence
of the newborn within the thermal video frames; and third, we evaluate and
post-process the model's predictions to estimate the ToB. A precision of 88.1\%
and a recall of 89.3\% are reported in the detection of the newborn within
thermal frames during performance evaluation. Our approach achieves an absolute
median deviation of 2.7 seconds in estimating the ToB relative to the manual
annotations.

ÊëòË¶ÅÔºö<paragraph>Á¥Ñ 5-10% ÁöÑÊñ∞ÁîüÂÖíÈúÄË¶ÅÂçîÂä©ÊâçËÉΩÈñãÂßãÂëºÂê∏„ÄÇÁõÆÂâçÔºåÁº∫‰πèÂü∫ÊñºË≠âÊìöÁöÑÁ†îÁ©∂„ÄÅÂÆ¢ËßÄÁöÑË≥áÊñôËíêÈõÜÔºå‰ª•ÂèäÂæûÂØ¶ÈöõÊñ∞ÁîüÂÖíÂæ©Áî¶Á∑äÊÄ•‰∫ã‰ª∂‰∏≠Â≠∏ÁøíÁöÑÊ©üÊúÉ„ÄÇÁîüÊàê‰∏¶Ë©ï‰º∞Ëá™ÂãïÊñ∞ÁîüÂÖíÂæ©Áî¶ÊºîÁÆóÊ≥ïÊ¥ªÂãïÊôÇÈñìË°®ÔºåÁõ∏Â∞çÊñºÂá∫ÁîüÊôÇÈñì (ToB)ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊ©üÊúÉÔºåÂèØ‰ª•Â¢ûÂº∑Êñ∞ÁîüÂÖíÁÖßË≠∑ÂØ¶Âãô„ÄÇÈëëÊñºÂú®Âá∫ÁîüÂæåÁöÑ„ÄåÈªÉÈáë‰∏ÄÂàÜÈêò„ÄçÂÖßÈÄ≤Ë°åÁ´ãÂç≥Âæ©Áî¶Âπ≤È†êÁöÑÈáçË¶ÅÊÄßÔºåÊìÅÊúâÊ∫ñÁ¢∫Âà∞ÁßíÁöÑ ToB Â∞çÊñºÊúâÊïàÂàÜÊûêÊñ∞ÁîüÂÖíÂæ©Áî¶‰∫ã‰ª∂Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåToB ÈÄöÂ∏∏ÊòØÊâãÂãïË®òÈåÑÁöÑÔºåÈÄöÂ∏∏Âè™ÊúâÂàÜÈêòÁöÑÁ≤æÁ¢∫Â∫¶ÔºåÈÄô‰ΩøÂæóÈÄôÂÄãÈÅéÁ®ãÊïàÁéá‰Ωé‰∏ãÔºåÂÆπÊòìÂá∫ÈåØ‰∏î‰∏çÁ≤æÁ¢∫„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫∫Â∑•Êô∫ÊÖß (AI) ÂíåÁÜ±ÂΩ±ÂÉèËûçÂêàÔºå‰ª•ÈñãÁôºÁ¨¨‰∏ÄÂÄãÁî± AI È©ÖÂãïÁöÑ ToB ÂÅµÊ∏¨Âô®„ÄÇÊ∫´Â∫¶Ë≥áË®äÁöÑ‰ΩøÁî®Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂèØ‰ª•Âú®Â∞äÈáçÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖÂíåÊØçË¶™Èö±ÁßÅÁöÑÂêåÊôÇÂÅµÊ∏¨Êñ∞ÁîüÂÖí„ÄÇÁÑ∂ËÄåÔºåÁÜ±ÈáèÊ∏¨Èáè‰∏≠ÁöÑÈ†ªÁπÅ‰∏ç‰∏ÄËá¥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÈè°È†≠Ë®≠ÂÆö‰∏≠Ôºå‰ΩøÂæóÊ≠£Ë¶èÂåñÁ≠ñÁï•Ëá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨‰∏ÄÂÄã‰∏âÊ≠•È©üÊµÅÁ®ãÔºöÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÈ´òÊñØÊ∑∑ÂêàÊ®°Âûã (GMM) ÁöÑËá™ÈÅ©ÊáâÊ≠£Ë¶èÂåñÊñπÊ≥ïÔºå‰ª•Ê∏õËºïËàáÊ∫´Â∫¶ËÆäÂåñÁõ∏ÈóúÁöÑÂïèÈ°åÔºõÂÖ∂Ê¨°ÔºåÊàëÂÄëÂØ¶‰Ωú‰∏¶ÈÉ®ÁΩ≤‰∏ÄÂÄã AI Ê®°ÂûãÔºå‰ª•ÂÅµÊ∏¨Êñ∞ÁîüÂÖíÂú®ÁÜ±ÂΩ±ÂÉèÊ°Ü‰∏≠ÁöÑÂ≠òÂú®ÔºõÁ¨¨‰∏âÔºåÊàëÂÄëË©ï‰º∞‰∏¶ÂæåËôïÁêÜÊ®°ÂûãÁöÑÈ†êÊ∏¨Ôºå‰ª•‰º∞Ë®à ToB„ÄÇÂú®ÊïàËÉΩË©ï‰º∞ÊúüÈñìÔºåÂú®ÁÜ±ÂΩ±ÂÉèÊ°Ü‰∏≠ÂÅµÊ∏¨Êñ∞ÁîüÂÖíÊôÇÔºåÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 88.1%ÔºåÂè¨ÂõûÁéáÁÇ∫ 89.3%„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®‰º∞Ë®àÁõ∏Â∞çÊñºÊâãÂãïË®ªËß£ÁöÑ ToB ÊôÇÔºåÈÅîÂà∞ 2.7 ÁßíÁöÑÁµïÂ∞ç‰∏≠‰ΩçÊï∏ÂÅèÂ∑Æ„ÄÇ</paragraph>

##### **Affinity-Graph-Guided Contractive Learning for Pretext-Free Medical Image Segmentation with Minimal Annotation**
2410.10366v1 by Zehua Cheng, Di Yuan, Thomas Lukasiewicz

The combination of semi-supervised learning (SemiSL) and contrastive learning
(CL) has been successful in medical image segmentation with limited
annotations. However, these works often rely on pretext tasks that lack the
specificity required for pixel-level segmentation, and still face overfitting
issues due to insufficient supervision signals resulting from too few
annotations. Therefore, this paper proposes an affinity-graph-guided
semi-supervised contrastive learning framework (Semi-AGCL) by establishing
additional affinity-graph-based supervision signals between the student and
teacher network, to achieve medical image segmentation with minimal annotations
without pretext. The framework first designs an average-patch-entropy-driven
inter-patch sampling method, which can provide a robust initial feature space
without relying on pretext tasks. Furthermore, the framework designs an
affinity-graph-guided loss function, which can improve the quality of the
learned representation and the model generalization ability by exploiting the
inherent structure of the data, thus mitigating overfitting. Our experiments
indicate that with merely 10% of the complete annotation set, our model
approaches the accuracy of the fully annotated baseline, manifesting a marginal
deviation of only 2.52%. Under the stringent conditions where only 5% of the
annotations are employed, our model exhibits a significant enhancement in
performance surpassing the second best baseline by 23.09% on the dice metric
and achieving an improvement of 26.57% on the notably arduous CRAG and ACDC
datasets.

ÊëòË¶ÅÔºöÂçäÁõëÁù£Â≠¶‰π† (SemiSL) ÂíåÂØπÊØîÂ≠¶‰π† (CL) ÁöÑÁªìÂêàÂ∑≤ÊàêÂäüÁî®‰∫éÂåªÁñóÂõæÂÉèÂàÜÂâ≤Ôºå‰∏îÊ†áÊ≥®ÊúâÈôê„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÂ∑•‰ΩúÈÄöÂ∏∏‰æùËµñ‰∫éÁº∫‰πèÂÉèÁ¥†Á∫ßÂàÜÂâ≤ÊâÄÈúÄÁâπÂºÇÊÄßÁöÑÂÄüÂè£‰ªªÂä°ÔºåÂπ∂‰∏îÁî±‰∫éÊ†áÊ≥®Â§™Â∞ëÂØºËá¥ÁõëÁù£‰ø°Âè∑‰∏çË∂≥Ôºå‰ªçÁÑ∂Èù¢‰∏¥ËøáÂ∫¶ÊãüÂêàÈóÆÈ¢ò„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÈÄöËøáÂú®Â≠¶ÁîüÁΩëÁªúÂíåÊïôÂ∏àÁΩëÁªú‰πãÈó¥Âª∫Á´ãÂü∫‰∫é‰∫≤ÂíåÂõæÁöÑÈôÑÂä†ÁõëÁù£‰ø°Âè∑ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∫≤ÂíåÂõæÂºïÂØºÁöÑÂçäÁõëÁù£ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂ (Semi-AGCL)Ôºå‰ª•Âú®Ê≤°ÊúâÂÄüÂè£ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ÂåªÁñóÂõæÂÉèÂàÜÂâ≤Ôºå‰∏îÊ†áÊ≥®ÊúÄÂ∞ë„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàËÆæËÆ°‰∫Ü‰∏ÄÁßçÂπ≥ÂùáË°•‰∏ÅÁÜµÈ©±Âä®ÁöÑË°•‰∏ÅÈó¥ÈááÊ†∑ÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Âú®‰∏ç‰æùËµñÂÄüÂè£‰ªªÂä°ÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõÈ≤ÅÊ£íÁöÑÂàùÂßãÁâπÂæÅÁ©∫Èó¥„ÄÇÊ≠§Â§ñÔºåËØ•Ê°ÜÊû∂ËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∫≤ÂíåÂõæÂºïÂØºÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞ÂèØ‰ª•ÈÄöËøáÂà©Áî®Êï∞ÊçÆÁöÑÂõ∫ÊúâÁªìÊûÑÊù•ÊèêÈ´òÂ≠¶‰π†Ë°®Á§∫ÂíåÊ®°ÂûãÊ≥õÂåñËÉΩÂäõÔºå‰ªéËÄåÂáèËΩªËøáÂ∫¶ÊãüÂêà„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°Âûã‰ªÖ‰ΩøÁî® 10% ÁöÑÂÆåÊï¥Ê†áÊ≥®ÈõÜÔºåÂ∞±Êé•Ëøë‰∫ÜÂÆåÂÖ®Ê†áÊ≥®Âü∫ÂáÜÁöÑÂáÜÁ°ÆÂ∫¶Ôºå‰ªÖÊúâ 2.52% ÁöÑËæπÈôÖÂÅèÂ∑Æ„ÄÇÂú®‰ªÖ‰ΩøÁî® 5% Ê†áÊ≥®ÁöÑ‰∏•Ê†ºÊù°‰ª∂‰∏ãÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äË°®Áé∞Âá∫ÊòæÁùÄÊèêÂçáÔºåÂú®È™∞Â≠êÊåáÊ†á‰∏äÊØîÁ¨¨‰∫åÂ•ΩÁöÑÂü∫ÂáÜÈ´òÂá∫ 23.09%ÔºåÂπ∂Âú®ÈùûÂ∏∏Ëâ∞Â∑®ÁöÑ CRAG Âíå ACDC Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 26.57%„ÄÇ

##### **Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning**
2410.10144v1 by Hongyi Yuan, Suqi Liu, Kelly Cho, Katherine Liao, Alexandre Pereira, Tianxi Cai

We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a
framework designed to bridge genetic and biomedical knowledge bases. What sets
GENEREL apart is its ability to fine-tune language models to infuse biological
knowledge behind clinical concepts such as diseases and medications. This
fine-tuning enables the model to capture complex biomedical relationships more
effectively, enriching the understanding of how genomic data connects to
clinical outcomes. By constructing a unified embedding space for biomedical
concepts and a wide range of common SNPs from sources such as patient-level
data, biomedical knowledge graphs, and GWAS summaries, GENEREL aligns the
embeddings of SNPs and clinical concepts through multi-task contrastive
learning. This allows the model to adapt to diverse natural language
representations of biomedical concepts while bypassing the limitations of
traditional code mapping systems across different data sources. Our experiments
demonstrate GENEREL's ability to effectively capture the nuanced relationships
between SNPs and clinical concepts. GENEREL also emerges to discern the degree
of relatedness, potentially allowing for a more refined identification of
concepts. This pioneering approach in constructing a unified embedding system
for both SNPs and biomedical concepts enhances the potential for data
integration and discovery in biomedical research.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄë‰ªãÁ¥π GENomic Encoding REpresentation with Language Model (GENEREL)Ôºå‰∏ÄÂÄãÊó®Âú®Ê©ãÊé•ÈÅ∫ÂÇ≥ÂíåÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂ∫´ÁöÑÊ°ÜÊû∂„ÄÇGENEREL ÁöÑÁç®Áâπ‰πãËôïÂú®ÊñºÂÆÉÂæÆË™øË™ûË®ÄÊ®°ÂûãÔºå‰ª•ÁÅåËº∏ÁñæÁóÖÂíåËó•Áâ©Á≠âËá®Â∫äÊ¶ÇÂøµËÉåÂæåÁöÑÁîüÁâ©Áü•Ë≠ò„ÄÇÈÄôÁ®ÆÂæÆË™ø‰ΩøÊ®°ÂûãËÉΩÂ§†Êõ¥ÊúâÊïàÂú∞ÊçïÊçâË§áÈõúÁöÑÁîüÁâ©ÈÜ´Â≠∏Èóú‰øÇÔºåË±êÂØåÂ∞çÂü∫Âõ†ÁµÑÊï∏ÊìöÂ¶Ç‰ΩïÈÄ£Êé•Ëá®Â∫äÁµêÊûúÁöÑÁêÜËß£„ÄÇÈÄöÈÅéÊßãÂª∫‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÁîüÁâ©ÈÜ´Â≠∏Ê¶ÇÂøµÂµåÂÖ•Á©∫ÈñìÂíå‰æÜËá™ÊÇ£ËÄÖÁ¥öÂà•Êï∏Êìö„ÄÅÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂúñË≠úÂíå GWAS Á∏ΩÁµêÁ≠â‰æÜÊ∫êÁöÑÂª£Ê≥õÂ∏∏Ë¶ã SNPÔºåGENEREL ÈÄöÈÅéÂ§ö‰ªªÂãôÂ∞çÊØîÂ≠∏ÁøíÂ∞çÈΩä SNP ÂíåËá®Â∫äÊ¶ÇÂøµÁöÑÂµåÂÖ•„ÄÇÈÄôÂÖÅË®±Ê®°ÂûãÈÅ©ÊáâÁîüÁâ©ÈÜ´Â≠∏Ê¶ÇÂøµÁöÑÂ§öÂÖÉËá™ÁÑ∂Ë™ûË®ÄË°®Á§∫ÔºåÂêåÊôÇÁπûÈÅé‰∏çÂêåÊï∏ÊìöÊ∫ê‰∏≠ÂÇ≥Áµ±‰ª£Á¢ºÊò†Â∞ÑÁ≥ªÁµ±ÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫Ü GENEREL ÊúâÊïàÊçïÊçâ SNP ÂíåËá®Â∫äÊ¶ÇÂøµ‰πãÈñìÁ¥∞ÂæÆÈóú‰øÇÁöÑËÉΩÂäõ„ÄÇGENEREL ‰πüÂá∫Áèæ‰∫ÜËæ®Âà•Áõ∏ÈóúÁ®ãÂ∫¶ÔºåÊΩõÂú®Âú∞ÂÖÅË®±Êõ¥Á≤æÁ¢∫Âú∞Ë≠òÂà•Ê¶ÇÂøµ„ÄÇÈÄôÁ®ÆÊßãÂª∫ SNP ÂíåÁîüÁâ©ÈÜ´Â≠∏Ê¶ÇÂøµÁµ±‰∏ÄÂµåÂÖ•Á≥ªÁµ±ÁöÑÂÖàÈ©ÖÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÁîüÁâ©ÈÜ´Â≠∏Á†îÁ©∂‰∏≠Êï∏ÊìöÊï¥ÂêàÂíåÁôºÁèæÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **REHRSeg: Unleashing the Power of Self-Supervised Super-Resolution for Resource-Efficient 3D MRI Segmentation**
2410.10097v1 by Zhiyun Song, Yinjie Zhao, Xiaomin Li, Manman Fei, Xiangyu Zhao, Mengjun Liu, Cunjian Chen, Chung-Hsing Yeh, Qian Wang, Guoyan Zheng, Songtao Ai, Lichi Zhang

High-resolution (HR) 3D magnetic resonance imaging (MRI) can provide detailed
anatomical structural information, enabling precise segmentation of regions of
interest for various medical image analysis tasks. Due to the high demands of
acquisition device, collection of HR images with their annotations is always
impractical in clinical scenarios. Consequently, segmentation results based on
low-resolution (LR) images with large slice thickness are often unsatisfactory
for subsequent tasks. In this paper, we propose a novel Resource-Efficient
High-Resolution Segmentation framework (REHRSeg) to address the above-mentioned
challenges in real-world applications, which can achieve HR segmentation while
only employing the LR images as input. REHRSeg is designed to leverage
self-supervised super-resolution (self-SR) to provide pseudo supervision,
therefore the relatively easier-to-acquire LR annotated images generated by 2D
scanning protocols can be directly used for model training. The main
contribution to ensure the effectiveness in self-SR for enhancing segmentation
is three-fold: (1) We mitigate the data scarcity problem in the medical field
by using pseudo-data for training the segmentation model. (2) We design an
uncertainty-aware super-resolution (UASR) head in self-SR to raise the
awareness of segmentation uncertainty as commonly appeared on the ROI
boundaries. (3) We align the spatial features for self-SR and segmentation
through structural knowledge distillation to enable a better capture of region
correlations. Experimental results demonstrate that REHRSeg achieves
high-quality HR segmentation without intensive supervision, while also
significantly improving the baseline performance for LR segmentation.

ÊëòË¶ÅÔºöÈ´òËß£ÊûêÂ∫¶ (HR) 3D Á£ÅÂÖ±ÊåØÈÄ†ÂΩ± (MRI) ÂèØÊèê‰æõË©≥Á¥∞ÁöÑËß£ÂâñÁµêÊßãË≥áË®äÔºåËÉΩÈáùÂ∞çÂêÑÁ®ÆÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰ªªÂãôÁ≤æÁ¢∫ÂàÜÂâ≤ÊÑüËààË∂£ÁöÑÂçÄÂüü„ÄÇÁî±ÊñºÂèñÂæóË®≠ÂÇôË¶ÅÊ±ÇÈ´òÔºåÂú®Ëá®Â∫äÂ†¥ÊôØ‰∏≠Á∏ΩÊòØÈõ£‰ª•Êî∂ÈõÜÂ∏∂ÊúâË®ªËß£ÁöÑ HR ÂΩ±ÂÉè„ÄÇÂõ†Ê≠§ÔºåÂü∫ÊñºÂàáÁâáÂéöÂ∫¶Â§ßÁöÑ‰ΩéËß£ÊûêÂ∫¶ (LR) ÂΩ±ÂÉèÁöÑÂàÜÂâ≤ÁµêÊûúÂæÄÂæÄÁÑ°Ê≥ï‰ª§‰∫∫ÊªøÊÑèÔºåÁÑ°Ê≥ïÁî®ÊñºÂæåÁ∫å‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑË≥áÊ∫êÊúâÊïàÈ´òËß£ÊûêÂ∫¶ÂàÜÂâ≤Êû∂Êßã (REHRSeg)Ôºå‰ª•Ëß£Ê±∫ÂØ¶ÈöõÊáâÁî®‰∏≠‰∏äËø∞ÊåëÊà∞ÔºåË©≤Êû∂ÊßãÂÉÖ‰ΩøÁî® LR ÂΩ±ÂÉè‰ΩúÁÇ∫Ëº∏ÂÖ•Â∞±ËÉΩÂØ¶Áèæ HR ÂàÜÂâ≤„ÄÇREHRSeg Ë¢´Ë®≠Ë®àÁÇ∫Âà©Áî®Ëá™Áõ£Áù£Ë∂ÖËß£ÊûêÂ∫¶ (self-SR) ‰æÜÊèê‰æõÂÅΩÁõ£Áù£ÔºåÂõ†Ê≠§ÂèØ‰ª•Áõ¥Êé•‰ΩøÁî® 2D ÊéÉÊèèÂçîË≠∞Áî¢ÁîüÁöÑÁõ∏Â∞çÂÆπÊòìÂèñÂæóÁöÑ LR Ë®ªËß£ÂΩ±ÂÉè‰æÜÈÄ≤Ë°åÊ®°ÂûãË®ìÁ∑¥„ÄÇÁ¢∫‰øùËá™Áõ£Áù£Ë∂ÖËß£ÊûêÂ∫¶ (self-SR) Âú®Â¢ûÂº∑ÂàÜÂâ≤‰∏≠ÊúâÊïàÊÄßÁöÑ‰∏ªË¶ÅË≤¢ÁçªÊúâ‰∏âÈªûÔºö(1) ÊàëÂÄëÈÄèÈÅé‰ΩøÁî®ÂÅΩË≥áÊñô‰æÜË®ìÁ∑¥ÂàÜÂâ≤Ê®°ÂûãÔºå‰ª•Á∑©Ëß£ÈÜ´ÁôÇÈ†òÂüü‰∏≠ÁöÑË≥áÊñôÁ®ÄÂ∞ëÂïèÈ°å„ÄÇ(2) ÊàëÂÄëÂú®Ëá™Áõ£Áù£Ë∂ÖËß£ÊûêÂ∫¶ (self-SR) ‰∏≠Ë®≠Ë®à‰∫Ü‰∏ÄÂÄã‰∏çÁ¢∫ÂÆöÊÄßÊÑüÁü•Ë∂ÖËß£ÊûêÂ∫¶ (UASR) È†≠Ôºå‰ª•ÊèêÈ´òÂ∞çÂàÜÂâ≤‰∏çÁ¢∫ÂÆöÊÄßÁöÑÊÑüÁü•ÔºåÈÄôÁ®Æ‰∏çÁ¢∫ÂÆöÊÄßÈÄöÂ∏∏Âá∫ÁèæÂú®ÊÑüËààË∂£ÂçÄÂüü (ROI) ÈÇäÁïå‰∏ä„ÄÇ(3) ÊàëÂÄëÈÄèÈÅéÁµêÊßãÁü•Ë≠òËí∏È§æÂ∞áËá™Áõ£Áù£Ë∂ÖËß£ÊûêÂ∫¶ (self-SR) ÂíåÂàÜÂâ≤ÁöÑÁ©∫ÈñìÁâπÂæµÂ∞çÈΩäÔºå‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâÂçÄÂüüÈóúËÅØÊÄß„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåREHRSeg Âú®Ê≤íÊúâÂØÜÈõÜÁõ£Áù£ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶Áèæ‰∫ÜÈ´òÂìÅË≥™ÁöÑ HR ÂàÜÂâ≤ÔºåÂêåÊôÇ‰πüÈ°ØËëóÊèêÈ´ò‰∫Ü LR ÂàÜÂâ≤ÁöÑÂü∫Ê∫ñÊïàËÉΩ„ÄÇ

##### **IMAS: A Comprehensive Agentic Approach to Rural Healthcare Delivery**
2410.12868v1 by Agasthya Gangavarapu, Ananya Gangavarapu

Since the onset of COVID-19, rural communities worldwide have faced
significant challenges in accessing healthcare due to the migration of
experienced medical professionals to urban centers. Semi-trained caregivers,
such as Community Health Workers (CHWs) and Registered Medical Practitioners
(RMPs), have stepped in to fill this gap, but often lack formal training. This
paper proposes an advanced agentic medical assistant system designed to improve
healthcare delivery in rural areas by utilizing Large Language Models (LLMs)
and agentic approaches. The system is composed of five crucial components:
translation, medical complexity assessment, expert network integration, final
medical advice generation, and response simplification. Our innovative
framework ensures context-sensitive, adaptive, and reliable medical assistance,
capable of clinical triaging, diagnostics, and identifying cases requiring
specialist intervention. The system is designed to handle cultural nuances and
varying literacy levels, providing clear and actionable medical advice in local
languages. Evaluation results using the MedQA, PubMedQA, and JAMA datasets
demonstrate that this integrated approach significantly enhances the
effectiveness of rural healthcare workers, making healthcare more accessible
and understandable for underserved populations. All code and supplemental
materials associated with the paper and IMAS are available at
https://github.com/uheal/imas.

ÊëòË¶ÅÔºöËá™ COVID-19 Áñ´ÊÉÖÁàÜÂèë‰ª•Êù•ÔºåÂÖ®ÁêÉÂÜúÊùëÁ§æÂå∫Âú®Ëé∑ÂæóÂåªÁñó‰øùÂÅ•ÊñπÈù¢Èù¢‰∏¥ÈáçÂ§ßÊåëÊàòÔºåÂéüÂõ†ÊòØÁªèÈ™å‰∏∞ÂØåÁöÑÂåªÁñó‰∏ì‰∏ö‰∫∫ÂëòÁ∫∑Á∫∑ËøÅÂæÄÂüéÂ∏Ç‰∏≠ÂøÉ„ÄÇÂçäËÆ≠ÁªÉÊúâÁ¥†ÁöÑÁÖßÊä§ËÄÖÔºå‰æãÂ¶ÇÁ§æÂå∫Âç´ÁîüÂ∑•‰ΩúËÄÖ (CHW) ÂíåÊ≥®ÂÜåÂåªÁñó‰ªé‰∏öËÄÖ (RMP)ÔºåÂ∑≤Áªè‰ªãÂÖ•Â°´Ë°•Ëøô‰∏ÄÁ©∫ÁôΩÔºå‰ΩÜÂæÄÂæÄÁº∫‰πèÊ≠£ÂºèÂüπËÆ≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖàËøõÁöÑ‰ª£ÁêÜÂåªÁñóÂä©ÁêÜÁ≥ªÁªüÔºåÊó®Âú®ÈÄöËøáÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âíå‰ª£ÁêÜÊñπÊ≥ïÊù•ÊîπÂñÑÂÜúÊùëÂú∞Âå∫ÁöÑÂåªÁñó‰øùÂÅ•ÊúçÂä°„ÄÇËØ•Á≥ªÁªüÁî±‰∫î‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÁªÑÊàêÔºöÁøªËØë„ÄÅÂåªÁñóÂ§çÊùÇÊÄßËØÑ‰º∞„ÄÅ‰∏ìÂÆ∂ÁΩëÁªúÈõÜÊàê„ÄÅÊúÄÁªàÂåªÁñóÂª∫ËÆÆÁîüÊàêÂíåÂìçÂ∫îÁÆÄÂåñ„ÄÇÊàë‰ª¨ÂàõÊñ∞ÁöÑÊ°ÜÊû∂Á°Æ‰øù‰∫ÜÊÉÖÂ¢ÉÊïèÊÑü„ÄÅÈÄÇÂ∫îÊÄßÂíåÂèØÈù†ÁöÑÂåªÁñóÊè¥Âä©ÔºåËÉΩÂ§üËøõË°å‰∏¥Â∫äÂàÜËØä„ÄÅËØäÊñ≠ÂíåËØÜÂà´ÈúÄË¶Å‰∏ìÂÆ∂Âπ≤È¢ÑÁöÑÁóÖ‰æã„ÄÇËØ•Á≥ªÁªüÊó®Âú®Â§ÑÁêÜÊñáÂåñÂ∑ÆÂºÇÂíå‰∏çÂêåÁöÑËØÜÂ≠óÊ∞¥Âπ≥ÔºåÁî®ÂΩìÂú∞ËØ≠Ë®ÄÊèê‰æõÊ∏ÖÊô∞‰∏îÂèØÊìç‰ΩúÁöÑÂåªÁñóÂª∫ËÆÆ„ÄÇ‰ΩøÁî® MedQA„ÄÅPubMedQA Âíå JAMA Êï∞ÊçÆÈõÜËøõË°åÁöÑËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåËøôÁßçÁªºÂêàÊñπÊ≥ïÊòæÁùÄÊèêÈ´ò‰∫ÜÂÜúÊùëÂåªÁñó‰øùÂÅ•Â∑•‰ΩúËÄÖÁöÑÊúâÊïàÊÄßÔºå‰ΩøÂåªÁñó‰øùÂÅ•ÊúçÂä°ÂØπÊúçÂä°‰∏çË∂≥ÁöÑ‰∫∫Áæ§Êù•ËØ¥Êõ¥Êòì‰∫éËé∑ÂæóÂíåÁêÜËß£„ÄÇ‰∏éËÆ∫ÊñáÂíå IMAS Áõ∏ÂÖ≥ÁöÑÊâÄÊúâ‰ª£Á†ÅÂíåË°•ÂÖÖÊùêÊñôÂùáÂèØÂú® https://github.com/uheal/imas Ëé∑Âæó„ÄÇ

##### **Adaptive Reasoning and Acting in Medical Language Agents**
2410.10020v1 by Abhishek Dutta, Yen-Che Hsiao

This paper presents an innovative large language model (LLM) agent framework
for enhancing diagnostic accuracy in simulated clinical environments using the
AgentClinic benchmark. The proposed automatic correction enables doctor agents
to iteratively refine their reasoning and actions following incorrect
diagnoses, fostering improved decision-making over time. Experiments show that
the implementation of the adaptive LLM-based doctor agents achieve correct
diagnoses through dynamic interactions with simulated patients. The evaluations
highlight the capacity of autonomous agents to adapt and improve in complex
medical scenarios. Future enhancements will focus on refining the algorithm and
expanding its applicability across a wider range of tasks and different large
language models.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª£ÁêÜÊû∂ÊßãÔºåÁî®Êñº‰ΩøÁî® AgentClinic Âü∫Ê∫ñÊèêÈ´òÊ®°Êì¨Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶„ÄÇÂª∫Ë≠∞ÁöÑËá™ÂãïÊ†°Ê≠£‰ΩøÈÜ´Áîü‰ª£ÁêÜËÉΩÂ§†Âú®‰∏çÊ≠£Á¢∫ÁöÑË®∫Êñ∑ÂæåÂèçË¶ÜË™øÊï¥ÂÖ∂Êé®ÁêÜÂíåË°åÂãïÔºåÂæûËÄåÈö®ËëóÊôÇÈñìÁöÑÊé®Áßª‰øÉÈÄ≤ÊîπÂñÑÊ±∫Á≠ñ„ÄÇÂØ¶È©óË°®ÊòéÔºåÂü∫ÊñºËá™ÈÅ©Êáâ LLM ÁöÑÈÜ´Áîü‰ª£ÁêÜÁöÑÂØ¶ÊñΩÈÄöÈÅéËàáÊ®°Êì¨ÊÇ£ËÄÖÁöÑÂãïÊÖã‰∫íÂãïÂØ¶Áèæ‰∫ÜÊ≠£Á¢∫ÁöÑË®∫Êñ∑„ÄÇË©ï‰º∞Âº∑Ë™ø‰∫ÜËá™‰∏ª‰ª£ÁêÜÂú®Ë§áÈõúÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÈÅ©ÊáâÂíåÊîπÈÄ≤ÁöÑËÉΩÂäõ„ÄÇÊú™‰æÜÁöÑÊîπÈÄ≤Â∞áÈáçÈªûÊîæÂú®Ë™øÊï¥ÁÆóÊ≥ïÂíåÊì¥Â±ïÂÖ∂Âú®Êõ¥Âª£Ê≥õÁöÑ‰ªªÂãôÂíå‰∏çÂêåÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠ÁöÑÈÅ©Áî®ÊÄß‰∏ä„ÄÇ

##### **Improving 3D Few-Shot Segmentation with Inference-Time Pseudo-Labeling**
2410.09967v1 by Mohammad Mozafari, Hosein Hasani, Reza Vahidimajd, Mohamadreza Fereydooni, Mahdieh Soleymani Baghshah

In recent years, few-shot segmentation (FSS) models have emerged as a
promising approach in medical imaging analysis, offering remarkable
adaptability to segment novel classes with limited annotated data. Existing
approaches to few-shot segmentation have often overlooked the potential of the
query itself, failing to fully utilize the valuable information it contains.
However, treating the query as unlabeled data provides an opportunity to
enhance prediction accuracy. Specifically in the domain of medical imaging, the
volumetric structure of queries offers a considerable source of valuable
information that can be used to improve the target slice segmentation. In this
work, we present a novel strategy to efficiently leverage the intrinsic
information of the query sample for final segmentation during inference. First,
we use the support slices from a reference volume to generate an initial
segmentation score for the query slices through a prototypical approach.
Subsequently, we apply a confidence-aware pseudo-labeling procedure to transfer
the most informative parts of query slices to the support set. The final
prediction is performed based on the new expanded support set, enabling the
prediction of a more accurate segmentation mask for the query volume. Extensive
experiments show that the proposed method can effectively boost performance
across diverse settings and datasets.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÂ∞èÊ®£Êú¨ÂàÜÂâ≤ (FSS) Ê®°ÂûãÂ∑≤ÊàêÁÇ∫ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠‰∏ÄÁ®ÆÂâçÊôØÁúãÂ•ΩÁöÑÊñπÊ≥ïÔºåÂÆÉÁÇ∫‰ΩøÁî®ÊúâÈôêÊ®ôË®ªË≥áÊñôÂàÜÂâ≤Êñ∞È°ûÂà•Êèê‰æõ‰∫ÜÈ°ØËëóÁöÑÈÅ©ÊáâÊÄß„ÄÇÁèæÊúâÁöÑÂ∞èÊ®£Êú¨ÂàÜÂâ≤ÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•‰∫ÜÊü•Ë©¢Êú¨Ë∫´ÁöÑÊΩõÂäõÔºåÊú™ËÉΩÂÖÖÂàÜÂà©Áî®ÂÖ∂‰∏≠ÂåÖÂê´ÁöÑÂØ∂Ë≤¥Ë≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÂ∞áÊü•Ë©¢Ë¶ñÁÇ∫Êú™Ê®ôË®ªË≥áÊñôÊèê‰æõ‰∫ÜÂ¢ûÂº∑È†êÊ∏¨Á≤æÁ¢∫Â∫¶ÁöÑÊ©üÊúÉ„ÄÇÁâπÂà•ÊòØÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüüÔºåÊü•Ë©¢ÁöÑÈ´îÁ©çÁµêÊßãÊèê‰æõ‰∫ÜÂ§ßÈáèÁöÑÂØ∂Ë≤¥Ë≥áË®ä‰æÜÊ∫êÔºåÂèØÁî®ÊñºÊîπÂñÑÁõÆÊ®ôÂàáÁâáÂàÜÂâ≤„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á≠ñÁï•ÔºåÂú®Êé®Ë´ñÊúüÈñìÊúâÊïàÂà©Áî®Êü•Ë©¢Ê®£Êú¨ÁöÑÂÖßÂú®Ë≥áË®äÈÄ≤Ë°åÊúÄÁµÇÂàÜÂâ≤„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ΩøÁî®ÂèÉËÄÉÈ´îÁ©ç‰∏≠ÁöÑÊîØÊè¥ÂàáÁâáÔºåÈÄèÈÅéÂéüÂûãÂåñÊñπÊ≥ïÁÇ∫Êü•Ë©¢ÂàáÁâáÁî¢ÁîüÂàùÂßãÂàÜÂâ≤ÂàÜÊï∏„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊáâÁî®‰∏ÄÂÄãÂÖ∑ÂÇôË≠òÂà•ËÉΩÂäõÁöÑÂÅΩÊ®ôÁ±§Á®ãÂ∫èÔºåÂ∞áÊü•Ë©¢ÂàáÁâá‰∏≠ÊúÄÂÖ∑Ë≥áË®äÊÄßÁöÑÈÉ®ÂàÜËΩâÁßªÂà∞ÊîØÊè¥ÈõÜ‰∏≠„ÄÇÊúÄÁµÇÈ†êÊ∏¨ÊòØÊ†πÊìöÊñ∞ÁöÑÊì¥ÂÖÖÊîØÊè¥ÈõÜÈÄ≤Ë°åÁöÑÔºåÈÄô‰ΩøÂæóËÉΩÂ§†ÁÇ∫Êü•Ë©¢È´îÁ©çÈ†êÊ∏¨Âá∫Êõ¥Ê∫ñÁ¢∫ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•Âú®‰∏çÂêåÁöÑË®≠ÂÆöÂíåË≥áÊñôÈõÜ‰∏äÊúâÊïàÊèêÂçáÊïàËÉΩ„ÄÇ

##### **Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble for Zero-shot Learning**
2410.09908v1 by Pengfei Jin, Peng Shu, Sekeun Kim, Qing Xiao, Sifan Song, Cheng Chen, Tianming Liu, Xiang Li, Quanzheng Li

Foundation models have become a cornerstone in deep learning, with techniques
like Low-Rank Adaptation (LoRA) offering efficient fine-tuning of large models.
Similarly, methods such as Retrieval-Augmented Generation (RAG), which leverage
vectorized databases, have further improved model performance by grounding
outputs in external information. While these approaches have demonstrated
notable success, they often require extensive training or labeled data, which
can limit their adaptability in resource-constrained environments. To address
these challenges, we introduce Retrieval-based Parameter Ensemble (RPE), a new
method that creates a vectorized database of LoRAs, enabling efficient
retrieval and application of model adaptations to new tasks. RPE minimizes the
need for extensive training and eliminates the requirement for labeled data,
making it particularly effective for zero-shot learning. Additionally, RPE is
well-suited for privacy-sensitive domains like healthcare, as it modifies model
parameters without accessing raw data. When applied to tasks such as medical
report generation and image segmentation, RPE not only proved effective but
also surpassed supervised fine-tuning methods in certain cases, highlighting
its potential to enhance both computational efficiency and privacy in deep
learning applications.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂ∑≤ÊàêÁÇ∫Ê∑±Â∫¶Â≠∏ÁøíÁöÑÂü∫Áü≥ÔºåÂÖ∂‰∏≠‰ΩéÁß©ÈÅ©Êáâ (LoRA) Á≠âÊäÄË°ìÊèê‰æõÂ§ßÂûãÊ®°ÂûãÁöÑÊúâÊïàÂæÆË™ø„ÄÇ
È°û‰ººÂú∞ÔºåÂà©Áî®ÂêëÈáèÂåñË≥áÊñôÂ∫´ÁöÑÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG) Á≠âÊñπÊ≥ïÔºåÈÄèÈÅéÂú®Â§ñÈÉ®Ë≥áË®ä‰∏≠Âª∫Á´ãËº∏Âá∫ÔºåÈÄ≤‰∏ÄÊ≠•ÊîπÂñÑÊ®°ÂûãÊïàËÉΩ„ÄÇÈõñÁÑ∂ÈÄô‰∫õÊñπÊ≥ïÂ∑≤Â±ïÁèæÈ°ØËëóÁöÑÊàêÂäüÔºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑË®ìÁ∑¥ÊàñÊ®ôË®òË≥áÊñôÔºåÈÄôÂèØËÉΩÊúÉÈôêÂà∂ÂÆÉÂÄëÂú®Ë≥áÊ∫êÂèóÈôêÁí∞Â¢É‰∏≠ÁöÑÈÅ©ÊáâÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÈÄ≤Âü∫ÊñºÊ™¢Á¥¢ÁöÑÂèÉÊï∏ÈõÜÂêà (RPE)Ôºå‰∏ÄÁ®ÆÂª∫Á´ã LoRA ÂêëÈáèÂåñË≥áÊñôÂ∫´ÁöÑÊñ∞ÊñπÊ≥ïÔºåËÉΩÊúâÊïàÂú∞Ê™¢Á¥¢ÂíåÂ∞áÊ®°ÂûãÊîπÁ∑®ÊáâÁî®ÊñºÊñ∞‰ªªÂãô„ÄÇRPE Â∞áÂ§ßÈáèË®ìÁ∑¥ÁöÑÈúÄÊ±ÇÈôçËá≥ÊúÄ‰ΩéÔºå‰∏¶Ê∂àÈô§‰∫ÜÂ∞çÊ®ôË®òË≥áÊñôÁöÑÈúÄÊ±ÇÔºå‰ΩøÂÖ∂Âú®Èõ∂Ê¨°Â≠∏Áøí‰∏≠ÁâπÂà•ÊúâÊïà„ÄÇÊ≠§Â§ñÔºåRPE ÈùûÂ∏∏ÈÅ©ÂêàÈÜ´ÁôÇ‰øùÂÅ•Á≠âÊ≥®ÈáçÈö±ÁßÅÁöÑÈ†òÂüüÔºåÂõ†ÁÇ∫ÂÆÉ‰øÆÊîπÊ®°ÂûãÂèÉÊï∏ËÄå‰∏çÊúÉÂ≠òÂèñÂéüÂßãË≥áÊñô„ÄÇÁï∂ÊáâÁî®ÊñºÈÜ´ÁôÇÂ†±ÂëäÁîüÊàêÂíåÂΩ±ÂÉèÂàÜÂâ≤Á≠â‰ªªÂãôÊôÇÔºåRPE ‰∏çÂÉÖË¢´Ë≠âÊòéÊúâÊïàÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÈÇÑË∂ÖË∂ä‰∫ÜÊúâÁõ£Áù£ÁöÑÂæÆË™øÊñπÊ≥ïÔºåÁ™ÅÈ°ØÂá∫ÂÆÉÂú®Ê∑±Â∫¶Â≠∏ÁøíÊáâÁî®‰∏≠ÊèêÂçáÈÅãÁÆóÊïàÁéáÂíåÈö±ÁßÅÁöÑÊΩõÂäõ„ÄÇ

##### **Equitable Access to Justice: Logical LLMs Show Promise**
2410.09904v1 by Manuj Kant, Manav Kant, Marzieh Nabi, Preston Carlson, Megan Ma

The costs and complexity of the American judicial system limit access to
legal solutions for many Americans. Large language models (LLMs) hold great
potential to improve access to justice. However, a major challenge in applying
AI and LLMs in legal contexts, where consistency and reliability are crucial,
is the need for System 2 reasoning. In this paper, we explore the integration
of LLMs with logic programming to enhance their ability to reason, bringing
their strategic capabilities closer to that of a skilled lawyer. Our objective
is to translate laws and contracts into logic programs that can be applied to
specific legal cases, with a focus on insurance contracts. We demonstrate that
while GPT-4o fails to encode a simple health insurance contract into logical
code, the recently released OpenAI o1-preview model succeeds, exemplifying how
LLMs with advanced System 2 reasoning capabilities can expand access to
justice.

ÊëòË¶ÅÔºöÁæéÂúãÂè∏Ê≥ïÈ´îÁ≥ªÁöÑÊàêÊú¨ÂíåË§áÈõúÊÄßÈôêÂà∂‰∫ÜË®±Â§öÁæéÂúã‰∫∫Áç≤ÂæóÊ≥ïÂæãËß£Ê±∫ÊñπÊ°àÁöÑÊ©üÊúÉ„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂÖ∑ÊúâÊîπÂñÑÂè∏Ê≥ïÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅìÂèñÂæóÁÆ°ÈÅìÁÆ°ÈÅìÁÆ°ÈÅì

##### **Large-Scale 3D Medical Image Pre-training with Geometric Context Priors**
2410.09890v1 by Linshan Wu, Jiaxin Zhuang, Hao Chen

The scarcity of annotations poses a significant challenge in medical image
analysis. Large-scale pre-training has emerged as a promising label-efficient
solution, owing to the utilization of large-scale data, large models, and
advanced pre-training techniques. However, its development in medical images
remains underexplored. The primary challenge lies in harnessing large-scale
unlabeled data and learning high-level semantics without annotations. We
observe that 3D medical images exhibit consistent geometric context, i.e.,
consistent geometric relations between different organs, which leads to a
promising way for learning consistent representations. Motivated by this, we
introduce a simple-yet-effective Volume Contrast (VoCo) framework to leverage
geometric context priors for self-supervision. Given an input volume, we
extract base crops from different regions to construct positive and negative
pairs for contrastive learning. Then we predict the contextual position of a
random crop by contrasting its similarity to the base crops. In this way, VoCo
encodes the inherent geometric context into model representations, facilitating
high-level semantic learning without annotations. Specifically, we (1)
introduce the largest medical pre-training dataset PreCT-160K; (2) investigate
scaling laws and propose guidelines for tailoring different model sizes to
various medical tasks; (3) build a benchmark encompassing 48 medical tasks.
Extensive experiments highlight the superiority of VoCo. Codes at
https://github.com/Luffy03/Large-Scale-Medical.

ÊëòË¶ÅÔºö<paragraph>Ê®ôË®ªÁöÑÁ®ÄÁº∫ÊÄßÂ∞çÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁî±ÊñºÂà©Áî®Â§ßË¶èÊ®°Êï∏Êìö„ÄÅÂ§ßÂûãÊ®°ÂûãÂíåÂÖàÈÄ≤ÁöÑÈ†êË®ìÁ∑¥ÊäÄË°ìÔºåÂ§ßË¶èÊ®°È†êË®ìÁ∑¥Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊ®ôÁ±§ÊïàÁéáËß£Ê±∫ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁôºÂ±ï‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊà∞Âú®ÊñºÂà©Áî®Â§ßË¶èÊ®°Êú™Ê®ôË®ªÊï∏Êìö‰∏¶Âú®Ê≤íÊúâÊ®ôË®ªÁöÑÊÉÖÊ≥Å‰∏ãÂ≠∏ÁøíÈ´òÁ¥öË™ûÁæ©„ÄÇÊàëÂÄëËßÄÂØüÂà∞ 3D ÈÜ´Â≠∏ÂΩ±ÂÉèË°®ÁèæÂá∫‰∏ÄËá¥ÁöÑÂπæ‰ΩïËÉåÊôØÔºåÂç≥‰∏çÂêåÂô®ÂÆò‰πãÈñìÁöÑ‰∏ÄËá¥Âπæ‰ΩïÈóú‰øÇÔºåÈÄôÁÇ∫Â≠∏Áøí‰∏ÄËá¥ÁöÑË°®Á§∫Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ï„ÄÇÂèóÊ≠§ÂïüÁôºÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆËÄåÊúâÊïàÁöÑÈ´îÁ©çÂ∞çÊØî (VoCo) Ê°ÜÊû∂Ôºå‰ª•Âà©Áî®Âπæ‰ΩïËÉåÊôØÂÖàÈ©óÈÄ≤Ë°åËá™ÊàëÁõ£Áù£„ÄÇÁµ¶ÂÆö‰∏ÄÂÄãËº∏ÂÖ•È´îÁ©çÔºåÊàëÂÄëÂæû‰∏çÂêåÁöÑÂçÄÂüüÊèêÂèñÂü∫Á§éË£ÅÂâ™Ôºå‰ª•ÊßãÈÄ†Â∞çÊØîÂ≠∏ÁøíÁöÑÊ≠£Ë≤†Â∞ç„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÈÄöÈÅéÂ∞çÊØîÂÖ∂ËàáÂü∫Á§éË£ÅÂâ™ÁöÑÁõ∏‰ººÊÄß‰æÜÈ†êÊ∏¨Èö®Ê©üË£ÅÂâ™ÁöÑ‰∏ä‰∏ãÊñá‰ΩçÁΩÆ„ÄÇÈÄöÈÅéÈÄôÁ®ÆÊñπÂºèÔºåVoCo Â∞áÂõ∫ÊúâÁöÑÂπæ‰ΩïËÉåÊôØÁ∑®Á¢ºÂà∞Ê®°ÂûãË°®Á§∫‰∏≠ÔºåÂæûËÄå‰øÉÈÄ≤‰∫ÜÂú®Ê≤íÊúâÊ®ôË®ªÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åÈ´òÁ¥öË™ûÁæ©Â≠∏Áøí„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë (1) ÂºïÂÖ•‰∫ÜÊúÄÂ§ßÁöÑÈÜ´Â≠∏È†êË®ìÁ∑¥Êï∏ÊìöÈõÜ PreCT-160KÔºõ(2) Ë™øÊü•Á∏ÆÊîæÂÆöÂæã‰∏¶ÊèêÂá∫ÊåáÂ∞éÊñπÈáùÔºå‰ª•Ê†πÊìö‰∏çÂêåÁöÑÈÜ´ÁôÇ‰ªªÂãôË™øÊï¥‰∏çÂêåÁöÑÊ®°ÂûãÂ§ßÂ∞èÔºõ(3) ÊßãÂª∫‰∫Ü‰∏ÄÂÄãÊ∂µËìã 48 ÂÄãÈÜ´ÁôÇ‰ªªÂãôÁöÑÂü∫Ê∫ñ„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óÁ™ÅÂá∫‰∫Ü VoCo ÁöÑÂÑ™Ë∂äÊÄß„ÄÇ‰ª£Á¢ºË¶ã https://github.com/Luffy03/Large-Scale-Medical„ÄÇ</paragraph>

##### **HypomimiaCoach: An AU-based Digital Therapy System for Hypomimia Detection & Rehabilitation with Parkinson's Disease**
2410.09772v1 by Yingjing Xu, Xueyan Cai, Zihong Zhou, Mengru Xue, Bo Wang, Haotian Wang, Zhengke Li, Chentian Weng, Wei Luo, Cheng Yao, Bo Lin, Jianwei Yin

Hypomimia is a non-motor symptom of Parkinson's disease that manifests as
delayed facial movements and expressions, along with challenges in articulation
and emotion. Currently, subjective evaluation by neurologists is the primary
method for hypomimia detection, and conventional rehabilitation approaches
heavily rely on verbal prompts from rehabilitation physicians. There remains a
deficiency in accessible, user-friendly and scientifically rigorous assistive
tools for hypomimia treatments. To investigate this, we developed
HypomimaCoach, an Action Unit (AU)-based digital therapy system for hypomimia
detection and rehabilitation in Parkinson's disease. The HypomimaCoach system
was designed to facilitate engagement through the incorporation of both relaxed
and controlled rehabilitation exercises, while also stimulating initiative
through the integration of digital therapies that incorporated traditional face
training methods. We extract action unit(AU) features and their relationship
for hypomimia detection. In order to facilitate rehabilitation, a series of
training programmes have been devised based on the Action Units (AUs) and
patients are provided with real-time feedback through an additional AU
recognition model, which guides them through their training routines. A pilot
study was conducted with seven participants in China, all of whom exhibited
symptoms of Parkinson's disease hypomimia. The results of the pilot study
demonstrated a positive impact on participants' self-efficacy, with favourable
feedback received. Furthermore, physician evaluations validated the system's
applicability in a therapeutic setting for patients with Parkinson's disease,
as well as its potential value in clinical applications.

ÊëòË¶ÅÔºö<paragraph>È°èÈù¢Ë°®ÊÉÖÊ∏õÂ∞ëÁóáÊòØÂ∏ïÈáëÊ£ÆÊ∞èÁóáÁöÑ‰∏ÄÁ®ÆÈùûÈÅãÂãïÁóáÁãÄÔºåË°®ÁèæÁÇ∫Èù¢ÈÉ®Âãï‰ΩúÂíåË°®ÊÉÖÈÅ≤Á∑©Ôºå‰ª•ÂèäË®ÄË™ûË°®ÈÅîÂíåÊÉÖÁ∑íË°®ÈÅîÂõ∞Èõ£„ÄÇÁõÆÂâçÔºåÁ•ûÁ∂ìÁßëÈÜ´Â∏´ÁöÑ‰∏ªËßÄË©ï‰º∞ÊòØÈ°èÈù¢Ë°®ÊÉÖÊ∏õÂ∞ëÁóáÊ™¢Ê∏¨ÁöÑ‰∏ªË¶ÅÊñπÊ≥ïÔºåÂÇ≥Áµ±ÁöÑÂæ©ÂÅ•ÊñπÊ≥ïÈ´òÂ∫¶‰æùË≥¥Âæ©ÂÅ•ÈÜ´Â∏´ÁöÑË®ÄË™ûÊèêÁ§∫„ÄÇÈ°èÈù¢Ë°®ÊÉÖÊ∏õÂ∞ëÁóáÊ≤ªÁôÇ‰∏≠‰ªçÁÑ∂Áº∫‰πèÂèØÂèñÂæó„ÄÅ‰ΩøÁî®ËÄÖÂèãÂñÑ‰∏îÁßëÂ≠∏Âö¥Ë¨πÁöÑËºîÂä©Â∑•ÂÖ∑„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∫Ü HypomimaCoachÔºå‰∏ÄÁ®ÆÂü∫ÊñºÂãï‰ΩúÂñÆÂÖÉ (AU) ÁöÑÊï∏‰ΩçÊ≤ªÁôÇÁ≥ªÁµ±ÔºåÁî®ÊñºÂ∏ïÈáëÊ£ÆÊ∞èÁóáÁöÑÈ°èÈù¢Ë°®ÊÉÖÊ∏õÂ∞ëÁóáÊ™¢Ê∏¨ÂíåÂæ©ÂÅ•„ÄÇHypomimaCoach Á≥ªÁµ±Êó®Âú®ÈÄèÈÅéÊï¥ÂêàÊîæÈ¨ÜÂíåÊéßÂà∂ÁöÑÂæ©ÂÅ•ÈÅãÂãï‰æÜ‰øÉÈÄ≤ÂèÉËàáÔºåÂêåÊôÇÈÄèÈÅéÊï¥ÂêàÂÇ≥Áµ±Èù¢ÈÉ®Ë®ìÁ∑¥ÊñπÊ≥ïÁöÑÊï∏‰ΩçÊ≤ªÁôÇ‰æÜÊøÄÂãµ‰∏ªÂãïÊÄß„ÄÇÊàëÂÄëËêÉÂèñÂãï‰ΩúÂñÆÂÖÉ (AU) ÁâπÂæµÂèäÂÖ∂ËàáÈ°èÈù¢Ë°®ÊÉÖÊ∏õÂ∞ëÁóáÊ™¢Ê∏¨ÁöÑÈóú‰øÇ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Âæ©ÂÅ•ÔºåÊàëÂÄëÊ†πÊìöÂãï‰ΩúÂñÆÂÖÉ (AU) Ë®≠Ë®à‰∫Ü‰∏ÄÁ≥ªÂàóË®ìÁ∑¥Ë®àÁï´Ôºå‰∏¶ÈÄèÈÅé‰∏ÄÂÄãÈ°çÂ§ñÁöÑ AU Ëæ®Ë≠òÊ®°ÂûãÊèê‰æõÊÇ£ËÄÖÂç≥ÊôÇÂõûÈ•ãÔºåÂºïÂ∞é‰ªñÂÄëÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÂú®‰∏≠ÂúãÂ∞ç‰∏É‰ΩçÂèÉËàáËÄÖÈÄ≤Ë°å‰∫ÜË©¶È©óÁ†îÁ©∂ÔºåÊâÄÊúâÂèÉËàáËÄÖÂùáË°®ÁèæÂá∫Â∏ïÈáëÊ£ÆÊ∞èÁóáÈ°èÈù¢Ë°®ÊÉÖÊ∏õÂ∞ëÁóáÁöÑÁóáÁãÄ„ÄÇË©¶È©óÁ†îÁ©∂ÁöÑÁµêÊûúÈ°ØÁ§∫Â∞çÂèÉËàáËÄÖÁöÑËá™ÊàëÊïàËÉΩÁî¢Áîü‰∫ÜÊ≠£Èù¢ÁöÑÂΩ±ÈüøÔºå‰∏¶Áç≤Âæó‰∫ÜÊ≠£Èù¢ÁöÑÂõûÈ•ã„ÄÇÊ≠§Â§ñÔºåÈÜ´Â∏´Ë©ï‰º∞È©óË≠â‰∫ÜË©≤Á≥ªÁµ±Âú®Â∏ïÈáëÊ£ÆÊ∞èÁóáÊÇ£ËÄÖÊ≤ªÁôÇÁí∞Â¢É‰∏≠ÁöÑÈÅ©Áî®ÊÄßÔºå‰ª•ÂèäÂÖ∂Âú®Ëá®Â∫äÊáâÁî®‰∏≠ÁöÑÊΩõÂú®ÂÉπÂÄº„ÄÇ</paragraph>

##### **STA-Unet: Rethink the semantic redundant for Medical Imaging Segmentation**
2410.11578v1 by Vamsi Krishna Vasa, Wenhui Zhu, Xiwen Chen, Peijie Qiu, Xuanzhao Dong, Yalin Wang

In recent years, significant progress has been made in the medical image
analysis domain using convolutional neural networks (CNNs). In particular, deep
neural networks based on a U-shaped architecture (UNet) with skip connections
have been adopted for several medical imaging tasks, including organ
segmentation. Despite their great success, CNNs are not good at learning global
or semantic features. Especially ones that require human-like reasoning to
understand the context. Many UNet architectures attempted to adjust with the
introduction of Transformer-based self-attention mechanisms, and notable gains
in performance have been noted. However, the transformers are inherently flawed
with redundancy to learn at shallow layers, which often leads to an increase in
the computation of attention from the nearby pixels offering limited
information. The recently introduced Super Token Attention (STA) mechanism
adapts the concept of superpixels from pixel space to token space, using super
tokens as compact visual representations. This approach tackles the redundancy
by learning efficient global representations in vision transformers, especially
for the shallow layers. In this work, we introduce the STA module in the UNet
architecture (STA-UNet), to limit redundancy without losing rich information.
Experimental results on four publicly available datasets demonstrate the
superiority of STA-UNet over existing state-of-the-art architectures in terms
of Dice score and IOU for organ segmentation tasks. The code is available at
\url{https://github.com/Retinal-Research/STA-UNet}.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºå‰ΩøÁî®Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÈ†òÂüü‰∏≠ÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇÁâπÂà•ÊòØÔºåÂü∫Êñº U ÂΩ¢Êû∂Êßã (UNet) ÁöÑÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂÖ∑ÊúâË∑≥Ë∫çÈÄ£Êé•ÔºåÂ∑≤Ë¢´Êé°Áî®ÊñºÂ§öÈ†ÖÈÜ´Â≠∏ÂΩ±ÂÉè‰ªªÂãôÔºåÂåÖÊã¨Âô®ÂÆòÂàÜÂâ≤„ÄÇÂÑòÁÆ° CNN Áç≤ÂæóÂ∑®Â§ßÁöÑÊàêÂäüÔºå‰ΩÜÂÆÉÂÄë‰∏¶‰∏çÊìÖÈï∑Â≠∏ÁøíÂÖ®Â±ÄÊàñË™ûÁæ©ÁâπÂæµ„ÄÇÂ∞§ÂÖ∂ÊòØÈÇ£‰∫õÈúÄË¶ÅÈ°û‰ºº‰∫∫È°ûÁöÑÊé®ÁêÜÊâçËÉΩÁêÜËß£ËÑàÁµ°ÁöÑÁâπÂæµ„ÄÇË®±Â§ö UNet Êû∂ÊßãÂòóË©¶ÈÄèÈÅéÂ∞éÂÖ•Âü∫Êñº Transformer ÁöÑËá™ÊàëÊ≥®ÊÑèÊ©üÂà∂ÈÄ≤Ë°åË™øÊï¥Ôºå‰∏¶Â∑≤Ê≥®ÊÑèÂà∞ÊïàËÉΩÁöÑÈ°ØËëóÊèêÂçá„ÄÇÁÑ∂ËÄåÔºåTransformer Âú®Êú¨Ë≥™‰∏äÂ≠òÂú®Â≠∏ÁøíÊ∑∫Â±§ÁöÑÂÜóÈ§òÁº∫Èô∑ÔºåÈÄôÈÄöÂ∏∏ÊúÉÂ∞éËá¥Ë®àÁÆó‰æÜËá™ÈôÑËøëÂÉèÁ¥†ÁöÑÊ≥®ÊÑèÔºåËÄåÈÄô‰∫õÂÉèÁ¥†Êèê‰æõÁöÑË≥áË®äÊúâÈôê„ÄÇÊúÄËøëÊé®Âá∫ÁöÑË∂ÖÊ®ôË®òÊ≥®ÊÑè (STA) Ê©üÂà∂Â∞áË∂ÖÂÉèÁ¥†ÁöÑÊ¶ÇÂøµÂæûÂÉèÁ¥†Á©∫ÈñìË™øÊï¥Âà∞Ê®ôË®òÁ©∫ÈñìÔºå‰ΩøÁî®Ë∂ÖÊ®ôË®ò‰ΩúÁÇ∫Á∑äÊπäÁöÑË¶ñË¶∫Ë°®Á§∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈÄèÈÅéÂ≠∏ÁøíË¶ñË¶∫ Transformer ‰∏≠ÊúâÊïàÁéáÁöÑÂÖ®Â±ÄË°®Á§∫ÔºåÁâπÂà•ÊòØÂ∞çÊñºÊ∑∫Â±§Ôºå‰æÜËß£Ê±∫ÂÜóÈ§òÂïèÈ°å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂú® UNet Êû∂Êßã (STA-UNet) ‰∏≠Â∞éÂÖ• STA Ê®°ÁµÑÔºå‰ª•ÈôêÂà∂ÂÜóÈ§òÔºåÂêåÊôÇ‰∏çÈÅ∫Â§±Ë±êÂØåÁöÑË≥áË®ä„ÄÇÂú®ÂõõÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü STA-UNet Âú®Âô®ÂÆòÂàÜÂâ≤‰ªªÂãôÁöÑ Dice ÂàÜÊï∏Âíå IOU ÊñπÈù¢ÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤Êû∂Êßã„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/Retinal-Research/STA-UNet} ÂèñÂæó„ÄÇ</paragraph>

##### **MIRAGE: Multimodal Identification and Recognition of Annotations in Indian General Prescriptions**
2410.09729v1 by Tavish Mankash, V. S. Chaithanya Kota, Anish De, Praveen Prakash, Kshitij Jadhav

Hospitals generate thousands of handwritten prescriptions, a practice that
remains prevalent despite the availability of Electronic Medical Records (EMR).
This method of record-keeping hinders the examination of long-term medication
effects, impedes statistical analysis, and makes the retrieval of records
challenging. Handwritten prescriptions pose a unique challenge, requiring
specialized data for training models to recognize medications and their
patterns of recommendation. While current handwriting recognition approaches
typically employ 2-D LSTMs, recent studies have explored the use of Large
Language Models (LLMs) for Optical Character Recognition (OCR). Building on
this approach, we focus on extracting medication names from medical records.
Our methodology MIRAGE (Multimodal Identification and Recognition of
Annotations in indian GEneral prescriptions) involves fine-tuning the LLaVA 1.6
and Idefics2 models. Our research utilizes a dataset provided by Medyug
Technology, consisting of 743,118 fully annotated high-resolution simulated
medical records from 1,133 doctors across India. We demonstrate that our
methodology exhibits 82% accuracy in medication name and dosage extraction. We
provide a detailed account of our research methodology and results, notes about
HWR with Multimodal LLMs, and release a small dataset of 100 medical records
with labels.

ÊëòË¶ÅÔºöÈÜ´Èô¢ÊúÉÁî¢ÁîüÊï∏ÂçÉ‰ªΩÊâãÂØ´ËôïÊñπÔºåÂÑòÁÆ°ÊúâÈõªÂ≠êÁóÖÊ≠∑ (EMR) ÂèØÁî®Ôºå‰ΩÜÈÄôÁ®ÆÂÅöÊ≥ï‰ªçÁÑ∂ÂæàÊôÆÈÅç„ÄÇÈÄôÁ®ÆË®òÈåÑ‰øùÂ≠òÊñπÂºèÊúÉÈòªÁ§ôÈï∑ÊúüËó•Áâ©ÊïàÊûúÁöÑÊ™¢Êü•ÔºåÂ¶®Á§ôÁµ±Ë®àÂàÜÊûêÔºå‰∏¶ËÆìË®òÈåÑÁöÑÊ™¢Á¥¢ËÆäÂæóÂõ∞Èõ£„ÄÇÊâãÂØ´ËôïÊñπÊßãÊàê‰∫Ü‰∏ÄÈ†ÖÁç®ÁâπÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅÂ∞àÊ•≠ÁöÑË≥áÊñô‰æÜË®ìÁ∑¥Ê®°Âûã‰ª•Ëæ®Ë≠òËó•Áâ©ÂèäÂÖ∂Êé®Ëñ¶Ê®°Âºè„ÄÇÈõñÁÑ∂ÁõÆÂâçÁöÑËæ®Ë≠òÊâãÂØ´Â≠óÊñπÊ≥ïÈÄöÂ∏∏Êé°Áî® 2-D LSTMÔºå‰ΩÜÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤Êé¢Ë®é‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÂÖâÂ≠∏Â≠óÂÖÉËæ®Ë≠ò (OCR)„ÄÇÊ†πÊìöÊ≠§ÊñπÊ≥ïÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂæûÁóÖÊ≠∑‰∏≠Êì∑ÂèñËó•Áâ©ÂêçÁ®±„ÄÇÊàëÂÄëÁöÑ MIRAGE ÊñπÊ≥ïÔºàÂç∞Â∫¶‰∏ÄËà¨ËôïÊñπ‰∏≠ÁöÑÂ§öÊ®°ÂºèË®ªËß£Ëæ®Ë≠òËàáËæ®Ë≠òÔºâÊ∂âÂèäÂæÆË™ø LLaVA 1.6 Âíå Idefics2 Ê®°Âûã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ΩøÁî® Medyug Technology Êèê‰æõÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™Âç∞Â∫¶ 1,133 ‰ΩçÈÜ´ÁîüÁöÑ 743,118 ‰ªΩÁ∂ìÈÅéÂÆåÊï¥Ë®ªËß£ÁöÑÈ´òËß£ÊûêÂ∫¶Ê®°Êì¨ÁóÖÊ≠∑„ÄÇÊàëÂÄëË≠âÊòéÊàëÂÄëÁöÑÊäÄË°ìÂú®Ëó•Áâ©ÂêçÁ®±ÂíåÂäëÈáèÊì∑ÂèñÊñπÈù¢Â±ïÁèæÂá∫ 82% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫ÜÊàëÂÄëÁöÑÁ†îÁ©∂ÊñπÊ≥ïÂíåÁµêÊûúÔºå‰ª•ÂèäÊúâÈóú‰ΩøÁî®Â§öÊ®°Âºè LLM ÁöÑ HWR ÁöÑÊ≥®ÊÑè‰∫ãÈ†ÖÔºå‰∏¶ÁôºÂ∏É‰∫Ü‰∏ÄÂ∞èÈÉ®ÂàÜÂåÖÂê´Ê®ôÁ±§ÁöÑ 100 ‰ªΩÁóÖÊ≠∑Ë≥áÊñôÈõÜ„ÄÇ

##### **3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation**
2410.10901v1 by Hongxin Ding, Yue Fang, Runchuan Zhu, Xinke Jiang, Jinyang Zhang, Yongxin Xu, Xu Chu, Junfeng Zhao, Yasha Wang

Large Language Models(LLMs) excel in general tasks but struggle in
specialized domains like healthcare due to limited domain-specific
knowledge.Supervised Fine-Tuning(SFT) data construction for domain adaptation
often relies on heuristic methods, such as GPT-4 annotation or manual data
selection, with a data-centric focus on presumed diverse, high-quality
datasets. However, these methods overlook the model's inherent knowledge
distribution, introducing noise, redundancy, and irrelevant data, leading to a
mismatch between the selected data and the model's learning task, resulting in
suboptimal performance. To address this, we propose a two-stage model-centric
data selection framework, Decomposed Difficulty Data Selection (3DS), which
aligns data with the model's knowledge distribution for optimized adaptation.
In Stage1, we apply Prompt-Driven Data Selection via Explicit Alignment, where
the the model filters irrelevant or redundant data based on its internal
knowledge. In Stage2, we perform Decomposed Difficulty Data Selection, where
data selection is guided by our defined difficulty decomposition, using three
metrics: Instruction Understanding, Response Confidence, and Response
Correctness. Additionally, an attention-based importance weighting mechanism
captures token importance for more accurate difficulty calibration. This
two-stage approach ensures the selected data is not only aligned with the
model's knowledge and preferences but also appropriately challenging for the
model to learn, leading to more effective and targeted domain adaptation. In
the case study of the medical domain, our extensive experiments on real-world
healthcare datasets demonstrate the superiority of 3DS over exisiting methods
in accuracy by over 5.29%. Our dataset and code will be open-sourced at
https://anonymous.4open.science/r/3DS-E67F.

ÊëòË¶ÅÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®‰∏ÄËà¨‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÂåªÁñó‰øùÂÅ•Á≠â‰∏ì‰∏öÈ¢ÜÂüü‰∏≠Âç¥Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂõ†‰∏∫Áº∫‰πèÁâπÂÆöÈ¢ÜÂüüÁöÑÁü•ËØÜ„ÄÇÈ¢ÜÂüüÈÄÇÂ∫îÁöÑÁõëÁù£ÂæÆË∞É (SFT) Êï∞ÊçÆÊûÑÂª∫ÈÄöÂ∏∏‰æùËµñÂêØÂèëÂºèÊñπÊ≥ïÔºå‰æãÂ¶Ç GPT-4 Ê≥®ÈáäÊàñÊâãÂä®Êï∞ÊçÆÈÄâÊã©ÔºåÂÖ∂Êï∞ÊçÆ‰∏≠ÂøÉÂåñÈáçÁÇπÂú®‰∫éÂÅáÂÆöÁöÑÂ§öÊ†∑Âåñ„ÄÅÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂøΩÁï•‰∫ÜÊ®°ÂûãÂõ∫ÊúâÁöÑÁü•ËØÜÂàÜÂ∏ÉÔºåÂºïÂÖ•‰∫ÜÂô™Èü≥„ÄÅÂÜó‰ΩôÂíåÊó†ÂÖ≥Êï∞ÊçÆÔºåÂØºËá¥ÊâÄÈÄâÊï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÂ≠¶‰π†‰ªªÂä°‰∏çÂåπÈÖçÔºå‰ªéËÄåÂØºËá¥ÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏§Èò∂ÊÆµ‰ª•Ê®°Âûã‰∏∫‰∏≠ÂøÉÁöÑÊï∞ÊçÆÈÄâÊã©Ê°ÜÊû∂ÔºåÂç≥ÂàÜËß£ÈöæÂ∫¶Êï∞ÊçÆÈÄâÊã© (3DS)ÔºåÂÆÉ‰ΩøÊï∞ÊçÆ‰∏éÊ®°ÂûãÁöÑÁü•ËØÜÂàÜÂ∏É‰øùÊåÅ‰∏ÄËá¥Ôºå‰ª•ËøõË°å‰ºòÂåñÈÄÇÂ∫î„ÄÇÂú®Á¨¨ 1 Èò∂ÊÆµÔºåÊàë‰ª¨ÈÄöËøáÊòæÂºèÂØπÈΩêÂ∫îÁî®ÊèêÁ§∫È©±Âä®ÁöÑÂü∫‰∫éÊï∞ÊçÆÁöÑÈÄâÊã©ÔºåÂÖ∂‰∏≠Ê®°ÂûãÊ†πÊçÆÂÖ∂ÂÜÖÈÉ®Áü•ËØÜËøáÊª§Êó†ÂÖ≥ÊàñÂÜó‰ΩôÁöÑÊï∞ÊçÆ„ÄÇÂú®Á¨¨ 2 Èò∂ÊÆµÔºåÊàë‰ª¨ÊâßË°åÂàÜËß£ÈöæÂ∫¶Êï∞ÊçÆÈÄâÊã©ÔºåÂÖ∂‰∏≠Êï∞ÊçÆÈÄâÊã©Áî±Êàë‰ª¨ÂÆö‰πâÁöÑÈöæÂ∫¶ÂàÜËß£ÊåáÂØºÔºå‰ΩøÁî®‰∏â‰∏™ÊåáÊ†áÔºöÊåá‰ª§ÁêÜËß£„ÄÅÂìçÂ∫îÁΩÆ‰ø°Â∫¶ÂíåÂìçÂ∫îÊ≠£Á°ÆÊÄß„ÄÇÊ≠§Â§ñÔºåÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÈáçË¶ÅÊÄßÂä†ÊùÉÊú∫Âà∂ÊçïËé∑Ê†áËÆ∞ÈáçË¶ÅÊÄßÔºå‰ª•‰æøÊõ¥ÂáÜÁ°ÆÂú∞Ê†°ÂáÜÈöæÂ∫¶„ÄÇËøôÁßç‰∏§Èò∂ÊÆµÊñπÊ≥ïÁ°Æ‰øùÊâÄÈÄâÊï∞ÊçÆ‰∏ç‰ªÖ‰∏éÊ®°ÂûãÁöÑÁü•ËØÜÂíåÂÅèÂ•Ω‰øùÊåÅ‰∏ÄËá¥ÔºåËÄå‰∏îÂØπÊ®°ÂûãÂ≠¶‰π†ËÄåË®Ä‰πüÂÖ∑ÊúâÈÄÇÂΩìÁöÑÊåëÊàòÊÄßÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÂíåÊõ¥ÊúâÈíàÂØπÊÄßÁöÑÈ¢ÜÂüüÈÄÇÂ∫î„ÄÇÂú®ÂåªÂ≠¶È¢ÜÂüüÁöÑÊ°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨Âú®ÁúüÂÆû‰∏ñÁïåÂåªÁñó‰øùÂÅ•Êï∞ÊçÆÈõÜ‰∏äËøõË°åÁöÑÂπøÊ≥õÂÆûÈ™åË°®ÊòéÔºå3DS Âú®ÂáÜÁ°ÆÊÄßÊñπÈù¢ÊØîÁé∞ÊúâÊñπÊ≥ïÈ´òÂá∫ 5.29%„ÄÇÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÂíå‰ª£Á†ÅÂ∞ÜÂú® https://anonymous.4open.science/r/3DS-E67F ÂºÄÊ∫ê„ÄÇ

##### **Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions**
2410.09643v1 by Abdullah Mamun, Krista S. Leonard, Megan E. Petrov, Matthew P. Buman, Hassan Ghasemzadeh

Objective: This research aims to develop a lifestyle intervention system,
called MoveSense, that forecasts a patient's activity behavior to allow for
early and personalized interventions in real-world clinical environments.
Methods: We conducted two clinical studies involving 58 prediabetic veterans
and 60 patients with obstructive sleep apnea to gather multimodal behavioral
data using wearable devices. We develop multimodal long short-term memory
(LSTM) network models, which are capable of forecasting the number of step
counts of a patient up to 24 hours in advance by examining data from activity
and engagement modalities. Furthermore, we design goal-based forecasting models
to predict whether a person's next-day steps will be over a certain threshold.
Results: Multimodal LSTM with early fusion achieves 33% and 37% lower mean
absolute errors than linear regression and ARIMA respectively on the
prediabetes dataset. LSTM also outperforms linear regression and ARIMA with a
margin of 13% and 32% on the sleep dataset. Multimodal forecasting models also
perform with 72% and 79% accuracy on the prediabetes dataset and sleep dataset
respectively on goal-based forecasting. Conclusion: Our experiments conclude
that multimodal LSTM models with early fusion are better than multimodal LSTM
with late fusion and unimodal LSTM models and also than ARIMA and linear
regression models. Significance: We address an important and challenging task
of time-series forecasting in uncontrolled environments. Effective forecasting
of a person's physical activity can aid in designing adaptive behavioral
interventions to keep the user engaged and adherent to a prescribed routine.

ÊëòË¶ÅÔºöÁõÆÊ®ôÔºöÊú¨Á†îÁ©∂Êó®Âú®ÈñãÁôº‰∏ÄÁ®ÆÁîüÊ¥ªÂûãÊÖã‰ªãÂÖ•Á≥ªÁµ±ÔºåÁ®±ÁÇ∫ MoveSenseÔºåÂèØÈ†êÊ∏¨ÁóÖÊÇ£ÁöÑÊ¥ªÂãïË°åÁÇ∫Ôºå‰ª•‰æøÂú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÊó©Êúü‰∏îÂÄã‰∫∫ÂåñÁöÑ‰ªãÂÖ•„ÄÇ
ÊñπÊ≥ïÔºöÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂÖ©È†ÖËá®Â∫äÁ†îÁ©∂ÔºåÊ∂âÂèä 58 ‰ΩçÁ≥ñÂ∞øÁóÖÂâçÊúüÈÄÄ‰ºçËªç‰∫∫Âíå 60 ‰ΩçÈòªÂ°ûÊÄßÁù°Áú†ÂëºÂê∏‰∏≠Ê≠¢ÁóáÊÇ£ËÄÖÔºå‰ª•‰ΩøÁî®Á©øÊà¥ÂºèË£ùÁΩÆÊî∂ÈõÜÂ§öÊ®°ÂºèË°åÁÇ∫Êï∏Êìö„ÄÇÊàëÂÄëÈñãÁôº‰∫ÜÂ§öÊ®°ÂºèÈï∑Áü≠ÊúüË®òÊÜ∂ (LSTM) Á∂≤Ë∑ØÊ®°ÂûãÔºåÂÆÉËÉΩÂ§†ÈÄèÈÅéÊ™¢Êü•Ê¥ªÂãïÂíåÂèÉËàáÊ®°ÂºèÁöÑÊï∏ÊìöÔºåÈ†êÊ∏¨ÁóÖÊÇ£Âú® 24 Â∞èÊôÇÂÖßË∏èÂá∫ÁöÑÊ≠•Êï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÂü∫ÊñºÁõÆÊ®ôÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºå‰ª•È†êÊ∏¨Êüê‰∫∫ÁöÑÈöîÊó•Ê≠•Êï∏ÊòØÂê¶ÊúÉË∂ÖÈÅéÊüêÂÄãÈñæÂÄº„ÄÇ
ÁµêÊûúÔºöÂ§öÊ®°Âºè LSTM ËàáÊó©ÊúüËûçÂêàÂú®Á≥ñÂ∞øÁóÖÂâçÊúüÊï∏ÊìöÈõÜ‰∏äÂØ¶ÁèæÁöÑÂπ≥ÂùáÁµïÂ∞çË™§Â∑ÆÊØîÁ∑öÊÄßÂõûÊ≠∏Âíå ARIMA ÂàÜÂà•‰Ωé 33% Âíå 37%„ÄÇLSTM Âú®Áù°Áú†Êï∏ÊìöÈõÜ‰∏ä‰πü‰ª• 13% Âíå 32% ÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÁ∑öÊÄßÂõûÊ≠∏Âíå ARIMA„ÄÇÂ§öÊ®°ÂºèÈ†êÊ∏¨Ê®°ÂûãÂú®Á≥ñÂ∞øÁóÖÂâçÊúüÊï∏ÊìöÈõÜÂíåÁù°Áú†Êï∏ÊìöÈõÜ‰∏ä‰πüÂàÜÂà•‰ª• 72% Âíå 79% ÁöÑÊ∫ñÁ¢∫Â∫¶Âü∑Ë°åÂü∫ÊñºÁõÆÊ®ôÁöÑÈ†êÊ∏¨„ÄÇÁµêË´ñÔºöÊàëÂÄëÁöÑÂØ¶È©óÂæóÂá∫ÁµêË´ñÔºåÂÖ∑ÊúâÊó©ÊúüËûçÂêàÁöÑÂ§öÊ®°Âºè LSTM Ê®°ÂûãÊØîÂÖ∑ÊúâÂæåÊúüËûçÂêàÁöÑÂ§öÊ®°Âºè LSTM Ê®°ÂûãÂíåÂñÆÊ®°Âºè LSTM Ê®°ÂûãÊõ¥Â•ΩÔºå‰πüÊØî ARIMA ÂíåÁ∑öÊÄßÂõûÊ≠∏Ê®°ÂûãÊõ¥Â•Ω„ÄÇÊÑèÁæ©ÔºöÊàëÂÄëËß£Ê±∫‰∫ÜÂú®‰∏çÂèóÊéßÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ÁöÑ‰∏ÄÈ†ÖÈáçË¶Å‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÊúâÊïàÈ†êÊ∏¨ÂÄã‰∫∫ÁöÑË∫´È´îÊ¥ªÂãïÊúâÂä©ÊñºË®≠Ë®àÈÅ©ÊáâÊÄßË°åÁÇ∫‰ªãÂÖ•Êé™ÊñΩÔºå‰ª•‰øùÊåÅ‰ΩøÁî®ËÄÖÂèÉËàá‰∏¶ÈÅµÂÆàË¶èÂÆöÁöÑ‰æãË°åÂÖ¨‰∫ã„ÄÇ

##### **Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health**
2410.09635v1 by Abdullah Mamun, Lawrence D. Devoe, Mark I. Evans, David W. Britt, Judith Klein-Seetharaman, Hassan Ghasemzadeh

Early detection of intrapartum risk enables interventions to potentially
prevent or mitigate adverse labor outcomes such as cerebral palsy. Currently,
there is no accurate automated system to predict such events to assist with
clinical decision-making. To fill this gap, we propose "Artificial Intelligence
(AI) for Modeling and Explaining Neonatal Health" (AIMEN), a deep learning
framework that not only predicts adverse labor outcomes from maternal, fetal,
obstetrical, and intrapartum risk factors but also provides the model's
reasoning behind the predictions made. The latter can provide insights into
what modifications in the input variables of the model could have changed the
predicted outcome. We address the challenges of imbalance and small datasets by
synthesizing additional training data using Adaptive Synthetic Sampling
(ADASYN) and Conditional Tabular Generative Adversarial Networks (CTGAN). AIMEN
uses an ensemble of fully-connected neural networks as the backbone for its
classification with the data augmentation supported by either ADASYN or CTGAN.
AIMEN, supported by CTGAN, outperforms AIMEN supported by ADASYN in
classification. AIMEN can predict a high risk for adverse labor outcomes with
an average F1 score of 0.784. It also provides counterfactual explanations that
can be achieved by changing 2 to 3 attributes on average. Resources available:
https://github.com/ab9mamun/AIMEN.

ÊëòË¶ÅÔºöÁî¢Á®ã‰∏≠È¢®Èö™ÁöÑÊó©ÊúüÂÅµÊ∏¨ÊúâÂä©ÊñºÈÄ≤Ë°åÂπ≤È†êÊé™ÊñΩÔºå‰ª•È†êÈò≤ÊàñÊ∏õËºï‰∏çÂà©ÁöÑÁîüÁî¢ÁµêÊûúÔºå‰æãÂ¶ÇËÖ¶ÊÄßÈ∫ªÁó∫„ÄÇÁõÆÂâçÔºåÊ≤íÊúâÊ∫ñÁ¢∫ÁöÑËá™ÂãïÂåñÁ≥ªÁµ±ÂèØ‰ª•È†êÊ∏¨Ê≠§È°û‰∫ã‰ª∂Ôºå‰ª•ÂçîÂä©Ëá®Â∫äÊ±∫Á≠ñ„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄô‰∏ÄÁ©∫ÁôΩÔºåÊàëÂÄëÊèêÂá∫„ÄåÁî®ÊñºÂª∫Ê®°ÂíåËß£ÈáãÊñ∞ÁîüÂÖíÂÅ•Â∫∑ÁöÑ‰∫∫Â∑•Êô∫ÊÖß„Äç(AIMEN)ÔºåÈÄôÊòØ‰∏ÄÂÄãÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÂÆÉ‰∏çÂÉÖÂèØ‰ª•Ê†πÊìöÂ≠ïÁî¢Â©¶„ÄÅËÉéÂÖí„ÄÅÁî¢ÁßëÂíåÁî¢Á®ãÈ¢®Èö™Âõ†Á¥†È†êÊ∏¨‰∏çÂà©ÁöÑÁîüÁî¢ÁµêÊûúÔºåÈÇÑËÉΩÊèê‰æõÊ®°ÂûãÂÅöÂá∫È†êÊ∏¨ËÉåÂæåÁöÑÂéüÂõ†„ÄÇÂæåËÄÖÂèØ‰ª•Êèê‰æõË¶ãËß£ÔºåË™™ÊòéÊ®°ÂûãËº∏ÂÖ•ËÆäÊï∏‰∏≠ÁöÑÂì™‰∫õ‰øÆÊîπÂèØËÉΩÊúÉÊîπËÆäÈ†êÊ∏¨ÁµêÊûú„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî®ÈÅ©ÊáâÊÄßÂêàÊàêÊäΩÊ®£ (ADASYN) ÂíåÊ¢ù‰ª∂Ë°®Ê†ºÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (CTGAN) ‰æÜÂêàÊàêÈ°çÂ§ñÁöÑË®ìÁ∑¥Ë≥áÊñôÔºå‰ª•Ëß£Ê±∫‰∏çÂπ≥Ë°°ÂíåÂ∞èÂûãË≥áÊñôÈõÜÁöÑÊåëÊà∞„ÄÇAIMEN ‰ΩøÁî®ÂÖ®ÈÄ£Êé•Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÈõÜÂêà‰ΩúÁÇ∫ÂÖ∂ÂàÜÈ°ûÁöÑÈ™®ÂππÔºå‰∏¶ÈÄèÈÅé ADASYN Êàñ CTGAN ÊîØÊè¥Ë≥áÊñôÊì¥ÂÖÖ„ÄÇÁî± CTGAN ÊîØÊè¥ÁöÑ AIMEN Âú®ÂàÜÈ°ûÊñπÈù¢ÂÑ™ÊñºÁî± ADASYN ÊîØÊè¥ÁöÑ AIMEN„ÄÇAIMEN ÂèØ‰ª•È†êÊ∏¨‰∏çÂà©ÁöÑÁîüÁî¢ÁµêÊûúÁöÑÈ´òÈ¢®Èö™ÔºåÂπ≥Âùá F1 ÂàÜÊï∏ÁÇ∫ 0.784„ÄÇÂÆÉÈÇÑÊèê‰æõÂèç‰∫ãÂØ¶Ëß£ÈáãÔºåÂèØÈÄèÈÅéÂπ≥ÂùáËÆäÊõ¥ 2 Ëá≥ 3 ÂÄãÂ±¨ÊÄß‰æÜÈÅîÊàê„ÄÇÂèØÁî®Ë≥áÊ∫êÔºöhttps://github.com/ab9mamun/AIMEN„ÄÇ

##### **AuD-Former: A Hierarchical Transformer Network for Multimodal Audio-Based Disease Prediction**
2410.09289v1 by Jinjin Cai, Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Victoria McKenna, Aaron Friedman, Rachel Foot, Susan Storey, Ryan Boente, Sudip Vhaduri, Byung-Cheol Min

Audio-based disease prediction is emerging as a promising supplement to
traditional medical diagnosis methods, facilitating early, convenient, and
non-invasive disease detection and prevention. Multimodal fusion, which
integrates features from various domains within or across bio-acoustic
modalities, has proven effective in enhancing diagnostic performance. However,
most existing methods in the field employ unilateral fusion strategies that
focus solely on either intra-modal or inter-modal fusion. This approach limits
the full exploitation of the complementary nature of diverse acoustic feature
domains and bio-acoustic modalities. Additionally, the inadequate and isolated
exploration of latent dependencies within modality-specific and modality-shared
spaces curtails their capacity to manage the inherent heterogeneity in
multimodal data. To fill these gaps, we propose AuD-Former, a hierarchical
transformer network designed for general multimodal audio-based disease
prediction. Specifically, we seamlessly integrate intra-modal and inter-modal
fusion in a hierarchical manner and proficiently encode the necessary
intra-modal and inter-modal complementary correlations, respectively.
Comprehensive experiments demonstrate that AuD-Former achieves state-of-the-art
performance in predicting three diseases: COVID-19, Parkinson's disease, and
pathological dysarthria, showcasing its promising potential in a broad context
of audio-based disease prediction tasks. Additionally, extensive ablation
studies and qualitative analyses highlight the significant benefits of each
main component within our model.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºÈü≥Ë®äÁöÑÁñæÁóÖÈ†êÊ∏¨Ê≠£ÈÄêÊº∏ÊàêÁÇ∫ÂÇ≥Áµ±ÈÜ´ÁôÇË®∫Êñ∑ÊñπÊ≥ïÁöÑÊúâÂäõË£úÂÖÖÔºåÊúâÂä©ÊñºÊó©Êúü„ÄÅ‰æøÂà©‰∏îÈùû‰æµÂÖ•ÂºèÂú∞ÂÅµÊ∏¨ÂíåÈ†êÈò≤ÁñæÁóÖ„ÄÇÂ§öÊ®°ÊÖãËûçÂêàÊï¥Âêà‰æÜËá™ÁîüÁâ©ËÅ≤Â≠∏Ê®°ÂºèÂÖßÈÉ®ÊàñË∑®Ê®°ÂºèÁöÑÂêÑÁ®ÆÈ†òÂüüÁöÑÁâπÂæµÔºåÂ∑≤Ë¢´Ë≠âÂØ¶ËÉΩÊúâÊïàÊèêÂçáË®∫Êñ∑ÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÂ§ßÂ§öÊï∏ÊñπÊ≥ïÊé°Áî®ÂñÆÈÇäËûçÂêàÁ≠ñÁï•ÔºåÂÉÖÂ∞àÊ≥®ÊñºÊ®°ÂºèÂÖßÊàñÊ®°ÂºèÈñìËûçÂêà„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈôêÂà∂‰∫ÜÂ∞ç‰∏çÂêåËÅ≤Â≠∏ÁâπÂæµÈ†òÂüüÂíåÁîüÁâ©ËÅ≤Â≠∏Ê®°ÂºèÁöÑ‰∫íË£úÁâπÊÄßÁöÑÂÖÖÂàÜÂà©Áî®„ÄÇÊ≠§Â§ñÔºåÂ∞çÊ®°ÂºèÁâπÂÆöÂíåÊ®°ÂºèÂÖ±‰∫´Á©∫ÈñìÂÖßÊΩõÂú®‰æùË≥¥ÊÄß‰∏çË∂≥‰∏îÂ≠§Á´ãÁöÑÊé¢Á¥¢Ôºå‰πüÈôêÂà∂‰∫ÜÂÖ∂ÁÆ°ÁêÜÂ§öÊ®°ÊÖãË≥áÊñô‰∏≠Âõ∫ÊúâÁï∞Ë≥™ÊÄßÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄô‰∫õÁ©∫ÁôΩÔºåÊàëÂÄëÊèêÂá∫ AuD-FormerÔºå‰∏ÄÂÄãÈöéÂ±§ÂºèTransformerÁ∂≤Ë∑ØÔºåÂ∞àÁÇ∫‰∏ÄËà¨Â§öÊ®°ÊÖãÂü∫ÊñºÈü≥Ë®äÁöÑÁñæÁóÖÈ†êÊ∏¨ËÄåË®≠Ë®à„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ª•ÈöéÂ±§ÊñπÂºèÁÑ°Á∏´Êï¥ÂêàÊ®°ÂºèÂÖßÂíåÊ®°ÂºèÈñìËûçÂêàÔºå‰∏¶ÂàÜÂà•ÁÜüÁ∑¥Âú∞Á∑®Á¢ºÂøÖË¶ÅÁöÑÊ®°ÂºèÂÖßÂíåÊ®°ÂºèÈñì‰∫íË£úÈóúËÅØ„ÄÇÂÖ®Èù¢ÁöÑÂØ¶È©óË≠âÊòéÔºåAuD-Former Âú®È†êÊ∏¨‰∏âÁ®ÆÁñæÁóÖÔºàCOVID-19„ÄÅÂ∏ïÈáëÊ£ÆÊ∞èÁóáÂíåÁóÖÁêÜÊÄßÊßãÈü≥ÈöúÁ§ôÔºâÊñπÈù¢ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Âª£Ê≥õÁöÑÂü∫ÊñºÈü≥Ë®äÁöÑÁñæÁóÖÈ†êÊ∏¨‰ªªÂãô‰∏≠ÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÂª£Ê≥õÁöÑÊ∂àËûçÁ†îÁ©∂ÂíåÂÆöÊÄßÂàÜÊûêÁ™ÅÂá∫‰∫ÜÊàëÂÄëÊ®°Âûã‰∏≠ÊØèÂÄã‰∏ªË¶ÅÁµÑÊàêÁöÑÈ°ØËëóÂÑ™Èªû„ÄÇ</paragraph>

##### **LLMD: A Large Language Model for Interpreting Longitudinal Medical Records**
2410.12860v1 by Robert Porter, Adam Diehl, Benjamin Pastel, J. Henry Hinnefeld, Lawson Nerenberg, Pye Maung, Sebastien Kerbrat, Gillian Hanson, Troy Astorino, Stephen J. Tarsa

We introduce LLMD, a large language model designed to analyze a patient's
medical history based on their medical records. Along with domain knowledge,
LLMD is trained on a large corpus of records collected over time and across
facilities, as well as tasks and labels that make nuanced connections among
them. This approach is critical to an accurate picture of patient health, and
has distinctive advantages over models trained on knowledge alone, unlabeled
records, structured EHR data, or records from a single health system.
  The recipe for LLMD continues pretraining a foundational model on both domain
knowledge and the contents of millions of records. These span an average of 10
years of care and as many as 140 care sites per patient. LLMD is then
instruction fine-tuned on structuring and abstraction tasks. The former jointly
identify and normalize document metadata, provenance information, clinical
named-entities, and ontology mappings, while the latter roll these into
higher-level representations, such a continuous era of time a patient was on a
medication. LLMD is deployed within a layered validation system that includes
continual random audits and review by experts, e.g. based on uncertainty,
disease-specific rules, or use-case.
  LLMD exhibits large gains over both more-powerful generalized models and
domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state
of the art accuracy on PubMedQA text responses, besting orders-of-magnitude
larger models. On production tasks, we show that LLMD significantly outperforms
all other models evaluated, and among alternatives, large general purpose LLMs
like GPT-4o are more accurate than models emphasizing medical knowledge. We
find strong evidence that accuracy on today's medical benchmarks is not the
most significant factor when analyzing real-world patient data, an insight with
implications for future medical LLMs.'

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂºïÂÖ•‰∫Ü LLMDÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÊó®Âú®Ê†πÊìöÁóÖÊ≠∑ÂàÜÊûêÊÇ£ËÄÖÁöÑÁóÖÂè≤„ÄÇÈô§‰∫ÜÈ†òÂüüÁü•Ë≠òÂ§ñÔºåLLMD ÈÇÑÊé•Âèó‰∫ÜÂ§ßÈáèÈö®ËëóÊôÇÈñìÊé®ÁßªÂíåË∑®Ë®≠ÊñΩÊî∂ÈõÜÁöÑË®òÈåÑÁöÑË®ìÁ∑¥Ôºå‰ª•ÂèäÂú®ÂÆÉÂÄë‰πãÈñìÂª∫Á´ãÁ¥∞ÂæÆËÅØÁπ´ÁöÑ‰ªªÂãôÂíåÊ®ôÁ±§„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ∞çÊñºÊ∫ñÁ¢∫ÊèèÁπ™ÊÇ£ËÄÖÂÅ•Â∫∑ÁãÄÊ≥ÅËá≥ÈóúÈáçË¶ÅÔºå‰∏¶‰∏îËàáÂÉÖÊé•ÂèóÁü•Ë≠òË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÅÊú™Ê®ôË®òË®òÈåÑ„ÄÅÁµêÊßãÂåñÁöÑ EHR Êï∏ÊìöÊàñ‰æÜËá™ÂñÆ‰∏ÄÂÅ•Â∫∑Á≥ªÁµ±ÁöÑË®òÈåÑÁõ∏ÊØîÔºåÂÖ∑ÊúâÈ°ØËëóÂÑ™Âã¢„ÄÇ
LLMD ÁöÑÁßòË®£ÊòØÂ∞çÂü∫Á§éÊ®°ÂûãÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÊó¢ÂåÖÊã¨È†òÂüüÁü•Ë≠òÔºå‰πüÂåÖÊã¨Êï∏ÁôæËê¨Ê¢ùË®òÈåÑÁöÑÂÖßÂÆπ„ÄÇÈÄô‰∫õË®òÈåÑÂπ≥ÂùáÊ∂µËìã‰∫ÜÊØè‰ΩçÊÇ£ËÄÖ 10 Âπ¥ÁöÑË≠∑ÁêÜÊôÇÈñìÂíåÂ§öÈÅî 140 ÂÄãË≠∑ÁêÜÂú∞Èªû„ÄÇÁÑ∂ÂæåÂ∞ç LLMD ÈÄ≤Ë°åÁµêÊßãÂåñÂíåÊäΩË±°‰ªªÂãôÁöÑÊåá‰ª§ÂæÆË™ø„ÄÇÂâçËÄÖÂÖ±ÂêåË≠òÂà•ÂíåÊ®ôÊ∫ñÂåñÊñáÊ™îÂÖÉÊï∏Êìö„ÄÅ‰æÜÊ∫ê‰ø°ÊÅØ„ÄÅËá®Â∫äÂëΩÂêçÂØ¶È´îÂíåÊú¨‰ΩìÊò†Â∞ÑÔºåËÄåÂæåËÄÖÂ∞áÈÄô‰∫õÂÖßÂÆπËΩâÊèõÁÇ∫Êõ¥È´òÁ¥öÂà•ÁöÑË°®Á§∫Ôºå‰æãÂ¶ÇÊÇ£ËÄÖÊúçËó•ÁöÑÈÄ£Á∫åÊôÇÈñìÊÆµ„ÄÇLLMD Âú®‰∏ÄÂÄãÂàÜÂ±§È©óË≠âÁ≥ªÁµ±‰∏≠ÈÉ®ÁΩ≤ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊåÅÁ∫åÁöÑÈö®Ê©üÂØ©Ê†∏ÂíåÂ∞àÂÆ∂ÂØ©Êü•Ôºå‰æãÂ¶ÇÂü∫Êñº‰∏çÁ¢∫ÂÆöÊÄß„ÄÅÁâπÂÆöÁñæÁóÖË¶èÂâáÊàñÁî®‰æã„ÄÇ
LLMD Âú®ÂäüËÉΩÊõ¥Âº∑Â§ßÁöÑÈÄöÁî®Ê®°ÂûãÂíåÁâπÂÆöÈ†òÂüüÊ®°ÂûãÊñπÈù¢ÈÉΩË°®ÁèæÂá∫Â∑®Â§ßÁöÑÂÑ™Âã¢„ÄÇÂú®ÈÜ´Â≠∏Áü•Ë≠òÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåLLMD-8B Âú® PubMedQA ÊñáÊú¨ÈüøÊáâÊñπÈù¢ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÂÑ™ÊñºÊï∏ÈáèÁ¥öÊõ¥Â§ßÁöÑÊ®°Âûã„ÄÇÂú®ÁîüÁî¢‰ªªÂãô‰∏≠ÔºåÊàëÂÄëË°®Êòé LLMD ÊòéÈ°ØÂÑ™ÊñºÊâÄÊúâÂÖ∂‰ªñË©ï‰º∞Ê®°ÂûãÔºå‰∏¶‰∏îÂú®Êõø‰ª£ÊñπÊ°à‰∏≠ÔºåÂÉè GPT-4o ÈÄôÊ®£ÁöÑÂ§ßÂûãÈÄöÁî® LLM ÊØîÂº∑Ë™øÈÜ´Â≠∏Áü•Ë≠òÁöÑÊ®°ÂûãÊõ¥Ê∫ñÁ¢∫„ÄÇÊàëÂÄëÁôºÁèæÂº∑ÊúâÂäõÁöÑË≠âÊìöË°®ÊòéÔºåÂú®ÂàÜÊûêÁèæÂØ¶‰∏ñÁïåÁöÑÊÇ£ËÄÖÊï∏ÊìöÊôÇÔºåÁï∂‰ªäÈÜ´Â≠∏Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÊ∫ñÁ¢∫ÊÄß‰∏¶ÈùûÊúÄÈáçË¶ÅÁöÑÂõ†Á¥†ÔºåÈÄôÂ∞çÊú™‰æÜÁöÑÈÜ´Â≠∏ LLM ‰πüÊúâÂΩ±Èüø„ÄÇ</paragraph>

##### **Large Language Models for Medical OSCE Assessment: A Novel Approach to Transcript Analysis**
2410.12858v1 by Ameer Hamza Shakur, Michael J. Holcomb, David Hein, Shinyoung Kang, Thomas O. Dalton, Krystle K. Campbell, Daniel J. Scott, Andrew R. Jamieson

Grading Objective Structured Clinical Examinations (OSCEs) is a
time-consuming and expensive process, traditionally requiring extensive manual
effort from human experts. In this study, we explore the potential of Large
Language Models (LLMs) to assess skills related to medical student
communication. We analyzed 2,027 video-recorded OSCE examinations from the
University of Texas Southwestern Medical Center (UTSW), spanning four years
(2019-2022), and several different medical cases or "stations." Specifically,
our focus was on evaluating students' ability to summarize patients' medical
history: we targeted the rubric item 'did the student summarize the patients'
medical history?' from the communication skills rubric. After transcribing
speech audio captured by OSCE videos using Whisper-v3, we studied the
performance of various LLM-based approaches for grading students on this
summarization task based on their examination transcripts. Using various
frontier-level open-source and proprietary LLMs, we evaluated different
techniques such as zero-shot chain-of-thought prompting, retrieval augmented
generation, and multi-model ensemble methods. Our results show that frontier
LLM models like GPT-4 achieved remarkable alignment with human graders,
demonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential
for LLM-based OSCE grading to augment the current grading process. Open-source
models also showed promising results, suggesting potential for widespread,
cost-effective deployment. Further, we present a failure analysis identifying
conditions where LLM grading may be less reliable in this context and recommend
best practices for deploying LLMs in medical education settings.

ÊëòË¶ÅÔºöË©ïÂàÜÂÆ¢ËßÄÁµêÊßãÂºèËá®Â∫äËÄÉË©¶ (OSCE) ÊòØÂÄãËÄóÊôÇÂèàÊòÇË≤¥ÁöÑÈÅéÁ®ãÔºåÂÇ≥Áµ±‰∏äÈúÄË¶Å‰∫∫È°ûÂ∞àÂÆ∂Â§ßÈáèÊâãÂãïÂ∑•‰Ωú„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë©ï‰º∞ËàáÈÜ´Â≠∏ÁîüÊ∫ùÈÄöÁõ∏ÈóúÊäÄËÉΩÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂàÜÊûê‰∫ÜÂæ∑ÂÖãËñ©ÊñØÂ§ßÂ≠∏Ë•øÂçóÈÜ´Â≠∏‰∏≠ÂøÉ (UTSW) 2,027 Â†¥ÈåÑË£ΩÁöÑ OSCE ËÄÉË©¶ÔºåÊôÇÈñìË∑®Â∫¶ÁÇ∫ÂõõÂπ¥ (2019-2022)ÔºåÊ∂µËìã‰∫ÜÊï∏ÂÄã‰∏çÂêåÁöÑÈÜ´ÁôÇÊ°à‰æãÊàñ„ÄåÁ´ô„Äç„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºË©ï‰º∞Â≠∏ÁîüÁ∏ΩÁµêÁóÖÊ≠∑ÁöÑËÉΩÂäõÔºöÊàëÂÄë‰ª•Ê∫ùÈÄöÊäÄËÉΩË©ïÂàÜÊ®ôÊ∫ñ‰∏≠ÁöÑË©ïÂàÜÈ†ÖÁõÆ„ÄåÂ≠∏ÁîüÊòØÂê¶Á∏ΩÁµê‰∫ÜÁóÖ‰∫∫ÁöÑÁóÖÊ≠∑Ôºü„ÄçÁÇ∫ÁõÆÊ®ô„ÄÇÂú®‰ΩøÁî® Whisper-v3 ËΩâÈåÑ OSCE ÂΩ±ÁâáÊâÄÊì∑ÂèñÁöÑË™ûÈü≥Èü≥Ë®äÂæåÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂêÑÁ®ÆÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂú®Ë©ïÂàÜÂ≠∏ÁîüÊ≠§È†ÖÊëòË¶Å‰ªªÂãôÔºàÊ†πÊìöÂÖ∂ËÄÉË©¶ÊàêÁ∏æÂñÆÔºâÊñπÈù¢ÁöÑË°®Áèæ„ÄÇÊàëÂÄë‰ΩøÁî®ÂêÑÁ®ÆÂâçÊ≤øÁöÑÈñãÊ∫êÂíåÂ∞àÊúâ LLMÔºåË©ï‰º∞‰∫Ü‰∏çÂêåÁöÑÊäÄË°ìÔºå‰æãÂ¶ÇÈõ∂Ê¨°Â≠∏ÁøíÊÄùËÄÉÈèàÊèêÁ§∫„ÄÅÊ™¢Á¥¢Âº∑ÂåñÁîüÊàêÂíåÂ§öÊ®°ÂûãÈõÜÊàêÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåGPT-4 Á≠âÂâçÊ≤ø LLM Ê®°ÂûãËàá‰∫∫È°ûË©ïÂàÜËÄÖÈÅîÊàê‰∫ÜÈ°ØËëóÁöÑ‰∏ÄËá¥ÊÄßÔºåÂ±ïÁ§∫‰∫Ü 0.88 ÁöÑ Cohen kappa ‰∏ÄËá¥ÊÄßÔºå‰∏¶È°ØÁ§∫‰∫ÜÂü∫Êñº LLM ÁöÑ OSCE Ë©ïÂàÜÂú®Êì¥ÂÖÖÁèæË°åË©ïÂàÜÊµÅÁ®ãÊñπÈù¢ÂÖ∑ÊúâÂº∑Â§ßÁöÑÊΩõÂäõ„ÄÇÈñãÊ∫êÊ®°Âûã‰πüÈ°ØÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÈ°ØÁ§∫‰∫ÜÂª£Ê≥õ„ÄÅÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÈÉ®ÁΩ≤ÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ§±ÊïóÂàÜÊûêÔºåË≠òÂà•Âá∫ LLM Ë©ïÂàÜÂú®Ê≠§ÊÉÖÊ≥Å‰∏ãÂèØËÉΩËºÉ‰∏çÂèØÈù†ÁöÑÊ¢ù‰ª∂Ôºå‰∏¶Âª∫Ë≠∞Âú®ÈÜ´Â≠∏ÊïôËÇ≤Áí∞Â¢É‰∏≠ÈÉ®ÁΩ≤ LLM ÁöÑÊúÄ‰Ω≥ÂØ¶Âãô„ÄÇ

##### **Optimized Biomedical Question-Answering Services with LLM and Multi-BERT Integration**
2410.12856v1 by Cheng Qian, Xianglong Shi, Shanshan Yao, Yichen Liu, Fengming Zhou, Zishu Zhang, Junaid Akram, Ali Braytee, Ali Anaissi

We present a refined approach to biomedical question-answering (QA) services
by integrating large language models (LLMs) with Multi-BERT configurations. By
enhancing the ability to process and prioritize vast amounts of complex
biomedical data, this system aims to support healthcare professionals in
delivering better patient outcomes and informed decision-making. Through
innovative use of BERT and BioBERT models, combined with a multi-layer
perceptron (MLP) layer, we enable more specialized and efficient responses to
the growing demands of the healthcare sector. Our approach not only addresses
the challenge of overfitting by freezing one BERT model while training another
but also improves the overall adaptability of QA services. The use of extensive
datasets, such as BioASQ and BioMRC, demonstrates the system's ability to
synthesize critical information. This work highlights how advanced language
models can make a tangible difference in healthcare, providing reliable and
responsive tools for professionals to manage complex information, ultimately
serving the broader goal of improved care and data-driven insights.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ≤æÈÄ≤ÁöÑÊñπÊ≥ï‰æÜÈÄ≤Ë°åÁîüÁâ©ÈÜ´Â≠∏ÂïèÈ°åËß£Á≠î (QA) ÊúçÂãôÔºåÊñπÊ≥ïÊòØÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëàá Multi-BERT ÁµÑÊÖãÊï¥ÂêàÂú®‰∏ÄËµ∑„ÄÇÈÄèÈÅéÂä†Âº∑ËôïÁêÜÂíåÂÑ™ÂÖàËôïÁêÜÂ§ßÈáèË§áÈõúÁîüÁâ©ÈÜ´Â≠∏Ë≥áÊñôÁöÑËÉΩÂäõÔºåÊ≠§Á≥ªÁµ±Êó®Âú®ÂçîÂä©ÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Êèê‰æõÊõ¥Â•ΩÁöÑÊÇ£ËÄÖÁµêÊûúÂíåÊòéÊô∫ÁöÑÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÈÄèÈÅéÂâµÊñ∞‰ΩøÁî® BERT Âíå BioBERT Ê®°ÂûãÔºå‰∏¶ÁµêÂêàÂ§öÂ±§ÊÑüÁü•Âô® (MLP) Â±§ÔºåÊàëÂÄëËÉΩÂ§†ÈáùÂ∞çÈÜ´ÁôÇ‰øùÂÅ•ÈÉ®ÈñÄÊó•ÁõäÂ¢ûÈï∑ÁöÑÈúÄÊ±ÇÊèê‰æõÊõ¥Â∞àÊ•≠‰∏îÊõ¥ÊúâÊïàÁéáÁöÑÂõûÊáâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÈÄèÈÅéÂáçÁµê‰∏ÄÂÄã BERT Ê®°Âûã‰∏¶Ë®ìÁ∑¥Âè¶‰∏ÄÂÄãÊ®°Âûã‰æÜËß£Ê±∫ÈÅéÂ∫¶Êì¨ÂêàÁöÑÊåëÊà∞ÔºåÂêåÊôÇ‰πüÊîπÂñÑ‰∫Ü QA ÊúçÂãôÁöÑÊï¥È´îÈÅ©ÊáâÊÄß„ÄÇ‰ΩøÁî®Âª£Ê≥õÁöÑË≥áÊñôÈõÜÔºå‰æãÂ¶Ç BioASQ Âíå BioMRCÔºåË≠âÊòé‰∫ÜË©≤Á≥ªÁµ±Á∂úÂêàÈáçË¶ÅË≥áË®äÁöÑËÉΩÂäõ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂº∑Ë™ø‰∫ÜÈÄ≤ÈöéË™ûË®ÄÊ®°ÂûãÂ¶Ç‰ΩïËÉΩÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Áî¢ÁîüÂÖ∑È´îÂ∑ÆÁï∞ÔºåÁÇ∫Â∞àÊ•≠‰∫∫Âì°Êèê‰æõÂèØÈù†‰∏îÂÖ∑ÂõûÊáâÊÄßÁöÑÂ∑•ÂÖ∑‰æÜÁÆ°ÁêÜË§áÈõúË≥áË®äÔºåÊúÄÁµÇÊúçÂãôÊñºÊîπÂñÑÁÖßË≠∑ÂíåË≥áÊñôÈ©ÖÂãïË¶ãËß£ÁöÑÊõ¥Âª£Ê≥õÁõÆÊ®ô„ÄÇ

##### **Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models**
2410.08731v1 by Yeeun Kim, Young Rok Choi, Eunkyung Choi, Jinhwan Choi, Hai Jin Park, Wonseok Hwang

Large language models (LLMs) have demonstrated remarkable performance in the
legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However
their efficacy remains limited for non-standardized tasks and tasks in
languages other than English. This underscores the need for careful evaluation
of LLMs within each legal system before application. Here, we introduce KBL, a
benchmark for assessing the Korean legal language understanding of LLMs,
consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning
tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510
examples). First two datasets were developed in close collaboration with
lawyers to evaluate LLMs in practical scenarios in a certified manner.
Furthermore, considering legal practitioners' frequent use of extensive legal
documents for research, we assess LLMs in both a closed book setting, where
they rely solely on internal knowledge, and a retrieval-augmented generation
(RAG) setting, using a corpus of Korean statutes and precedents. The results
indicate substantial room and opportunities for improvement.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Ê≥ïÂæãÈ†òÂüüÂ±ïÁèæÂá∫ÂçìË∂äÁöÑË°®ÁèæÔºåGPT-4 ÁîöËá≥ÈÄöÈÅé‰∫ÜÁæéÂúãÁöÑÁµ±‰∏ÄÂæãÂ∏´ËÄÉË©¶„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÊïàËÉΩÂ∞çÊñºÈùûÊ®ôÊ∫ñÂåñ‰ªªÂãôÂíåÈùûËã±Ë™ûË™ûË®Ä‰ªªÂãô‰ªçÁÑ∂ÊúâÈôê„ÄÇÈÄôÂá∏È°Ø‰∫ÜÂú®ÊáâÁî® LLM ‰πãÂâçÔºåÈúÄË¶Å‰ªîÁ¥∞Ë©ï‰º∞ÊØèÂÄãÊ≥ïÂæãÂà∂Â∫¶ÁöÑÂøÖË¶ÅÊÄß„ÄÇÂú®Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π KBLÔºå‰∏ÄÂÄãÁî®ÊñºË©ï‰º∞ LLM ÈüìË™ûÊ≥ïÂæãË™ûË®ÄÁêÜËß£ÂäõÁöÑÂü∫Ê∫ñÔºåÂåÖÂê´ (1) 7 È†ÖÊ≥ïÂæãÁü•Ë≠ò‰ªªÂãôÔºà510 ÂÄãÁØÑ‰æãÔºâ„ÄÅ(2) 4 È†ÖÊ≥ïÂæãÊé®ÁêÜ‰ªªÂãôÔºà288 ÂÄãÁØÑ‰æãÔºâÂíå (3) ÈüìÂúãÂæãÂ∏´ËÄÉË©¶Ôºà4 ÂÄãÈ†òÂüüÔºå53 È†Ö‰ªªÂãôÔºå2,510 ÂÄãÁØÑ‰æãÔºâ„ÄÇÂâçÂÖ©ÂÄãË≥áÊñôÈõÜÊòØËàáÂæãÂ∏´ÂØÜÂàáÂêà‰ΩúÈñãÁôºÔºå‰ª•Ë™çË≠âÁöÑÊñπÂºèË©ï‰º∞ LLM Âú®ÂØ¶ÈöõÊÉÖÂ¢É‰∏≠ÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåËÄÉÈáèÂà∞Ê≥ïÂæãÂæûÊ•≠‰∫∫Âì°Á∂ìÂ∏∏‰ΩøÁî®Â§ßÈáèÁöÑÊ≥ïÂæãÊñá‰ª∂ÈÄ≤Ë°åÁ†îÁ©∂ÔºåÊàëÂÄëÂú®Â∞ÅÈñâÂºèË®≠ÂÆö‰∏≠Ë©ï‰º∞ LLMÔºåÂÖ∂‰∏≠‰ªñÂÄëÂÉÖ‰æùË≥¥ÂÖßÈÉ®Áü•Ë≠òÔºå‰ª•Âèä‰ΩøÁî®ÈüìÂúãÊ≥ïË¶èÂíåÂà§‰æãË™ûÊñôÂ∫´ÁöÑÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Ë®≠ÂÆö„ÄÇÁµêÊûúÈ°ØÁ§∫‰ªçÊúâÂ§ßÂπÖÈÄ≤Ê≠•ÁöÑÁ©∫ÈñìÂíåÊ©üÊúÉ„ÄÇ

##### **ViT3D Alignment of LLaMA3: 3D Medical Image Report Generation**
2410.08588v1 by Siyou Li, Beining Xu, Yihao Luo, Dong Nie, Le Zhang

Automatic medical report generation (MRG), which aims to produce detailed
text reports from medical images, has emerged as a critical task in this
domain. MRG systems can enhance radiological workflows by reducing the time and
effort required for report writing, thereby improving diagnostic efficiency. In
this work, we present a novel approach for automatic MRG utilizing a multimodal
large language model. Specifically, we employed the 3D Vision Transformer
(ViT3D) image encoder introduced from M3D-CLIP to process 3D scans and use the
Asclepius-Llama3-8B as the language model to generate the text reports by
auto-regressive decoding. The experiment shows our model achieved an average
Green score of 0.3 on the MRG task validation set and an average accuracy of
0.61 on the visual question answering (VQA) task validation set, outperforming
the baseline model. Our approach demonstrates the effectiveness of the ViT3D
alignment of LLaMA3 for automatic MRG and VQA tasks by tuning the model on a
small dataset.

ÊëòË¶ÅÔºöËá™ÂãïÂåñÈÜ´ÁôÇÂ†±ÂëäÁîüÊàê (MRG) ÁöÑÁõÆÊ®ôÊòØÊ†πÊìöÈÜ´Â≠∏ÂΩ±ÂÉèÁî¢ÁîüË©≥Á¥∞ÁöÑÊñáÂ≠óÂ†±ÂëäÔºåÂ∑≤ÊàêÁÇ∫Ê≠§È†òÂüüÁöÑ‰∏ÄÈ†ÖÈáçË¶Å‰ªªÂãô„ÄÇMRG Á≥ªÁµ±ÂèØ‰ª•Ê∏õÂ∞ëÊí∞ÂØ´Â†±ÂëäÊâÄÈúÄÁöÑÊôÇÈñìÂíåÁ≤æÂäõÔºåÂæûËÄåÊèêÂçáÊîæÂ∞ÑÁßëÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÈÄ≤ËÄåÊîπÂñÑË®∫Êñ∑ÊïàÁéá„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂà©Áî®Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°åËá™ÂãïÂåñ MRG ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®‰∫ÜÂæû M3D-CLIP ÂºïÂÖ•ÁöÑ 3D Ë¶ñË¶∫Transformer (ViT3D) ÂΩ±ÂÉèÁ∑®Á¢ºÂô®‰æÜËôïÁêÜ 3D ÊéÉÊèèÔºå‰∏¶‰ΩøÁî® Asclepius-Llama3-8B ‰ΩúÁÇ∫Ë™ûË®ÄÊ®°ÂûãÔºåÈÄèÈÅéËá™Ëø¥Ê≠∏Ëß£Á¢º‰æÜÁî¢ÁîüÊñáÂ≠óÂ†±Âëä„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® MRG ‰ªªÂãôÈ©óË≠âÈõÜ‰∏äÈÅîÂà∞‰∫ÜÂπ≥Âùá 0.3 ÁöÑ Green ÂàÜÊï∏ÔºåÂú®Ë¶ñË¶∫ÂïèÁ≠î (VQA) ‰ªªÂãôÈ©óË≠âÈõÜ‰∏äÈÅîÂà∞‰∫ÜÂπ≥Âùá 0.61 ÁöÑÊ∫ñÁ¢∫ÁéáÔºåÂÑ™ÊñºÂü∫Á∑öÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïË≠âÊòé‰∫Ü ViT3D Â∞çÈΩä LLaMA3 Âú®Ëá™ÂãïÂåñ MRG Âíå VQA ‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÊñπÊ≥ïÊòØÂú®Â∞èÂûãË≥áÊñôÈõÜ‰∏äË™øÊï¥Ê®°Âûã„ÄÇ

##### **oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness**
2410.08431v1 by Yu He Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Chang-Fu Kuo, Shao-Chun Wu, Vesela P. Kovacheva, Daniel Shu Wei Ting

Large Language Models (LLMs) show potential for medical applications but
often lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)
allows customization with domain-specific information, making it suitable for
healthcare. This study evaluates the accuracy, consistency, and safety of RAG
models in determining fitness for surgery and providing preoperative
instructions. We developed LLM-RAG models using 35 local and 23 international
preoperative guidelines and tested them against human-generated responses. A
total of 3,682 responses were evaluated. Clinical documents were processed
using Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were
assessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects
of preoperative instructions. Established guidelines and expert judgment were
used to determine correct responses, with human-generated answers serving as
comparisons. The LLM-RAG models generated responses within 20 seconds,
significantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model
achieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no
hallucinations and producing correct instructions comparable to clinicians.
Results were consistent across both local and international guidelines. This
study demonstrates the potential of LLM-RAG models for preoperative healthcare
tasks, highlighting their efficiency, scalability, and reliability.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) È°ØÁ§∫Âá∫Âú®ÈÜ´ÁôÇÊáâÁî®ÊñπÈù¢ÁöÑÊΩõÂäõÔºå‰ΩÜÈÄöÂ∏∏Áº∫‰πèÂ∞àÊ•≠ÁöÑËá®Â∫äÁü•Ë≠ò„ÄÇÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG) ÂÖÅË®±‰ΩøÁî®ÁâπÂÆöÈ†òÂüüÁöÑË≥áË®äÈÄ≤Ë°åËá™Ë®ÇÔºå‰ΩøÂÖ∂ÈÅ©Áî®ÊñºÈÜ´ÁôÇ‰øùÂÅ•„ÄÇÊú¨Á†îÁ©∂Ë©ï‰º∞ RAG Ê®°ÂûãÂú®Á¢∫ÂÆöÊâãË°ìÈÅ©ÊáâÁóáÂíåÊèê‰æõË°ìÂâçË™™ÊòéÊñπÈù¢ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅ‰∏ÄËá¥ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊàëÂÄë‰ΩøÁî® 35 ‰ªΩÁï∂Âú∞Âíå 23 ‰ªΩÂúãÈöõË°ìÂâçÊåáÂçóÈñãÁôº‰∫Ü LLM-RAG Ê®°ÂûãÔºå‰∏¶Â∞áÂÆÉÂÄëËàá‰∫∫ÁÇ∫Áî¢ÁîüÁöÑÂõûÊáâÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶„ÄÇÁ∏ΩÂÖ±Ë©ï‰º∞‰∫Ü 3,682 ‰ªΩÂõûÊáâ„ÄÇËá®Â∫äÊñá‰ª∂‰ΩøÁî® Llamaindex ËôïÁêÜÔºå‰∏¶Ë©ï‰º∞‰∫Ü 10 ÂÄã LLMÔºåÂåÖÊã¨ GPT3.5„ÄÅGPT4 Âíå Claude-3„ÄÇÂàÜÊûê‰∫Ü 14 ÂÄãËá®Â∫äÂ†¥ÊôØÔºåÈáçÈªûÈóúÊ≥®Ë°ìÂâçË™™ÊòéÁöÑ‰∏ÉÂÄãÊñπÈù¢„ÄÇ‰ΩøÁî®Êó¢ÂÆöÁöÑÊåáÂçóÂíåÂ∞àÂÆ∂Âà§Êñ∑‰æÜÁ¢∫ÂÆöÊ≠£Á¢∫ÁöÑÂõûÊáâÔºå‰∏¶‰ª•‰∫∫ÁÇ∫Áî¢ÁîüÁöÑÁ≠îÊ°à‰ΩúÁÇ∫ÊØîËºÉ„ÄÇLLM-RAG Ê®°ÂûãÂú® 20 ÁßíÂÖßÁî¢ÁîüÂõûÊáâÔºåÈ°ØËëóÂø´ÊñºËá®Â∫äÈÜ´Áîü (10 ÂàÜÈêò)„ÄÇGPT4 LLM-RAG Ê®°ÂûãÈÅîÂà∞‰∫ÜÊúÄÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶ (96.4% Â∞çÊØî 86.6%Ôºåp=0.016)ÔºåÊ≤íÊúâÂá∫ÁèæÂπªË¶∫Ôºå‰∏¶Áî¢Áîü‰∫ÜËàáËá®Â∫äÈÜ´ÁîüÁõ∏Áï∂ÁöÑÊ≠£Á¢∫Ë™™Êòé„ÄÇÁµêÊûúÂú®Áï∂Âú∞ÂíåÂúãÈöõÊåáÂçó‰∏≠ÊòØ‰∏ÄËá¥ÁöÑ„ÄÇÊú¨Á†îÁ©∂Â±ïÁ§∫‰∫Ü LLM-RAG Ê®°ÂûãÂú®Ë°ìÂâçÈÜ´ÁôÇ‰øùÂÅ•‰ªªÂãô‰∏≠ÁöÑÊΩõÂäõÔºåÁ™ÅÂá∫‰∫ÜÂÆÉÂÄëÁöÑÊïàÁéá„ÄÅÂèØÊì¥ÂÖÖÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

##### **VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis**
2410.08397v1 by Andrew Hoopes, Victor Ion Butoi, John V. Guttag, Adrian V. Dalca

We present VoxelPrompt, an agent-driven vision-language framework that
tackles diverse radiological tasks through joint modeling of natural language,
image volumes, and analytical metrics. VoxelPrompt is multi-modal and
versatile, leveraging the flexibility of language interaction while providing
quantitatively grounded image analysis. Given a variable number of 3D medical
volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that
iteratively predicts executable instructions to solve a task specified by an
input prompt. These instructions communicate with a vision network to encode
image features and generate volumetric outputs (e.g., segmentations).
VoxelPrompt interprets the results of intermediate instructions and plans
further actions to compute discrete measures (e.g., tumor growth across a
series of scans) and present relevant outputs to the user. We evaluate this
framework in a sandbox of diverse neuroimaging tasks, and we show that the
single VoxelPrompt model can delineate hundreds of anatomical and pathological
features, measure many complex morphological properties, and perform
open-language analysis of lesion characteristics. VoxelPrompt carries out these
objectives with accuracy similar to that of fine-tuned, single-task models for
segmentation and visual question-answering, while facilitating a much larger
range of tasks. Therefore, by supporting accurate image processing with
language interaction, VoxelPrompt provides comprehensive utility for numerous
imaging tasks that traditionally require specialized models to address.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊèêÂá∫ VoxelPromptÔºå‰∏ÄÁ®ÆÁî±‰ª£ÁêÜÈ©ÖÂãïÁöÑË¶ñË¶∫Ë™ûË®ÄÊ°ÜÊû∂ÔºåÂÆÉÈÄèÈÅéËá™ÁÑ∂Ë™ûË®Ä„ÄÅÂΩ±ÂÉèÈ´îÁ©çÂíåÂàÜÊûêÊåáÊ®ôÁöÑËÅØÂêàÂª∫Ê®°Ôºå‰æÜËôïÁêÜÂ§öÊ®£ÁöÑÊîæÂ∞ÑÂ≠∏‰ªªÂãô„ÄÇVoxelPrompt ÊòØÂ§öÊ®°ÊÖã‰∏îÂ§öÂäüËÉΩÁöÑÔºåÂÆÉÂà©Áî®Ë™ûË®Ä‰∫íÂãïÁöÑÈùàÊ¥ªÊÄßÔºåÂêåÊôÇÊèê‰æõÈáèÂåñÂü∫Á§éÁöÑÂΩ±ÂÉèÂàÜÊûê„ÄÇÁµ¶ÂÆöÂèØËÆäÊï∏ÈáèÁöÑ 3D ÈÜ´Â≠∏È´îÁ©çÔºå‰æãÂ¶Ç MRI Âíå CT ÊéÉÊèèÔºåVoxelPrompt ‰ΩøÁî®Ë™ûË®Ä‰ª£ÁêÜÔºåÂèçË¶ÜÈ†êÊ∏¨ÂèØÂü∑Ë°åÊåá‰ª§Ôºå‰ª•Ëß£Ê±∫Áî±Ëº∏ÂÖ•ÊèêÁ§∫ÊåáÂÆöÁöÑ‰ªªÂãô„ÄÇÈÄô‰∫õÊåá‰ª§ËàáË¶ñË¶∫Á∂≤Ë∑ØÊ∫ùÈÄöÔºå‰ª•Á∑®Á¢ºÂΩ±ÂÉèÁâπÂæµ‰∏¶Áî¢ÁîüÈ´îÁ©çËº∏Âá∫Ôºà‰æãÂ¶ÇÔºåÂàÜÂâ≤Ôºâ„ÄÇVoxelPrompt Ëß£Èáã‰∏≠ÈñìÊåá‰ª§ÁöÑÁµêÊûúÔºå‰∏¶Ë¶èÂäÉÈÄ≤‰∏ÄÊ≠•ÁöÑÂãï‰ΩúÔºå‰ª•Ë®àÁÆóÈõ¢Êï£Ê∏¨ÈáèÔºà‰æãÂ¶ÇÔºå‰∏ÄÁ≥ªÂàóÊéÉÊèè‰∏≠ÁöÑËÖ´Áò§ÁîüÈï∑ÔºâÔºå‰∏¶Âêë‰ΩøÁî®ËÄÖÊèê‰æõÁõ∏ÈóúËº∏Âá∫„ÄÇÊàëÂÄëÂú®‰∏ÄÂÄãÂ§öÊ®£ÂåñÁöÑÁ•ûÁ∂ìÂΩ±ÂÉè‰ªªÂãôÊ≤ôÁõí‰∏≠Ë©ï‰º∞ÈÄôÂÄãÊ°ÜÊû∂ÔºåÊàëÂÄëË°®ÊòéÂñÆ‰∏ÄÁöÑ VoxelPrompt Ê®°ÂûãÂèØ‰ª•ÊèèËø∞Êï∏ÁôæÂÄãËß£ÂâñÂíåÁóÖÁêÜÁâπÂæµÔºåÊ∏¨ÈáèË®±Â§öË§áÈõúÁöÑÂΩ¢ÊÖãÂ±¨ÊÄßÔºå‰∏¶Âü∑Ë°åÁóÖÁÅ∂ÁâπÂæµÁöÑÈñãÊîæË™ûË®ÄÂàÜÊûê„ÄÇVoxelPrompt Âü∑Ë°åÈÄô‰∫õÁõÆÊ®ôÁöÑÊ∫ñÁ¢∫Â∫¶ËàáÈáùÂ∞çÂàÜÂâ≤ÂíåË¶ñË¶∫ÂïèÁ≠îÈÄ≤Ë°åÂæÆË™øÁöÑÂñÆ‰∏Ä‰ªªÂãôÊ®°ÂûãÈ°û‰ººÔºåÂêåÊôÇ‰øÉÈÄ≤‰∫ÜÊõ¥Â§ßÁöÑ‰ªªÂãôÁØÑÂúç„ÄÇÂõ†Ê≠§ÔºåÈÄèÈÅéÊîØÊè¥‰ΩøÁî®Ë™ûË®Ä‰∫íÂãïÁöÑÊ∫ñÁ¢∫ÂΩ±ÂÉèËôïÁêÜÔºåVoxelPrompt ÁÇ∫ÂÇ≥Áµ±‰∏äÈúÄË¶ÅÂ∞àÈñÄÊ®°Âûã‰æÜËôïÁêÜÁöÑÁúæÂ§öÂΩ±ÂÉè‰ªªÂãôÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶Áî®ÊÄß„ÄÇ</paragraph>

##### **Optimizing Vital Sign Monitoring in Resource-Constrained Maternal Care: An RL-Based Restless Bandit Approach**
2410.08377v1 by Niclas Boehmer, Yunfan Zhao, Guojun Xiong, Paula Rodriguez-Diaz, Paola Del Cueto Cibrian, Joseph Ngonzi, Adeline Boatin, Milind Tambe

Maternal mortality remains a significant global public health challenge. One
promising approach to reducing maternal deaths occurring during facility-based
childbirth is through early warning systems, which require the consistent
monitoring of mothers' vital signs after giving birth. Wireless vital sign
monitoring devices offer a labor-efficient solution for continuous monitoring,
but their scarcity raises the critical question of how to allocate them most
effectively. We devise an allocation algorithm for this problem by modeling it
as a variant of the popular Restless Multi-Armed Bandit (RMAB) paradigm. In
doing so, we identify and address novel, previously unstudied constraints
unique to this domain, which render previous approaches for RMABs unsuitable
and significantly increase the complexity of the learning and planning problem.
To overcome these challenges, we adopt the popular Proximal Policy Optimization
(PPO) algorithm from reinforcement learning to learn an allocation policy by
training a policy and value function network. We demonstrate in simulations
that our approach outperforms the best heuristic baseline by up to a factor of
$4$.

ÊëòË¶ÅÔºöÁî¢Â©¶Ê≠ª‰∫°Áéá‰ªçÁÑ∂ÊòØÂÖ®ÁêÉÂÖ¨ÂÖ±Ë°õÁîüÁöÑÈáçÂ§ßÊåëÊà∞„ÄÇ‰∏ÄÁ®ÆÊúâÊúõÊ∏õÂ∞ëÂú®ÈÜ´ÁôÇÊ©üÊßãÁîüÁî¢ÈÅéÁ®ã‰∏≠Áî¢Â©¶Ê≠ª‰∫°ÁöÑÊñπÊ≥ïÊòØÈÄèÈÅéÈ†êË≠¶Á≥ªÁµ±ÔºåÈÄôÈúÄË¶ÅÂú®Áî¢ÂæåÊåÅÁ∫åÁõ£Ê∏¨Áî¢Â©¶ÁöÑÁîüÂëΩÂæµË±°„ÄÇÁÑ°Á∑öÁîüÂëΩÂæµË±°Áõ£Ê∏¨Ë£ùÁΩÆÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÁúÅÂäõÁöÑÈÄ£Á∫åÁõ£Ê∏¨Ëß£Ê±∫ÊñπÊ°àÔºå‰ΩÜÂÖ∂Á®ÄÁº∫ÊÄßÂºïÁôº‰∫Ü‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÔºåÂç≥Â¶Ç‰ΩïÊúÄÊúâÊïàÂú∞ÂàÜÈÖçÈÄô‰∫õË£ùÁΩÆ„ÄÇÊàëÂÄëÁÇ∫ÈÄôÂÄãÂïèÈ°åË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂàÜÈÖçÊºîÁÆóÊ≥ïÔºåÂ∞áÂÖ∂Âª∫Ê®°ÁÇ∫ÊµÅË°åÁöÑ‰∏çËÄêÁÖ©Â§öËáÇË≥≠Âæí (RMAB) ÁØÑ‰æãÁöÑËÆäÈ´î„ÄÇÂú®ÈÄôÊ®£ÂÅöÁöÑÈÅéÁ®ã‰∏≠ÔºåÊàëÂÄëË≠òÂà•‰∏¶Ëß£Ê±∫‰∫ÜÈÄôÂÄãÈ†òÂüüÁç®ÊúâÁöÑ„ÄÅ‰ª•ÂâçÊú™Á†îÁ©∂ÈÅéÁöÑÊñ∞Á¥ÑÊùüÔºåÈÄô‰∫õÁ¥ÑÊùüËÆìÂÖàÂâçÈáùÂ∞ç RMAB ÁöÑÊñπÊ≥ïËÆäÂæó‰∏çÈÅ©Áî®Ôºå‰∏¶È°ØËëóÂ¢ûÂä†‰∫ÜÂ≠∏ÁøíÂíåË¶èÂäÉÂïèÈ°åÁöÑË§áÈõúÊÄß„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊé°Áî®‰∫ÜÂº∑ÂåñÂ≠∏Áøí‰∏≠ÊµÅË°åÁöÑËøëÁ´ØÁ≠ñÁï•ÊúÄ‰Ω≥Âåñ (PPO) ÊºîÁÆóÊ≥ïÔºåÈÄèÈÅéË®ìÁ∑¥Á≠ñÁï•ÂíåÂÉπÂÄºÂáΩÊï∏Á∂≤Ë∑Ø‰æÜÂ≠∏ÁøíÂàÜÈÖçÁ≠ñÁï•„ÄÇÊàëÂÄëÂú®Ê®°Êì¨‰∏≠Ë≠âÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊØîÊúÄ‰Ω≥ÂïüÁôºÂºèÂü∫Ê∫ñÈ´òÂá∫ $4$ ÂÄç„ÄÇ

##### **ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation**
2410.07908v2 by L√©o Machado, H√©l√®ne Philippe, √âlodie Ferreres, Julien Khlaut, Julie Dupuis, Korentin Le Floch, Denis Habip Gatenyo, Pascal Roux, Jules Gr√©gory, Maxime Ronot, Corentin Dancette, Daniel Tordjman, Pierre Manceron, Paul H√©rent

Carcinogenesis is a proteiform phenomenon, with tumors emerging in various
locations and displaying complex, diverse shapes. At the crucial intersection
of research and clinical practice, it demands precise and flexible assessment.
However, current biomarkers, such as RECIST 1.1's long and short axis
measurements, fall short of capturing this complexity, offering an approximate
estimate of tumor burden and a simplistic representation of a more intricate
process. Additionally, existing supervised AI models face challenges in
addressing the variability in tumor presentations, limiting their clinical
utility. These limitations arise from the scarcity of annotations and the
models' focus on narrowly defined tasks.
  To address these challenges, we developed ONCOPILOT, an interactive
radiological foundation model trained on approximately 7,500 CT scans covering
the whole body, from both normal anatomy and a wide range of oncological cases.
ONCOPILOT performs 3D tumor segmentation using visual prompts like point-click
and bounding boxes, outperforming state-of-the-art models (e.g., nnUnet) and
achieving radiologist-level accuracy in RECIST 1.1 measurements. The key
advantage of this foundation model is its ability to surpass state-of-the-art
performance while keeping the radiologist in the loop, a capability that
previous models could not achieve. When radiologists interactively refine the
segmentations, accuracy improves further. ONCOPILOT also accelerates
measurement processes and reduces inter-reader variability, facilitating
volumetric analysis and unlocking new biomarkers for deeper insights.
  This AI assistant is expected to enhance the precision of RECIST 1.1
measurements, unlock the potential of volumetric biomarkers, and improve
patient stratification and clinical care, while seamlessly integrating into the
radiological workflow.

ÊëòË¶ÅÔºö<paragraph>Ëá¥Áôå‰ΩúÁî®ÊòØ‰∏ÄÁ®ÆËÆäÂΩ¢ÁèæË±°ÔºåËÖ´Áò§Âá∫ÁèæÂú®‰∏çÂêå‰ΩçÁΩÆÔºå‰∏¶ÂëàÁèæÂá∫Ë§áÈõú„ÄÅÂ§öÊ®£ÁöÑÂΩ¢ÁãÄ„ÄÇÂú®Á†îÁ©∂ÂíåËá®Â∫äÂØ¶ÂãôÁöÑÈáçË¶Å‰∫§ÊúÉÈªûÔºåÂÆÉÈúÄË¶ÅÁ≤æÁ¢∫‰∏îÈùàÊ¥ªÁöÑË©ï‰º∞„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÁîüÁâ©Ê®ôË®òÔºå‰æãÂ¶Ç RECIST 1.1 ÁöÑÈï∑Ëª∏ÂíåÁü≠Ëª∏Ê∏¨ÈáèÔºåÊú™ËÉΩÊçïÊçâÂà∞ÈÄôÁ®ÆË§áÈõúÊÄßÔºåÂÉÖÊèê‰æõËÖ´Áò§Ë≤†ÊìîÁöÑËøë‰ºº‰º∞Ë®àÂÄºÔºå‰ª•ÂèäÂ∞çÊõ¥Ë§áÈõúÈÅéÁ®ãÁöÑÁ∞°ÂåñË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÁèæÊúâÁöÑÁõ£Áù£Âºè AI Ê®°ÂûãÂú®ËôïÁêÜËÖ´Áò§Ë°®ÁèæÁöÑËÆäÁï∞ÊÄßÊôÇÈù¢Ëá®ÊåëÊà∞ÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑËá®Â∫äÊïàÁî®„ÄÇÈÄô‰∫õÈôêÂà∂‰æÜËá™ÊñºÊ®ôË®ªÁöÑÁ®ÄÂ∞ëÊÄßÔºå‰ª•ÂèäÊ®°ÂûãÂ∞àÊ≥®ÊñºÁãπÁæ©ÂÆöÁæ©ÁöÑ‰ªªÂãô„ÄÇ
ÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÈñãÁôº‰∫Ü ONCOPILOTÔºåÈÄôÊòØ‰∏ÄÂÄã‰∫íÂãïÂºèÊîæÂ∞ÑÂ≠∏Âü∫Á§éÊ®°ÂûãÔºåË®ìÁ∑¥ÊñºÂ§ßÁ¥Ñ 7,500 ÂÄãÊ∂µËìãÂÖ®Ë∫´ÁöÑ CT ÊéÉÊèèÔºåÂåÖÊã¨Ê≠£Â∏∏Ëß£ÂâñÁµêÊßãÂíåÂêÑÁ®ÆËÖ´Áò§ÁóÖ‰æã„ÄÇONCOPILOT ‰ΩøÁî®Ë¶ñË¶∫ÊèêÁ§∫Ôºà‰æãÂ¶ÇÈªûÈÅ∏ÂíåÈÇäÁïåÊ°ÜÔºâÂü∑Ë°å 3D ËÖ´Áò§ÂàÜÂâ≤ÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºà‰æãÂ¶Ç nnUnetÔºâÔºå‰∏¶Âú® RECIST 1.1 Ê∏¨Èáè‰∏≠ÈÅîÂà∞ÊîæÂ∞ÑÁßëÈÜ´Â∏´Á≠âÁ¥öÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÈÄôÂÄãÂü∫Á§éÊ®°ÂûãÁöÑ‰∏ªË¶ÅÂÑ™ÈªûÊòØÂÆÉËÉΩÂ§†Ë∂ÖË∂äÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂêåÊôÇËÆìÊîæÂ∞ÑÁßëÈÜ´Â∏´ÂèÉËàáÂÖ∂‰∏≠ÔºåÈÄôÊòØ‰ª•ÂâçÁöÑÊ®°ÂûãÁÑ°Ê≥ïÈÅîÂà∞ÁöÑËÉΩÂäõ„ÄÇÁï∂ÊîæÂ∞ÑÁßëÈÜ´Â∏´‰∫íÂãïÂºèÂú∞Ë™øÊï¥ÂàÜÂâ≤ÊôÇÔºåÊ∫ñÁ¢∫Â∫¶ÊúÉÈÄ≤‰∏ÄÊ≠•ÊèêÈ´ò„ÄÇONCOPILOT ‰πüÂä†ÈÄü‰∫ÜÊ∏¨ÈáèÈÅéÁ®ã‰∏¶Ê∏õÂ∞ë‰∫ÜËÆÄËÄÖÈñìÁöÑËÆäÁï∞ÊÄßÔºå‰øÉÈÄ≤‰∫ÜÈ´îÁ©çÂàÜÊûêÔºå‰∏¶Ëß£Èéñ‰∫ÜÊñ∞ÁöÑÁîüÁâ©Ê®ôË®òÔºå‰ª•Áç≤ÂæóÊõ¥Ê∑±ÂÖ•ÁöÑË¶ãËß£„ÄÇ
È†êË®àÈÄôÂÄã AI Âä©ÁêÜÂ∞áÊèêÈ´ò RECIST 1.1 Ê∏¨ÈáèÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÈáãÊîæÈ´îÁ©çÁîüÁâ©Ê®ôË®òÁöÑÊΩõÂäõÔºå‰∏¶ÊîπÂñÑÊÇ£ËÄÖÂàÜÂ±§ÂíåËá®Â∫äÁÖßË≠∑ÔºåÂêåÊôÇÁÑ°Á∏´Êï¥ÂêàÂà∞ÊîæÂ∞ÑÂ≠∏Â∑•‰ΩúÊµÅÁ®ã‰∏≠„ÄÇ</paragraph>

##### **Exploring ASR-Based Wav2Vec2 for Automated Speech Disorder Assessment: Insights and Analysis**
2410.08250v1 by Tuan Nguyen, Corinne Fredouille, Alain Ghio, Mathieu Balaguer, Virginie Woisard

With the rise of SSL and ASR technologies, the Wav2Vec2 ASR-based model has
been fine-tuned for automated speech disorder quality assessment tasks,
yielding impressive results and setting a new baseline for Head and Neck Cancer
speech contexts. This demonstrates that the ASR dimension from Wav2Vec2 closely
aligns with assessment dimensions. Despite its effectiveness, this system
remains a black box with no clear interpretation of the connection between the
model ASR dimension and clinical assessments. This paper presents the first
analysis of this baseline model for speech quality assessment, focusing on
intelligibility and severity tasks. We conduct a layer-wise analysis to
identify key layers and compare different SSL and ASR Wav2Vec2 models based on
pre-trained data. Additionally, post-hoc XAI methods, including Canonical
Correlation Analysis (CCA) and visualization techniques, are used to track
model evolution and visualize embeddings for enhanced interpretability.

ÊëòË¶ÅÔºöÈö®Ëëó SSL Âíå ASR ÊäÄË°ìÁöÑËààËµ∑ÔºåÂü∫Êñº Wav2Vec2 ÁöÑ ASR Ê®°ÂûãÂ∑≤ÈáùÂ∞çËá™ÂãïÂåñË™ûË®ÄÈöúÁ§ôÂìÅË≥™Ë©ï‰º∞‰ªªÂãôÈÄ≤Ë°åÂæÆË™øÔºåÁî¢Áîü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÁµêÊûúÔºå‰∏¶ÁÇ∫È†≠È†∏ÁôåË™ûÈü≥Áí∞Â¢ÉË®≠ÂÆöÊñ∞ÁöÑÂü∫Ê∫ñ„ÄÇÈÄôË≠âÊòé‰∫Ü Wav2Vec2 ÁöÑ ASR Á∂≠Â∫¶ËàáË©ï‰º∞Á∂≠Â∫¶Á∑äÂØÜÂ∞çÈΩä„ÄÇÂÑòÁÆ°ÈÄôÂÄãÁ≥ªÁµ±ÂæàÊúâÊïàÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈªëÁõíÂ≠êÔºåÁÑ°Ê≥ïÊ∏ÖÊ•öËß£ÈáãÊ®°Âûã ASR Á∂≠Â∫¶ËàáËá®Â∫äË©ï‰º∞‰πãÈñìÁöÑÈóúËÅØ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÁ¨¨‰∏ÄÂÄãÈáùÂ∞çË™ûÈü≥ÂìÅË≥™Ë©ï‰º∞ÁöÑÂü∫Ê∫ñÊ®°ÂûãÂàÜÊûêÔºåÈáçÈªûÊîæÂú®Ê∏ÖÊô∞Â∫¶ÂíåÂö¥ÈáçÊÄß‰ªªÂãô‰∏ä„ÄÇÊàëÂÄëÈÄ≤Ë°åÈÄêÂ±§ÂàÜÊûêÔºå‰ª•Ë≠òÂà•ÈóúÈçµÂ±§Ôºå‰∏¶Ê†πÊìöÈ†êË®ìÁ∑¥Êï∏ÊìöÊØîËºÉ‰∏çÂêåÁöÑ SSL Âíå ASR Wav2Vec2 Ê®°Âûã„ÄÇÊ≠§Â§ñÔºå‰∫ãÂæå XAI ÊñπÊ≥ïÔºàÂåÖÊã¨ÂÖ∏ÂûãÁõ∏ÈóúÂàÜÊûê (CCA) ÂíåË¶ñË¶∫ÂåñÊäÄË°ìÔºâÁî®ÊñºËøΩËπ§Ê®°ÂûãÊºîÂåñÔºå‰∏¶Ë¶ñË¶∫ÂåñÂµåÂÖ•‰ª•Â¢ûÂº∑ÂèØËß£ÈáãÊÄß„ÄÇ

##### **Forecasting mortality associated emergency department crowding**
2410.08247v1 by Jalmari Nevanlinna, Anna Eidst√∏, Jari Yl√§-Mattila, Teemu Koivistoinen, Niku Oksala, Juho Kanniainen, Ari Palom√§ki, Antti Roine

Emergency department (ED) crowding is a global public health issue that has
been repeatedly associated with increased mortality. Predicting future service
demand would enable preventative measures aiming to eliminate crowding along
with it's detrimental effects. Recent findings in our ED indicate that
occupancy ratios exceeding 90% are associated with increased 10-day mortality.
In this paper, we aim to predict these crisis periods using retrospective data
from a large Nordic ED with a LightGBM model. We provide predictions for the
whole ED and individually for it's different operational sections. We
demonstrate that afternoon crowding can be predicted at 11 a.m. with an AUC of
0.82 (95% CI 0.78-0.86) and at 8 a.m. with an AUC up to 0.79 (95% CI
0.75-0.83). Consequently we show that forecasting mortality-associated crowding
using anonymous administrative data is feasible.

ÊëòË¶ÅÔºöÊÄ•Ë®∫ÂÆ§ÔºàEDÔºâÊìÅÊì†ÊòØÂÖ®ÁêÉÂÖ¨ÂÖ±Ë°õÁîüÂïèÈ°åÔºåÂ∑≤ÂèçË¶ÜËàáÊ≠ª‰∫°Áéá‰∏äÂçáÁõ∏Èóú„ÄÇÈ†êÊ∏¨Êú™‰æÜÁöÑÊúçÂãôÈúÄÊ±ÇÂ∞áÊúâÂä©ÊñºÊé°ÂèñÈ†êÈò≤Êé™ÊñΩÔºå‰ª•Ê∂àÈô§ÊìÅÊì†ÂèäÂÖ∂‰∏çÂà©ÂΩ±Èüø„ÄÇÊàëÂÄëÊÄ•Ë®∫ÂÆ§ÁöÑÊúÄÊñ∞ÁôºÁèæË°®ÊòéÔºåÂÖ•‰ΩèÁéáË∂ÖÈÅé 90% Ëàá 10 Â§©Ê≠ª‰∫°ÁéáÂ¢ûÂä†Áõ∏Èóú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊó®Âú®‰ΩøÁî®‰æÜËá™Â§ßÂûãÂåóÊ≠êÊÄ•Ë®∫ÂÆ§ÁöÑÂõûÈ°ßÊÄßÊï∏ÊìöÂíå LightGBM Ê®°Âûã‰æÜÈ†êÊ∏¨ÈÄô‰∫õÂç±Ê©üÊôÇÊúü„ÄÇÊàëÂÄëÊèê‰æõÊï¥ÂÄãÊÄ•Ë®∫ÂÆ§ÁöÑÈ†êÊ∏¨Ôºå‰∏¶ÂàÜÂà•ÈáùÂ∞çÂÖ∂‰∏çÂêåÁöÑÈÅã‰ΩúÈÉ®ÈñÄÈÄ≤Ë°åÈ†êÊ∏¨„ÄÇÊàëÂÄëË≠âÊòé‰∏ãÂçàÊìÅÊì†ÂèØ‰ª•Âú®‰∏äÂçà 11 ÈªûÈ†êÊ∏¨ÔºåAUC ÁÇ∫ 0.82Ôºà95% CI 0.78-0.86ÔºâÔºåÂú®‰∏äÂçà 8 ÈªûÈ†êÊ∏¨ÔºåAUC È´òÈÅî 0.79Ôºà95% CI 0.75-0.83Ôºâ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëË°®Êòé‰ΩøÁî®ÂåøÂêçÁÆ°ÁêÜÊï∏ÊìöÈ†êÊ∏¨ËàáÊ≠ª‰∫°ÁéáÁõ∏ÈóúÁöÑÊìÅÊì†ÊòØÂèØË°åÁöÑ„ÄÇ

##### **Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions**
2410.12848v1 by Per Niklas Waaler, Musarrat Hussain, Igor Molchanov, Lars Ailo Bongo, Brita Elvev√•g

Patients with schizophrenia often present with cognitive impairments that may
hinder their ability to learn about their condition. These individuals could
benefit greatly from education platforms that leverage the adaptability of
Large Language Models (LLMs) such as GPT-4. While LLMs have the potential to
make topical mental health information more accessible and engaging, their
black-box nature raises concerns about ethics and safety. Prompting offers a
way to produce semi-scripted chatbots with responses anchored in instructions
and validated information, but prompt-engineered chatbots may drift from their
intended identity as the conversation progresses. We propose a Critical
Analysis Filter for achieving better control over chatbot behavior. In this
system, a team of prompted LLM agents are prompt-engineered to critically
analyze and refine the chatbot's response and deliver real-time feedback to the
chatbot. To test this approach, we develop an informational schizophrenia
chatbot and converse with it (with the filter deactivated) until it oversteps
its scope. Once drift has been observed, AI-agents are used to automatically
generate sample conversations in which the chatbot is being enticed to talk
about out-of-bounds topics. We manually assign to each response a compliance
score that quantifies the chatbot's compliance to its instructions;
specifically the rules about accurately conveying sources and being transparent
about limitations. Activating the Critical Analysis Filter resulted in an
acceptable compliance score (>=2) in 67.0% of responses, compared to only 8.7%
when the filter was deactivated. These results suggest that a self-reflection
layer could enable LLMs to be used effectively and safely in mental health
platforms, maintaining adaptability while reliably limiting their scope to
appropriate use cases.

ÊëòË¶ÅÔºö<paragraph>Á≤æÁ•ûÂàÜË£ÇÁóáÊÇ£ËÄÖÂ∏∏Âá∫ÁèæË™çÁü•ÈöúÁ§ôÔºåÈÄôÂèØËÉΩÊúÉÈòªÁ§ô‰ªñÂÄëÂ≠∏ÁøíËá™Â∑±ÁóÖÊÉÖÁöÑËÉΩÂäõ„ÄÇÈÄô‰∫õ‰∫∫ÂèØ‰ª•ÂæûÊïôËÇ≤Âπ≥Âè∞‰∏≠ÂèóÁõäËâØÂ§öÔºåÈÄô‰∫õÂπ≥Âè∞Âà©Áî®‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºà‰æãÂ¶Ç GPT-4ÔºâÁöÑÈÅ©ÊáâÊÄß„ÄÇÂÑòÁÆ° LLM ÊúâÂèØËÉΩ‰Ωø‰∏ªÈ°åÂøÉÁêÜÂÅ•Â∫∑‰ø°ÊÅØÊõ¥ÊòìÊñºË®™ÂïèÂíåÊõ¥ÂÖ∑Âê∏ÂºïÂäõÔºå‰ΩÜÂÆÉÂÄëÁöÑÈªëÁÆ±ÊÄßË≥™ÂºïËµ∑‰∫Ü‰∫∫ÂÄëÂ∞çÈÅìÂæ∑ÂíåÂÆâÂÖ®ÁöÑÊìîÊÜÇ„ÄÇÊèêÁ§∫Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ï‰æÜÁîüÊàêÂçäËÖ≥Êú¨ÁöÑËÅäÂ§©Ê©üÂô®‰∫∫ÔºåÂÖ∂ÈüøÊáâÊ§çÊ†πÊñºÊåá‰ª§ÂíåÁ∂ìÈÅéÈ©óË≠âÁöÑ‰ø°ÊÅØ‰∏≠Ôºå‰ΩÜÊèêÁ§∫Â∑•Á®ãËÅäÂ§©Ê©üÂô®‰∫∫ÂèØËÉΩÊúÉÈö®ËëóÂ∞çË©±ÁöÑÈÄ≤Ë°åËÄåÂÅèÈõ¢ÂÖ∂Êó¢ÂÆöÁöÑË∫´‰ªΩ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊâπÂà§ÊÄßÂàÜÊûêÈÅéÊøæÂô®Ôºå‰ª•Êõ¥Â•ΩÂú∞ÊéßÂà∂ËÅäÂ§©Ê©üÂô®‰∫∫ÁöÑË°åÁÇ∫„ÄÇÂú®ÈÄôÂÄãÁ≥ªÁµ±‰∏≠Ôºå‰∏ÄÂÄãÁî±ÊèêÁ§∫ÁöÑ LLM ‰ª£ÁêÜÁµÑÊàêÁöÑÂúòÈöäË¢´ÊèêÁ§∫Â∑•Á®ãÂåñÔºå‰ª•ÊâπÂà§ÊÄßÂú∞ÂàÜÊûêÂíåÂÑ™ÂåñËÅäÂ§©Ê©üÂô®‰∫∫ÁöÑÈüøÊáâÔºå‰∏¶ÂêëËÅäÂ§©Ê©üÂô®‰∫∫Êèê‰æõÂØ¶ÊôÇÂèçÈ•ã„ÄÇÁÇ∫‰∫ÜÊ∏¨Ë©¶ÈÄôÁ®ÆÊñπÊ≥ïÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ø°ÊÅØÊÄßÁ≤æÁ•ûÂàÜË£ÇÁóáËÅäÂ§©Ê©üÂô®‰∫∫Ôºå‰∏¶ËàáÂÆÉ‰∫§Ë´áÔºàÂú®ÈÅéÊøæÂô®ÂÅúÁî®ÁöÑÊÉÖÊ≥Å‰∏ãÔºâÔºåÁõ¥Âà∞ÂÆÉË∂ÖÂá∫ÂÖ∂ÁØÑÂúç„ÄÇ‰∏ÄÊó¶ËßÄÂØüÂà∞ÊºÇÁßªÔºåAI ‰ª£ÁêÜÂ∞±ÊúÉÁî®ÊñºËá™ÂãïÁîüÊàêÁ§∫‰æãÂ∞çË©±ÔºåÂÖ∂‰∏≠ËÅäÂ§©Ê©üÂô®‰∫∫Ë¢´Ë™òÂ∞éË´áË´ñË∂ÖÂá∫ÁïåÈôêÁöÑË©±È°å„ÄÇÊàëÂÄëÊâãÂãïÁÇ∫ÊØèÂÄãÈüøÊáâÂàÜÈÖç‰∏ÄÂÄãÂêàË¶èÂàÜÊï∏ÔºåË©≤ÂàÜÊï∏ÈáèÂåñ‰∫ÜËÅäÂ§©Ê©üÂô®‰∫∫Â∞çÂÖ∂Êåá‰ª§ÁöÑÂêàË¶èÊÄßÔºõÂÖ∑È´î‰æÜË™™ÔºåÂ∞±ÊòØÊ∫ñÁ¢∫ÂÇ≥ÈÅî‰æÜÊ∫êÂíåÂ∞çÈôêÂà∂‰øùÊåÅÈÄèÊòéÁöÑË¶èÂâá„ÄÇÊøÄÊ¥ªÊâπÂà§ÊÄßÂàÜÊûêÈÅéÊøæÂô®ÂæåÔºå67.0% ÁöÑÈüøÊáâÁç≤Âæó‰∫ÜÂèØÊé•ÂèóÁöÑÂêàË¶èÂàÜÊï∏Ôºà>=2ÔºâÔºåËÄåÈÅéÊøæÂô®ÂÅúÁî®ÊôÇÂè™Êúâ 8.7%„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºå‰∏ÄÂÄãËá™ÊàëÂèçÁúÅÂ±§ÂèØ‰ª•‰Ωø LLM Âú®ÂøÉÁêÜÂÅ•Â∫∑Âπ≥Âè∞‰∏≠ÂæóÂà∞ÊúâÊïàÂíåÂÆâÂÖ®ÁöÑÂà©Áî®ÔºåÂú®‰øùÊåÅÈÅ©ÊáâÊÄßÁöÑÂêåÊôÇÔºåÂèØÈù†Âú∞Â∞áÂÖ∂ÁØÑÂúçÈôêÂà∂Âú®ÈÅ©Áï∂ÁöÑÁî®‰æã‰∏≠„ÄÇ</paragraph>

##### **Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts**
2410.08245v1 by Sukwon Yun, Inyoung Choi, Jie Peng, Yangfan Wu, Jingxuan Bao, Qiyiwen Zhang, Jiayi Xin, Qi Long, Tianlong Chen

Multimodal learning has gained increasing importance across various fields,
offering the ability to integrate data from diverse sources such as images,
text, and personalized records, which are frequently observed in medical
domains. However, in scenarios where some modalities are missing, many existing
frameworks struggle to accommodate arbitrary modality combinations, often
relying heavily on a single modality or complete data. This oversight of
potential modality combinations limits their applicability in real-world
situations. To address this challenge, we propose Flex-MoE (Flexible
Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary
modality combinations while maintaining robustness to missing data. The core
idea of Flex-MoE is to first address missing modalities using a new missing
modality bank that integrates observed modality combinations with the
corresponding missing ones. This is followed by a uniquely designed Sparse MoE
framework. Specifically, Flex-MoE first trains experts using samples with all
modalities to inject generalized knowledge through the generalized router
($\mathcal{G}$-Router). The $\mathcal{S}$-Router then specializes in handling
fewer modality combinations by assigning the top-1 gate to the expert
corresponding to the observed modality combination. We evaluate Flex-MoE on the
ADNI dataset, which encompasses four modalities in the Alzheimer's Disease
domain, as well as on the MIMIC-IV dataset. The results demonstrate the
effectiveness of Flex-MoE highlighting its ability to model arbitrary modality
combinations in diverse missing modality scenarios. Code is available at
https://github.com/UNITES-Lab/flex-moe.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ≠¶‰π†Âú®ÂêÑ‰∏™È¢ÜÂüü‰∏≠Ëé∑ÂæóË∂äÊù•Ë∂äÂ§öÁöÑÈáçËßÜÔºå
ÂÆÉÊèê‰æõ‰∫ÜÊï¥ÂêàÊù•Ëá™ÂõæÂÉè„ÄÅ
ÊñáÊú¨Âíå‰∏™ÊÄßÂåñËÆ∞ÂΩïÁ≠â‰∏çÂêåÊù•Ê∫êÁöÑÊï∞ÊçÆÁöÑËÉΩÂäõÔºåËøô‰∫õÊï∞ÊçÆÈÄöÂ∏∏Âú®ÂåªÂ≠¶
È¢ÜÂüü‰∏≠ËßÇÂØüÂà∞„ÄÇÁÑ∂ËÄåÔºåÂú®Êüê‰∫õÊ®°ÊÄÅÁº∫Â§±ÁöÑÊÉÖÂÜµ‰∏ãÔºåËÆ∏Â§öÁé∞ÊúâÁöÑ
Ê°ÜÊû∂Èöæ‰ª•ÈÄÇÂ∫î‰ªªÊÑèÊ®°ÊÄÅÁªÑÂêàÔºåÈÄöÂ∏∏‰∏•Èáç‰æùËµñ‰∫éÂçï‰∏ÄÊ®°ÊÄÅÊàñÂÆåÊï¥Êï∞ÊçÆ„ÄÇËøôÁßçÂØπ
ÊΩúÂú®Ê®°ÊÄÅÁªÑÂêàÁöÑÂøΩËßÜÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÈÄÇÁî®ÊÄß
ÊÉÖÂÜµ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü Flex-MoEÔºàÁÅµÊ¥ª
‰∏ìÂÆ∂Ê∑∑ÂêàÔºâÔºå‰∏Ä‰∏™Êó®Âú®ÁÅµÊ¥ªÂú∞Á∫≥ÂÖ•‰ªªÊÑè
Ê®°ÊÄÅÁªÑÂêàÔºåÂêåÊó∂‰øùÊåÅÂØπÁº∫Â§±Êï∞ÊçÆÁöÑÈ≤ÅÊ£íÊÄß„ÄÇFlex-MoE ÁöÑÊ†∏ÂøÉ
ÊÄùÊÉ≥ÊòØÈ¶ñÂÖà‰ΩøÁî®Êñ∞ÁöÑÁº∫Â§±Ê®°ÊÄÅÂ∫ìÊù•Ëß£ÂÜ≥Áº∫Â§±Ê®°ÊÄÅÔºåËØ•Â∫ìÂ∞ÜËßÇÂØüÂà∞ÁöÑÊ®°ÊÄÅÁªÑÂêà‰∏é
Áõ∏Â∫îÁöÑÁº∫Â§±Ê®°ÊÄÅÁõ∏ÁªìÂêà„ÄÇÊé•‰∏ãÊù•ÊòØ‰∏Ä‰∏™Áã¨ÁâπËÆæËÆ°ÁöÑÁ®ÄÁñè MoE
Ê°ÜÊû∂„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåFlex-MoE È¶ñÂÖà‰ΩøÁî®ÂÖ∑ÊúâÊâÄÊúâ
Ê®°ÊÄÅÁöÑÊ†∑Êú¨ËÆ≠ÁªÉ‰∏ìÂÆ∂Ôºå‰ª•ÈÄöËøáÂπø‰πâË∑ØÁî±Âô®Ê≥®ÂÖ•Âπø‰πâÁü•ËØÜ
Ôºà$\mathcal{G}$-RouterÔºâ„ÄÇÁÑ∂ÂêéÔºå$\mathcal{S}$-Router ÈÄöËøáÂ∞Ü top-1 Èó®ÂàÜÈÖçÁªô‰∏ìÂÆ∂Êù•‰∏ìÈó®Â§ÑÁêÜËæÉÂ∞ëÁöÑÊ®°ÊÄÅÁªÑÂêà
ÂØπÂ∫î‰∫éËßÇÂØüÂà∞ÁöÑÊ®°ÊÄÅÁªÑÂêà„ÄÇÊàë‰ª¨Âú®
ADNI Êï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞ Flex-MoEÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÈòøÂ∞îËå®Êµ∑ÈªòÁóÖ‰∏≠ÁöÑÂõõÁßçÊ®°ÊÄÅ
È¢ÜÂüüÔºå‰ª•Âèä MIMIC-IV Êï∞ÊçÆÈõÜ„ÄÇÁªìÊûúËØÅÊòé‰∫Ü
Flex-MoE ÁöÑÊúâÊïàÊÄßÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂Âú®‰∏çÂêåÁº∫Â§±Ê®°ÊÄÅÂú∫ÊôØ‰∏≠ÂØπ‰ªªÊÑèÊ®°ÊÄÅÁªÑÂêàËøõË°åÂª∫Ê®°ÁöÑËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂèØÂú®
https://github.com/UNITES-Lab/flex-moe Ëé∑Âæó„ÄÇ

##### **Artificial intelligence techniques in inherited retinal diseases: A review**
2410.09105v1 by Han Trinh, Jordan Vice, Jason Charng, Zahra Tajbakhsh, Khyber Alam, Fred K. Chen, Ajmal Mian

Inherited retinal diseases (IRDs) are a diverse group of genetic disorders
that lead to progressive vision loss and are a major cause of blindness in
working-age adults. The complexity and heterogeneity of IRDs pose significant
challenges in diagnosis, prognosis, and management. Recent advancements in
artificial intelligence (AI) offer promising solutions to these challenges.
However, the rapid development of AI techniques and their varied applications
have led to fragmented knowledge in this field. This review consolidates
existing studies, identifies gaps, and provides an overview of AI's potential
in diagnosing and managing IRDs. It aims to structure pathways for advancing
clinical applications by exploring AI techniques like machine learning and deep
learning, particularly in disease detection, progression prediction, and
personalized treatment planning. Special focus is placed on the effectiveness
of convolutional neural networks in these areas. Additionally, the integration
of explainable AI is discussed, emphasizing its importance in clinical settings
to improve transparency and trust in AI-based systems. The review addresses the
need to bridge existing gaps in focused studies on AI's role in IRDs, offering
a structured analysis of current AI techniques and outlining future research
directions. It concludes with an overview of the challenges and opportunities
in deploying AI for IRDs, highlighting the need for interdisciplinary
collaboration and the continuous development of robust, interpretable AI models
to advance clinical applications.

ÊëòË¶ÅÔºöÈÅ∫ÂÇ≥ÊÄßË¶ñÁ∂≤ËÜúÁñæÁóÖ (IRD) ÊòØ‰∏ÄÁµÑÂ§öÊ®£ÂåñÁöÑÈÅ∫ÂÇ≥ÁñæÁóÖÔºå
ÊúÉÂ∞éËá¥Ë¶ñÂäõÈÄêÊº∏Âñ™Â§±ÔºåÊòØÂ∑•‰ΩúÂπ¥ÈΩ°Êàê‰∫∫Â§±ÊòéÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇIRD ÁöÑË§áÈõúÊÄßÂíåÁï∞Ë≥™ÊÄßÂ∞çË®∫Êñ∑„ÄÅÈ†êÂæåÂíåÁÆ°ÁêÜÊèêÂá∫‰∫ÜÈáçÂ§ßÊåëÊà∞„ÄÇÊúÄËøë‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÈÄ≤Ê≠•ÁÇ∫ÈÄô‰∫õÊåëÊà∞Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ
ÁÑ∂ËÄåÔºåAI ÊäÄË°ìÁöÑÂø´ÈÄüÁôºÂ±ïÂèäÂÖ∂Â§öÁ®ÆÊáâÁî®Â∞éËá¥‰∫ÜË©≤È†òÂüüÁöÑÁü•Ë≠òÂàÜÊï£„ÄÇÊú¨Á∂úËø∞Êï¥Âêà‰∫ÜÁèæÊúâÁ†îÁ©∂ÔºåÊâæÂá∫Â∑ÆË∑ùÔºå‰∏¶Ê¶ÇËø∞‰∫Ü AI Âú®Ë®∫Êñ∑ÂíåÁÆ°ÁêÜ IRD ‰∏≠ÁöÑÊΩõÂäõ„ÄÇÂÆÉÊó®Âú®ÈÄöÈÅéÊé¢Á¥¢Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÁ≠â AI ÊäÄË°ìÔºåÁâπÂà•ÊòØÂú®ÁñæÁóÖÊ™¢Ê∏¨„ÄÅÈÄ≤Á®ãÈ†êÊ∏¨ÂíåÂÄãÊÄßÂåñÊ≤ªÁôÇË®àÂäÉ‰∏≠ÔºåÁÇ∫Êé®ÈÄ≤Ëá®Â∫äÊáâÁî®ÊßãÂª∫ÈÄîÂæë„ÄÇÁâπÂà•ÈóúÊ≥®ÈÄô‰∫õÈ†òÂüü‰∏≠Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåË®éË´ñ‰∫ÜÂèØËß£Èáã AI ÁöÑÊï¥ÂêàÔºåÂº∑Ë™ø‰∫ÜÂÖ∂Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÊèêÈ´òÈÄèÊòéÂ∫¶ÂíåÂ∞çÂü∫Êñº AI ÁöÑÁ≥ªÁµ±ÁöÑ‰ø°‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇË©≤Á∂úËø∞Ëß£Ê±∫‰∫ÜÂΩåÂêà AI Âú® IRD ‰∏≠‰ΩúÁî®ÁöÑÈáçÈªûÁ†îÁ©∂‰∏≠ÁèæÊúâÂ∑ÆË∑ùÁöÑÂøÖË¶ÅÊÄßÔºåÊèê‰æõ‰∫ÜÂ∞çÁï∂Ââç AI ÊäÄË°ìÁöÑÁµêÊßãÂåñÂàÜÊûêÔºå‰∏¶Ê¶ÇËø∞‰∫ÜÊú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊúÄÂæåÊ¶ÇËø∞‰∫ÜÂú® IRD ‰∏≠ÈÉ®ÁΩ≤ AI ÁöÑÊåëÊà∞ÂíåÊ©üÈÅáÔºåÂº∑Ë™ø‰∫ÜË∑®Â≠∏ÁßëÂêà‰ΩúÂíåÊåÅÁ∫åÈñãÁôºÂº∑Â§ß„ÄÅÂèØËß£ÈáãÁöÑ AI Ê®°Âûã‰ª•Êé®ÈÄ≤Ëá®Â∫äÊáâÁî®ÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **Toward Relieving Clinician Burden by Automatically Generating Progress Notes using Interim Hospital Data**
2410.12845v1 by Sarvesh Soni, Dina Demner-Fushman

Regular documentation of progress notes is one of the main contributors to
clinician burden. The abundance of structured chart information in medical
records further exacerbates the burden, however, it also presents an
opportunity to automate the generation of progress notes. In this paper, we
propose a task to automate progress note generation using structured or tabular
information present in electronic health records. To this end, we present a
novel framework and a large dataset, ChartPNG, for the task which contains
$7089$ annotation instances (each having a pair of progress notes and interim
structured chart data) across $1616$ patients. We establish baselines on the
dataset using large language models from general and biomedical domains. We
perform both automated (where the best performing Biomistral model achieved a
BERTScore F1 of $80.53$ and MEDCON score of $19.61$) and manual (where we found
that the model was able to leverage relevant structured data with $76.9\%$
accuracy) analyses to identify the challenges with the proposed task and
opportunities for future research.

ÊëòË¶ÅÔºöÂÆöÊúüËÆ∞ÂΩïÈÄ≤Â∫¶Á≠ÜË®òÊòØÈÄ†ÊàêËá®Â∫äÈÜ´Â∏´Ë≤†ÊìîÁöÑ‰∏ªË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇÁóÖÊ≠∑‰∏≠Ë±êÂØåÁöÑÁµêÊßãÂåñÂúñË°®Ë≥áË®äÈÄ≤‰∏ÄÊ≠•Âä†Âäá‰∫ÜË≤†ÊìîÔºå‰ΩÜÂÆÉ‰πüÊèê‰æõ‰∫ÜËá™ÂãïÂåñÁîüÊàêÈÄ≤Â∫¶Á≠ÜË®òÁöÑÊ©üÊúÉ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰ΩøÁî®ÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ‰∏≠Â≠òÂú®ÁöÑÁµêÊßãÂåñÊàñË°®Ê†ºË≥áË®äËá™ÂãïÂåñÁîüÊàêÈÄ≤Â∫¶Á≠ÜË®òÁöÑ‰ªªÂãô„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü ChartPNG ÁöÑ‰∏ÄÂÄãÊñ∞Ê°ÜÊû∂Âíå‰∏ÄÂÄãÂ§ßÂûãË≥áÊñôÈõÜÔºåË©≤‰ªªÂãôÂåÖÂê´ 1616 ‰ΩçÊÇ£ËÄÖÁöÑ 7089 ÂÄãË®ªËß£ÂØ¶‰æãÔºàÊØèÂÄãÂØ¶‰æãÈÉΩÊúâ‰∏ÄÂ∞çÈÄ≤Â∫¶Á≠ÜË®òÂíåËá®ÊôÇÁµêÊßãÂåñÂúñË°®Ë≥áÊñôÔºâ„ÄÇÊàëÂÄë‰ΩøÁî®‰æÜËá™‰∏ÄËà¨ÂíåÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Ë≥áÊñôÈõÜ‰∏äÂª∫Á´ãÂü∫Á∑ö„ÄÇÊàëÂÄëÂü∑Ë°åËá™ÂãïÂåñÔºàË°®ÁèæÊúÄ‰Ω≥ÁöÑ Biomistral Ê®°ÂûãÈÅîÂà∞ BERTScore F1 ÁÇ∫ 80.53 Âíå MEDCON ÂàÜÊï∏ÁÇ∫ 19.61ÔºâÂíåÊâãÂãïÔºàÊàëÂÄëÁôºÁèæË©≤Ê®°ÂûãËÉΩÂ§†‰ª• 76.9% ÁöÑÊ∫ñÁ¢∫Â∫¶Âà©Áî®Áõ∏ÈóúÁµêÊßãÂåñË≥áÊñôÔºâÂàÜÊûêÔºå‰ª•ÊâæÂá∫ÊâÄÊèêÂá∫‰ªªÂãôÁöÑÊåëÊà∞ÂíåÊú™‰æÜÁ†îÁ©∂ÁöÑÊ©üÊúÉ„ÄÇ

##### **Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare**
2410.07525v2 by Nan Fang, Guiliang Liu, Wei Gong

Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical
decisions and treatment, such as excessive dosages or abrupt changes, often due
to agents overlooking common-sense constraints. Consequently, Constrained
Reinforcement Learning (CRL) is a natural choice for safe decisions. However,
specifying the exact cost function is inherently difficult in healthcare.
Recent Inverse Constrained Reinforcement Learning (ICRL) is a promising
approach that infers constraints from expert demonstrations. ICRL algorithms
model Markovian decisions in an interactive environment. These settings do not
align with the practical requirement of a decision-making system in healthcare,
where decisions rely on historical treatment recorded in an offline dataset. To
tackle these issues, we propose the Constraint Transformer (CT). Specifically,
1) we utilize a causal attention mechanism to incorporate historical decisions
and observations into the constraint modeling, while employing a Non-Markovian
layer for weighted constraints to capture critical states. 2) A generative
world model is used to perform exploratory data augmentation, enabling offline
RL methods to simulate unsafe decision sequences. In multiple medical
scenarios, empirical results demonstrate that CT can capture unsafe states and
achieve strategies that approximate lower mortality rates, reducing the
occurrence probability of unsafe behaviors.

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏Áøí (RL) ÊáâÁî®ÊñºÈÜ´ÁôÇ‰øùÂÅ•ÂèØËÉΩÊúÉÂ∞éËá¥‰∏çÂÆâÂÖ®ÁöÑÈÜ´ÁôÇÊ±∫Á≠ñÂíåÊ≤ªÁôÇÔºå‰æãÂ¶ÇÈÅéÈáèÂäëÈáèÊàñÁ™ÅÁÑ∂ÊîπËÆäÔºåÈÄöÂ∏∏ÊòØÂõ†ÁÇ∫‰ª£ÁêÜ‰∫∫ÂøΩË¶ñÂ∏∏Ë≠òÈôêÂà∂„ÄÇÂõ†Ê≠§ÔºåÂèóÁ¥ÑÊùüÂº∑ÂåñÂ≠∏Áøí (CRL) ÊòØÂÆâÂÖ®Ê±∫Á≠ñÁöÑËá™ÁÑ∂ÈÅ∏Êìá„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÊòéÁ¢∫ÊåáÂÆöÁ¢∫ÂàáÁöÑÊàêÊú¨ÂáΩÊï∏Êú¨Ë≥™‰∏äÂæàÂõ∞Èõ£„ÄÇÊúÄËøëÁöÑÂèçÂêëÂèóÁ¥ÑÊùüÂº∑ÂåñÂ≠∏Áøí (ICRL) ÊòØ‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂÆÉÂæûÂ∞àÂÆ∂Á§∫ÁØÑ‰∏≠Êé®Êñ∑Âá∫Á¥ÑÊùü„ÄÇICRL ÊºîÁÆóÊ≥ïÂú®‰∫íÂãïÁí∞Â¢É‰∏≠Âª∫ÊßãÈ¶¨ÂèØÂ§´Ê±∫Á≠ñ„ÄÇÈÄô‰∫õË®≠ÂÆöËàáÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Ê±∫Á≠ñÁ≥ªÁµ±ÁöÑÂØ¶ÈöõË¶ÅÊ±Ç‰∏çÁ¨¶ÔºåÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåÊ±∫Á≠ñ‰æùË≥¥ÊñºÈõ¢Á∑öË≥áÊñôÈõÜ‰∏≠Ë®òÈåÑÁöÑÊ≠∑Âè≤Ê≤ªÁôÇ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁ¥ÑÊùüËΩâÊèõÂô® (CT)„ÄÇÂÖ∑È´î‰æÜË™™Ôºå1) ÊàëÂÄëÂà©Áî®Âõ†ÊûúÊ≥®ÊÑèÊ©üÂà∂Â∞áÊ≠∑Âè≤Ê±∫Á≠ñÂíåËßÄÂØüÁ¥çÂÖ•Á¥ÑÊùüÂª∫Ê®°ÔºåÂêåÊôÇÊé°Áî®ÈùûÈ¶¨ÂèØÂ§´Â±§Ôºå‰ª•Âä†Ê¨äÁ¥ÑÊùü‰æÜÊçïÊçâÈóúÈçµÁãÄÊÖã„ÄÇ2) ÁîüÊàêÂºè‰∏ñÁïåÊ®°ÂûãÁî®ÊñºÂü∑Ë°åÊé¢Á¥¢ÊÄßË≥áÊñôÊì¥ÂÖÖÔºå‰ΩøÈõ¢Á∑ö RL ÊñπÊ≥ïËÉΩÂ§†Ê®°Êì¨‰∏çÂÆâÂÖ®ÁöÑÊ±∫Á≠ñÂ∫èÂàó„ÄÇÂú®Â§öÁ®ÆÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÔºåÂØ¶Ë≠âÁµêÊûúË°®ÊòéÔºåCT ÂèØ‰ª•ÊçïÊçâ‰∏çÂÆâÂÖ®ÁöÑÁãÄÊÖãÔºå‰∏¶ÂØ¶ÁèæËøë‰ººÈôç‰ΩéÊ≠ª‰∫°ÁéáÁöÑÁ≠ñÁï•ÔºåÂæûËÄåÈôç‰Ωé‰∏çÂÆâÂÖ®Ë°åÁÇ∫ÁôºÁîüÁöÑÊ©üÁéá„ÄÇ

##### **A Two-Model Approach for Humour Style Recognition**
2410.12842v1 by Mary Ogbuka Kenneth, Foaad Khosmood, Abbas Edalat

Humour, a fundamental aspect of human communication, manifests itself in
various styles that significantly impact social interactions and mental health.
Recognising different humour styles poses challenges due to the lack of
established datasets and machine learning (ML) models. To address this gap, we
present a new text dataset for humour style recognition, comprising 1463
instances across four styles (self-enhancing, self-deprecating, affiliative,
and aggressive) and non-humorous text, with lengths ranging from 4 to 229
words. Our research employs various computational methods, including classic
machine learning classifiers, text embedding models, and DistilBERT, to
establish baseline performance. Additionally, we propose a two-model approach
to enhance humour style recognition, particularly in distinguishing between
affiliative and aggressive styles. Our method demonstrates an 11.61%
improvement in f1-score for affiliative humour classification, with consistent
improvements in the 14 models tested. Our findings contribute to the
computational analysis of humour in text, offering new tools for studying
humour in literature, social media, and other textual sources.

ÊëòË¶ÅÔºöÂπΩÈªòÔºåÊòØ‰∫∫È°ûÊ∫ùÈÄö‰∏≠‰∏ÄÂÄãÂü∫Êú¨ÁöÑÈù¢ÂêëÔºåÂÆÉ‰ª•ÂêÑÁ®Æ‰∏çÂêåÁöÑÈ¢®Ê†ºÂ±ïÁèæÔºåÂ∞çÁ§æÊúÉ‰∫íÂãïÂíåÂøÉÁêÜÂÅ•Â∫∑ÊúâÈ°ØËëóÁöÑÂΩ±Èüø„ÄÇÁî±ÊñºÁº∫‰πèÊó¢ÂÆöÁöÑË≥áÊñôÈõÜÂíåÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÔºåËæ®Ë≠ò‰∏çÂêåÁöÑÂπΩÈªòÈ¢®Ê†ºÊúÉÂ∏∂‰æÜÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊñ∞ÁöÑÊñáÊú¨Ë≥áÊñôÈõÜÔºåÁî®ÊñºÂπΩÈªòÈ¢®Ê†ºËæ®Ë≠òÔºåÂåÖÂê´ 1463 ÂÄãÂØ¶‰æãÔºåÊ©´Ë∑®ÂõõÁ®ÆÈ¢®Ê†ºÔºàËá™ÊàëÂ¢ûÂº∑„ÄÅËá™ÊàëË≤∂Êäë„ÄÅË¶™ÂíåÂíåÊîªÊìäÔºâÔºå‰ª•ÂèäÈùûÂπΩÈªòÊñáÊú¨ÔºåÈï∑Â∫¶Âæû 4 Âà∞ 229 ÂÄãÂ≠ó‰∏çÁ≠â„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êé°Áî®‰∫ÜÂêÑÁ®ÆË®àÁÆóÊñπÊ≥ïÔºåÂåÖÊã¨Á∂ìÂÖ∏Ê©üÂô®Â≠∏ÁøíÂàÜÈ°ûÂô®„ÄÅÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÂíå DistilBERTÔºå‰ª•Âª∫Á´ãÂü∫Ê∫ñÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈõôÊ®°ÂûãÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∑ÂπΩÈªòÈ¢®Ê†ºËæ®Ë≠òÔºåÁâπÂà•ÊòØÂú®ÂçÄÂàÜË¶™ÂíåÂíåÊîªÊìäÈ¢®Ê†ºÊñπÈù¢„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñπÊ≥ïÂú®Ë¶™ÂíåÂπΩÈªòÂàÜÈ°ûÁöÑ f1 ÂàÜÊï∏‰∏äÂ±ïÁèæ‰∫Ü 11.61% ÁöÑÈÄ≤Ê≠•ÔºåÂú®Ê∏¨Ë©¶ÁöÑ 14 ÂÄãÊ®°Âûã‰∏≠ÈÉΩÊúâÁ©©ÂÆöÁöÑÈÄ≤Ê≠•„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊúâÂä©ÊñºÂ∞çÊñáÊú¨‰∏≠ÁöÑÂπΩÈªòÈÄ≤Ë°åË®àÁÆóÂàÜÊûêÔºåÁÇ∫Á†îÁ©∂ÊñáÂ≠∏„ÄÅÁ§æÁæ§Â™íÈ´îÂíåÂÖ∂‰ªñÊñáÊú¨‰æÜÊ∫ê‰∏≠ÁöÑÂπΩÈªòÊèê‰æõ‰∫ÜÊñ∞ÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel Parallelism for FPGA-Accelerated Processing**
2410.07364v1 by Ismail Erbas, Aporva Amarnath, Vikas Pandey, Karthik Swaminathan, Naigang Wang, Xavier Intes

Fluorescence lifetime imaging (FLI) is a widely used technique in the
biomedical field for measuring the decay times of fluorescent molecules,
providing insights into metabolic states, protein interactions, and
ligand-receptor bindings. However, its broader application in fast biological
processes, such as dynamic activity monitoring, and clinical use, such as in
guided surgery, is limited by long data acquisition times and computationally
demanding data processing. While deep learning has reduced post-processing
times, time-resolved data acquisition remains a bottleneck for real-time
applications. To address this, we propose a method to achieve real-time FLI
using an FPGA-based hardware accelerator. Specifically, we implemented a
GRU-based sequence-to-sequence (Seq2Seq) model on an FPGA board compatible with
time-resolved cameras. The GRU model balances accurate processing with the
resource constraints of FPGAs, which have limited DSP units and BRAM. The
limited memory and computational resources on the FPGA require efficient
scheduling of operations and memory allocation to deploy deep learning models
for low-latency applications. We address these challenges by using STOMP, a
queue-based discrete-event simulator that automates and optimizes task
scheduling and memory management on hardware. By integrating a GRU-based
Seq2Seq model and its compressed version, called Seq2SeqLite, generated through
knowledge distillation, we were able to process multiple pixels in parallel,
reducing latency compared to sequential processing. We explore various levels
of parallelism to achieve an optimal balance between performance and resource
utilization. Our results indicate that the proposed techniques achieved a 17.7x
and 52.0x speedup over manual scheduling for the Seq2Seq model and the
Seq2SeqLite model, respectively.

ÊëòË¶ÅÔºöËû¢ÂÖâÁîüÂëΩÈÄ±ÊúüÂΩ±ÂÉè (FLI) ÊòØÁîüÁâ©ÈÜ´Â≠∏È†òÂüü‰∏≠Âª£Ê≥õ‰ΩøÁî®ÁöÑÊäÄË°ìÔºåÁî®ÊñºÊ∏¨ÈáèËû¢ÂÖâÂàÜÂ≠êÁöÑË°∞ËÆäÊôÇÈñìÔºåÊèê‰æõ‰ª£Ë¨ùÁãÄÊÖã„ÄÅËõãÁôΩË≥™‰∫§‰∫í‰ΩúÁî®ÂíåÈÖçÈ´îÂèóÈ´îÁµêÂêàÁöÑË¶ãËß£„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Âú®Âø´ÈÄüÁîüÁâ©ÈÅéÁ®ãÔºà‰æãÂ¶ÇÂãïÊÖãÊ¥ªÂãïÁõ£Ê∏¨ÔºâÂíåËá®Â∫äÁî®ÈÄîÔºà‰æãÂ¶ÇÂºïÂ∞éÂºèÊâãË°ìÔºâ‰∏≠ÁöÑÂª£Ê≥õÊáâÁî®ÂèóÂà∞Èï∑ÊôÇÈñìË≥áÊñôÊì∑ÂèñÂíåË®àÁÆóÈúÄÊ±ÇÈ´òÁöÑË≥áÊñôËôïÁêÜÁöÑÈôêÂà∂„ÄÇÂÑòÁÆ°Ê∑±Â∫¶Â≠∏ÁøíÊ∏õÂ∞ë‰∫ÜÂæåËôïÁêÜÊôÇÈñìÔºå‰ΩÜÊôÇÈñìËß£ÊûêË≥áÊñôÊì∑Âèñ‰ªçÁÑ∂ÊòØÂç≥ÊôÇÊáâÁî®Á®ãÂºèÁöÑÁì∂È†∏„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Âü∫Êñº FPGA ÁöÑÁ°¨È´îÂä†ÈÄüÂô®‰æÜÂØ¶ÁèæÂç≥ÊôÇ FLI ÁöÑÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂú®ËàáÊôÇÈñìËß£ÊûêÁõ∏Ê©üÁõ∏ÂÆπÁöÑ FPGA Êùø‰∏äÂØ¶‰Ωú‰∫ÜÂü∫Êñº GRU ÁöÑÂ∫èÂàóÂ∞çÂ∫èÂàó (Seq2Seq) Ê®°Âûã„ÄÇGRU Ê®°ÂûãÂπ≥Ë°°‰∫ÜÊ∫ñÁ¢∫ÁöÑËôïÁêÜËàá FPGA ÁöÑË≥áÊ∫êÈôêÂà∂ÔºåFPGA ÁöÑ DSP ÂñÆÂÖÉÂíå BRAM ÊúâÈôê„ÄÇFPGA ‰∏äÊúâÈôêÁöÑË®òÊÜ∂È´îÂíåË®àÁÆóË≥áÊ∫êÈúÄË¶ÅÊúâÊïàÂú∞ÊéíÁ®ã‰ΩúÊ•≠ÂíåË®òÊÜ∂È´îÈÖçÁΩÆÔºåÊâçËÉΩÈÉ®ÁΩ≤Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰ª•ÈÄ≤Ë°å‰ΩéÂª∂ÈÅ≤ÊáâÁî®Á®ãÂºè„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî® STOMP ‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Êñº‰ΩáÂàóÁöÑÈõ¢Êï£‰∫ã‰ª∂Ê®°Êì¨Âô®ÔºåÂèØËá™ÂãïÂåñÂíåÊúÄ‰Ω≥ÂåñÁ°¨È´î‰∏äÁöÑ‰ªªÂãôÊéíÁ®ãÂíåË®òÊÜ∂È´îÁÆ°ÁêÜ„ÄÇÈÄèÈÅéÊï¥ÂêàÂü∫Êñº GRU ÁöÑ Seq2Seq Ê®°ÂûãÂèäÂÖ∂Â£ìÁ∏ÆÁâàÊú¨ Seq2SeqLiteÔºàÈÄèÈÅéÁü•Ë≠òËêÉÂèñÁî¢ÁîüÔºâÔºåÊàëÂÄëËÉΩÂ§†Âπ≥Ë°åËôïÁêÜÂ§öÂÄãÂÉèÁ¥†ÔºåËàáÈ†ÜÂ∫èËôïÁêÜÁõ∏ÊØîÔºåÂèØÊ∏õÂ∞ëÂª∂ÈÅ≤„ÄÇÊàëÂÄëÊé¢Á¥¢‰∫ÜÂêÑÁ®ÆÂπ≥Ë°åÂ±§Á¥öÔºå‰ª•Âú®ÊïàËÉΩÂíåË≥áÊ∫êÂà©Áî®Áéá‰πãÈñìÂèñÂæóÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàá Seq2Seq Ê®°ÂûãÂíå Seq2SeqLite Ê®°ÂûãÁöÑÊâãÂãïÊéíÁ®ãÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊäÄË°ìÂàÜÂà•ÈÅîÂà∞‰∫Ü 17.7 ÂÄçÂíå 52.0 ÂÄçÁöÑÂä†ÈÄü„ÄÇ

##### **Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy**
2410.07147v1 by Vivian Nguyen, Sang Min Jung, Lillian Lee, Thomas D. Hull, Cristian Danescu-Niculescu-Mizil

Mental-health therapy involves a complex conversation flow in which patients
and therapists continuously negotiate what should be talked about next. For
example, therapists might try to shift the conversation's direction to keep the
therapeutic process on track and avoid stagnation, or patients might push the
discussion towards issues they want to focus on.
  How do such patient and therapist redirections relate to the development and
quality of their relationship? To answer this question, we introduce a
probabilistic measure of the extent to which a certain utterance immediately
redirects the flow of the conversation, accounting for both the intention and
the actual realization of such a change. We apply this new measure to
characterize the development of patient-therapist relationships over multiple
sessions in a very large, widely-used online therapy platform. Our analysis
reveals that (1) patient control of the conversation's direction generally
increases relative to that of the therapist as their relationship progresses;
and (2) patients who have less control in the first few sessions are
significantly more likely to eventually express dissatisfaction with their
therapist and terminate the relationship.

ÊëòË¶ÅÔºöÂøÉÁêÜÂÅ•Â∫∑Ê≤ªÁôÇÊ∂âÂèäË§áÈõúÁöÑÂ∞çË©±ÊµÅÁ®ãÔºåÂÖ∂‰∏≠ÊÇ£ËÄÖÂíåÊ≤ªÁôÇÂ∏´ÊåÅÁ∫åÂçîÂïÜÊé•‰∏ã‰æÜÊáâË®éË´ñ‰ªÄÈ∫º„ÄÇ‰æãÂ¶ÇÔºåÊ≤ªÁôÇÂ∏´ÂèØËÉΩÊúÉÂòóË©¶ÊîπËÆäÂ∞çË©±ÊñπÂêëÔºå‰ª•‰ΩøÊ≤ªÁôÇÈÅéÁ®ã‰øùÊåÅÂú®Ê≠£Ëªå‰∏¶ÈÅøÂÖçÂÅúÊªØÔºåÊàñËÄÖÊÇ£ËÄÖÂèØËÉΩÊúÉÂ∞áË®éË´ñÂºïÂêë‰ªñÂÄëÊÉ≥ÈóúÊ≥®ÁöÑÂïèÈ°å„ÄÇ
ÊÇ£ËÄÖÂíåÊ≤ªÁôÇÂ∏´ÁöÑÈÄôÁ®ÆÈáçÊñ∞ÂÆöÂêëËàá‰ªñÂÄëÈóú‰øÇÁöÑÁôºÂ±ïÂíåÂìÅË≥™Êúâ‰ΩïÈóú‰øÇÔºüÁÇ∫‰∫ÜÂõûÁ≠îÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊ©üÁéáÊ∏¨ÈáèÔºåÁî®ÊñºË°°ÈáèÊüêÂÄãË©±Ë™ûÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÁ´ãÂç≥ÈáçÊñ∞ÂÆöÂêëÂ∞çË©±ÊµÅÁ®ãÔºåÂêåÊôÇËÄÉÈáèÊ≠§È°ûËÆäÂåñÁöÑÊÑèÂúñÂíåÂØ¶ÈöõÂØ¶Áèæ„ÄÇÊàëÂÄëÂ∞áÊ≠§Êñ∞Ê∏¨ÈáèÊáâÁî®ÊñºÊèèËø∞ÊÇ£ËÄÖ-Ê≤ªÁôÇÂ∏´Èóú‰øÇÂú®‰∏ÄÂÄãÈùûÂ∏∏ÈæêÂ§ß„ÄÅÂª£Ê≥õ‰ΩøÁî®ÁöÑÁ∑ö‰∏äÊ≤ªÁôÇÂπ≥Âè∞‰∏äÔºåÂú®Â§öÂÄãÁôÇÁ®ã‰∏≠ÁöÑÁôºÂ±ï„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫Ôºå(1) Èö®ËëóÊÇ£ËÄÖËàáÊ≤ªÁôÇÂ∏´Èóú‰øÇÁöÑÈÄ≤Â±ïÔºåÊÇ£ËÄÖÂ∞çÂ∞çË©±ÊñπÂêëÁöÑÊéßÂà∂ÈÄöÂ∏∏ÊúÉÁõ∏Â∞çÊñºÊ≤ªÁôÇÂ∏´ËÄåÂ¢ûÂä†Ôºõ(2) Âú®ÊúÄÂàùÂπæÊ¨°ÁôÇÁ®ã‰∏≠ÊéßÂà∂ËºÉÂ∞ëÁöÑÊÇ£ËÄÖÔºåÊúÄÁµÇÈ°ØËëóÊõ¥ÊúâÂèØËÉΩÂ∞çÂÖ∂Ê≤ªÁôÇÂ∏´Ë°®ÈÅî‰∏çÊªø‰∏¶ÁµÇÊ≠¢Èóú‰øÇ„ÄÇ

##### **Mental Disorders Detection in the Era of Large Language Models**
2410.07129v2 by Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Artem Shelmanov, Ivan Smirnov

This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications.

ÊëòË¶ÅÔºöÊú¨ÊñáÊØîËºÉ‰∫ÜÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ï„ÄÅÁ∑®Á¢ºÂô®Ê®°ÂûãÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂÅµÊ∏¨ÊÜÇÈ¨±ÁóáÂíåÁÑ¶ÊÖÆÁóá‰ªªÂãô‰∏äÁöÑÊúâÊïàÊÄß„ÄÇËÄÉÊÖÆ‰∫Ü‰∫îÂÄãË≥áÊñôÈõÜÔºåÊØèÂÄãË≥áÊñôÈõÜÂú®Ê†ºÂºèÂíåÁî®ÊñºÂÆöÁæ©ÁõÆÊ®ôÁóÖÁêÜÈ°ûÂà•ÁöÑÊñπÊ≥ï‰∏äÈÉΩ‰∏çÂêå„ÄÇÊàëÂÄëÊ∏¨Ë©¶‰∫ÜÂü∫ÊñºË™ûË®ÄÁâπÂæµÁöÑ AutoML Ê®°Âûã„ÄÅÂ§öÁ®ÆÁ∑®Á¢ºÂô®Ê®°ÂûãÁöÑËÆäÈ´îÔºå‰æãÂ¶Ç BERTÔºå‰ª•Âèä‰ΩúÁÇ∫ÁóÖÁêÜÂàÜÈ°ûÊ®°ÂûãÁöÑÊúÄÊñ∞ LLM„ÄÇÁµêÊûúË°®ÊòéÔºåLLM ÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®Ë®ìÁ∑¥ÁØÑ‰æãÂú®ÊñáÂ≠óÈï∑Â∫¶ÂíåÈ°ûÂûã‰∏äÂ∑ÆÁï∞ÂæàÂ§ßÁöÑÂòàÈõú‰∏îÂ∞èÂûãË≥áÊñôÈõÜ‰∏ä„ÄÇÁÑ∂ËÄåÔºåÁï∂Âú®Ëá®Â∫ä‰∏äÁ¢∫Ë®∫ÁΩπÊÇ£ÊÜÇÈ¨±ÁóáÁöÑÂÄãÈ´îÁöÑÊñáÂ≠ó‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÊôÇÔºåÂøÉÁêÜË™ûË®ÄÂ≠∏ÁâπÂæµÂíåÁ∑®Á¢ºÂô®Ê®°ÂûãÂèØ‰ª•ÈÅîÂà∞ËàáË™ûË®ÄÊ®°ÂûãÁõ∏Áï∂ÁöÑÊïàËÉΩÔºåÁ™ÅÈ°Ø‰∫ÜÂÖ∂Âú®ÁõÆÊ®ôËá®Â∫äÊáâÁî®‰∏≠ÁöÑÊΩõÂú®ÊúâÊïàÊÄß„ÄÇ

##### **MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders**
2410.06845v1 by Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, Heng Ji

Mental health disorders are one of the most serious diseases in the world.
Most people with such a disease lack access to adequate care, which highlights
the importance of training models for the diagnosis and treatment of mental
health disorders. However, in the mental health domain, privacy concerns limit
the accessibility of personalized treatment data, making it challenging to
build powerful models. In this paper, we introduce MentalArena, a self-play
framework to train language models by generating domain-specific personalized
data, where we obtain a better model capable of making a personalized diagnosis
and treatment (as a therapist) and providing information (as a patient). To
accurately model human-like mental health patients, we devise Symptom Encoder,
which simulates a real patient from both cognition and behavior perspectives.
To address intent bias during patient-therapist interactions, we propose
Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and
dynamically manage the dialogue between patient and therapist according to the
identified deviations. We evaluated MentalArena against 6 benchmarks, including
biomedicalQA and mental health tasks, compared to 6 advanced models. Our
models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform
their counterparts, including GPT-4o. We hope that our work can inspire future
research on personalized care. Code is available in
https://github.com/Scarelette/MentalArena/tree/main

ÊëòË¶ÅÔºöÂøÉÁêÜÂÅ•Â∫∑ÈöúÁ§ôÊòØ‰∏ñÁïå‰∏äÊúÄÂö¥ÈáçÁöÑÁñæÁóÖ‰πã‰∏Ä„ÄÇ
Â§ßÂ§öÊï∏ÊÇ£ÊúâÈÄôÁ®ÆÁñæÁóÖÁöÑ‰∫∫ÁÑ°Ê≥ïÁç≤ÂæóÈÅ©Áï∂ÁöÑÁÖßË≠∑ÔºåÈÄôÂá∏È°Ø‰∫ÜË®ìÁ∑¥Ê®°Âûã‰ª•Ë®∫Êñ∑ÂíåÊ≤ªÁôÇÂøÉÁêÜÂÅ•Â∫∑ÈöúÁ§ôÁöÑÈáçË¶ÅÊÄß„ÄÇÁÑ∂ËÄåÔºåÂú®ÂøÉÁêÜÂÅ•Â∫∑È†òÂüüÔºåÈö±ÁßÅÂïèÈ°åÈôêÂà∂‰∫ÜÂÄã‰∫∫ÂåñÊ≤ªÁôÇË≥áÊñôÁöÑÂèØÂèäÊÄßÔºåÈÄô‰ΩøÂæóÂª∫Á´ãÂº∑Â§ßÁöÑÊ®°ÂûãËÆäÂæóÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü MentalArenaÔºå‰∏ÄÂÄãËá™Áé©Ê°ÜÊû∂ÔºåÈÄöÈÅéÁîüÊàêÁâπÂÆöÈ†òÂüüÁöÑÂÄã‰∫∫ÂåñË≥áÊñô‰æÜË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÔºåÂú®ÂÖ∂‰∏≠ÊàëÂÄëÁç≤Âæó‰∫Ü‰∏ÄÂÄãÊõ¥Â•ΩÁöÑÊ®°ÂûãÔºåËÉΩÂ§†ÈÄ≤Ë°åÂÄã‰∫∫ÂåñË®∫Êñ∑ÂíåÊ≤ªÁôÇÔºà‰ΩúÁÇ∫Ê≤ªÁôÇÂ∏´Ôºâ‰∏¶Êèê‰æõË≥áË®äÔºà‰ΩúÁÇ∫ÊÇ£ËÄÖÔºâ„ÄÇÁÇ∫‰∫ÜÊ∫ñÁ¢∫Ê®°Êì¨È°û‰ºº‰∫∫È°ûÁöÑÂøÉÁêÜÂÅ•Â∫∑ÊÇ£ËÄÖÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÁóáÁãÄÁ∑®Á¢ºÂô®ÔºåÂÆÉÂæûË™çÁü•ÂíåË°åÁÇ∫ÁöÑËßíÂ∫¶Ê®°Êì¨‰∏ÄÂÄãÁúüÂØ¶ÁöÑÊÇ£ËÄÖ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÊÇ£ËÄÖËàáÊ≤ªÁôÇÂ∏´‰∫íÂãïÊúüÈñìÁöÑÊÑèÂúñÂÅèÂ∑ÆÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁóáÁãÄËß£Á¢ºÂô®ÔºåÂ∞áË®∫Êñ∑Âá∫ÁöÑÁóáÁãÄËàáÁ∑®Á¢ºÁóáÁãÄÈÄ≤Ë°åÊØîËºÉÔºå‰∏¶Ê†πÊìöË≠òÂà•Âá∫ÁöÑÂÅèÂ∑ÆÂãïÊÖãÁÆ°ÁêÜÊÇ£ËÄÖËàáÊ≤ªÁôÇÂ∏´‰πãÈñìÁöÑÂ∞çË©±„ÄÇÊàëÂÄëÈáùÂ∞ç 6 ÂÄãÂü∫Ê∫ñÂ∞ç MentalArena ÈÄ≤Ë°å‰∫ÜË©ï‰º∞ÔºåÂåÖÊã¨ÁîüÁâ©ÈÜ´Â≠∏ÂïèÁ≠îÂíåÂøÉÁêÜÂÅ•Â∫∑‰ªªÂãôÔºå‰∏¶Ëàá 6 ÂÄãÂÖàÈÄ≤Ê®°ÂûãÈÄ≤Ë°å‰∫ÜÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú® GPT-3.5 Âíå Llama-3-8b ‰∏äÈÉΩÈÄ≤Ë°å‰∫ÜÂæÆË™øÔºåÈ°ØËëóÂÑ™ÊñºÂÖ∂Â∞çÊáâÊ®°ÂûãÔºåÂåÖÊã¨ GPT-4o„ÄÇÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÁ†îÁ©∂ËÉΩÊøÄÂãµÊú™‰æÜÂ∞çÂÄã‰∫∫ÂåñÁÖßË≠∑ÁöÑÁ†îÁ©∂„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/Scarelette/MentalArena/tree/main ‰∏≠Áç≤Âæó

##### **An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion**
2410.06818v1 by Narjes Benameur, Ramzi Mahmoudi, Mohamed Deriche, Amira fayouka, Imene Masmoudi, Nessrine Zoghlami

Left ventricular ejection fraction (LVEF) is the most important clinical
parameter of cardiovascular function. The accuracy in estimating this parameter
is highly dependent upon the precise segmentation of the left ventricle (LV)
structure at the end diastole and systole phases. Therefore, it is crucial to
develop robust algorithms for the precise segmentation of the heart structure
during different phases. Methodology: In this work, an improved 3D UNet model
is introduced to segment the myocardium and LV, while excluding papillary
muscles, as per the recommendation of the Society for Cardiovascular Magnetic
Resonance. For the practical testing of the proposed framework, a total of
8,400 cardiac MRI images were collected and analysed from the military hospital
in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance
metrics, we used the Dice coefficient and the F1 score for validation/testing
of the LV and the myocardium segmentation. Results: The data was split into
70%, 10%, and 20% for training, validation, and testing, respectively. It is
worth noting that the proposed segmentation model was tested across three axis
views: basal, medio basal and apical at two different cardiac phases: end
diastole and end systole instances. The experimental results showed a Dice
index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end
diastolic and systolic phases, respectively. Additionally, clinical evaluation
outcomes revealed a significant difference in the LVEF and other clinical
parameters when the papillary muscles were included or excluded.

ÊëòË¶ÅÔºöÂ∑¶ÂøÉÂÆ§Â∞ÑË°ÄÂàÜÊï∏ (LVEF) ÊòØÂøÉË°ÄÁÆ°ÂäüËÉΩÊúÄÈáçË¶ÅÁöÑËá®Â∫äÂèÉÊï∏„ÄÇ‰º∞Ë®àÊ≠§ÂèÉÊï∏ÁöÑÊ∫ñÁ¢∫ÊÄßÈ´òÂ∫¶‰æùË≥¥ÊñºÂ∑¶ÂøÉÂÆ§ (LV) ÁµêÊßãÂú®ËàíÂºµÊú´ÊúüÂíåÊî∂Á∏ÆÊúüÁöÑÁ≤æÁ¢∫ÂàÜÂâ≤„ÄÇÂõ†Ê≠§ÔºåÈñãÁôºÁî®ÊñºÁ≤æÁ¢∫ÂàÜÂâ≤‰∏çÂêåÊôÇÊúüÂøÉËáüÁµêÊßãÁöÑÂº∑ÂÅ•ÊºîÁÆóÊ≥ïËá≥ÈóúÈáçË¶Å„ÄÇÊñπÊ≥ïÔºöÂú®Ê≠§Â∑•‰Ωú‰∏≠ÔºåÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãÊîπËâØÁöÑ 3D UNet Ê®°Âûã‰æÜÂàÜÂâ≤ÂøÉËÇåÂíåÂ∑¶ÂøÉÂÆ§ÔºåÂêåÊôÇÊ†πÊìöÂøÉË°ÄÁÆ°Á£ÅÂÖ±ÊåØÂ≠∏ÊúÉÁöÑÂª∫Ë≠∞ÊéíÈô§‰π≥È†≠ËÇå„ÄÇÁÇ∫‰∫ÜÂ∞çÊèêÂá∫ÁöÑÊû∂ÊßãÈÄ≤Ë°åÂØ¶ÈöõÊ∏¨Ë©¶ÔºåÂæûÁ™ÅÂ∞ºÊñØÁöÑËªç‰∫ãÈÜ´Èô¢ (HMPIT) ÂíåÊµÅË°åÁöÑ ACDC ÂÖ¨ÂÖ±Ë≥áÊñôÈõÜÊî∂ÈõÜ‰∏¶ÂàÜÊûê‰∫ÜÁ∏ΩÂÖ± 8,400 ÂºµÂøÉËáü MRI ÂΩ±ÂÉè„ÄÇ‰ΩúÁÇ∫ÊïàËÉΩÊåáÊ®ôÔºåÊàëÂÄë‰ΩøÁî® Dice ‰øÇÊï∏Âíå F1 ÂàÜÊï∏‰æÜÈ©óË≠â/Ê∏¨Ë©¶Â∑¶ÂøÉÂÆ§ÂíåÂøÉËÇåÂàÜÂâ≤„ÄÇÁµêÊûúÔºöË≥áÊñôË¢´ÂàÜÊàê 70%„ÄÅ10% Âíå 20% ÂàÜÂà•Áî®ÊñºË®ìÁ∑¥„ÄÅÈ©óË≠âÂíåÊ∏¨Ë©¶„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊâÄÊèêÂá∫ÁöÑÂàÜÂâ≤Ê®°ÂûãÂú®‰∏âÂÄãËª∏ÂêëË¶ñÂúñ‰∏≠ÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶ÔºöÂü∫Â∫ï„ÄÅ‰∏≠Âü∫Â∫ïÂíåÂøÉÂ∞ñÔºåÂú®ÂÖ©ÂÄã‰∏çÂêåÁöÑÂøÉËáüÊôÇÊúüÔºöËàíÂºµÊú´ÊúüÂíåÊî∂Á∏ÆÊú´Êúü„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂú®ËàíÂºµÊú´ÊúüÂíåÊî∂Á∏ÆÊúüÔºåDice ÊåáÊï∏ÂàÜÂà•ÁÇ∫ 0.965 Âíå 0.945ÔºåF1 ÂàÜÊï∏ÂàÜÂà•ÁÇ∫ 0.801 Âíå 0.799„ÄÇÊ≠§Â§ñÔºåËá®Â∫äË©ï‰º∞ÁµêÊûúÈ°ØÁ§∫ÔºåÁï∂‰π≥È†≠ËÇåË¢´Á¥çÂÖ•ÊàñÊéíÈô§ÊôÇÔºåLVEF ÂíåÂÖ∂‰ªñËá®Â∫äÂèÉÊï∏Â≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇ

##### **Deep Learning for Surgical Instrument Recognition and Segmentation in Robotic-Assisted Surgeries: A Systematic Review**
2410.07269v1 by Fatimaelzahraa Ali Ahmed, Mahmoud Yousef, Mariam Ali Ahmed, Hasan Omar Ali, Anns Mahboob, Hazrat Ali, Zubair Shah, Omar Aboumarzouk, Abdulla Al Ansari, Shidin Balakrishnan

Applying deep learning (DL) for annotating surgical instruments in
robot-assisted minimally invasive surgeries (MIS) represents a significant
advancement in surgical technology. This systematic review examines 48 studies
that and advanced DL methods and architectures. These sophisticated DL models
have shown notable improvements in the precision and efficiency of detecting
and segmenting surgical tools. The enhanced capabilities of these models
support various clinical applications, including real-time intraoperative
guidance, comprehensive postoperative evaluations, and objective assessments of
surgical skills. By accurately identifying and segmenting surgical instruments
in video data, DL models provide detailed feedback to surgeons, thereby
improving surgical outcomes and reducing complication risks. Furthermore, the
application of DL in surgical education is transformative. The review
underscores the significant impact of DL on improving the accuracy of skill
assessments and the overall quality of surgical training programs. However,
implementing DL in surgical tool detection and segmentation faces challenges,
such as the need for large, accurately annotated datasets to train these models
effectively. The manual annotation process is labor-intensive and
time-consuming, posing a significant bottleneck. Future research should focus
on automating the detection and segmentation process and enhancing the
robustness of DL models against environmental variations. Expanding the
application of DL models across various surgical specialties will be essential
to fully realize this technology's potential. Integrating DL with other
emerging technologies, such as augmented reality (AR), also offers promising
opportunities to further enhance the precision and efficacy of surgical
procedures.

ÊëòË¶ÅÔºöÊáâÁî®Ê∑±Â∫¶Â≠∏Áøí (DL) ‰æÜË®ªËß£Ê©üÂô®‰∫∫ËºîÂä©ÂæÆÂâµÊâãË°ì (MIS) ‰∏≠ÁöÑÂ§ñÁßëÂô®Ê¢∞‰ª£Ë°®‰∫ÜÂ§ñÁßëÊäÄË°ìÁöÑÈáçÂ§ßÈÄ≤Ê≠•„ÄÇÈÄôÈ†ÖÁ≥ªÁµ±ÊÄßÂõûÈ°ßÂØ©Êü•‰∫Ü 48 È†ÖÁ†îÁ©∂ÔºåÈÄô‰∫õÁ†îÁ©∂Êé°Áî®ÂÖàÈÄ≤ÁöÑ DL ÊñπÊ≥ïÂíåÊû∂Êßã„ÄÇÈÄô‰∫õË§áÈõúÁöÑ DL Ê®°ÂûãÂú®ÂÅµÊ∏¨ÂíåÂàÜÂâ≤Â§ñÁßëÊâãË°ìÂ∑•ÂÖ∑ÁöÑÁ≤æÊ∫ñÂ∫¶ÂíåÊïàÁéáÊñπÈù¢Â∑≤Â±ïÁèæÂá∫È°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÈÄô‰∫õÊ®°ÂûãÂ¢ûÂº∑ÁöÑÂäüËÉΩÊîØÊè¥ÂêÑÁ®ÆËá®Â∫äÊáâÁî®ÔºåÂåÖÊã¨Âç≥ÊôÇË°ì‰∏≠ÂºïÂ∞é„ÄÅÂÖ®Èù¢ÁöÑË°ìÂæåË©ï‰º∞ÂíåÂ§ñÁßëÊäÄË°ìÁöÑÂÆ¢ËßÄË©ï‰º∞„ÄÇÈÄèÈÅéÂú®ÂΩ±ÁâáË≥áÊñô‰∏≠Á≤æÁ¢∫Ë≠òÂà•ÂíåÂàÜÂâ≤Â§ñÁßëÂô®Ê¢∞ÔºåDL Ê®°ÂûãËÉΩÊèê‰æõË©≥Á¥∞ÁöÑÂõûÈ•ãÁµ¶Â§ñÁßëÈÜ´ÁîüÔºåÈÄ≤ËÄåÊîπÂñÑÊâãË°ìÁµêÊûú‰∏¶Èôç‰Ωé‰ΩµÁôºÁóáÈ¢®Èö™„ÄÇÊ≠§Â§ñÔºåDL Âú®Â§ñÁßëÊïôËÇ≤‰∏≠ÁöÑÊáâÁî®ÂÖ∑ÊúâËÆäÈù©ÊÄß„ÄÇÈÄôÈ†ÖÂõûÈ°ßÂº∑Ë™ø‰∫Ü DL Âú®ÊîπÂñÑÊäÄËÉΩË©ï‰º∞Ê∫ñÁ¢∫Â∫¶ÂíåÊï¥È´îÂ§ñÁßëË®ìÁ∑¥Ë®àÁï´ÂìÅË≥™ÊñπÈù¢ÁöÑÈáçÂ§ßÂΩ±Èüø„ÄÇÁÑ∂ËÄåÔºåÂú®Â§ñÁßëÂ∑•ÂÖ∑ÂÅµÊ∏¨ÂíåÂàÜÂâ≤‰∏≠ÂØ¶ÊñΩ DL Èù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶ÇÈúÄË¶ÅÂ§ßÈáèÊ∫ñÁ¢∫Ë®ªËß£ÁöÑË≥áÊñôÈõÜÊâçËÉΩÊúâÊïàË®ìÁ∑¥ÈÄô‰∫õÊ®°Âûã„ÄÇÊâãÂãïË®ªËß£ÈÅéÁ®ãËÄóÊôÇ‰∏îË≤ªÂäõÔºåÊßãÊàê‰∫Ü‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÁì∂È†∏„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂ÊáâÂ∞àÊ≥®ÊñºËá™ÂãïÂåñÂÅµÊ∏¨ÂíåÂàÜÂâ≤ÊµÅÁ®ãÔºå‰∏¶Â¢ûÂº∑ DL Ê®°ÂûãÂ∞çÁí∞Â¢ÉËÆäÂåñÁöÑÈ≠ØÊ£íÊÄß„ÄÇÊì¥Â±ï DL Ê®°ÂûãÂú®ÂêÑÁ®ÆÂ§ñÁßëÂ∞àÁßëÁöÑÊáâÁî®Â∞çÊñºÂÖÖÂàÜÂØ¶ÁèæÈÄôÈ†ÖÊäÄË°ìÁöÑÊΩõÂäõËá≥ÈóúÈáçË¶Å„ÄÇÂ∞á DL ËàáÂÖ∂‰ªñÊñ∞ËààÊäÄË°ìÔºà‰æãÂ¶ÇÊì¥Â¢ûÂØ¶Â¢É (AR)ÔºâÊï¥Âêà‰πüÊèê‰æõ‰∫ÜÊúâÊúõÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Â§ñÁßëÊâãË°ìÁ≤æÊ∫ñÂ∫¶ÂíåÊïàÁéáÁöÑÊ©üÊúÉ„ÄÇ

##### **Multimodal Representation Learning using Adaptive Graph Construction**
2410.06395v1 by Weichen Huang

Multimodal contrastive learning train neural networks by levergaing data from
heterogeneous sources such as images and text. Yet, many current multimodal
learning architectures cannot generalize to an arbitrary number of modalities
and need to be hand-constructed. We propose AutoBIND, a novel contrastive
learning framework that can learn representations from an arbitrary number of
modalites through graph optimization. We evaluate AutoBIND on Alzhiemer's
disease detection because it has real-world medical applicability and it
contains a broad range of data modalities. We show that AutoBIND outperforms
previous methods on this task, highlighting the generalizablility of the
approach.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÖãÂ∞çÊØîÂ≠∏ÁøíÈÄèÈÅéÂà©Áî®‰æÜËá™Áï∞Ë≥™‰æÜÊ∫êÔºà‰æãÂ¶ÇÂúñÂÉèÂíåÊñáÂ≠óÔºâÁöÑË≥áÊñô‰æÜË®ìÁ∑¥Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÁÑ∂ËÄåÔºåË®±Â§öÁõÆÂâçÁöÑÂ§öÊ®°ÊÖãÂ≠∏ÁøíÊû∂ÊßãÁÑ°Ê≥ïÊé®Âª£Âà∞‰ªªÊÑèÊï∏ÈáèÁöÑÊ®°ÊÖãÔºå‰∏¶‰∏îÈúÄË¶ÅÊâãÂãïÂª∫Êßã„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü AutoBINDÔºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ∞çÊØîÂ≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÂèØ‰ª•ÈÄèÈÅéÂúñÂΩ¢ÊúÄ‰Ω≥ÂåñÂæû‰ªªÊÑèÊï∏ÈáèÁöÑÊ®°ÊÖã‰∏≠Â≠∏ÁøíË°®Âæµ„ÄÇÊàëÂÄëÂú®ÈòøËå≤Êµ∑ÈªòÁóáÁöÑÂÅµÊ∏¨‰∏äË©ï‰º∞ AutoBINDÔºåÂõ†ÁÇ∫ÂÆÉÂÖ∑ÊúâÂØ¶ÈöõÁöÑÈÜ´ÁôÇÊáâÁî®ÊÄßÔºåËÄå‰∏îÂÆÉÂåÖÂê´Âª£Ê≥õÁöÑË≥áÊñôÊ®°ÊÖã„ÄÇÊàëÂÄëÂ±ïÁ§∫ AutoBIND Âú®ÈÄôÈ†Ö‰ªªÂãô‰∏äÂÑ™ÊñºÂÖàÂâçÁöÑÂêÑÁ®ÆÊñπÊ≥ïÔºåÁ™ÅÈ°Ø‰∫ÜÊ≠§ÊñπÊ≥ïÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

##### **Skin Cancer Machine Learning Model Tone Bias**
2410.06385v1 by James Pope, Md Hassanuzzaman, Mingmar Sherpa, Omar Emara, Ayush Joshi, Nirmala Adhikari

Background: Many open-source skin cancer image datasets are the result of
clinical trials conducted in countries with lighter skin tones. Due to this
tone imbalance, machine learning models derived from these datasets can perform
well at detecting skin cancer for lighter skin tones. Any tone bias in these
models could introduce fairness concerns and reduce public trust in the
artificial intelligence health field.
  Methods: We examine a subset of images from the International Skin Imaging
Collaboration (ISIC) archive that provide tone information. The subset has a
significant tone imbalance. These imbalances could explain a model's tone bias.
To address this, we train models using the imbalanced dataset and a balanced
dataset to compare against. The datasets are used to train a deep convolutional
neural network model to classify the images as malignant or benign. We then
evaluate the models' disparate impact, based on selection rate, relative to
dark or light skin tone.
  Results: Using the imbalanced dataset, we found that the model is
significantly better at detecting malignant images in lighter tone resulting in
a disparate impact of 0.577. Using the balanced dataset, we found that the
model is also significantly better at detecting malignant images in lighter
versus darker tones with a disparate impact of 0.684. Using the imbalanced or
balanced dataset to train the model still results in a disparate impact well
below the standard threshold of 0.80 which suggests the model is biased with
respect to skin tone.
  Conclusion: The results show that typical skin cancer machine learning models
can be tone biased. These results provide evidence that diagnosis or tone
imbalance is not the cause of the bias. Other techniques will be necessary to
identify and address the bias in these models, an area of future investigation.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöË®±Â§öÈñãÊîæÂéüÂßãÁ¢ºÁöÆËÜöÁôåÂúñÂÉèË≥áÊñôÈõÜÊòØÊ†πÊìöÂú®ËÜöËâ≤ËºÉÊ∑∫ÁöÑÂúãÂÆ∂ÈÄ≤Ë°åÁöÑËá®Â∫äË©¶È©óÁöÑÁµêÊûú„ÄÇÁî±ÊñºÈÄôÁ®ÆËâ≤Ë™ø‰∏çÂπ≥Ë°°ÔºåÂæûÈÄô‰∫õË≥áÊñôÈõÜÊ¥æÁîüÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®Ê™¢Ê∏¨ËÜöËâ≤ËºÉÊ∑∫ÁöÑÁöÆËÜöÁôåÊñπÈù¢Ë°®ÁèæËâØÂ•Ω„ÄÇÈÄô‰∫õÊ®°Âûã‰∏≠ÁöÑ‰ªª‰ΩïËâ≤Ë™øÂÅèÂ∑ÆÈÉΩÂèØËÉΩÂºïÁôºÂÖ¨Âπ≥ÊÄßÁöÑÂïèÈ°åÔºå‰∏¶Èôç‰ΩéÂÖ¨ÁúæÂ∞ç‰∫∫Â∑•Êô∫ÊÖßÂÅ•Â∫∑È†òÂüüÁöÑ‰ø°‰ªª„ÄÇ
ÊñπÊ≥ïÔºöÊàëÂÄëÊ™¢Êü•‰∫ÜÂúãÈöõÁöÆËÜöÂΩ±ÂÉèÂêà‰ΩúÁµÑÁπî (ISIC) Ê™îÊ°àÂ∫´‰∏≠Êèê‰æõËâ≤Ë™øË≥áË®äÁöÑÂúñÂÉèÂ≠êÈõÜ„ÄÇË©≤Â≠êÈõÜÂÖ∑ÊúâÈ°ØËëóÁöÑËâ≤Ë™ø‰∏çÂπ≥Ë°°„ÄÇÈÄô‰∫õ‰∏çÂπ≥Ë°°ÂèØËÉΩËß£Èáã‰∫ÜÊ®°ÂûãÁöÑËâ≤Ë™øÂÅèÂ∑Æ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄë‰ΩøÁî®‰∏çÂπ≥Ë°°ÁöÑË≥áÊñôÈõÜÂíåÂπ≥Ë°°ÁöÑË≥áÊñôÈõÜË®ìÁ∑¥Ê®°ÂûãÔºå‰ª•‰æøÈÄ≤Ë°åÊØîËºÉ„ÄÇÈÄô‰∫õË≥áÊñôÈõÜÁî®ÊñºË®ìÁ∑¥Ê∑±Â∫¶Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºåÂ∞áÂΩ±ÂÉèÂàÜÈ°ûÁÇ∫ÊÉ°ÊÄßÊàñËâØÊÄß„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊ†πÊìöÈÅ∏ÊìáÁéáË©ï‰º∞Ê®°ÂûãÁöÑ‰∏çÂêåÂΩ±ÈüøÔºåÁõ∏Â∞çÊñºÊ∑±Ëâ≤ÊàñÊ∑∫Ëâ≤ËÜöËâ≤„ÄÇ
ÁµêÊûúÔºö‰ΩøÁî®‰∏çÂπ≥Ë°°ÁöÑË≥áÊñôÈõÜÔºåÊàëÂÄëÁôºÁèæË©≤Ê®°ÂûãÂú®Ê™¢Ê∏¨Ê∑∫Ëâ≤Ëâ≤Ë™ø‰∏≠ÁöÑÊÉ°ÊÄßÂΩ±ÂÉèÊñπÈù¢È°ØËëóÂÑ™ÊñºÂú®Ê∑±Ëâ≤Ëâ≤Ë™ø‰∏≠Ê™¢Ê∏¨ÊÉ°ÊÄßÂΩ±ÂÉèÔºåÂ∞éËá¥ 0.577 ÁöÑ‰∏çÂêåÂΩ±Èüø„ÄÇ‰ΩøÁî®Âπ≥Ë°°ÁöÑË≥áÊñôÈõÜÔºåÊàëÂÄëÁôºÁèæË©≤Ê®°ÂûãÂú®Ê™¢Ê∏¨Ê∑∫Ëâ≤Ëâ≤Ë™ø‰∏≠ÁöÑÊÉ°ÊÄßÂΩ±ÂÉèÊñπÈù¢‰πüÈ°ØËëóÂÑ™ÊñºÊ∑±Ëâ≤Ëâ≤Ë™øÔºå‰∏çÂêåÂΩ±ÈüøÁÇ∫ 0.684„ÄÇ‰ΩøÁî®‰∏çÂπ≥Ë°°ÊàñÂπ≥Ë°°ÁöÑË≥áÊñôÈõÜË®ìÁ∑¥Ê®°Âûã‰ªçÁÑ∂ÊúÉÂ∞éËá¥‰∏çÂêåÂΩ±ÈüøÔºåÈÅ†‰ΩéÊñº 0.80 ÁöÑÊ®ôÊ∫ñÈñæÂÄºÔºåÈÄôË°®ÊòéÊ®°ÂûãÂú®ËÜöËâ≤ÊñπÈù¢ÊúâÂÅèÂ∑Æ„ÄÇ
ÁµêË´ñÔºöÁµêÊûúË°®ÊòéÔºåÂÖ∏ÂûãÁöÑÁöÆËÜöÁôåÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂèØËÉΩÊúÉÁî¢ÁîüËâ≤Ë™øÂÅèÂ∑Æ„ÄÇÈÄô‰∫õÁµêÊûúÊèê‰æõ‰∫ÜË≠âÊìöË°®ÊòéÔºåË®∫Êñ∑ÊàñËâ≤Ë™ø‰∏çÂπ≥Ë°°‰∏¶ÈùûÈÄ†ÊàêÂÅèÂ∑ÆÁöÑÂéüÂõ†„ÄÇÈúÄË¶ÅÂÖ∂‰ªñÊäÄË°ì‰æÜË≠òÂà•ÂíåËß£Ê±∫ÈÄô‰∫õÊ®°Âûã‰∏≠ÁöÑÂÅèÂ∑ÆÔºåÈÄôÊòØÊú™‰æÜÁ†îÁ©∂ÁöÑ‰∏ÄÂÄãÈ†òÂüü„ÄÇ</paragraph>

##### **HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid**
2410.06370v2 by Hemank Lamba, Anton Abilov, Ke Zhang, Elizabeth M. Olson, Henry k. Dambanemuya, Jo√£o c. B√°rcia, David S. Batista, Christina Wille, Aoife Cahill, Joel Tetreault, Alex Jaimes

Humanitarian organizations can enhance their effectiveness by analyzing data
to discover trends, gather aggregated insights, manage their security risks,
support decision-making, and inform advocacy and funding proposals. However,
data about violent incidents with direct impact and relevance for humanitarian
aid operations is not readily available. An automatic data collection and
NLP-backed classification framework aligned with humanitarian perspectives can
help bridge this gap. In this paper, we present HumVI - a dataset comprising
news articles in three languages (English, French, Arabic) containing instances
of different types of violent incidents categorized by the humanitarian sector
they impact, e.g., aid security, education, food security, health, and
protection. Reliable labels were obtained for the dataset by partnering with a
data-backed humanitarian organization, Insecurity Insight. We provide multiple
benchmarks for the dataset, employing various deep learning architectures and
techniques, including data augmentation and mask loss, to address different
task-related challenges, e.g., domain expansion. The dataset is publicly
available at https://github.com/dataminr-ai/humvi-dataset.

ÊëòË¶ÅÔºö‰∫∫ÈÅì‰∏ªÁæ©ÁµÑÁπîÂèØÈÄèÈÅéÂàÜÊûêË≥áÊñô‰æÜÊèêÂçáÂÖ∂ÊàêÊïàÔºå‰ª•ÊâæÂá∫Ë∂®Âã¢„ÄÅÊî∂ÈõÜÂΩôÊï¥ÁöÑË¶ãËß£„ÄÅÁÆ°ÁêÜÂÖ∂ÂÆâÂÖ®È¢®Èö™„ÄÅÊîØÊè¥Ê±∫Á≠ñÂà∂ÂÆöÔºå‰ª•ÂèäÂëäÁü•ÂÄ°Ë≠∞ÂíåÂãüÊ¨æÊèêÊ°à„ÄÇ‰∏çÈÅéÔºåËàá‰∫∫ÈÅì‰∏ªÁæ©Êè¥Âä©Ë°åÂãïÁõ¥Êé•Áõ∏Èóú‰∏îÂÖ∑ÂΩ±ÈüøÂäõÁöÑÊö¥Âäõ‰∫ã‰ª∂Ë≥áÊñô‰∏¶‰∏çÂÆπÊòìÂèñÂæó„ÄÇ‰∏ÄÂÄãËàá‰∫∫ÈÅì‰∏ªÁæ©ËßÄÈªû‰∏ÄËá¥ÁöÑËá™ÂãïË≥áÊñôÊî∂ÈõÜÂíå NLP ÊîØÊè¥ÂàÜÈ°ûÊû∂ÊßãÔºåÊúâÂä©ÊñºÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ HumVI - ‰∏ÄÂÄãÂåÖÂê´‰∏âÁ®ÆË™ûË®ÄÔºàËã±Ë™û„ÄÅÊ≥ïË™û„ÄÅÈòøÊãâ‰ºØË™ûÔºâÊñ∞ËÅûÊñáÁ´†ÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∫∫ÈÅì‰∏ªÁæ©ÈÉ®ÈñÄÂàÜÈ°ûÁöÑ‰∏çÂêåÈ°ûÂûãÊö¥Âäõ‰∫ã‰ª∂ÂØ¶‰æãÔºå‰æãÂ¶ÇÊè¥Âä©ÂÆâÂÖ®„ÄÅÊïôËÇ≤„ÄÅÁ≥ßÈ£üÂÆâÂÖ®„ÄÅÂÅ•Â∫∑Âíå‰øùË≠∑„ÄÇÊàëÂÄëÈÄèÈÅéËàáË≥áÊñôÊîØÊè¥ÁöÑ‰∫∫ÈÅì‰∏ªÁæ©ÁµÑÁπî Insecurity Insight Âêà‰ΩúÔºåÂèñÂæóË≥áÊñôÈõÜÁöÑÂèØÈù†Ê®ôÁ±§„ÄÇÊàëÂÄëÁÇ∫Ë≥áÊñôÈõÜÊèê‰æõÂ§öÂÄãÂü∫Ê∫ñÔºåÊé°Áî®ÂêÑÁ®ÆÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÂíåÊäÄË°ìÔºåÂåÖÊã¨Ë≥áÊñôÊì¥ÂÖÖÂíåÈÅÆÁΩ©ÊêçÂ§±Ôºå‰ª•ÊáâÂ∞ç‰∏çÂêåÁöÑ‰ªªÂãôÁõ∏ÈóúÊåëÊà∞Ôºå‰æãÂ¶ÇÈ†òÂüüÊì¥ÂÖÖ„ÄÇË≥áÊñôÈõÜÂ∑≤Êñº https://github.com/dataminr-ai/humvi-dataset ÂÖ¨Èñã„ÄÇ

##### **A Comparative Study of Hybrid Models in Health Misinformation Text Classification**
2410.06311v1 by Mkululi Sikosana, Oluwaseun Ajao, Sean Maudsley-Barton

This study evaluates the effectiveness of machine learning (ML) and deep
learning (DL) models in detecting COVID-19-related misinformation on online
social networks (OSNs), aiming to develop more effective tools for countering
the spread of health misinformation during the pan-demic. The study trained and
tested various ML classifiers (Naive Bayes, SVM, Random Forest, etc.), DL
models (CNN, LSTM, hybrid CNN+LSTM), and pretrained language models
(DistilBERT, RoBERTa) on the "COVID19-FNIR DATASET". These models were
evaluated for accuracy, F1 score, recall, precision, and ROC, and used
preprocessing techniques like stemming and lemmatization. The results showed
SVM performed well, achieving a 94.41% F1-score. DL models with Word2Vec
embeddings exceeded 98% in all performance metrics (accuracy, F1 score, recall,
precision & ROC). The CNN+LSTM hybrid models also exceeded 98% across
performance metrics, outperforming pretrained models like DistilBERT and
RoBERTa. Our study concludes that DL and hybrid DL models are more effective
than conventional ML algorithms for detecting COVID-19 misinformation on OSNs.
The findings highlight the importance of advanced neural network approaches and
large-scale pretraining in misinformation detection. Future research should
optimize these models for various misinformation types and adapt to changing
OSNs, aiding in combating health misinformation.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂Ë©ï‰º∞Ê©üÂô®Â≠∏Áøí (ML) ÂíåÊ∑±Â∫¶Â≠∏Áøí (DL) Ê®°ÂûãÂú®ÂÅµÊ∏¨Á∑ö‰∏äÁ§æÁæ§Á∂≤Ë∑Ø (OSN) ‰∏äËàá COVID-19 Áõ∏ÈóúÁöÑÈåØË™§Ë®äÊÅØÁöÑÊúâÊïàÊÄßÔºåÁõÆÊ®ôÊòØÈñãÁôºÊõ¥ÊúâÊïàÁöÑÂ∑•ÂÖ∑‰æÜÂ∞çÊäóÂ§ßÊµÅË°åÊúüÈñìÂÅ•Â∫∑ÈåØË™§Ë®äÊÅØÁöÑÊï£Â∏É„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ë®ìÁ∑¥‰∏¶Ê∏¨Ë©¶‰∫ÜÂêÑÁ®Æ ML ÂàÜÈ°ûÂô®ÔºàÊ®∏Á¥†Ë≤ùÊ∞è„ÄÅSVM„ÄÅÈö®Ê©üÊ£ÆÊûóÁ≠âÔºâ„ÄÅDL Ê®°ÂûãÔºàCNN„ÄÅLSTM„ÄÅÊ∑∑Âêà CNN+LSTMÔºâÂíåÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÔºàDistilBERT„ÄÅRoBERTaÔºâÂú®„ÄåCOVID19-FNIR Ë≥áÊñôÈõÜ„Äç‰∏ä„ÄÇÈÄô‰∫õÊ®°ÂûãÁ∂ìÈÅéË©ï‰º∞ÔºåÊ®ôÊ∫ñÁÇ∫Ê∫ñÁ¢∫Â∫¶„ÄÅF1 ÂàÜÊï∏„ÄÅÂè¨ÂõûÁéá„ÄÅÁ≤æÁ¢∫Â∫¶Âíå ROCÔºå‰∏¶‰ΩøÁî®‰∫ÜË©ûÂππÂåñÂíåË©ûÂΩ¢ÈÇÑÂéüÁ≠âÂâçËôïÁêÜÊäÄË°ì„ÄÇÁµêÊûúÈ°ØÁ§∫ SVM Ë°®ÁèæËâØÂ•ΩÔºåÈÅîÂà∞ 94.41% ÁöÑ F1 ÂàÜÊï∏„ÄÇ‰ΩøÁî® Word2Vec ÂµåÂÖ•ÁöÑ DL Ê®°ÂûãÂú®ÊâÄÊúâÊïàËÉΩÊåáÊ®ôÔºàÊ∫ñÁ¢∫Â∫¶„ÄÅF1 ÂàÜÊï∏„ÄÅÂè¨ÂõûÁéá„ÄÅÁ≤æÁ¢∫Â∫¶Âíå ROCÔºâ‰∏≠ÈÉΩË∂ÖÈÅé 98%„ÄÇCNN+LSTM Ê∑∑ÂêàÊ®°ÂûãÂú®ÊâÄÊúâÊïàËÉΩÊåáÊ®ô‰∏≠‰πüË∂ÖÈÅé 98%ÔºåÂÑ™Êñº DistilBERT Âíå RoBERTa Á≠âÈ†êË®ìÁ∑¥Ê®°Âûã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêË´ñÊòØÔºåDL ÂíåÊ∑∑Âêà DL Ê®°ÂûãÊØîÂÇ≥Áµ± ML ÊºîÁÆóÊ≥ïÊõ¥ËÉΩÊúâÊïàÂÅµÊ∏¨ OSN ‰∏äÁöÑ COVID-19 ÈåØË™§Ë®äÊÅØ„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÈÄ≤ÈöéÁ•ûÁ∂ìÁ∂≤Ë∑ØÊñπÊ≥ïÂíåÈåØË™§Ë®äÊÅØÂÅµÊ∏¨‰∏≠Â§ßË¶èÊ®°È†êË®ìÁ∑¥ÁöÑÈáçË¶ÅÊÄß„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂ÊáâÈáùÂ∞çÂêÑÁ®ÆÈåØË™§Ë®äÊÅØÈ°ûÂûãÊúÄ‰Ω≥ÂåñÈÄô‰∫õÊ®°ÂûãÔºå‰∏¶ÈÅ©Êáâ‰∏çÊñ∑ËÆäÂåñÁöÑ OSNÔºåÂçîÂä©ÊâìÊìäÂÅ•Â∫∑ÈåØË™§Ë®äÊÅØ„ÄÇ

##### **Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging**
2410.10869v1 by Ryota Tozuka, Hisashi Johno, Akitomo Amakawa, Junichi Sato, Mizuki Muto, Shoichiro Seki, Atsushi Komaba, Hiroshi Onishi

Purpose: In radiology, large language models (LLMs), including ChatGPT, have
recently gained attention, and their utility is being rapidly evaluated.
However, concerns have emerged regarding their reliability in clinical
applications due to limitations such as hallucinations and insufficient
referencing. To address these issues, we focus on the latest technology,
retrieval-augmented generation (RAG), which enables LLMs to reference reliable
external knowledge (REK). Specifically, this study examines the utility and
reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for
staging lung cancer.
  Materials and methods: We summarized the current lung cancer staging
guideline in Japan and provided this as REK to NotebookLM. We then tasked
NotebookLM with staging 100 fictional lung cancer cases based on CT findings
and evaluated its accuracy. For comparison, we performed the same task using a
gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.
  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer
staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the
REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in
searching reference locations within the REK.
  Conclusion: NotebookLM successfully performed lung cancer staging by
utilizing the REK, demonstrating superior performance compared to GPT-4o.
Additionally, it provided highly accurate reference locations within the REK,
allowing radiologists to efficiently evaluate the reliability of NotebookLM's
responses and detect possible hallucinations. Overall, this study highlights
the potential of NotebookLM, a RAG-LLM, in image diagnosis.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÁöÑÔºöÂú®ÊîæÂ∞ÑÂ≠∏‰∏≠ÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÂåÖÊã¨ ChatGPTÔºåÊúÄËøëÂèóÂà∞ÈóúÊ≥®Ôºå‰∏¶‰∏îÂÆÉÂÄëÁöÑÊïàÁî®Ê≠£Ë¢´ËøÖÈÄüË©ï‰º∞„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂπªË¶∫ÂíåÂèÉËÄÉ‰∏çË∂≥Á≠âÈôêÂà∂Ôºå‰∫∫ÂÄëÈñãÂßãÈóúÊ≥®ÂÆÉÂÄëÂú®Ëá®Â∫äÊáâÁî®‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÊúÄÊñ∞ÁöÑÊäÄË°ìÔºåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)ÔºåÂÆÉ‰Ωø LLM ËÉΩÂ§†ÂèÉËÄÉÂèØÈù†ÁöÑÂ§ñÈÉ®Áü•Ë≠ò (REK)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈÄôÈ†ÖÁ†îÁ©∂Êé¢Ë®é‰∫ÜÊúÄËøëÁôºÂ∏ÉÁöÑÈÖçÂÇô RAG ÁöÑ LLM (RAG-LLM)ÔºåNotebookLMÔºåÂú®ËÇ∫ÁôåÂàÜÊúüÁöÑÊïàÁî®ÂíåÂèØÈù†ÊÄß„ÄÇÊùêÊñôÂíåÊñπÊ≥ïÔºöÊàëÂÄëÁ∏ΩÁµê‰∫ÜÊó•Êú¨Áï∂ÂâçÁöÑËÇ∫ÁôåÂàÜÊúüÊåáÂçóÔºå‰∏¶Â∞áÂÖ∂‰ΩúÁÇ∫ REK Êèê‰æõÁµ¶ NotebookLM„ÄÇÁÑ∂ÂæåÊàëÂÄëËÆì NotebookLM Ê†πÊìö CT ÁµêÊûúÂ∞ç 100 ÂÄãËôõÊßãÁöÑËÇ∫ÁôåÁóÖ‰æãÈÄ≤Ë°åÂàÜÊúüÔºå‰∏¶Ë©ï‰º∞ÂÖ∂Ê∫ñÁ¢∫ÊÄß„ÄÇÁÇ∫‰∫ÜÈÄ≤Ë°åÊØîËºÉÔºåÊàëÂÄë‰ΩøÁî®ÈªÉÈáëÊ®ôÊ∫ñ LLMÔºåGPT-4 Omni (GPT-4o) Âü∑Ë°åÁõ∏ÂêåÁöÑ‰ªªÂãôÔºåÊúâÂíåÊ≤íÊúâ REK ÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÁµêÊûúÔºöNotebookLM Âú®ËÇ∫ÁôåÂàÜÊúüÂØ¶È©ó‰∏≠ÂØ¶Áèæ‰∫Ü 86% ÁöÑË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÔºåÂÑ™Êñº GPT-4oÔºåÂæåËÄÖÂú®Êúâ REK ÁöÑÊÉÖÊ≥Å‰∏ãÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 39%ÔºåÊ≤íÊúâ REK ÁöÑÊÉÖÊ≥Å‰∏ãÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 25%„ÄÇÊ≠§Â§ñÔºåNotebookLM Âú® REK ‰∏≠ÊêúÁ¥¢ÂèÉËÄÉ‰ΩçÁΩÆÁöÑÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 95%„ÄÇÁµêË´ñÔºöNotebookLM ÈÄöÈÅéÂà©Áî® REK ÊàêÂäüÂú∞ÈÄ≤Ë°å‰∫ÜËÇ∫ÁôåÂàÜÊúüÔºåËàá GPT-4o Áõ∏ÊØîË°®ÁèæÂá∫ÂÑ™Ë∂äÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂÆÉÂú® REK ‰∏≠Êèê‰æõ‰∫ÜÈ´òÂ∫¶Ê∫ñÁ¢∫ÁöÑÂèÉËÄÉ‰ΩçÁΩÆÔºå‰ΩøÊîæÂ∞ÑÁßëÈÜ´ÁîüËÉΩÂ§†ÊúâÊïàÂú∞Ë©ï‰º∞ NotebookLM ÁöÑÈüøÊáâÁöÑÂèØÈù†ÊÄß‰∏¶Ê™¢Ê∏¨ÂèØËÉΩÁöÑÂπªË¶∫„ÄÇÁ∏ΩÁöÑ‰æÜË™™ÔºåÈÄôÈ†ÖÁ†îÁ©∂Á™ÅÂá∫‰∫Ü NotebookLMÔºå‰∏ÄÁ®Æ RAG-LLMÔºåÂú®ÂΩ±ÂÉèË®∫Êñ∑‰∏≠ÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept**
2410.10866v1 by YuXuan Wu, Bonaventure F. P. Dossou, Dianbo Liu

Large Language Models (LLMs) offer extensive knowledge across various
domains, but they may inadvertently memorize sensitive, unauthorized, or
malicious data, such as personal information in the medical and financial
sectors. Machine unlearning methods aim to remove specific information from
models after training to address this. However, current approaches require
additional model training or struggle to effectively erase particular data
points and their associated context due to LLMs' complex, dense, and continuous
nature. In this study, we propose a novel amortized unlearning approach using
codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to
decompose the activation space and regulate information flow, our method
efficiently unlearns targeted information while preserving the model's
performance on unrelated data. To the best of our knowledge, this is the first
work that successfully enables unlearning specific topics with contextual
relevance in an LLM, marking a significant step towards real-world applications
of machine unlearning.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèØÊèê‰æõË∑®‰∏çÂêåÈ†òÂüüÁöÑÂª£Ê≥õÁü•Ë≠òÔºå‰ΩÜÂÆÉÂÄëÂèØËÉΩÊúÉ‰∏çÁ∂ìÊÑèÂú∞Ë®ò‰ΩèÊïèÊÑü„ÄÅÊú™Á∂ìÊéàÊ¨äÊàñÊÉ°ÊÑèÁöÑË≥áÊñôÔºå‰æãÂ¶ÇÈÜ´ÁôÇÂíåÈáëËûçÈ†òÂüüÁöÑÂÄã‰∫∫Ë≥áË®ä„ÄÇÊ©üÂô®ÂèñÊ∂àÂ≠∏ÁøíÊñπÊ≥ïÊó®Âú®ÈÄèÈÅéÂú®Ë®ìÁ∑¥ÂæåÂæûÊ®°Âûã‰∏≠ÁßªÈô§ÁâπÂÆöË≥áË®ä‰æÜËß£Ê±∫Ê≠§ÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ‰ΩúÊ≥ïÈúÄË¶ÅÈ°çÂ§ñÊ®°ÂûãË®ìÁ∑¥ÔºåÊàñÂõ† LLM Ë§áÈõú„ÄÅÂØÜÈõÜ‰∏îÊåÅÁ∫åÁöÑÁâπÊÄßËÄåÈõ£‰ª•ÊúâÊïàÂú∞Ê∏ÖÈô§ÁâπÂÆöË≥áÊñôÈªûÂèäÂÖ∂ÈóúËÅØËÑàÁµ°„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊî§Èä∑ÂèñÊ∂àÂ≠∏ÁøíÊñπÊ≥ïÔºå‰ΩøÁî®Á¢ºÊú¨ÁâπÂæµÂíåÁ®ÄÁñèËá™ÂãïÁ∑®Á¢ºÂô® (SAE)„ÄÇÈÄèÈÅéÂà©Áî®Áì∂È†∏‰æÜÂàÜËß£ÂïüÁî®Á©∫ÈñìÂíåË¶èÁØÑË≥áË®äÊµÅÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞ÂèñÊ∂àÂ≠∏ÁøíÁõÆÊ®ôË≥áË®äÔºåÂêåÊôÇ‰øùÁïôÊ®°ÂûãÂú®‰∏çÁõ∏ÈóúË≥áÊñô‰∏äÁöÑÊïàËÉΩ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÊàêÂäüËÆì LLM ÂèñÊ∂àÂ≠∏ÁøíÁâπÂÆö‰∏ªÈ°åÂèäÂÖ∂ËÑàÁµ°Áõ∏ÈóúÊÄßÁöÑ‰ΩúÂìÅÔºåÁÇ∫Ê©üÂô®ÂèñÊ∂àÂ≠∏ÁøíÁöÑÂØ¶ÈöõÊáâÁî®ÈÇÅÂá∫‰∏ÄÂ§ßÊ≠•„ÄÇ

##### **KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server**
2410.05725v2 by Wenhao Wang, Xiaoyu Liang, Rui Ye, Jingyi Chai, Siheng Chen, Yanfeng Wang

The success of large language models (LLMs) facilitate many parties to
fine-tune LLMs on their own private data. However, this practice raises privacy
concerns due to the memorization of LLMs. Existing solutions, such as utilizing
synthetic data for substitution, struggle to simultaneously improve performance
and preserve privacy. They either rely on a local model for generation,
resulting in a performance decline, or take advantage of APIs, directly
exposing the data to API servers. To address this issue, we propose
KnowledgeSG, a novel client-server framework which enhances synthetic data
quality and improves model performance while ensuring privacy. We achieve this
by learning local knowledge from the private data with differential privacy
(DP) and distilling professional knowledge from the server. Additionally,
inspired by federated learning, we transmit models rather than data between the
client and server to prevent privacy leakage. Extensive experiments in medical
and financial domains demonstrate the effectiveness of KnowledgeSG. Our code is
now publicly available at https://github.com/wwh0411/KnowledgeSG.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊàêÂäüËÆìË®±Â§ö‰∫∫ÂèØ‰ª•ÂæÆË™ø LLM ‰ª•Á¨¶Âêà‰ªñÂÄëÁöÑÁßÅ‰∫∫Ë≥áÊñô„ÄÇÁÑ∂ËÄåÔºåÁî±Êñº LLM ÁöÑË®òÊÜ∂ÂäüËÉΩÔºåÊ≠§ÂÅöÊ≥ïÂºïÁôº‰∫ÜÈö±ÁßÅÂïèÈ°å„ÄÇÁèæÊúâÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰æãÂ¶Ç‰ΩøÁî®ÂêàÊàêË≥áÊñôÈÄ≤Ë°åÊõøÊèõÔºåÈõ£‰ª•ÂêåÊôÇÊîπÂñÑÊïàËÉΩ‰∏¶Á∂≠Ë≠∑Èö±ÁßÅ„ÄÇÂÆÉÂÄë‰æùË≥¥ÊñºÂçÄÂüüÊ®°ÂûãÈÄ≤Ë°åÁî¢ÁîüÔºåÂ∞éËá¥ÊïàËÉΩ‰∏ãÈôçÔºåÊàñÂà©Áî® APIÔºåÁõ¥Êé•Â∞áË≥áÊñôÂÖ¨ÈñãÁµ¶ API ‰º∫ÊúçÂô®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ KnowledgeSGÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂÆ¢Êà∂Á´Ø‰º∫ÊúçÂô®Êû∂ÊßãÔºåÂÆÉËÉΩÊèêÂçáÂêàÊàêË≥áÊñôÂìÅË≥™‰∏¶ÊîπÂñÑÊ®°ÂûãÊïàËÉΩÔºåÂêåÊôÇÁ¢∫‰øùÈö±ÁßÅ„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî®Â∑ÆÂàÜÈö±ÁßÅ (DP) ÂæûÁßÅ‰∫∫Ë≥áÊñô‰∏≠Â≠∏ÁøíÂçÄÂüüÁü•Ë≠òÔºå‰∏¶Âæû‰º∫ÊúçÂô®‰∏≠ËêÉÂèñÂ∞àÊ•≠Áü•Ë≠ò‰æÜÈÅîÊàêÊ≠§ÁõÆÊ®ô„ÄÇÊ≠§Â§ñÔºåÂèóÂà∞ËÅØÈÇ¶Â≠∏ÁøíÁöÑÂïüÁôºÔºåÊàëÂÄëÂÇ≥Ëº∏Ê®°ÂûãËÄåÈùûË≥áÊñôÂú®ÂÆ¢Êà∂Á´ØÂíå‰º∫ÊúçÂô®‰πãÈñìÔºå‰ª•Èò≤Ê≠¢Èö±ÁßÅÂ§ñÊ¥©„ÄÇÂú®ÈÜ´ÁôÇÂíåÈáëËûçÈ†òÂüüÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü KnowledgeSG ÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÁèæÂú®ÂÖ¨ÈñãÊñº https://github.com/wwh0411/KnowledgeSG„ÄÇ

##### **Copiloting Diagnosis of Autism in Real Clinical Scenarios via LLMs**
2410.05684v2 by Yi Jiang, Qingyang Shen, Shuzhong Lai, Shunyu Qi, Qian Zheng, Lin Yao, Yueming Wang, Gang Pan

Autism spectrum disorder(ASD) is a pervasive developmental disorder that
significantly impacts the daily functioning and social participation of
individuals. Despite the abundance of research focused on supporting the
clinical diagnosis of ASD, there is still a lack of systematic and
comprehensive exploration in the field of methods based on Large Language
Models (LLMs), particularly regarding the real-world clinical diagnostic
scenarios based on Autism Diagnostic Observation Schedule, Second Edition
(ADOS-2). Therefore, we have proposed a framework called ADOS-Copilot, which
strikes a balance between scoring and explanation and explored the factors that
influence the performance of LLMs in this task. The experimental results
indicate that our proposed framework is competitive with the diagnostic results
of clinicians, with a minimum MAE of 0.4643, binary classification F1-score of
81.79\%, and ternary classification F1-score of 78.37\%. Furthermore, we have
systematically elucidated the strengths and limitations of current LLMs in this
task from the perspectives of ADOS-2, LLMs' capabilities, language, and model
scale aiming to inspire and guide the future application of LLMs in a broader
fields of mental health disorders. We hope for more research to be transferred
into real clinical practice, opening a window of kindness to the world for
eccentric children.

ÊëòË¶ÅÔºöËá™ÈñâÁóáË≠úÁ≥ªÈöúÁ§ô (ASD) ÊòØ‰∏ÄÁ®ÆÂª£Ê≥õÁöÑÁôºÂ±ïÈöúÁ§ôÔºåÊúÉÈ°ØËëóÂΩ±ÈüøÂÄãÈ´îÁöÑÊó•Â∏∏ÁîüÊ¥ªÂäüËÉΩÂíåÁ§æ‰∫§ÂèÉËàá„ÄÇÂÑòÁÆ°ÊúâÂ§ßÈáèÁöÑÁ†îÁ©∂Â∞àÊ≥®ÊñºÊîØÊåÅ ASD ÁöÑËá®Â∫äË®∫Êñ∑Ôºå‰ΩÜÂú®Âü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊñπÊ≥ïÈ†òÂüü‰∏≠Ôºå‰ªçÁÑ∂Áº∫‰πèÁ≥ªÁµ±‰∏îÂÖ®Èù¢ÁöÑÊé¢Á¥¢ÔºåÁâπÂà•ÊòØÈóúÊñºÂü∫ÊñºËá™ÈñâÁóáË®∫Êñ∑ËßÄÂØüÈáèË°®Á¨¨‰∫åÁâàÁöÑÁúüÂØ¶‰∏ñÁïåËá®Â∫äË®∫Êñ∑ÊÉÖÂ¢É (ADOS-2)„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ ADOS-Copilot ÁöÑÊ°ÜÊû∂ÔºåÂÆÉÂú®Ë©ïÂàÜÂíåËß£Èáã‰πãÈñìÂèñÂæóÂπ≥Ë°°Ôºå‰∏¶Êé¢Ë®é‰∫ÜÂΩ±Èüø LLM Âú®Ê≠§‰ªªÂãô‰∏≠Ë°®ÁèæÁöÑÂõ†Á¥†„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊ°ÜÊû∂ËàáËá®Â∫äÈÜ´ÁîüÁöÑË®∫Êñ∑ÁµêÊûúÂÖ∑ÊúâÁ´∂Áà≠ÂäõÔºåMAE ÊúÄÂ∞èÁÇ∫ 0.4643Ôºå‰∫åÂÖÉÂàÜÈ°û F1 ÂàÜÊï∏ÁÇ∫ 81.79%Ôºå‰∏âÂÖÉÂàÜÈ°û F1 ÂàÜÊï∏ÁÇ∫ 78.37%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæû ADOS-2„ÄÅLLM ÁöÑËÉΩÂäõ„ÄÅË™ûË®ÄÂíåÊ®°ÂûãË¶èÊ®°ÁöÑËßíÂ∫¶Á≥ªÁµ±Âú∞Èó°Êòé‰∫ÜÁï∂Ââç LLM Âú®Ê≠§‰ªªÂãô‰∏≠ÁöÑÂÑ™Âã¢ÂíåÂ±ÄÈôêÊÄßÔºåÊó®Âú®ÊøÄÂãµÂíåÊåáÂ∞é LLM Âú®Êõ¥Âª£Ê≥õÁöÑÁ≤æÁ•ûÁñæÁóÖÈ†òÂüü‰∏≠ÁöÑÊú™‰æÜÊáâÁî®„ÄÇÊàëÂÄëÂ∏åÊúõÊõ¥Â§öÁöÑÁ†îÁ©∂ËÉΩËΩâÂåñÁÇ∫ÁúüÊ≠£ÁöÑËá®Â∫äÂØ¶Ë∏êÔºåÁÇ∫Âè§ÊÄ™ÁöÑÂ≠©Â≠êÂÄëÊâìÈñã‰∏ÄÊâáÈÄöÂæÄ‰∏ñÁïåÁöÑÂñÑÊÑè‰πãÁ™ó„ÄÇ

##### **NegMerge: Consensual Weight Negation for Strong Machine Unlearning**
2410.05583v1 by Hyoseo Kim, Dongyoon Han, Junsuk Choe

Machine unlearning aims to selectively remove specific knowledge from a
model. Current methods, such as task arithmetic, rely on fine-tuning models on
the forget set, generating a task vector, and subtracting it from the original
model. However, we argue the effectiveness of this approach is highly sensitive
to hyperparameter selection, necessitating careful validation to identify the
best model among many fine-tuned candidates. In this paper, we propose a novel
method that leverages all given fine-tuned models rather than selecting a
single one. By constructing task vectors from models trained with varied
hyperparameters and merging only the components of the task vectors with
consistent signs, we perform unlearning by negating the merged task vector from
the original model. Given that existing methods also utilize multiple
fine-tuned models, our approach delivers more effective unlearning without
incurring additional computational costs. We demonstrate the effectiveness of
our method on both vision-language models and standard image classification
models, showing improved unlearning performance with minimal degradation on the
retain set, outperforming state-of-the-art techniques.

ÊëòË¶ÅÔºöÊ©üÂô®ÂéªÂ≠∏ÁøíÊó®Âú®ÈÅ∏ÊìáÊÄßÂú∞ÂæûÊ®°Âûã‰∏≠ÁßªÈô§ÁâπÂÆöÁü•Ë≠ò„ÄÇÁõÆÂâçÁöÑÊñπÊ≥ïÔºå‰æãÂ¶Ç‰ªªÂãôÁÆóË°ìÔºå‰æùË≥¥ÊñºÂú®ÈÅ∫ÂøòÈõÜ‰∏äÂæÆË™øÊ®°ÂûãÔºåÁîüÊàê‰ªªÂãôÂêëÈáèÔºå‰∏¶ÂæûÂéüÂßãÊ®°Âûã‰∏≠Ê∏õÂéªÂÆÉ„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëË™çÁÇ∫ÈÄôÁ®ÆÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∞çË∂ÖÂèÉÊï∏ÈÅ∏ÊìáÈ´òÂ∫¶ÊïèÊÑüÔºåÈúÄË¶Å‰ªîÁ¥∞È©óË≠â‰ª•Âú®Ë®±Â§öÂæÆË™øÂÄôÈÅ∏ËÄÖ‰∏≠ÊâæÂá∫ÊúÄ‰Ω≥Ê®°Âûã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®ÊâÄÊúâÁµ¶ÂÆöÁöÑÂæÆË™øÊ®°ÂûãÔºåËÄå‰∏çÊòØÈÅ∏Êìá‰∏ÄÂÄã„ÄÇÈÄöÈÅé‰ΩøÁî®ÂÖ∑Êúâ‰∏çÂêåË∂ÖÂèÉÊï∏Ë®ìÁ∑¥ÁöÑÊ®°ÂûãÊßãÂª∫‰ªªÂãôÂêëÈáèÔºå‰∏¶ÂÉÖÂêà‰ΩµÂÖ∑ÊúâÁõ∏ÂêåÁ¨¶ËôüÁöÑ‰ªªÂãôÂêëÈáèÁöÑÁµÑÊàêÈÉ®ÂàÜÔºåÊàëÂÄëÈÄöÈÅéÂæûÂéüÂßãÊ®°Âûã‰∏≠Âê¶ÂÆöÂêà‰ΩµÁöÑ‰ªªÂãôÂêëÈáè‰æÜÂü∑Ë°åÂéªÂ≠∏Áøí„ÄÇÈëëÊñºÁèæÊúâÊñπÊ≥ï‰πüÂà©Áî®Â§öÂÄãÂæÆË™øÊ®°ÂûãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®‰∏çÁî¢ÁîüÈ°çÂ§ñË®àÁÆóÊàêÊú¨ÁöÑÊÉÖÊ≥Å‰∏ãÊèê‰æõ‰∫ÜÊõ¥ÊúâÊïàÁöÑÂéªÂ≠∏Áøí„ÄÇÊàëÂÄëÂú®Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÂíåÊ®ôÊ∫ñÂúñÂÉèÂàÜÈ°ûÊ®°Âûã‰∏äÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÈ°ØÁ§∫Âá∫ÊîπÈÄ≤ÁöÑÂéªÂ≠∏ÁøíÊÄßËÉΩÔºåÂêåÊôÇÂ∞ç‰øùÁïôÈõÜÁöÑ‰∏ãÈôçÂπÖÂ∫¶ÊúÄÂ∞èÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊäÄË°ì„ÄÇ

##### **AI-Driven Early Mental Health Screening with Limited Data: Analyzing Selfies of Pregnant Women**
2410.05450v1 by Gustavo A. Bas√≠lio, Thiago B. Pereira, Alessandro L. Koerich, Ludmila Dias, Maria das Gra√ßas da S. Teixeira, Rafael T. Sousa, Wilian H. Hisatugu, Amanda S. Mota, Anilton S. Garcia, Marco Aur√©lio K. Galletta, Hermano Tavares, Thiago M. Paix√£o

Major Depressive Disorder and anxiety disorders affect millions globally,
contributing significantly to the burden of mental health issues. Early
screening is crucial for effective intervention, as timely identification of
mental health issues can significantly improve treatment outcomes. Artificial
intelligence (AI) can be valuable for improving the screening of mental
disorders, enabling early intervention and better treatment outcomes. AI-driven
screening can leverage the analysis of multiple data sources, including facial
features in digital images. However, existing methods often rely on controlled
environments or specialized equipment, limiting their broad applicability. This
study explores the potential of AI models for ubiquitous depression-anxiety
screening given face-centric selfies. The investigation focuses on high-risk
pregnant patients, a population that is particularly vulnerable to mental
health issues. To cope with limited training data resulting from our clinical
setup, pre-trained models were utilized in two different approaches:
fine-tuning convolutional neural networks (CNNs) originally designed for facial
expression recognition and employing vision-language models (VLMs) for
zero-shot analysis of facial expressions. Experimental results indicate that
the proposed VLM-based method significantly outperforms CNNs, achieving an
accuracy of 77.6% and an F1-score of 56.0%. Although there is significant room
for improvement, the results suggest that VLMs can be a promising approach for
mental health screening, especially in scenarios with limited data.

ÊëòË¶ÅÔºöÈáçÂ∫¶ÊÜÇÈ¨±ÁóáÂíåÁÑ¶ÊÖÆÁóáÂΩ±ÈüøÂÖ®ÁêÉÊï∏ÁôæËê¨‰∫∫Ôºå
Â∞çÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°åÁöÑË≤†ÊìîÊúâÈ°ØËëóÁöÑÂΩ±Èüø„ÄÇÊó©Êúü
ÁØ©Ê™¢Â∞çÊñºÊúâÊïàÂπ≤È†êËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂèäÊôÇË≠òÂà•
ÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°åÂèØ‰ª•È°ØËëóÊîπÂñÑÊ≤ªÁôÇÁµêÊûú„ÄÇ‰∫∫Â∑•
Êô∫ÊÖß (AI) ÂèØ‰ª•ÁÇ∫ÊîπÂñÑÂøÉÁêÜÁñæÁóÖÁöÑÁØ©Ê™¢Êèê‰æõÊúâÂÉπÂÄºÁöÑÂπ´Âä©Ôºå
ÂØ¶ÁèæÊó©ÊúüÂπ≤È†êÂíåÊõ¥Â•ΩÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÇAI È©ÖÂãïÁöÑ
ÁØ©Ê™¢ÂèØ‰ª•Âà©Áî®Â§öÂÄãÊï∏Êìö‰æÜÊ∫êÁöÑÂàÜÊûêÔºåÂåÖÊã¨Êï∏‰ΩçÂΩ±ÂÉè‰∏≠ÁöÑËáâÈÉ®
ÁâπÂæµ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùË≥¥ÂèóÊéß
Áí∞Â¢ÉÊàñÂ∞àÊ•≠Ë®≠ÂÇôÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑÂª£Ê≥õÈÅ©Áî®ÊÄß„ÄÇÊú¨
Á†îÁ©∂Êé¢Ë®é AI Ê®°ÂûãÂú®ÁÑ°ÊâÄ‰∏çÂú®ÁöÑÊÜÇÈ¨±ÁóáÁÑ¶ÊÖÆÁóá
ÁØ©Ê™¢‰∏≠Ôºå‰ª•ËáâÈÉ®ÁÇ∫‰∏≠ÂøÉÁöÑËá™ÊãçÁöÑÊΩõÂäõ„ÄÇË™øÊü•ÈáçÈªûÈóúÊ≥®È´òÈ¢®Èö™
Â≠ïÂ©¶ÔºåÈÄôÊòØ‰∏ÄÂÄãÁâπÂà•ÂÆπÊòìÂèóÂà∞ÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°åÂΩ±ÈüøÁöÑ‰∫∫Áæ§„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂõ†ÊàëÂÄëÁöÑËá®Â∫ä
Ë®≠ÁΩÆËÄåÁî¢ÁîüÁöÑÊúâÈôêË®ìÁ∑¥Ë≥áÊñôÔºåÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãË¢´Áî®ÊñºÂÖ©Á®Æ‰∏çÂêåÁöÑÊñπÊ≥ïÔºö
ÂæÆË™øÂéüÊú¨Ë®≠Ë®àÁî®ÊñºËáâÈÉ®Ë°®ÊÉÖËæ®Ë≠òÁöÑÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN)Ôºå‰∏¶Êé°Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÈÄ≤Ë°å
Èõ∂Ê¨°Â≠∏ÁøíÁöÑËáâÈÉ®Ë°®ÊÉÖÂàÜÊûê„ÄÇÂØ¶È©óÁµêÊûúË°®Êòé
ÊèêÂá∫ÁöÑÂü∫Êñº VLM ÁöÑÊñπÊ≥ïÈ°ØËëóÂÑ™Êñº CNNÔºåÈÅîÂà∞ 77.6% ÁöÑÊ∫ñÁ¢∫ÁéáÂíå 56.0% ÁöÑ F1 ÂàÜÊï∏„ÄÇÂÑòÁÆ°ÊúâÈ°ØËëóÁöÑÊîπÈÄ≤Á©∫ÈñìÔºå
ÁµêÊûúË°®Êòé VLM ÂèØ‰ª•ÊàêÁÇ∫ÂøÉÁêÜÂÅ•Â∫∑ÁØ©Ê™¢ÁöÑ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇ

##### **Improving Predictor Reliability with Selective Recalibration**
2410.05407v1 by Thomas P. Zollo, Zhun Deng, Jake C. Snell, Toniann Pitassi, Richard Zemel

A reliable deep learning system should be able to accurately express its
confidence with respect to its predictions, a quality known as calibration. One
of the most effective ways to produce reliable confidence estimates with a
pre-trained model is by applying a post-hoc recalibration method. Popular
recalibration methods like temperature scaling are typically fit on a small
amount of data and work in the model's output space, as opposed to the more
expressive feature embedding space, and thus usually have only one or a handful
of parameters. However, the target distribution to which they are applied is
often complex and difficult to fit well with such a function. To this end we
propose \textit{selective recalibration}, where a selection model learns to
reject some user-chosen proportion of the data in order to allow the
recalibrator to focus on regions of the input space that can be well-captured
by such a model. We provide theoretical analysis to motivate our algorithm, and
test our method through comprehensive experiments on difficult medical imaging
and zero-shot classification tasks. Our results show that selective
recalibration consistently leads to significantly lower calibration error than
a wide range of selection and recalibration baselines.

ÊëòË¶ÅÔºö‰∏ÄÂÄãÂèØÈù†ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÁ≥ªÁµ±ÊáâË©≤ËÉΩÂ§†Ê∫ñÁ¢∫Âú∞Ë°®ÈÅîÂÖ∂Â∞çÈ†êÊ∏¨ÁöÑ‰ø°ÂøÉÔºåÈÄôÈ†ÖÂìÅË≥™Á®±ÁÇ∫Ê†°Ê∫ñ„ÄÇ‰ΩøÁî®È†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÁî¢ÁîüÂèØÈù†ÁöÑ‰ø°ÂøÉ‰º∞Ë®àÂÄºÊúÄÊúâÊïàÁöÑÊñπÊ≥ï‰πã‰∏ÄÊòØÊáâÁî®‰∫ãÂæåÈáçÊñ∞Ê†°Ê∫ñÊñπÊ≥ï„ÄÇÁÜ±ÈñÄÁöÑÈáçÊñ∞Ê†°Ê∫ñÊñπÊ≥ïÔºà‰æãÂ¶ÇÊ∫´Â∫¶Á∏ÆÊîæÔºâÈÄöÂ∏∏ÈÅ©Áî®ÊñºÂ∞ëÈáèË≥áÊñôÔºå‰∏¶Âú®Ê®°ÂûãÁöÑËº∏Âá∫Á©∫Èñì‰∏≠ÈÅã‰ΩúÔºåËÄå‰∏çÊòØÊõ¥ÂÖ∑Ë°®ÁèæÂäõÁöÑÁâπÂæµÂµåÂÖ•Á©∫ÈñìÔºåÂõ†Ê≠§ÈÄöÂ∏∏Âè™Êúâ‰∏ÄÂÄãÊàñÂ∞ëÊï∏ÂπæÂÄãÂèÉÊï∏„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÊâÄÊáâÁî®ÁöÑÁõÆÊ®ôÂàÜ‰ΩàÈÄöÂ∏∏ÂæàË§áÈõúÔºå‰∏îÈõ£‰ª•Áî®Ê≠§È°ûÂáΩÊï∏ÂÅöËâØÂ•ΩÁöÑÊì¨Âêà„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫„ÄåÈÅ∏ÊìáÊÄßÈáçÊñ∞Ê†°Ê∫ñ„ÄçÔºåÂÖ∂‰∏≠ÈÅ∏ÊìáÊ®°ÂûãÊúÉÂ≠∏ÁøíÊãíÁµï‰ΩøÁî®ËÄÖÈÅ∏ÊìáÁöÑÊüê‰∫õË≥áÊñôÊØî‰æãÔºå‰ª•ÂÖÅË®±ÈáçÊñ∞Ê†°Ê∫ñÂô®Â∞àÊ≥®ÊñºËº∏ÂÖ•Á©∫Èñì‰∏≠ËÉΩË¢´Ê≠§È°ûÊ®°ÂûãËâØÂ•ΩÊçïÊçâÂà∞ÁöÑÂçÄÂüü„ÄÇÊàëÂÄëÊèê‰æõÁêÜË´ñÂàÜÊûê‰æÜÊøÄÂãµÊàëÂÄëÁöÑÊºîÁÆóÊ≥ïÔºå‰∏¶ÈÄèÈÅéÂú®Âõ∞Èõ£ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÈõ∂Ê¨°ÂàÜÈ°û‰ªªÂãô‰∏≠ÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂØ¶È©ó‰æÜÊ∏¨Ë©¶ÊàëÂÄëÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÈÅ∏ÊìáÊÄßÈáçÊñ∞Ê†°Ê∫ñÊåÅÁ∫åÂ∞éËá¥Ê†°Ê∫ñË™§Â∑ÆÈ°ØËëó‰ΩéÊñºÂêÑÁ®ÆÈÅ∏ÊìáÂíåÈáçÊñ∞Ê†°Ê∫ñÂü∫Á∑ö„ÄÇ

##### **CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures**
2410.05235v2 by Ekaterina Sviridova, Anar Yeginbergen, Ainara Estarrona, Elena Cabrio, Serena Villata, Rodrigo Agerri

Explaining Artificial Intelligence (AI) decisions is a major challenge
nowadays in AI, in particular when applied to sensitive scenarios like medicine
and law. However, the need to explain the rationale behind decisions is a main
issue also for human-based deliberation as it is important to justify
\textit{why} a certain decision has been taken. Resident medical doctors for
instance are required not only to provide a (possibly correct) diagnosis, but
also to explain how they reached a certain conclusion. Developing new tools to
aid residents to train their explanation skills is therefore a central
objective of AI in education. In this paper, we follow this direction, and we
present, to the best of our knowledge, the first multilingual dataset for
Medical Question Answering where correct and incorrect diagnoses for a clinical
case are enriched with a natural language explanation written by doctors. These
explanations have been manually annotated with argument components (i.e.,
premise, claim) and argument relations (i.e., attack, support), resulting in
the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases
in four languages (English, Spanish, French, Italian) with explanations, where
we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106
attack relations. We conclude by showing how competitive baselines perform over
this challenging dataset for the argument mining task.

ÊëòË¶ÅÔºöËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊ±∫Á≠ñÊòØÁèæÂú® AI ÁöÑ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞ÔºåÁâπÂà•ÊòØÊáâÁî®ÊñºÂÉèÈÜ´Â≠∏ÂíåÊ≥ïÂæãÁ≠âÊïèÊÑüÊÉÖÂ¢ÉÊôÇ„ÄÇÁÑ∂ËÄåÔºåËß£ÈáãÊ±∫Á≠ñËÉåÂæåÁêÜÁî±ÁöÑÈúÄÊ±Ç‰πüÊòØÂü∫Êñº‰∫∫È°ûÁöÑËÄÉÈáèÁöÑ‰∏ÄÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÂõ†ÁÇ∫ÊúâÂøÖË¶ÅË≠âÊòéÁÇ∫‰ªÄÈ∫ºÂÅöÂá∫ÊüêÂÄãÊ±∫Á≠ñ„ÄÇ‰æãÂ¶ÇÔºå‰ΩèÈô¢ÈÜ´Â∏´‰∏çÂÉÖÈúÄË¶ÅÊèê‰æõÔºàÂèØËÉΩÊòØÊ≠£Á¢∫ÁöÑÔºâË®∫Êñ∑ÔºåÈÇÑÈúÄË¶ÅËß£Èáã‰ªñÂÄëÂ¶Ç‰ΩïÈÅîÊàêÊüêÂÄãÁµêË´ñ„ÄÇÂõ†Ê≠§ÔºåÈñãÁôºÊñ∞ÁöÑÂ∑•ÂÖ∑‰æÜÂπ´Âä©‰ΩèÈô¢ÈÜ´Â∏´Ë®ìÁ∑¥‰ªñÂÄëÁöÑËß£ÈáãÊäÄÂ∑ßÊòØÊïôËÇ≤‰∏≠ AI ÁöÑ‰∏ÄÈ†ÖÊ†∏ÂøÉÁõÆÊ®ô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÅµÂæ™ÈÄôÂÄãÊñπÂêëÔºå‰∏¶‰∏îÊ†πÊìöÊàëÂÄëÁöÑ‰∫ÜËß£ÔºåÊèêÂá∫Á¨¨‰∏ÄÂÄãÂ§öË™ûË®ÄÈÜ´Â≠∏ÂïèÁ≠îË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠Ëá®Â∫äÁóÖ‰æãÁöÑÊ≠£Á¢∫Âíå‰∏çÊ≠£Á¢∫Ë®∫Êñ∑ÈÉΩÈôÑÊúâÁî±ÈÜ´ÁîüÊí∞ÂØ´ÁöÑËá™ÁÑ∂Ë™ûË®ÄËß£Èáã„ÄÇÈÄô‰∫õËß£ÈáãÂ∑≤‰ΩøÁî®Ë´ñË≠âÁµÑÊàêÔºàÂç≥ÂâçÊèê„ÄÅ‰∏ªÂºµÔºâÂíåË´ñË≠âÈóú‰øÇÔºàÂç≥ÊîªÊìä„ÄÅÊîØÊåÅÔºâÈÄ≤Ë°åÊâãÂãïË®ªËß£ÔºåÁî¢ÁîüÂ§öË™ûË®Ä CasiMedicos-Arg Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 558 ÂÄãÂÖ∑ÊúâËß£ÈáãÁöÑÂõõÁ®ÆË™ûË®ÄÔºàËã±Ë™û„ÄÅË•øÁè≠ÁâôË™û„ÄÅÊ≥ïË™û„ÄÅÁæ©Â§ßÂà©Ë™ûÔºâÁöÑËá®Â∫äÁóÖ‰æãÔºåÊàëÂÄëË®ªËß£‰∫Ü 5021 ÂÄã‰∏ªÂºµ„ÄÅ2313 ÂÄãÂâçÊèê„ÄÅ2431 ÂÄãÊîØÊåÅÈóú‰øÇÂíå 1106 ÂÄãÊîªÊìäÈóú‰øÇ„ÄÇÊàëÂÄëÊúÄÂæåÂ±ïÁ§∫‰∫ÜÁ´∂Áà≠Âü∫Ê∫ñÂ¶Ç‰ΩïÈáùÂ∞çË´ñË≠âÊé¢Âãò‰ªªÂãôÂü∑Ë°åÊ≠§ÂÖ∑ÊåëÊà∞ÊÄßÁöÑË≥áÊñôÈõÜ„ÄÇ

##### **RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction**
2410.05361v1 by Yuwei Zhang, Tong Xia, Aaqib Saeed, Cecilia Mascolo

The high incidence and mortality rates associated with respiratory diseases
underscores the importance of early screening. Machine learning models can
automate clinical consultations and auscultation, offering vital support in
this area. However, the data involved, spanning demographics, medical history,
symptoms, and respiratory audio, are heterogeneous and complex. Existing
approaches are insufficient and lack generalizability, as they typically rely
on limited training data, basic fusion techniques, and task-specific models. In
this paper, we propose RespLLM, a novel multimodal large language model (LLM)
framework that unifies text and audio representations for respiratory health
prediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs
and enables effective audio-text fusion through cross-modal attentions.
Instruction tuning is employed to integrate diverse data from multiple sources,
ensuring generalizability and versatility of the model. Experiments on five
real-world datasets demonstrate that RespLLM outperforms leading baselines by
an average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates
zero-shot predictions for new tasks. Our work lays the foundation for
multimodal models that can perceive, listen to, and understand heterogeneous
data, paving the way for scalable respiratory health diagnosis.

ÊëòË¶ÅÔºöÈ´òÁôºÁîüÁéáÂíåÊ≠ª‰∫°ÁéáÁöÑÂëºÂê∏ÈÅìÁñæÁóÖÁ™ÅÈ°Ø‰∫ÜÊó©ÊúüÁØ©Ê™¢ÁöÑÈáçË¶ÅÊÄß„ÄÇÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂèØ‰ª•Ëá™ÂãïÂåñËá®Â∫äË´ÆË©¢ÂíåËÅΩË®∫ÔºåÂú®Ê≠§È†òÂüüÊèê‰æõÈáçË¶ÅÁöÑÊîØÊè¥„ÄÇÁÑ∂ËÄåÔºåÊâÄÊ∂âÂèäÁöÑË≥áÊñôÊ∂µËìã‰∫∫Âè£Áµ±Ë®à„ÄÅÁóÖÂè≤„ÄÅÁóáÁãÄÂíåÂëºÂê∏Èü≥Ë®äÔºåÊó¢Áï∞Ë≥™ÂèàË§áÈõú„ÄÇÁèæÊúâÁöÑÊñπÊ≥ï‰∏çË∂≥‰∏îÁº∫‰πèÊ¶ÇÊã¨ÊÄßÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÈÄöÂ∏∏‰æùË≥¥ÊñºÊúâÈôêÁöÑË®ìÁ∑¥Ë≥áÊñô„ÄÅÂü∫Êú¨ÁöÑËûçÂêàÊäÄË°ìÂíåÁâπÂÆöÊñº‰ªªÂãôÁöÑÊ®°Âûã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RespLLMÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ê°ÜÊû∂ÔºåÂÆÉÁµ±‰∏Ä‰∫ÜÊñáÊú¨ÂíåÈü≥Ë®äË°®Á§∫Ôºå‰ª•ÈÄ≤Ë°åÂëºÂê∏ÈÅìÂÅ•Â∫∑È†êÊ∏¨„ÄÇRespLLM Âà©Áî®È†êË®ìÁ∑¥ LLM ÁöÑÂª£Ê≥õÂÖàÈ©óÁü•Ë≠òÔºå‰∏¶ÈÄèÈÅéË∑®Ê®°ÊÖãÊ≥®ÊÑèÂäõÂØ¶ÁèæÊúâÊïàÁöÑÈü≥Ë®äÊñáÊú¨ËûçÂêà„ÄÇÊåáÁ§∫Ë™øÊï¥Áî®ÊñºÊï¥Âêà‰æÜËá™Â§öÂÄã‰æÜÊ∫êÁöÑ‰∏çÂêåË≥áÊñôÔºåÁ¢∫‰øùÊ®°ÂûãÁöÑÊ¶ÇÊã¨ÊÄßÂíåÂ§öÂäüËÉΩÊÄß„ÄÇÂú®‰∫îÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåRespLLM Âú®Ë®ìÁ∑¥‰ªªÂãô‰∏äÊØîÈ†òÂÖàÁöÑÂü∫Ê∫ñÈ´òÂá∫Âπ≥Âùá 4.6%ÔºåÂú®Êú™Ë¶ãË≥áÊñôÈõÜ‰∏äÈ´òÂá∫ 7.9%Ôºå‰∏¶‰øÉÈÄ≤Êñ∞‰ªªÂãôÁöÑÈõ∂Ê¨°Â≠∏ÁøíÈ†êÊ∏¨„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁÇ∫Â§öÊ®°ÊÖãÊ®°ÂûãÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÈÄô‰∫õÊ®°ÂûãÂèØ‰ª•ÊÑüÁü•„ÄÅËÅÜËÅΩÂíåÁêÜËß£Áï∞Ë≥™Ë≥áÊñôÔºåÁÇ∫ÂèØÊì¥ÂÖÖÁöÑÂëºÂê∏ÈÅìÂÅ•Â∫∑Ë®∫Êñ∑Èã™Âπ≥ÈÅìË∑Ø„ÄÇ

##### **Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization**
2410.05114v1 by Rohan Reddy Mekala, Frederik Pahde, Simon Baur, Sneha Chandrashekar, Madeline Diep, Markus Wenzel, Eric L. Wisotzky, Galip √úmit Yolcu, Sebastian Lapuschkin, Jackie Ma, Peter Eisert, Mikael Lindvall, Adam Porter, Wojciech Samek

In the realm of dermatological diagnoses, where the analysis of dermatoscopic
and microscopic skin lesion images is pivotal for the accurate and early
detection of various medical conditions, the costs associated with creating
diverse and high-quality annotated datasets have hampered the accuracy and
generalizability of machine learning models. We propose an innovative
unsupervised augmentation solution that harnesses Generative Adversarial
Network (GAN) based models and associated techniques over their latent space to
generate controlled semiautomatically-discovered semantic variations in
dermatoscopic images. We created synthetic images to incorporate the semantic
variations and augmented the training data with these images. With this
approach, we were able to increase the performance of machine learning models
and set a new benchmark amongst non-ensemble based models in skin lesion
classification on the HAM10000 dataset; and used the observed analytics and
generated models for detailed studies on model explainability, affirming the
effectiveness of our solution.

ÊëòË¶ÅÔºöÂú®ÁöÆËÜöÁßëË®∫Êñ∑È†òÂüüÔºåÁöÆËÜöÈè°Ê™¢Êü•ÂíåÈ°ØÂæÆÈè°ÁöÆËÜöÁóÖËÆäÂΩ±ÂÉèÁöÑÂàÜÊûêÂ∞çÊñºÊ∫ñÁ¢∫‰∏îÊó©ÊúüÂÅµÊ∏¨ÂêÑÁ®ÆÈÜ´ÁôÇÁãÄÊ≥ÅËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂª∫Á´ãÂ§öÊ®£Âåñ‰∏îÈ´òÂìÅË≥™ÁöÑÊ®ôË®òË≥áÊñôÈõÜÁõ∏ÈóúÊàêÊú¨Â∑≤ÈòªÁ§ôÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊôÆÈÅçÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ÂâµÊñ∞ÁöÑÈùûÁõ£Áù£ÂºèÊì¥ÂÖÖËß£Ê±∫ÊñπÊ°àÔºåÂà©Áî®ÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (GAN) Âü∫Á§éÊ®°ÂûãÂèäÂÖ∂Âú®ÊΩõÂú®Á©∫Èñì‰∏äÁöÑÁõ∏ÈóúÊäÄË°ìÔºå‰ª•Âú®ÁöÆËÜöÈè°ÂΩ±ÂÉè‰∏≠Áî¢ÁîüÂèóÊéßÁöÑÂçäËá™ÂãïÁôºÁèæË™ûÁæ©ËÆäÂåñ„ÄÇÊàëÂÄëÂª∫Á´ãÂêàÊàêÂΩ±ÂÉè‰ª•Á¥çÂÖ•Ë™ûÁæ©ËÆäÂåñÔºå‰∏¶‰ΩøÁî®ÈÄô‰∫õÂΩ±ÂÉèÊì¥ÂÖÖË®ìÁ∑¥Ë≥áÊñô„ÄÇÈÄèÈÅéÊ≠§ÊñπÊ≥ïÔºåÊàëÂÄëÂæó‰ª•ÊèêÂçáÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩÔºå‰∏¶Âú® HAM10000 Ë≥áÊñôÈõÜÁöÑÁöÆËÜöÁóÖËÆäÂàÜÈ°û‰∏≠Ë®≠ÂÆöÈùûÊï¥È´îÂºèÊ®°ÂûãÁöÑÊñ∞Âü∫Ê∫ñÔºõ‰∏¶‰ΩøÁî®ËßÄÂØüÂà∞ÁöÑÂàÜÊûêÂíåÂª∫Á´ãÁöÑÊ®°ÂûãÈÄ≤Ë°åÊ®°ÂûãÂèØËß£ÈáãÊÄßÁöÑË©≥Á¥∞Á†îÁ©∂ÔºåÁ¢∫Ë™çÊàëÂÄëËß£Ê±∫ÊñπÊ°àÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Named Clinical Entity Recognition Benchmark**
2410.05046v1 by Wadood M Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Cl√©ment Christophe, Praveen K Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan

This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP.

ÊëòË¶ÅÔºöÈÄô‰ªΩÊäÄË°ìÂ†±Âëä‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂëΩÂêçËá®Â∫äÂØ¶È´îËæ®Ë≠òÂü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑË™ûË®ÄÊ®°ÂûãÔºåËß£Ê±∫ÂæûËá®Â∫äÊïòËø∞‰∏≠ËêÉÂèñÁµêÊßãÂåñË≥áË®äÁöÑÈóúÈçµËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰ªªÂãôÔºå‰ª•ÊîØÊè¥Ëá™ÂãïÁ∑®Á¢º„ÄÅËá®Â∫äË©¶È©óÁæ§ÁµÑË≠òÂà•ÂíåËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≠âÊáâÁî®Á®ãÂºè„ÄÇ
ÊéíË°åÊ¶úÊèê‰æõ‰∏ÄÂÄãÊ®ôÊ∫ñÂåñÂπ≥Âè∞ÔºåÁî®ÊñºË©ï‰º∞ÂêÑÁ®ÆË™ûË®ÄÊ®°ÂûãÔºåÂåÖÊã¨Á∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®Êû∂ÊßãÔºå‰ª•ÂèäÂÆÉÂÄëË∑®Â§öÂÄãÈÜ´ÁôÇÈ†òÂüüË≠òÂà•ÂíåÂàÜÈ°ûËá®Â∫äÂØ¶È´îÁöÑËÉΩÂäõ„ÄÇÂà©Áî®Á≤æÂøÉÊï¥ÁêÜÁöÑÂÖ¨ÈñãËá®Â∫äË≥áÊñôÈõÜÔºåÊ∂µËìãÁñæÁóÖ„ÄÅÁóáÁãÄ„ÄÅËó•Áâ©„ÄÅÁ®ãÂ∫èÂíåÂØ¶È©óÂÆ§Ê∏¨ÈáèÁ≠âÂØ¶È´î„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÈÄô‰∫õÂØ¶È´îÊ†πÊìöËßÄÂØüÊÄßÈÜ´ÁôÇÁµêÊûúÂêà‰ΩúÂ§•‰º¥Èóú‰øÇ (OMOP) Â∏∏Ë¶ãË≥áÊñôÊ®°ÂûãÊ®ôÊ∫ñÂåñÔºåÁ¢∫‰øù‰∏çÂêåÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÂíåË≥áÊñôÈõÜ‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄßÂíå‰∫íÈÄöÊÄßÔºå‰ª•ÂèäÊ®°ÂûãÊïàËÉΩÁöÑÂÖ®Èù¢Ë©ï‰º∞„ÄÇÊ®°ÂûãÊïàËÉΩ‰∏ªË¶Å‰ΩøÁî® F1 ÂàÜÊï∏Ë©ï‰º∞Ôºå‰∏¶Ëºî‰ª•ÂêÑÁ®ÆË©ï‰º∞Ê®°ÂºèÔºåÊèê‰æõÂ∞çÊ®°ÂûãÊïàËÉΩÁöÑÂÖ®Èù¢Ë¶ãËß£„ÄÇÂ†±ÂëäÈÇÑÂåÖÊã¨Â∞çËøÑ‰ªäË©ï‰º∞Ê®°ÂûãÁöÑÁ∞°Ë¶ÅÂàÜÊûêÔºåÈáçÈªûË™™ÊòéËßÄÂØüÂà∞ÁöÑË∂®Âã¢ÂíåÈôêÂà∂„ÄÇ
ÈÄèÈÅéÂª∫Á´ãÊ≠§Âü∫Ê∫ñÊû∂ÊßãÔºåÊéíË°åÊ¶úÊó®Âú®‰øÉÈÄ≤ÈÄèÊòéÂ∫¶„ÄÅ‰øÉÈÄ≤ÊØîËºÉÂàÜÊûêÔºå‰∏¶Êé®ÂãïËá®Â∫äÂØ¶È´îËæ®Ë≠ò‰ªªÂãôÁöÑÂâµÊñ∞ÔºåÊªøË∂≥ÈÜ´ÁôÇ‰øùÂÅ• NLP ‰∏≠Â∞çÂÅ•ÂÖ®Ë©ï‰º∞ÊñπÊ≥ïÁöÑÈúÄÊ±Ç„ÄÇ

##### **Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data**
2410.04814v1 by Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz

In science, we are often interested in obtaining a generative model of the
underlying system dynamics from observed time series. While powerful methods
for dynamical systems reconstruction (DSR) exist when data come from a single
domain, how to best integrate data from multiple dynamical regimes and leverage
it for generalization is still an open question. This becomes particularly
important when individual time series are short, and group-level information
may help to fill in for gaps in single-domain data. At the same time, averaging
is not an option in DSR, as it will wipe out crucial dynamical properties
(e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is
needed that enables to efficiently harvest group-level (multi-domain)
information while retaining all single-domain dynamical characteristics. Here
we provide such a hierarchical approach and showcase it on popular DSR
benchmarks, as well as on neuroscientific and medical time series. In addition
to faithful reconstruction of all individual dynamical regimes, our
unsupervised methodology discovers common low-dimensional feature spaces in
which datasets with similar dynamics cluster. The features spanning these
spaces were further dynamically highly interpretable, surprisingly in often
linear relation to control parameters that govern the dynamics of the
underlying system. Finally, we illustrate transfer learning and generalization
to new parameter regimes.

ÊëòË¶ÅÔºöÂú®ÁßëÂ≠∏‰∏≠ÔºåÊàëÂÄëÂ∏∏Â∏∏ÊúâËààË∂£ÂæûËßÄÂØüÂà∞ÁöÑÊôÇÈñìÂ∫èÂàó‰∏≠Áç≤ÂæóÂü∫Á§éÁ≥ªÁµ±ÂãïÊÖãÁöÑÁîüÊàêÊ®°Âûã„ÄÇÈõñÁÑ∂Áï∂Ë≥áÊñô‰æÜËá™ÂñÆ‰∏ÄÈ†òÂüüÊôÇÔºåÂº∑Â§ßÁöÑÂãïÊÖãÁ≥ªÁµ±ÈáçÂª∫ (DSR) ÊñπÊ≥ïÂ∑≤Á∂ìÂ≠òÂú®Ôºå‰ΩÜÂ¶Ç‰ΩïÊúÄ‰Ω≥Êï¥Âêà‰æÜËá™Â§öÂÄãÂãïÊÖãÊ©üÂà∂ÁöÑË≥áÊñô‰∏¶Âà©Áî®ÂÆÉÈÄ≤Ë°åÊ¶ÇÊã¨‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈñãÊîæÁöÑÂïèÈ°å„ÄÇÁï∂ÂÄãÂà•ÊôÇÈñìÂ∫èÂàóÂæàÁü≠ÊôÇÔºåÈÄô‰∏ÄÈªûÂ∞§ÂÖ∂ÈáçË¶ÅÔºåËÄå‰∏îÁæ§ÁµÑÂ±§Á¥öÁöÑË≥áË®äÂèØËÉΩÊúâÂä©ÊñºÂ°´Ë£úÂñÆ‰∏ÄÈ†òÂüüË≥áÊñô‰∏≠ÁöÑÁ©∫ÁôΩ„ÄÇÂêåÊôÇÔºåÂπ≥ÂùáÂåñ‰∏¶Èùû DSR ‰∏≠ÁöÑÈÅ∏È†ÖÔºåÂõ†ÁÇ∫ÂÆÉÊúÉÊ∂àÈô§ÈóúÈçµÁöÑÂãïÊÖãÁâπÊÄßÔºà‰æãÂ¶ÇÔºå‰∏ÄÂÄãÈ†òÂüü‰∏≠ÁöÑÊ•µÈôêÈÄ±ÊúüÁõ∏Â∞çÊñºÂè¶‰∏ÄÂÄãÈ†òÂüü‰∏≠ÁöÑÊ∑∑‰∫ÇÔºâ„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶Å‰∏ÄÂÄãÊ°ÜÊû∂ÔºåËÉΩÂ§†ÊúâÊïàÊî∂ÈõÜÁæ§ÁµÑÂ±§Á¥öÔºàÂ§öÈ†òÂüüÔºâË≥áË®äÔºåÂêåÊôÇ‰øùÁïôÊâÄÊúâÂñÆ‰∏ÄÈ†òÂüüÂãïÊÖãÁâπÊÄß„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÊèê‰æõÈÄôÁ®ÆÈöéÂ±§ÂºèÊñπÊ≥ïÔºå‰∏¶Âú®ÊµÅË°åÁöÑ DSR Âü∫Ê∫ñ‰ª•ÂèäÁ•ûÁ∂ìÁßëÂ≠∏ÂíåÈÜ´Â≠∏ÊôÇÈñìÂ∫èÂàó‰∏≠Â±ïÁ§∫ÂÆÉ„ÄÇÈô§‰∫ÜÂø†ÂØ¶ÈáçÂª∫ÊâÄÊúâÂÄãÂà•ÂãïÊÖãÊ©üÂà∂‰πãÂ§ñÔºåÊàëÂÄëÁöÑÈùûÁõ£Áù£ÊñπÊ≥ïÈÇÑÁôºÁèæ‰∫ÜÂ∏∏Ë¶ãÁöÑ‰ΩéÁ∂≠ÁâπÂæµÁ©∫ÈñìÔºåÂÖ∂‰∏≠ÂÖ∑ÊúâÁõ∏‰ººÂãïÊÖãÁöÑË≥áÊñôÈõÜÊúÉÊàêÁæ§„ÄÇË∑®Ë∂äÈÄô‰∫õÁ©∫ÈñìÁöÑÁâπÂæµÂú®ÂãïÊÖã‰∏äÈÄ≤‰∏ÄÊ≠•ÂÖ∑ÊúâÈ´òÂ∫¶ÂèØËß£ÈáãÊÄßÔºå‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÂÆÉÂÄëÈÄöÂ∏∏ËàáÊéßÂà∂Âü∫Á§éÁ≥ªÁµ±ÂãïÊÖãÁöÑÊéßÂà∂ÂèÉÊï∏ÂëàÁ∑öÊÄßÈóú‰øÇ„ÄÇÊúÄÂæåÔºåÊàëÂÄëË™™Êòé‰∫ÜÈÅ∑ÁßªÂºèÂ≠∏ÁøíÂíåÂ∞çÊñ∞ÂèÉÊï∏Ê©üÂà∂ÁöÑÊ¶ÇÊã¨„ÄÇ

##### **$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization**
2410.04717v2 by Dylan Zhang, Justin Wang, Francois Charton

Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.

ÊëòË¶ÅÔºöÁêÜËß£‰∏¶Ê∫ñÁ¢∫ÈÅµÂæ™ÊåáÁ§∫Â∞çÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁôºÊèÆ‰ΩúÁî®Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂö¥Ê†ºÂØ©Êü•‰∫Ü‰ΩøÊ®°ÂûãËÉΩÂ§†Ê¶ÇÊã¨ÁÇ∫Êú™Ë¶ãÊåáÁ§∫ÁöÑÈóúÈçµÂõ†Á¥†Ôºå‰∏¶Êèê‰æõË¶ãËß£‰ª•ÊåáÂ∞éÊî∂ÈõÜÁî®ÊñºÊåáÁ§∫ÂæÆË™øÁöÑÊï∏Êìö„ÄÇÈÄöÈÅéÂèóÂúñÈùàÂÆåÂÇôÈ¶¨ÂèØÂ§´ÊºîÁÆóÊ≥ïÂïüÁôºÁöÑÂèóÊéßÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé‰∫ÜÈÄôÁ®ÆÊ¶ÇÊã¨ÂÉÖÂú®Ë®ìÁ∑¥Êï∏ÊìöÂú®Ë™ûÁæ©È†òÂüü‰∏≠Ë∂≥Â§†Â§öÊ®£ÂåñÊôÇÊâçÊúÉÂá∫Áèæ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈÇÑË°®ÊòéÔºåÂÉÖÂú®ÊúâÈôêÁöÑÈ†òÂüüÂÖßÈÄ≤Ë°åÂ§öÊ®£Âåñ‰∏çË∂≥‰ª•Á¢∫‰øùÁ©©ÂÅ•ÁöÑÊ¶ÇÊã¨„ÄÇÁõ∏ÂèçÔºåÂç≥‰ΩøÂú®ÂèóÈôêÁöÑÊï∏ÊìöÈ†êÁÆó‰∏ãÔºåË∑®È†òÂüüÊï∏ÊìöÂ§öÊ®£Âåñ‰πüÊúÉÈ°ØËëóÂ¢ûÂº∑Ê®°ÂûãÁöÑÈÅ©ÊáâÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Â∞áÊàëÂÄëÁöÑÂàÜÊûêÊì¥Â±ïÂà∞ÁèæÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØÔºåÂåÖÊã¨ÂæÆË™øÂ∞àÂÆ∂ÂíåÈÄöÊâçÊ®°Âûã„ÄÇÂú®ÂÖ©Á®ÆÊÉÖÊ≥Å‰∏ãÔºåÊàëÂÄëÈÉΩË≠âÊòé‰∫Ü 1) ÂèØ‰ª•Âú®‰øùÊåÅÊï∏ÊìöÂ§ßÂ∞è‰∏çËÆäÁöÑÊÉÖÊ≥Å‰∏ãÈÄöÈÅéÂ¢ûÂä†Êó¢ÊúâÊï∏ÊìöÈõÜÁöÑÂ§öÊ®£ÊÄß‰æÜÂØ¶ÁèæÊõ¥Â•ΩÁöÑÊÄßËÉΩÔºå‰ª•Âèä 2) Âú®Êì¥Â±ïÊï∏ÊìöÊôÇÔºåÂ§öÊ®£ÂåñÊåáÁ§∫ÁöÑË™ûÁæ©ÊØîÁ∞°ÂñÆÂú∞Â¢ûÂä†È°û‰ººÊï∏ÊìöÁöÑÊï∏ÈáèÊõ¥ÊúâÊïà„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁÇ∫Êï∏ÊìöÈõÜÊï¥ÁêÜÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑË¶ãËß£ÔºåÁâπÂà•ÊòØÂú®ÈÄöÈÅéÊì¥Â±ïÂ∞àÂÆ∂ÂíåÈÄöÊâçÂ†¥ÊôØÁöÑË®ìÁ∑¥Êï∏Êìö‰æÜÂÑ™ÂåñÊ®°ÂûãÊÄßËÉΩÊôÇ„ÄÇÊàëÂÄëË°®Êòé‰ªîÁ¥∞ËÄÉÊÖÆÊï∏ÊìöÂ§öÊ®£ÂåñËá≥ÈóúÈáçË¶ÅÔºö‰ΩøÁî®Ë∂ÖÂá∫ÂÖ∂Ê†∏ÂøÉÈ†òÂüüÁöÑÊï∏ÊìöË®ìÁ∑¥Â∞àÂÆ∂Ê®°ÂûãÊúÉÂ∞éËá¥È°ØËëóÁöÑÊÄßËÉΩÊîπÈÄ≤ÔºåËÄåÈÄöÊâçÊ®°ÂûãÂèóÁõäÊñºÂ§öÊ®£ÂåñÁöÑÊï∏ÊìöÊ∑∑ÂêàÔºåÈÄô‰∫õÊ∑∑ÂêàÂ¢ûÂº∑‰∫ÜÂÆÉÂÄëÂú®Âª£Ê≥õÊáâÁî®‰∏≠ÁöÑÊï¥È´îÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÂá∫‰∫ÜÁ≠ñÁï•ÊÄßÂ§öÊ®£ÂåñÁöÑÈóúÈçµ‰ΩúÁî®Ôºå‰∏¶ÁÇ∫ÊèêÈ´òÊï∏ÊìöË≥™ÈáèÊèê‰æõ‰∫ÜÊòéÁ¢∫ÁöÑÊåáÂ∞éÊñπÈáù„ÄÇ

##### **Rule-based Data Selection for Large Language Models**
2410.04715v1 by Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, Hong Hu

The quality of training data significantly impacts the performance of large
language models (LLMs). There are increasing studies using LLMs to rate and
select data based on several human-crafted metrics (rules). However, these
conventional rule-based approaches often depend too heavily on human
heuristics, lack effective metrics for assessing rules, and exhibit limited
adaptability to new tasks. In our study, we introduce an innovative rule-based
framework that utilizes the orthogonality of score vectors associated with
rules as a novel metric for rule evaluations. Our approach includes an
automated pipeline that first uses LLMs to generate a diverse set of rules,
encompassing various rating dimensions to evaluate data quality. Then it rates
a batch of data based on these rules and uses the determinantal point process
(DPP) from random matrix theory to select the most orthogonal score vectors,
thereby identifying a set of independent rules. These rules are subsequently
used to evaluate all data, selecting samples with the highest average scores
for downstream tasks such as LLM training. We verify the effectiveness of our
method through two experimental setups: 1) comparisons with ground truth
ratings and 2) benchmarking LLMs trained with the chosen data. Our
comprehensive experiments cover a range of scenarios, including general
pre-training and domain-specific fine-tuning in areas such as IMDB, Medical,
Math, and Code. The outcomes demonstrate that our DPP-based rule rating method
consistently outperforms other approaches, including rule-free rating, uniform
sampling, importance resampling, and QuRating, in terms of both rating
precision and model performance.

ÊëòË¶ÅÔºöË®ìÁ∑¥Ë≥áÊñôÁöÑÂìÅË≥™ÊúÉÈ°ØËëóÂΩ±ÈüøÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊïàËÉΩ„ÄÇÊúâÊÑà‰æÜÊÑàÂ§öÁ†îÁ©∂‰ΩøÁî® LLM ‰æÜË©ïÂàÜ‰∏¶Ê†πÊìöÂ§öÈ†Ö‰∫∫ÁÇ∫Âª∫Á´ãÁöÑÊåáÊ®ô (Ë¶èÂâá) ÈÅ∏ÊìáË≥áÊñô„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂÇ≥Áµ±ÁöÑÂü∫ÊñºË¶èÂâáÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÈÅéÂ∫¶‰æùË≥¥‰∫∫È°ûÁöÑÂïüÁôºÊ≥ïÔºåÁº∫‰πèË©ï‰º∞Ë¶èÂâáÁöÑÊúâÊïàÊåáÊ®ôÔºå‰∏îÂú®ÈÅ©ÊáâÊñ∞‰ªªÂãôÊñπÈù¢Â±ïÁèæÂá∫ÊúâÈôêÁöÑÈùàÊ¥ªÊÄß„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÈÄ≤‰∏ÄÂÄãÂâµÊñ∞ÁöÑÂü∫ÊñºË¶èÂâáÁöÑÊû∂ÊßãÔºåÂÆÉÂà©Áî®ËàáË¶èÂâáÁõ∏ÈóúËÅØÁöÑÂàÜÊï∏ÂêëÈáèÁöÑÊ≠£‰∫§ÊÄß‰ΩúÁÇ∫Ë¶èÂâáË©ï‰º∞ÁöÑÊñ∞ÊåáÊ®ô„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨‰∏ÄÂÄãËá™ÂãïÂåñÊµÅÁ®ãÔºåË©≤ÊµÅÁ®ãÈ¶ñÂÖà‰ΩøÁî® LLM Áî¢Áîü‰∏ÄÁµÑÂ§öÊ®£ÂåñÁöÑË¶èÂâáÔºåÊ∂µËìãÂêÑÁ®ÆË©ïÂàÜÈù¢Âêë‰ª•Ë©ï‰º∞Ë≥áÊñôÂìÅË≥™„ÄÇÊé•ËëóÔºåÂÆÉÊ†πÊìöÈÄô‰∫õË¶èÂâáË©ïÂàÜ‰∏ÄÊâπË≥áÊñôÔºå‰∏¶‰ΩøÁî®Èö®Ê©üÁü©Èô£ÁêÜË´ñ‰∏≠ÁöÑË°åÂàóÂºèÈªûÈÅéÁ®ã (DPP) ‰æÜÈÅ∏Âá∫ÊúÄÊ≠£‰∫§ÁöÑÂàÜÊï∏ÂêëÈáèÔºåÂæûËÄåÊâæÂá∫Áç®Á´ãË¶èÂâáÁöÑÈõÜÂêà„ÄÇÈÄô‰∫õË¶èÂâáÈö®ÂæåÁî®ÊñºË©ï‰º∞ÊâÄÊúâË≥áÊñôÔºåÈáùÂ∞ç‰∏ãÊ∏∏‰ªªÂãôÔºà‰æãÂ¶Ç LLM Ë®ìÁ∑¥ÔºâÈÅ∏Âá∫Âπ≥ÂùáÂàÜÊï∏ÊúÄÈ´òÁöÑÊ®£Êú¨„ÄÇÊàëÂÄëÈÄèÈÅéÂÖ©ÂÄãÂØ¶È©óË®≠ÂÆöÈ©óË≠âÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºö1) ËàáÁúüÂØ¶Ë©ïÂàÜÈÄ≤Ë°åÊØîËºÉÔºå‰ª•Âèä 2) Â∞ç‰ΩøÁî®ÊâÄÈÅ∏Ë≥áÊñôË®ìÁ∑¥ÁöÑ LLM ÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇÊàëÂÄëÂÖ®Èù¢ÁöÑÂØ¶È©óÊ∂µËìã‰∏ÄÁ≥ªÂàóÊÉÖÂ¢ÉÔºåÂåÖÊã¨Âú® IMDB„ÄÅÈÜ´Â≠∏„ÄÅÊï∏Â≠∏ÂíåÁ®ãÂºèÁ¢ºÁ≠âÈ†òÂüüÁöÑ‰∏ÄËà¨È†êË®ìÁ∑¥ÂíåÁâπÂÆöÈ†òÂüüÁöÑÂæÆË™ø„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÂü∫Êñº DPP ÁöÑË¶èÂâáË©ïÂàÜÊñπÊ≥ïÂú®Ë©ïÂàÜÁ≤æÊ∫ñÂ∫¶ÂíåÊ®°ÂûãÊïàËÉΩÊñπÈù¢ÂßãÁµÇÂÑ™ÊñºÂÖ∂‰ªñÊñπÊ≥ïÔºåÂåÖÊã¨ÁÑ°Ë¶èÂâáË©ïÂàÜ„ÄÅÂùáÂãªÊäΩÊ®£„ÄÅÈáçË¶ÅÊÄßÂÜçÊäΩÊ®£Âíå QuRating„ÄÇ

##### **Knowledge Graph Based Agent for Complex, Knowledge-Intensive QA in Medicine**
2410.04660v1 by Xiaorui Su, Yibo Wang, Shanghua Gao, Xiaolong Liu, Valentina Giunchiglia, Djork-Arn√© Clevert, Marinka Zitnik

Biomedical knowledge is uniquely complex and structured, requiring distinct
reasoning strategies compared to other scientific disciplines like physics or
chemistry. Biomedical scientists do not rely on a single approach to reasoning;
instead, they use various strategies, including rule-based, prototype-based,
and case-based reasoning. This diversity calls for flexible approaches that
accommodate multiple reasoning strategies while leveraging in-domain knowledge.
We introduce KGARevion, a knowledge graph (KG) based agent designed to address
the complexity of knowledge-intensive medical queries. Upon receiving a query,
KGARevion generates relevant triplets by using the knowledge base of the LLM.
These triplets are then verified against a grounded KG to filter out erroneous
information and ensure that only accurate, relevant data contribute to the
final answer. Unlike RAG-based models, this multi-step process ensures
robustness in reasoning while adapting to different models of medical
reasoning. Evaluations on four gold-standard medical QA datasets show that
KGARevion improves accuracy by over 5.2%, outperforming 15 models in handling
complex medical questions. To test its capabilities, we curated three new
medical QA datasets with varying levels of semantic complexity, where KGARevion
achieved a 10.4% improvement in accuracy.

ÊëòË¶ÅÔºöÁîüÁâ©ÂåªÂ≠¶Áü•Ë≠òÁç®ÁâπÂú∞Ë§áÈõú‰∏îÁµêÊßãÂåñÔºåÈúÄË¶ÅËàáÂÖ∂‰ªñÁßëÂ≠∏È†òÂüüÔºàÂ¶ÇÁâ©ÁêÜÊàñÂåñÂ≠∏Ôºâ‰∏çÂêåÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇÁîüÁâ©ÈÜ´Â≠∏ÁßëÂ≠∏ÂÆ∂‰∏ç‰æùË≥¥ÂñÆ‰∏ÄÁöÑÊé®ÁêÜÊñπÊ≥ïÔºõÁõ∏ÂèçÔºå‰ªñÂÄë‰ΩøÁî®ÂêÑÁ®ÆÁ≠ñÁï•ÔºåÂåÖÊã¨Âü∫ÊñºË¶èÂâá„ÄÅÂü∫ÊñºÂéüÂûãÂíåÂü∫ÊñºÊ°à‰æãÁöÑÊé®ÁêÜ„ÄÇÈÄôÁ®ÆÂ§öÊ®£ÊÄßÈúÄË¶ÅÈùàÊ¥ªÁöÑÊñπÊ≥ïÔºåÂêåÊôÇÂà©Áî®È†òÂüüÁü•Ë≠ò‰æÜÈÅ©ÊáâÂ§öÁ®ÆÊé®ÁêÜÁ≠ñÁï•„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü KGARevionÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òÂúñË≠ú (KG) ÁöÑ‰ª£ÁêÜÔºåÊó®Âú®Ëß£Ê±∫Áü•Ë≠òÂØÜÈõÜÂûãÈÜ´ÁôÇÊü•Ë©¢ÁöÑË§áÈõúÊÄß„ÄÇÂú®Êî∂Âà∞Êü•Ë©¢ÂæåÔºåKGARevion ‰ΩøÁî® LLM ÁöÑÁü•Ë≠òÂ∫´ÁîüÊàêÁõ∏ÈóúÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÁÑ∂ÂæåÂ∞áÈÄô‰∫õ‰∏âÂÖÉÁµÑËàáÂü∫Á§é KG ÈÄ≤Ë°åÈ©óË≠âÔºå‰ª•ÈÅéÊøæÊéâÈåØË™§‰ø°ÊÅØ‰∏¶Á¢∫‰øùÂè™ÊúâÊ∫ñÁ¢∫„ÄÅÁõ∏ÈóúÁöÑÊï∏ÊìöÊúâÂä©ÊñºÊúÄÁµÇÁ≠îÊ°à„ÄÇËàáÂü∫Êñº RAG ÁöÑÊ®°Âûã‰∏çÂêåÔºåÈÄôÁ®ÆÂ§öÊ≠•È©üÈÅéÁ®ãÁ¢∫‰øù‰∫ÜÊé®ÁêÜÁöÑÁ©©ÂÅ•ÊÄßÔºåÂêåÊôÇÈÅ©Êáâ‰∏çÂêåÁöÑÈÜ´ÁôÇÊé®ÁêÜÊ®°Âûã„ÄÇÂ∞çÂõõÂÄãÈªÉÈáëÊ®ôÊ∫ñÈÜ´ÁôÇ QA Êï∏ÊìöÈõÜÁöÑË©ï‰º∞Ë°®ÊòéÔºåKGARevion Â∞áÊ∫ñÁ¢∫ÁéáÊèêÈ´ò‰∫Ü 5.2%ÔºåÂú®ËôïÁêÜË§áÈõúÁöÑÈÜ´ÁôÇÂïèÈ°åÊñπÈù¢ÂÑ™Êñº 15 ÂÄãÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÊ∏¨Ë©¶ÂÖ∂ËÉΩÂäõÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫Ü‰∏âÂÄãÊñ∞ÁöÑÈÜ´ÁôÇ QA Êï∏ÊìöÈõÜÔºåÂÖ∑Êúâ‰∏çÂêåÁöÑË™ûÁæ©Ë§áÈõúÊÄßÔºåÂÖ∂‰∏≠ KGARevion Âú®Ê∫ñÁ¢∫Áéá‰∏äÊèêÈ´ò‰∫Ü 10.4%„ÄÇ

##### **Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task**
2410.11860v1 by Chengyuan Xu, Kuo-Chin Lien, Tobias H√∂llerer

When designing an AI-assisted decision-making system, there is often a
tradeoff between precision and recall in the AI's recommendations. We argue
that careful exploitation of this tradeoff can harness the complementary
strengths in the human-AI collaboration to significantly improve team
performance. We investigate a real-world video anonymization task for which
recall is paramount and more costly to improve. We analyze the performance of
78 professional annotators working with a) no AI assistance, b) a
high-precision "restrained" AI, and c) a high-recall "zealous" AI in over 3,466
person-hours of annotation work. In comparison, the zealous AI helps human
teammates achieve significantly shorter task completion time and higher recall.
In a follow-up study, we remove AI assistance for everyone and find negative
training effects on annotators trained with the restrained AI. These findings
and our analysis point to important implications for the design of AI
assistance in recall-demanding scenarios.

ÊëòË¶ÅÔºöÂú®Ë®≠Ë®à AI ËºîÂä©Ê±∫Á≠ñÁ≥ªÁµ±ÊôÇÔºåAI Âª∫Ë≠∞‰∏≠ÁöÑÁ≤æÊ∫ñÂ∫¶ËàáÂè¨ÂõûÁéá‰πãÈñìÈÄöÂ∏∏Â≠òÂú®ÂèñÊç®„ÄÇÊàëÂÄë‰∏ªÂºµÔºåÂ∞èÂøÉÂà©Áî®ÈÄôÁ®ÆÂèñÊç®ÂèØ‰ª•Âà©Áî®‰∫∫Ê©üÂçî‰Ωú‰∏≠ÁöÑ‰∫íË£úÂÑ™Âã¢ÔºåÈ°ØËëóÊèêÂçáÂúòÈöäÁ∏æÊïà„ÄÇÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏ÄÈ†ÖÁúüÂØ¶‰∏ñÁïåÁöÑÂΩ±ÁâáÂåøÂêçÂåñ‰ªªÂãôÔºåÂè¨ÂõûÁéáËá≥ÈóúÈáçË¶Å‰∏îÊõ¥Èõ£‰ª•ÊèêÂçá„ÄÇÊàëÂÄëÂàÜÊûê‰∫Ü 78 ‰ΩçÂ∞àÊ•≠Ë®ªËß£Âì°ÁöÑË°®ÁèæÔºå‰ªñÂÄëÂàÜÂà•‰ΩøÁî® a) Ê≤íÊúâ AI ÂçîÂä©„ÄÅb) È´òÁ≤æÊ∫ñÂ∫¶ÁöÑ„ÄåÁ¥ÑÊùü„ÄçAIÔºå‰ª•Âèä c) È´òÂè¨ÂõûÁéáÁöÑ„ÄåÁÜ±ÂøÉ„ÄçAIÔºåÈÄ≤Ë°åË∂ÖÈÅé 3,466 ‰∫∫Â∞èÊôÇÁöÑË®ªËß£Â∑•‰Ωú„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåÁÜ±ÂøÉ AI ËÉΩÂπ´Âä©‰∫∫È°ûÈöäÂèãÈ°ØËëóÁ∏ÆÁü≠‰ªªÂãôÂÆåÊàêÊôÇÈñìÔºå‰∏¶ÊèêÈ´òÂè¨ÂõûÁéá„ÄÇÂú®ÂæåÁ∫åÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁßªÈô§ÊâÄÊúâ‰∫∫ÁöÑ AI ÂçîÂä©ÔºåÁôºÁèæÂ∞ç‰ΩøÁî®Á¥ÑÊùü AI ÈÄ≤Ë°åË®ìÁ∑¥ÁöÑË®ªËß£Âì°Áî¢ÁîüË≤†Èù¢ÁöÑË®ìÁ∑¥ÊïàÊûú„ÄÇÈÄô‰∫õÁôºÁèæÂíåÊàëÂÄëÁöÑÂàÜÊûêÊåáÂá∫ÔºåÂú®Ë¶ÅÊ±ÇÂè¨ÂõûÁéáÁöÑÊÉÖÂ¢É‰∏≠Ë®≠Ë®à AI ÂçîÂä©ÂÖ∑ÊúâÈáçË¶ÅÁöÑÊÑèÁæ©„ÄÇ

##### **Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry (MWR) Breast Cancer Detection**
2410.04636v1 by Christoforos Galazis, Huiyi Wu, Igor Goryanin

The pursuit of enhanced breast cancer detection and monitoring techniques is
a paramount healthcare objective, driving the need for innovative imaging
technologies and diagnostic approaches. This study introduces a novel
multi-tiered self-contrastive model tailored for the application of microwave
radiometry (MWR) breast cancer detection. Our approach encompasses three
distinct models: Local-MWR (L-MWR), Regional-MWR (R-MWR), and Global-MWR
(G-MWR), each engineered to analyze varying sub-regional comparisons within the
breasts. These models are cohesively integrated through the Joint-MWR (J-MWR)
network, which leverages the self-contrastive data generated at each analytical
level to enhance detection capabilities. Employing a dataset comprising 4,932
cases of female patients, our research showcases the effectiveness of our
proposed models. Notably, the J-MWR model distinguishes itself by achieving a
Matthews correlation coefficient of 0.74 $\pm$ 0.018, surpassing existing MWR
neural networks and contrastive methods. These results highlight the
significant potential of self-contrastive learning techniques in improving both
the diagnostic accuracy and generalizability of MWR-based breast cancer
detection processes. Such advancements hold considerable promise for further
investigative and clinical endeavors. The source code is available at:
https://github.com/cgalaz01/self_contrastive_mwr

ÊëòË¶ÅÔºöËøΩÊ±ÇÂ¢ûÂº∑‰π≥ÁôåÊ™¢Ê∏¨ÂíåÁõ£Ê∏¨ÊäÄË°ìÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÁõÆÊ®ôÔºåÊé®Âãï‰∫ÜÂâµÊñ∞ÂΩ±ÂÉèÊäÄË°ìÂíåË®∫Êñ∑ÊñπÊ≥ïÁöÑÈúÄÊ±Ç„ÄÇÊú¨Á†îÁ©∂‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ§öÂ±§Ëá™Â∞çÊØîÊ®°ÂûãÔºåÂ∞àÈñÄÁî®ÊñºÂæÆÊ≥¢ËºªÂ∞ÑÊ∏¨Èáè (MWR) ‰π≥ÁôåÊ™¢Ê∏¨„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÂê´‰∏âÂÄã‰∏çÂêåÁöÑÊ®°ÂûãÔºöÂ±ÄÈÉ® MWR (L-MWR)„ÄÅÂçÄÂüü MWR (R-MWR) ÂíåÂÖ®Â±Ä MWR (G-MWR)ÔºåÊØèÂÄãÊ®°ÂûãÈÉΩË®≠Ë®àÁî®ÊñºÂàÜÊûê‰π≥ÊàøÂÖß‰∏çÂêåÁöÑÊ¨°ÂçÄÂüüÊØîËºÉ„ÄÇÈÄô‰∫õÊ®°ÂûãÈÄöÈÅéËÅØÂêà MWR (J-MWR) Á∂≤Ë∑ØÁ∑äÂØÜÊï¥ÂêàÔºåÂà©Áî®Âú®ÊØèÂÄãÂàÜÊûêÂ±§Á¥öÁî¢ÁîüÁöÑËá™Â∞çÊØîË≥áÊñô‰æÜÂ¢ûÂº∑Ê™¢Ê∏¨ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êé°Áî®ÂåÖÂê´ 4,932 ‰æãÂ•≥ÊÄßÊÇ£ËÄÖÁöÑË≥áÊñôÈõÜÔºåÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåJ-MWR Ê®°Âûã‰ª•ÈÅîÂà∞ 0.74 ¬± 0.018 ÁöÑÈ¶¨‰øÆÊñØÁõ∏Èóú‰øÇÊï∏ËÄåÂçÄÂà•ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºåË∂ÖË∂ä‰∫ÜÁèæÊúâÁöÑ MWR Á•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ∞çÊØîÊñπÊ≥ï„ÄÇÈÄô‰∫õÁµêÊûúÁ™ÅÈ°Ø‰∫ÜËá™Â∞çÊØîÂ≠∏ÁøíÊäÄË°ìÂú®ÊîπÂñÑÂü∫Êñº MWR ÁöÑ‰π≥ÁôåÊ™¢Ê∏¨Á®ãÂ∫èÁöÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÊ¶ÇÊã¨ÊÄßÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõ„ÄÇÈÄô‰∫õÈÄ≤Â±ïÁÇ∫ÈÄ≤‰∏ÄÊ≠•ÁöÑË™øÊü•ÂíåËá®Â∫äÂ∑•‰ΩúÊèê‰æõ‰∫ÜÁõ∏Áï∂Â§ßÁöÑÂ∏åÊúõ„ÄÇÂéüÂßãÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttps://github.com/cgalaz01/self_contrastive_mwr

##### **Semi-Markovian Planning to Coordinate Aerial and Maritime Medical Evacuation Platforms**
2410.04523v1 by Mahdi Al-Husseini, Kyle H. Wray, Mykel J. Kochenderfer

The transfer of patients between two aircraft using an underway watercraft
increases medical evacuation reach and flexibility in maritime environments.
The selection of any one of multiple underway watercraft for patient exchange
is complicated by participating aircraft utilization history and a
participating watercraft position and velocity. The selection problem is
modeled as a semi-Markov decision process with an action space including both
fixed land and moving watercraft exchange points. Monte Carlo tree search with
root parallelization is used to select optimal exchange points and determine
aircraft dispatch times. Model parameters are varied in simulation to identify
representative scenarios where watercraft exchange points reduce incident
response times. We find that an optimal policy with watercraft exchange points
outperforms an optimal policy without watercraft exchange points and a greedy
policy by 35% and 40%, respectively. In partnership with the United States
Army, we deploy for the first time the watercraft exchange point by executing a
mock patient transfer with a manikin between two HH-60M medical evacuation
helicopters and an underway Army Logistic Support Vessel south of the Hawaiian
island of Oahu. Both helicopters were dispatched in accordance with our
optimized decision strategy.

ÊëòË¶ÅÔºö‰ΩøÁî®Ëà™Ë°å‰∏≠ÁöÑÊ∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑Âú®‰∏§Êû∂È£ûÊú∫‰πãÈó¥ËΩ¨ËøêÊÇ£ËÄÖÔºåÂèØÂ¢ûÂä†Êµ∑‰∏äÁéØÂ¢É‰∏≠ÁöÑÂåªÁñóÂêéÈÄÅËåÉÂõ¥ÂíåÁÅµÊ¥ªÊÄß„ÄÇ
Áî±‰∫éÂèÇ‰∏éÈ£ûÊú∫ÁöÑ‰ΩøÁî®ÂéÜÂè≤‰ª•ÂèäÂèÇ‰∏éÊ∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑ÁöÑ‰ΩçÁΩÆÂíåÈÄüÂ∫¶ÔºåÈÄâÊã©Â§ö‰∏™Ëà™Ë°å‰∏≠ÁöÑÊ∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑‰∏≠ÁöÑ‰ªª‰Ωï‰∏Ä‰∏™ËøõË°åÊÇ£ËÄÖ‰∫§Êç¢ÂèòÂæóÂ§çÊùÇ„ÄÇÈÄâÊã©ÈóÆÈ¢òË¢´Âª∫Ê®°‰∏∫ÂçäÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÂÖ∂Âä®‰ΩúÁ©∫Èó¥ÂåÖÊã¨Âõ∫ÂÆöÈôÜÂú∞ÂíåÁßªÂä®Ê∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑‰∫§Êç¢ÁÇπ„ÄÇ‰ΩøÁî®Ê†πÂπ∂Ë°åÂåñÁöÑËíôÁâπÂç°ÁΩóÊ†ëÊêúÁ¥¢Êù•ÈÄâÊã©ÊúÄ‰Ω≥‰∫§Êç¢ÁÇπÂπ∂Á°ÆÂÆöÈ£ûÊú∫Ë∞ÉÂ∫¶Êó∂Èó¥„ÄÇÂú®‰ªøÁúü‰∏≠ÊîπÂèòÊ®°ÂûãÂèÇÊï∞Ôºå‰ª•ËØÜÂà´Ê∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑‰∫§Êç¢ÁÇπÂáèÂ∞ë‰∫ã‰ª∂ÂìçÂ∫îÊó∂Èó¥ÁöÑ‰ª£Ë°®ÊÄßÂú∫ÊôØ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂÖ∑ÊúâÊ∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑‰∫§Êç¢ÁÇπÁöÑÊúÄ‰ºòÁ≠ñÁï•ÊØîÊ≤°ÊúâÊ∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑‰∫§Êç¢ÁÇπÁöÑÊúÄ‰ºòÁ≠ñÁï•ÂíåË¥™Â©™Á≠ñÁï•ÂàÜÂà´È´òÂá∫ 35% Âíå 40%„ÄÇ‰∏éÁæéÂõΩÈôÜÂÜõÂêà‰ΩúÔºåÊàë‰ª¨È¶ñÊ¨°ÈÄöËøáÂú®‰∏§Êû∂ HH-60M ÂåªÁñóÂêéÈÄÅÁõ¥ÂçáÊú∫ÂíåÂ§èÂ®ÅÂ§∑Ê¨ßËÉ°Â≤õÂçóÈÉ®Ëà™Ë°å‰∏≠ÁöÑÈôÜÂÜõÂêéÂã§ÊîØÊè¥Ëàπ‰πãÈó¥ÊâßË°åÊ®°ÊãüÊÇ£ËÄÖËΩ¨ËøêÔºåÈÉ®ÁΩ≤‰∫ÜÊ∞¥‰∏ä‰∫§ÈÄöÂ∑•ÂÖ∑‰∫§Êç¢ÁÇπ„ÄÇ‰∏§Êû∂Áõ¥ÂçáÊú∫ÂùáÊåâÁÖßÊàë‰ª¨‰ºòÂåñÁöÑÂÜ≥Á≠ñÁ≠ñÁï•ËøõË°åË∞ÉÂ∫¶„ÄÇ

##### **Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support**
2410.10853v1 by Abdul Muqtadir, Hafiz Syed Muhammad Bilal, Ayesha Yousaf, Hafiz Farooq Ahmed, Jamil Hussain

This research work delves into the manifestation of hallucination within
Large Language Models (LLMs) and its consequential impacts on applications
within the domain of mental health. The primary objective is to discern
effective strategies for curtailing hallucinatory occurrences, thereby
bolstering the dependability and security of LLMs in facilitating mental health
interventions such as therapy, counseling, and the dissemination of pertinent
information. Through rigorous investigation and analysis, this study seeks to
elucidate the underlying mechanisms precipitating hallucinations in LLMs and
subsequently propose targeted interventions to alleviate their occurrence. By
addressing this critical issue, the research endeavors to foster a more robust
framework for the utilization of LLMs within mental health contexts, ensuring
their efficacy and reliability in aiding therapeutic processes and delivering
accurate information to individuals seeking mental health support.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÂπªË¶∫ÁöÑË°®ÁèæÔºåÂèäÂÖ∂Â∞çÂøÉÁêÜÂÅ•Â∫∑È†òÂüüÊáâÁî®Áî¢ÁîüÁöÑÂæåÁ∫åÂΩ±Èüø„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØËæ®Âà•ÈÅèÂà∂ÂπªË¶∫ÁôºÁîüÁöÑÊúâÊïàÁ≠ñÁï•ÔºåÂæûËÄåÂä†Âº∑ LLM Âú®‰øÉÈÄ≤ÂøÉÁêÜÂÅ•Â∫∑Âπ≤È†êÊé™ÊñΩÔºà‰æãÂ¶ÇÊ≤ªÁôÇ„ÄÅË´ÆË©¢ÂíåÂÇ≥Êí≠Áõ∏ÈóúË≥áË®äÔºâÊñπÈù¢ÁöÑÂèØÈù†ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÈÄèÈÅéÂö¥Ë¨πÁöÑË™øÊü•ÂíåÂàÜÊûêÔºåÊú¨Á†îÁ©∂Ë©¶ÂúñÈó°ÊòéÂ∞éËá¥ LLM Áî¢ÁîüÂπªË¶∫ÁöÑÊΩõÂú®Ê©üÂà∂Ôºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÊèêÂá∫ÊúâÈáùÂ∞çÊÄßÁöÑÂπ≤È†êÊé™ÊñΩ‰æÜÊ∏õËºïÂÖ∂ÁôºÁîü„ÄÇÈÄèÈÅéËß£Ê±∫ÈÄôÂÄãÈóúÈçµÂïèÈ°åÔºåÊú¨Á†îÁ©∂Ëá¥ÂäõÊñºÂª∫Á´ã‰∏ÄÂÄãÊõ¥Á©©ÂÅ•ÁöÑÊû∂ÊßãÔºå‰ª•‰æøÂú®ÂøÉÁêÜÂÅ•Â∫∑ÊÉÖÂ¢É‰∏≠‰ΩøÁî® LLMÔºåÁ¢∫‰øùÂÖ∂Âú®ÂçîÂä©Ê≤ªÁôÇÈÅéÁ®ãÂíåÂêëÂ∞ãÊ±ÇÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅÁöÑÂÄã‰∫∫Êèê‰æõÊ∫ñÁ¢∫Ë≥áË®äÊñπÈù¢ÁöÑÊïàËÉΩÂíåÂèØÈù†ÊÄß„ÄÇ

##### **On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts**
2410.10850v2 by Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmotaleb El Saddik

We investigate and observe the behaviour and performance of Large Language
Model (LLM)-backed chatbots in addressing misinformed prompts and questions
with demographic information within the domains of Climate Change and Mental
Health. Through a combination of quantitative and qualitative methods, we
assess the chatbots' ability to discern the veracity of statements, their
adherence to facts, and the presence of bias or misinformation in their
responses. Our quantitative analysis using True/False questions reveals that
these chatbots can be relied on to give the right answers to these close-ended
questions. However, the qualitative insights, gathered from domain experts,
shows that there are still concerns regarding privacy, ethical implications,
and the necessity for chatbots to direct users to professional services. We
conclude that while these chatbots hold significant promise, their deployment
in sensitive areas necessitates careful consideration, ethical oversight, and
rigorous refinement to ensure they serve as a beneficial augmentation to human
expertise rather than an autonomous solution.

ÊëòË¶ÅÔºöÊàëÂÄëË™øÊü•‰∏¶ËßÄÂØüÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊîØÊåÅÁöÑËÅäÂ§©Ê©üÂô®‰∫∫Âú®Ê∞£ÂÄôËÆäÈÅ∑ÂíåÂøÉÁêÜÂÅ•Â∫∑È†òÂüü‰∏≠ËôïÁêÜÈåØË™§Ë®äÊÅØÊèêÁ§∫ÂíåÂïèÈ°åÁöÑË°åÁÇ∫ÂíåË°®Áèæ„ÄÇÈÄèÈÅéÁµêÂêàÈáèÂåñÂíåË≥™ÊÄßÊñπÊ≥ïÔºåÊàëÂÄëË©ï‰º∞ËÅäÂ§©Ê©üÂô®‰∫∫Ëæ®Âà•Èô≥Ëø∞ÁúüÂØ¶ÊÄßÁöÑËÉΩÂäõ„ÄÅ‰ªñÂÄëÂ∞ç‰∫ãÂØ¶ÁöÑÂ†ÖÊåÅÔºå‰ª•Âèä‰ªñÂÄëÂõûÊáâ‰∏≠ÂÅèË¶ãÊàñÈåØË™§Ë®äÊÅØÁöÑÂ≠òÂú®„ÄÇÊàëÂÄë‰ΩøÁî®Áúü/ÂÅáÂïèÈ°åÁöÑÈáèÂåñÂàÜÊûêÈ°ØÁ§∫ÔºåÂèØ‰ª•‰æùË≥¥ÈÄô‰∫õËÅäÂ§©Ê©üÂô®‰∫∫Â∞çÈÄô‰∫õÂ∞ÅÈñâÂºèÂïèÈ°åÁµ¶Âá∫Ê≠£Á¢∫Á≠îÊ°à„ÄÇÁÑ∂ËÄåÔºåÂæûÈ†òÂüüÂ∞àÂÆ∂Êî∂ÈõÜÁöÑË≥™ÊÄßË¶ãËß£È°ØÁ§∫ÔºåÂ∞çÊñºÈö±ÁßÅ„ÄÅÂÄ´ÁêÜÂΩ±Èüø‰ª•ÂèäËÅäÂ§©Ê©üÂô®‰∫∫Â∞á‰ΩøÁî®ËÄÖÂ∞éÂêëÂ∞àÊ•≠ÊúçÂãôÁöÑÂøÖË¶ÅÊÄß‰ªçÊúâÁñëÊÖÆ„ÄÇÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÂÑòÁÆ°ÈÄô‰∫õËÅäÂ§©Ê©üÂô®‰∫∫Ê•µÂÖ∑ÂâçÊôØÔºå‰ΩÜÂú®ÊïèÊÑüÈ†òÂüüÈÉ®ÁΩ≤ÂÆÉÂÄëÈúÄË¶Å‰ªîÁ¥∞ËÄÉÈáè„ÄÅÈÅìÂæ∑Áõ£Áù£ÂíåÂö¥Ê†ºÊîπÈÄ≤Ôºå‰ª•Á¢∫‰øùÂÆÉÂÄë‰ΩúÁÇ∫‰∫∫È°ûÂ∞àÊ•≠Áü•Ë≠òÁöÑÊúâÁõäÊì¥ÂÖÖÔºåËÄå‰∏çÊòØËá™‰∏ªËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **RespDiff: An End-to-End Multi-scale RNN Diffusion Model for Respiratory Waveform Estimation from PPG Signals**
2410.04366v1 by Yuyang Miao, Zehua Chen, Chang Li, Danilo Mandic

Respiratory rate (RR) is a critical health indicator often monitored under
inconvenient scenarios, limiting its practicality for continuous monitoring.
Photoplethysmography (PPG) sensors, increasingly integrated into wearable
devices, offer a chance to continuously estimate RR in a portable manner. In
this paper, we propose RespDiff, an end-to-end multi-scale RNN diffusion model
for respiratory waveform estimation from PPG signals. RespDiff does not require
hand-crafted features or the exclusion of low-quality signal segments, making
it suitable for real-world scenarios. The model employs multi-scale encoders,
to extract features at different resolutions, and a bidirectional RNN to
process PPG signals and extract respiratory waveform. Additionally, a spectral
loss term is introduced to optimize the model further. Experiments conducted on
the BIDMC dataset demonstrate that RespDiff outperforms notable previous works,
achieving a mean absolute error (MAE) of 1.18 bpm for RR estimation while
others range from 1.66 to 2.15 bpm, showing its potential for robust and
accurate respiratory monitoring in real-world applications.

ÊëòË¶ÅÔºöÂëºÂê∏ÈÄüÁéá (RR) ÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑÂÅ•Â∫∑ÊåáÊ®ôÔºåÈÄöÂ∏∏Âú®‰∏çÊñπ‰æøÁöÑÊÉÖÊ≥Å‰∏ãÈÄ≤Ë°åÁõ£Ê∏¨ÔºåÈÄôÈôêÂà∂‰∫ÜÂÖ∂Âú®ÊåÅÁ∫åÁõ£Ê∏¨‰∏≠ÁöÑÂØ¶Áî®ÊÄß„ÄÇÂÖâÈõªÂÆπÁ©çÊèèË®òÊ≥ï (PPG) ÊÑüÊ∏¨Âô®Êó•ÁõäÊï¥ÂêàÂà∞ÂèØÁ©øÊà¥Ë£ùÁΩÆ‰∏≠ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁ®Æ‰ª•ÂèØÊîúÂºèÊñπÂºèÊåÅÁ∫å‰º∞Ë®à RR ÁöÑÊ©üÊúÉ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü RespDiffÔºåÈÄôÊòØ‰∏ÄÂÄãÁ´ØÂà∞Á´ØÁöÑÂ§öÂàÜËæ®Áéá RNN Êì¥Êï£Ê®°ÂûãÔºåÁî®ÊñºÂæû PPG Ë®äËôü‰∏≠‰º∞Ë®àÂëºÂê∏Ê≥¢ÂΩ¢„ÄÇRespDiff ‰∏çÈúÄË¶Å‰∫∫Â∑•ÁâπÂæµÊàñÊéíÈô§‰ΩéÂìÅË≥™Ë®äËôüÁâáÊÆµÔºå‰ΩøÂÖ∂ÈÅ©Áî®ÊñºÂØ¶ÈöõÂ†¥ÊôØ„ÄÇË©≤Ê®°ÂûãÊé°Áî®Â§öÂàÜËæ®ÁéáÁ∑®Á¢ºÂô®Ôºå‰ª•‰∏çÂêåËß£ÊûêÂ∫¶ÊèêÂèñÁâπÂæµÔºå‰∏¶Êé°Áî®ÈõôÂêë RNN ‰æÜËôïÁêÜ PPG Ë®äËôüÂíåÊèêÂèñÂëºÂê∏Ê≥¢ÂΩ¢„ÄÇÊ≠§Â§ñÔºåÈÇÑÂºïÂÖ•‰∫ÜÈ†ªË≠úÊêçÂ§±È†Ö‰ª•ÈÄ≤‰∏ÄÊ≠•ÊúÄ‰Ω≥ÂåñÊ®°Âûã„ÄÇÂú® BIDMC Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óË°®ÊòéÔºåRespDiff ÂÑ™Êñº‰πãÂâçÁöÑÈ°ØËëóÁ†îÁ©∂ÔºåÂú® RR ‰º∞Ë®à‰∏≠ÂØ¶Áèæ‰∫Ü 1.18 bpm ÁöÑÂπ≥ÂùáÁµïÂ∞çË™§Â∑Æ (MAE)ÔºåËÄåÂÖ∂‰ªñË™§Â∑ÆÁØÑÂúçÂæû 1.66 Âà∞ 2.15 bpmÔºåÈ°ØÁ§∫‰∫ÜÂÖ∂Âú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÈÄ≤Ë°åÁ©©ÂÅ•‰∏îÊ∫ñÁ¢∫ÁöÑÂëºÂê∏Áõ£Ê∏¨ÁöÑÊΩõÂäõ„ÄÇ

##### **Applying Quantum Autoencoders for Time Series Anomaly Detection**
2410.04154v2 by Robin Frehner, Kurt Stockinger

Anomaly detection is an important problem with applications in various
domains such as fraud detection, pattern recognition or medical diagnosis.
Several algorithms have been introduced using classical computing approaches.
However, using quantum computing for solving anomaly detection problems in time
series data is a widely unexplored research field.
  This paper explores the application of quantum autoencoders to time series
anomaly detection. We investigate two primary techniques for classifying
anomalies: (1) Analyzing the reconstruction error generated by the quantum
autoencoder and (2) latent representation analysis. Our simulated experimental
results, conducted across various ansaetze, demonstrate that quantum
autoencoders consistently outperform classical deep learning-based autoencoders
across multiple datasets. Specifically, quantum autoencoders achieve superior
anomaly detection performance while utilizing 60-230 times fewer parameters and
requiring five times fewer training iterations. In addition, we implement our
quantum encoder on real quantum hardware. Our experimental results demonstrate
that quantum autoencoders achieve anomaly detection performance on par with
their simulated counterparts.

ÊëòË¶ÅÔºöÁï∞Â∏∏ÂÅµÊ∏¨ÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÂïèÈ°åÔºåÂú®ÂêÑÂÄãÈ†òÂüüÈÉΩÊúâÊáâÁî®Ôºå‰æãÂ¶ÇË©êÊ¨∫ÂÅµÊ∏¨„ÄÅÊ®°ÂºèËæ®Ë≠òÊàñÈÜ´ÁôÇË®∫Êñ∑„ÄÇ
Â∑≤Á∂ìÊúâ‰ΩøÁî®ÂÇ≥Áµ±ÈÅãÁÆóÊñπÊ≥ïÊèêÂá∫ÁöÑÂ§öÁ®ÆÊºîÁÆóÊ≥ï„ÄÇ
ÁÑ∂ËÄåÔºå‰ΩøÁî®ÈáèÂ≠êÈÅãÁÆó‰æÜËß£Ê±∫ÊôÇÈñìÂ∫èÂàóË≥áÊñô‰∏≠ÁöÑÁï∞Â∏∏ÂÅµÊ∏¨ÂïèÈ°åÊòØ‰∏ÄÂÄãÂª£Ê≥õÊú™ÈñãÁôºÁöÑÁ†îÁ©∂È†òÂüü„ÄÇ
Êú¨ÊñáÊé¢Ë®éÈáèÂ≠êËá™ÂãïÁ∑®Á¢ºÂô®Âú®ÊôÇÈñìÂ∫èÂàóÁï∞Â∏∏ÂÅµÊ∏¨ÁöÑÊáâÁî®„ÄÇÊàëÂÄëÁ†îÁ©∂‰∫ÜÂÖ©Á®ÆÂàÜÈ°ûÁï∞Â∏∏ÁöÑ‰∏ªË¶ÅÊäÄË°ìÔºö(1) ÂàÜÊûêÈáèÂ≠êËá™ÂãïÁ∑®Á¢ºÂô®Áî¢ÁîüÁöÑÈáçÂª∫Ë™§Â∑ÆÔºå‰ª•Âèä (2) ÊΩõÂú®Ë°®Á§∫ÂàÜÊûê„ÄÇÊàëÂÄëÂú®ÂêÑÁ®Æ ansaetze ‰∏ãÈÄ≤Ë°åÊ®°Êì¨ÂØ¶È©óÔºåÁµêÊûúË°®ÊòéÔºåÈáèÂ≠êËá™ÂãïÁ∑®Á¢ºÂô®Âú®Â§öÂÄãË≥áÊñôÈõÜ‰∏äÂßãÁµÇÂÑ™ÊñºÂü∫ÊñºÁ∂ìÂÖ∏Ê∑±Â∫¶Â≠∏ÁøíÁöÑËá™ÂãïÁ∑®Á¢ºÂô®„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈáèÂ≠êËá™ÂãïÁ∑®Á¢ºÂô®Âú®‰ΩøÁî®Â∞ë 60-230 ÂÄçÁöÑÂèÉÊï∏ÂíåÈúÄË¶ÅÂ∞ë‰∫îÂÄçÁöÑË®ìÁ∑¥ÂèçË¶ÜÈÅãÁÆóÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂØ¶Áèæ‰∫ÜÂçìË∂äÁöÑÁï∞Â∏∏ÂÅµÊ∏¨ÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®ÁúüÂØ¶ÈáèÂ≠êÁ°¨È´î‰∏äÂØ¶Áèæ‰∫ÜÊàëÂÄëÁöÑÈáèÂ≠êÁ∑®Á¢ºÂô®„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÈáèÂ≠êËá™ÂãïÁ∑®Á¢ºÂô®ÂØ¶Áèæ‰∫ÜËàáÂÖ∂Ê®°Êì¨Â∞çÊáâÁâ©Áõ∏Áï∂ÁöÑÁï∞Â∏∏ÂÅµÊ∏¨ÊïàËÉΩ„ÄÇ

##### **DAMMI:Daily Activities in a Psychologically Annotated Multi-Modal IoT dataset**
2410.04152v1 by Mohsen Falah Rad, Kamrad Khoshhal Roudposhti, Mohammad Hassan Khoobkar, Mohsen Shirali, Zahra Ahmadi, Carlos Fernandez-Llatas

The growth in the elderly population and the shift in the age pyramid have
increased the demand for healthcare and well-being services. To address this
concern, alongside the rising cost of medical care, the concept of ageing at
home has emerged, driven by recent advances in medical and technological
solutions. Experts in computer science, communication technology, and
healthcare have collaborated to develop affordable health solutions by
employing sensors in living environments, wearable devices, and smartphones, in
association with advanced data mining and intelligent systems with learning
capabilities, to monitor, analyze, and predict the health status of elderly
individuals. However, implementing intelligent healthcare systems and
developing analytical techniques requires testing and evaluating algorithms on
real-world data. Despite the need, there is a shortage of publicly available
datasets that meet these requirements. To address this gap, we present the
DAMMI dataset in this work, designed to support researchers in the field. The
dataset includes daily activity data of an elderly individual collected via
home-installed sensors, smartphone data, and a wristband over 146 days. It also
contains daily psychological reports provided by a team of psychologists.
Furthermore, the data collection spans significant events such as the COVID-19
pandemic, New Year's holidays, and the religious month of Ramadan, offering
additional opportunities for analysis. In this paper, we outline detailed
information about the data collection system, the types of data recorded, and
pre-processed event logs. This dataset is intended to assist professionals in
IoT and data mining in evaluating and implementing their research ideas.

ÊëòË¶ÅÔºöÈö®ËëóËÄÅÂπ¥‰∫∫Âè£ÁöÑÂ¢ûÈï∑ÂíåÂπ¥ÈΩ°ÈáëÂ≠óÂ°îÁöÑËΩâËÆäÔºåÂ∞çÈÜ´ÁôÇ‰øùÂÅ•ÂíåÁ¶èÁ•âÊúçÂãôÁöÑÈúÄÊ±Ç‰πüÈö®‰πãÂ¢ûÂä†„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂä†‰∏äÈÜ´ÁôÇ‰øùÂÅ•ÊàêÊú¨ÁöÑ‰∏äÂçáÔºåÂú®ÂÆ∂‰∏≠ËÄÅÂåñÁöÑÊ¶ÇÂøµÊáâÈÅãËÄåÁîüÔºåÈÄôÂæóÁõäÊñºÈÜ´ÁôÇÂíåÊäÄË°ìËß£Ê±∫ÊñπÊ°àÁöÑÊúÄÊñ∞ÈÄ≤Â±ï„ÄÇÈõªËÖ¶ÁßëÂ≠∏„ÄÅÈÄöË®äÊäÄË°ìÂíåÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑÂ∞àÂÆ∂Âêà‰ΩúÈñãÁôº‰∫ÜÁ∂ìÊøüÂØ¶ÊÉ†ÁöÑÂÅ•Â∫∑Ëß£Ê±∫ÊñπÊ°àÔºåÊñπÊ≥ïÊòØÂú®ÁîüÊ¥ªÁí∞Â¢É‰∏≠‰ΩøÁî®ÊÑüÊ∏¨Âô®„ÄÅÁ©øÊà¥ÂºèË£ùÁΩÆÂíåÊô∫ÊÖßÂûãÊâãÊ©üÔºå‰∏¶ÁµêÂêàÂÖàÈÄ≤ÁöÑË≥áÊñôÊé¢ÂãòÂíåÂÖ∑ÂÇôÂ≠∏ÁøíËÉΩÂäõÁöÑÊô∫ÊÖßÁ≥ªÁµ±Ôºå‰æÜÁõ£Êéß„ÄÅÂàÜÊûêÂíåÈ†êÊ∏¨ËÄÅÂπ¥‰∫∫ÁöÑÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÇÁÑ∂ËÄåÔºåÂØ¶ÊñΩÊô∫ÊÖßÂûãÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÂíåÈñãÁôºÂàÜÊûêÊäÄË°ìÈúÄË¶ÅÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñô‰∏äÊ∏¨Ë©¶ÂíåË©ï‰º∞ÊºîÁÆóÊ≥ï„ÄÇÂÑòÁÆ°ÊúâÈÄôÂÄãÈúÄÊ±ÇÔºå‰ΩÜÁ¨¶ÂêàÈÄô‰∫õË¶ÅÊ±ÇÁöÑÂÖ¨ÈñãÂèØÁî®Ë≥áÊñôÈõÜÂçªÂæàÁº∫‰πè„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÊèêÂá∫‰∫Ü DAMMI Ë≥áÊñôÈõÜÔºåÊó®Âú®ÊîØÊè¥Ë©≤È†òÂüüÁöÑÁ†îÁ©∂‰∫∫Âì°„ÄÇË©≤Ë≥áÊñôÈõÜÂåÖÊã¨ÈÄèÈÅéÂÆâË£ùÂú®ÂÆ∂ÁöÑÊÑüÊ∏¨Âô®„ÄÅÊô∫ÊÖßÂûãÊâãÊ©üË≥áÊñôÂíå‰∏ÄÂÄãÊâãÁí∞Âú® 146 Â§©ÂÖßÊî∂ÈõÜÂà∞ÁöÑËÄÅÂπ¥‰∫∫Êó•Â∏∏Ê¥ªÂãïË≥áÊñô„ÄÇÂÆÉÈÇÑÂåÖÂê´Áî±ÂøÉÁêÜÂ≠∏ÂÆ∂ÂúòÈöäÊèê‰æõÁöÑÊØèÊó•ÂøÉÁêÜÂ†±Âëä„ÄÇÊ≠§Â§ñÔºåË≥áÊñôÊî∂ÈõÜÊ∂µËìã‰∫ÜÈáçË¶ÅÁöÑ‰∫ã‰ª∂Ôºå‰æãÂ¶Ç COVID-19 Â§ßÊµÅË°å„ÄÅÊñ∞Âπ¥ÂÅáÊúüÂíåÈΩãÊàíÊúàÔºåÁÇ∫ÂàÜÊûêÊèê‰æõ‰∫ÜÈ°çÂ§ñÁöÑÊ©üÊúÉ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊ¶ÇËø∞‰∫ÜÊúâÈóúË≥áÊñôÊî∂ÈõÜÁ≥ªÁµ±„ÄÅË®òÈåÑÁöÑË≥áÊñôÈ°ûÂûãÂíåÈ†êËôïÁêÜ‰∫ã‰ª∂Ë®òÈåÑÁöÑË©≥Á¥∞Ë≥áË®ä„ÄÇÊ≠§Ë≥áÊñôÈõÜÊó®Âú®ÂçîÂä©Áâ©ËÅØÁ∂≤ÂíåË≥áÊñôÊé¢ÂãòÊñπÈù¢ÁöÑÂ∞àÊ•≠‰∫∫Âì°Ë©ï‰º∞ÂíåÂØ¶ÊñΩÂÖ∂Á†îÁ©∂ÊßãÊÉ≥„ÄÇ

##### **From Hospital to Portables: A Universal ECG Foundation Model Built on 10+ Million Diverse Recordings**
2410.04133v1 by Jun Li, Aaron Aguirre, Junior Moura, Che Liu, Lanhai Zhong, Chenxi Sun, Gari Clifford, Brandon Westover, Shenda Hong

Artificial Intelligence (AI) has shown great promise in electrocardiogram
(ECG) analysis and cardiovascular disease detection. However, developing a
general AI-ECG model has been challenging due to inter-individual variability
and the diversity of ECG diagnoses, limiting existing models to specific
diagnostic tasks and datasets. Moreover, current AI-ECG models struggle to
achieve comparable performance between single-lead and 12-lead ECGs, limiting
the application of AI-ECG to portable and wearable ECG devices. To address
these limitations, we introduce an ECG Foundation Model (ECGFounder), a
general-purpose model that leverages real-world ECG annotations from cardiology
experts to broaden the diagnostic capabilities of ECG analysis. ECGFounder is
trained on over 10 million ECGs with 150 label categories from the
Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease
diagnosis through ECG analysis. The model is designed to be both effective
out-of-the-box and fine-tunable for downstream tasks, maximizing usability.
More importantly, we extend its application to single-lead ECGs, enabling
complex condition diagnoses and supporting various downstream tasks in mobile
and remote monitoring scenarios. Experimental results demonstrate that
ECGFounder achieves expert-level performance on internal validation sets for
both 12-lead and single-lead ECGs, while also exhibiting strong classification
performance and generalization across various diagnoses on external validation
sets. When fine-tuned, ECGFounder outperforms baseline models in demographics
detection, clinical event detection, and cross-modality cardiac rhythm
diagnosis. The trained model and data will be publicly released upon
publication through the bdsp.io. Our code is available at
https://github.com/bdsp-core/ECGFounder.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂú®ÂøÉÈõªÂúñÔºàECGÔºâÂàÜÊûêÂíåÂøÉË°ÄÁÆ°ÁñæÁóÖÊ™¢Ê∏¨ÊñπÈù¢Â∑≤Â±ïÁèæÊ•µ‰Ω≥ÁöÑÂâçÊôØ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂÄãÈ´îÈñìËÆäÁï∞ÊÄßÂíå ECG Ë®∫Êñ∑ÁöÑÂ§öÊ®£ÊÄßÔºåÈñãÁôºÈÄöÁî® AI-ECG Ê®°Âûã‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÊåëÊà∞ÔºåÈÄôÈôêÂà∂‰∫ÜÁèæÊúâÊ®°ÂûãÂè™ËÉΩÁî®ÊñºÁâπÂÆöÁöÑË®∫Êñ∑‰ªªÂãôÂíåË≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÁõÆÂâçÁöÑ AI-ECG Ê®°ÂûãÈõ£‰ª•Âú®ÂñÆÂ∞éÁ®ãÂíå 12 Â∞éÁ®ã ECG ‰πãÈñìÂèñÂæóÁõ∏Áï∂ÁöÑÊïàËÉΩÔºåÈÄôÈôêÂà∂‰∫Ü AI-ECG ÊáâÁî®ÊñºÂèØÊîúÂºèÂíåÂèØÁ©øÊà¥ ECG Ë£ùÁΩÆ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÁ®Æ ECG Âü∫Á§éÊ®°ÂûãÔºàECGFounderÔºâÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄöÁî®Ê®°ÂûãÔºåÂÆÉÂà©Áî®‰æÜËá™ÂøÉËáüÁóÖÂ∞àÂÆ∂ÁöÑÁúüÂØ¶‰∏ñÁïå ECG Ê®ôË®ªÔºå‰ª•Êì¥Â±ï ECG ÂàÜÊûêÁöÑË®∫Êñ∑ËÉΩÂäõ„ÄÇECGFounder Á∂ìÁî±Âìà‰Ωõ-ËâæÈªòÈáå ECG Ë≥áÊñôÂ∫´‰∏≠Ë∂ÖÈÅé 1 ÂçÉËê¨ÂÄã ECG Âíå 150 ÂÄãÊ®ôÁ±§È°ûÂà•ÈÄ≤Ë°åË®ìÁ∑¥ÔºåËÉΩÈÄèÈÅé ECG ÂàÜÊûêÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂøÉË°ÄÁÆ°ÁñæÁóÖË®∫Êñ∑„ÄÇË©≤Ê®°ÂûãË¢´Ë®≠Ë®àÁÇ∫ÈñãÁÆ±Âç≥Áî®‰∏îÂèØÂæÆË™ø‰ª•ÈÄ≤Ë°å‰∏ãÊ∏∏‰ªªÂãôÔºå‰ª•ÊúÄÂ§ßÂåñÂèØÁî®ÊÄß„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÂ∞áÂÖ∂ÊáâÁî®Âª∂‰º∏Ëá≥ÂñÆÂ∞éÁ®ã ECGÔºåËÉΩÈÄ≤Ë°åË§áÈõúÁãÄÊ≥ÅË®∫Êñ∑Ôºå‰∏¶ÊîØÊè¥Ë°åÂãïÂíåÈÅ†Á´ØÁõ£ÊéßÊÉÖÂ¢É‰∏≠ÁöÑÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåECGFounder Âú® 12 Â∞éÁ®ãÂíåÂñÆÂ∞éÁ®ã ECG ÁöÑÂÖßÈÉ®È©óË≠âÈõÜ‰∏≠ÈÅîÂà∞‰∫ÜÂ∞àÂÆ∂Á¥öÁöÑÊïàËÉΩÔºåÂêåÊôÇÂú®Â§ñÈÉ®È©óË≠âÈõÜ‰∏≠Â∞çÂêÑÁ®ÆË®∫Êñ∑‰πüÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÂàÜÈ°ûÊïàËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÁ∂ìÈÅéÂæÆË™øÂæåÔºåECGFounder Âú®‰∫∫Âè£Áµ±Ë®àË≥áÊñôÊ™¢Ê∏¨„ÄÅËá®Â∫ä‰∫ã‰ª∂Ê™¢Ê∏¨ÂíåË∑®Ê®°ÊÖãÂøÉÂæãË®∫Êñ∑ÊñπÈù¢ÂÑ™ÊñºÂü∫Ê∫ñÊ®°Âûã„ÄÇË®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÂíåË≥áÊñôÂ∞áÂú®Âá∫ÁâàÂæåÈÄèÈÅé bdsp.io ÂÖ¨ÈñãÁôºÂ∏É„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/bdsp-core/ECGFounder ÂèñÂæó„ÄÇ

##### **Taming the Tail: Leveraging Asymmetric Loss and Pade Approximation to Overcome Medical Image Long-Tailed Class Imbalance**
2410.04084v1 by Pankhi Kashyap, Pavni Tandon, Sunny Gupta, Abhishek Tiwari, Ritwik Kulkarni, Kshitij Sharad Jadhav

Long-tailed problems in healthcare emerge from data imbalance due to
variability in the prevalence and representation of different medical
conditions, warranting the requirement of precise and dependable classification
methods. Traditional loss functions such as cross-entropy and binary
cross-entropy are often inadequate due to their inability to address the
imbalances between the classes with high representation and the classes with
low representation found in medical image datasets. We introduce a novel
polynomial loss function based on Pade approximation, designed specifically to
overcome the challenges associated with long-tailed classification. This
approach incorporates asymmetric sampling techniques to better classify
under-represented classes. We conducted extensive evaluations on three publicly
available medical datasets and a proprietary medical dataset. Our
implementation of the proposed loss function is open-sourced in the public
repository:https://github.com/ipankhi/ALPA.

ÊëòË¶ÅÔºöÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÈï∑Â∞æÂïèÈ°åÊ∫êÊñºÊï∏Êìö‰∏çÂπ≥Ë°°ÔºåÈÄôÊòØÁî±Êñº‰∏çÂêåÈÜ´ÁôÇÁãÄÊ≥ÅÁöÑÊµÅË°åÁéáÂíåË°®ÁèæÂ≠òÂú®ËÆäÁï∞ÊÄßÔºåÈÄô‰øùË≠â‰∫ÜÂ∞çÁ≤æÁ¢∫‰∏îÂèØÈù†ÁöÑÂàÜÈ°ûÊñπÊ≥ïÁöÑÈúÄÊ±Ç„ÄÇÂÇ≥Áµ±ÊêçÂ§±ÂáΩÊï∏Ôºà‰æãÂ¶Ç‰∫§ÂèâÁÜµÂíå‰∫åÂÖÉ‰∫§ÂèâÁÜµÔºâÈÄöÂ∏∏‰∏çË∂≥ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁÑ°Ê≥ïËß£Ê±∫ÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏≠Ë°®Á§∫ÁéáÈ´òÁöÑÈ°ûÂà•ËàáË°®Á§∫Áéá‰ΩéÁöÑÈ°ûÂà•‰πãÈñìÁöÑ‰∏çÂπ≥Ë°°„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂü∫Êñº Pade Ëøë‰ººÁöÑÂ§öÈ†ÖÂºèÊêçÂ§±ÂáΩÊï∏ÔºåÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÂÖãÊúçËàáÈï∑Â∞æÂàÜÈ°ûÁõ∏ÈóúÁöÑÊåëÊà∞„ÄÇÊ≠§ÊñπÊ≥ïÁµêÂêà‰∫ÜÈùûÂ∞çÁ®±ÊäΩÊ®£ÊäÄË°ìÔºå‰ª•Êõ¥Â•ΩÂú∞Â∞ç‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑÈ°ûÂà•ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÂ∞ç‰∏âÂÄãÂÖ¨ÈñãÁöÑÈÜ´ÁôÇË≥áÊñôÈõÜÂíå‰∏ÄÂÄãÂ∞àÊúâÈÜ´ÁôÇË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑË©ï‰º∞„ÄÇÊàëÂÄëÂ∞çÊèêË≠∞ÊêçÂ§±ÂáΩÊï∏ÁöÑÂØ¶‰ΩúÂ∑≤Âú®ÂÖ¨ÂÖ±ÂÑ≤Â≠òÂ∫´‰∏≠ÈñãÊ∫êÔºöhttps://github.com/ipankhi/ALPA„ÄÇ

