{"2501.05891": {"publish_time": "2025-01-10", "title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific MCQs", "paper_summary": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.", "paper_summary_zh": "\u5728\u6559\u80b2\u4e2d\uff0c\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u6587\u672c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u80fd\u529b\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u5982\u4f55\u63d0\u9ad8\u5b66\u4e60\u548c\u6559\u5b66\u6548\u7387\u7684\u7814\u7a76\u3002\u6211\u4eec\u901a\u8fc7\u8c03\u67e5 LLM \u5982\u4f55\u5728\u786c\u4ef6\u9650\u5236\u548c\u4f18\u5316\u6280\u672f\u65b9\u9762\u56de\u7b54\u591a\u9879\u9009\u62e9\u9898 (MCQ) \u6765\u7814\u7a76\u8fd9\u4e9b\u6a21\u578b\u5bf9\u6559\u80b2\u8005\u548c\u5b66\u751f\u7684\u53ef\u8d1f\u62c5\u6027\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u901a\u7528\u7684\u9884\u8bad\u7ec3 LLM\uff08LLaMA-2 \u7684 7B\u300113B \u548c 70B \u53d8\u4f53\uff09\u6765\u63a2\u7d22\u8fd9\u4e2a\u7a7a\u95f4\uff0c\u4ee5\u56de\u7b54\u6765\u81ea\u7f16\u7a0b\u8bed\u8a00 (PL) \u8bfe\u7a0b\u7684 162 \u4e2a\u672c\u79d1\u751f\u7ea7\u522b\u7684 MCQ\u2014\u2014MCQ \u6570\u636e\u96c6\u662f\u8fd9\u9879\u5de5\u4f5c\u7684\u8d21\u732e\uff0c\u6211\u4eec\u516c\u5f00\u63d0\u4f9b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5256\u6790\u4e86\u4e0d\u540c\u56e0\u7d20\u5982\u4f55\u6539\u53d8\u54cd\u5e94\u7684\u51c6\u786e\u6027\uff0c\u4f8b\u5982\u4f7f\u7528\u73b0\u6210\u7684\u6750\u6599\u2014\u2014\uff08\u8bfe\u7a0b\u6559\u79d1\u4e66\u7684\uff09\u90e8\u5206\u2014\u2014\u8fdb\u884c\u5fae\u8c03\u548c\u91cf\u5316\uff08\u4ee5\u51cf\u5c11\u8d44\u6e90\u4f7f\u7528\uff09\u3002\u6700\u4e3b\u8981\u7684\u6536\u83b7\u662f\uff0c\u8f83\u5c0f\u7684\u57fa\u4e8e\u6559\u79d1\u4e66\u7684\u5fae\u8c03\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u7684\u8f83\u5927\u6a21\u578b\uff08\u5176\u9884\u8bad\u7ec3\u9700\u8981\u663e\u7740\u7684\u8d44\u6e90\uff09\uff0c\u8fd9\u4f7f\u5f97\u4f7f\u7528 LLM \u6765\u56de\u7b54 MCQ \u5728\u8d44\u6e90\u548c\u6750\u6599\u65b9\u9762\u53d8\u5f97\u53ef\u8d1f\u62c5\u3002", "author": "Bianca Raimondi et.al.", "authors": "Bianca Raimondi, Saverio Giallorenzo, Maurizio Gabbrielli", "id": "2501.05891v1", "paper_url": "http://arxiv.org/abs/2501.05891v1", "repo": "https://github.com/biancaraimondi/llama2_for_mcqs"}}