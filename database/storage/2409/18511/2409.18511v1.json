{"2409.18511": {"publish_time": "2024-09-27", "title": "Do We Need Domain-Specific Embedding Models? An Empirical Investigation", "paper_summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns, even when trained on large\ngeneral-purpose corpora. This study sheds light on the necessity of developing\ndomain-specific embedding models in the LLM era, offering valuable insights for\nresearchers and practitioners.", "paper_summary_zh": "<paragraph>\u5d4c\u5165\u5f0f\u6a21\u578b\u5728\u8868\u793a\u548c\u64f7\u53d6\u5404\u7a2e NLP \u61c9\u7528\u7a0b\u5f0f\u4e2d\u7684\u8cc7\u8a0a\u65b9\u9762\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u9032\u4e00\u6b65\u589e\u5f37\u4e86\u5d4c\u5165\u5f0f\u6a21\u578b\u7684\u6548\u80fd\uff0c\u9019\u4e9b\u6a21\u578b\u7d93\u7531\u6db5\u84cb\u5e7e\u4e4e\u6240\u6709\u9818\u57df\u7684\u5927\u91cf\u6587\u5b57\u8a13\u7df4\u800c\u6210\u3002\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u6703\u5728\u901a\u7528\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u4f8b\u5982\u5927\u898f\u6a21\u6587\u5b57\u5d4c\u5165\u57fa\u6e96 (MTEB)\uff0c\u5b83\u5011\u5728\u9019\u4e9b\u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u4e00\u500b\u95dc\u9375\u554f\u984c\u51fa\u73fe\u4e86\uff1a\u7576\u901a\u7528\u6a21\u578b\u5df2\u5728\u5305\u542b\u5c08\u696d\u9818\u57df\u6587\u5b57\u7684\u9f90\u5927\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u6642\uff0c\u958b\u767c\u7279\u5b9a\u9818\u57df\u7684\u5d4c\u5165\u5f0f\u6a21\u578b\u662f\u5426\u5fc5\u8981\uff1f\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ee5\u8ca1\u52d9\u9818\u57df\u70ba\u4f8b\uff0c\u5c0d\u6b64\u554f\u984c\u9032\u884c\u5be6\u8b49\u7814\u7a76\u3002\u6211\u5011\u5f15\u5165\u4e86\u8ca1\u52d9\u5927\u898f\u6a21\u6587\u5b57\u5d4c\u5165\u57fa\u6e96 (FinMTEB)\uff0c\u9019\u662f MTEB \u7684\u5c0d\u61c9\u7248\u672c\uff0c\u5305\u542b\u7279\u5b9a\u65bc\u8ca1\u52d9\u9818\u57df\u7684\u6587\u5b57\u8cc7\u6599\u96c6\u3002\u6211\u5011\u8a55\u4f30\u4e86\u4e03\u7a2e\u6700\u5148\u9032\u7684\u5d4c\u5165\u5f0f\u6a21\u578b\u5728 FinMTEB \u4e0a\u7684\u6548\u80fd\uff0c\u4e26\u89c0\u5bdf\u5230\u8207\u5b83\u5011\u5728 MTEB \u4e0a\u7684\u6548\u80fd\u76f8\u6bd4\uff0c\u51fa\u73fe\u986f\u8457\u7684\u6548\u80fd\u4e0b\u964d\u3002\u70ba\u4e86\u8aaa\u660e\u9019\u7a2e\u4e0b\u964d\u53ef\u80fd\u662f\u7531 FinMTEB \u8f03\u9ad8\u7684\u8907\u96dc\u5ea6\u6240\u9a45\u52d5\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u56db\u9805\u6307\u6a19\u4f86\u91cf\u5316\u8cc7\u6599\u96c6\u8907\u96dc\u5ea6\uff0c\u4e26\u5728\u6211\u5011\u7684\u5206\u6790\u4e2d\u63a7\u5236\u6b64\u56e0\u7d20\u3002\u6211\u5011\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u8b49\u64da\uff0c\u8b49\u660e\u6700\u5148\u9032\u7684\u5d4c\u5165\u5f0f\u6a21\u578b\u96e3\u4ee5\u64f7\u53d6\u7279\u5b9a\u9818\u57df\u7684\u8a9e\u8a00\u548c\u8a9e\u610f\u6a21\u5f0f\uff0c\u5373\u4f7f\u662f\u5728\u5927\u578b\u901a\u7528\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u4e5f\u662f\u5982\u6b64\u3002\u9019\u9805\u7814\u7a76\u95e1\u660e\u4e86\u5728 LLM \u6642\u4ee3\u958b\u767c\u7279\u5b9a\u9818\u57df\u5d4c\u5165\u5f0f\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u70ba\u7814\u7a76\u4eba\u54e1\u548c\u5be6\u52d9\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002</paragraph>", "author": "Yixuan Tang et.al.", "authors": "Yixuan Tang, Yi Yang", "id": "2409.18511v1", "paper_url": "http://arxiv.org/abs/2409.18511v1", "repo": "https://github.com/yixuantt/finmteb"}}