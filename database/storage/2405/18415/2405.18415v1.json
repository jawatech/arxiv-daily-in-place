{"2405.18415": {"publish_time": "2024-05-28", "title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "paper_summary": "Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.", "paper_summary_zh": "\u5f71\u50cf\u5206\u985e\u662f\u6a5f\u5668\u8996\u89ba\u667a\u80fd\u6700\u57fa\u672c\u7684\u6280\u80fd\u4e4b\u4e00\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u8996\u89ba\u57fa\u790e\u8a9e\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u4f8b\u5982 GPT-4V \u548c LLaVA\uff0c\u91cd\u65b0\u63a2\u8a0e\u5f71\u50cf\u5206\u985e\u4efb\u52d9\u3002\u6211\u5011\u767c\u73fe\u73fe\u6709\u7684\u5c08\u6709\u548c\u516c\u958b VLM\uff0c\u5118\u7ba1\u7d93\u5e38\u4f7f\u7528 CLIP \u4f5c\u70ba\u8996\u89ba\u7de8\u78bc\u5668\uff0c\u4e14\u5177\u6709\u66f4\u591a\u53c3\u6578\uff0c\u4f46\u5728 ImageNet \u7b49\u6a19\u6e96\u5f71\u50cf\u5206\u985e\u57fa\u6e96\u4e0a\u537b\u660e\u986f\u4e0d\u5982 CLIP\u3002\u70ba\u4e86\u4e86\u89e3\u539f\u56e0\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u95dc\u65bc VLM \u4e2d\u7684\u63a8\u8ad6\u6f14\u7b97\u6cd5\u3001\u8a13\u7df4\u76ee\u6a19\u548c\u8cc7\u6599\u8655\u7406\u7684\u5e7e\u500b\u5047\u8a2d\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u4e3b\u8981\u539f\u56e0\u8207\u8cc7\u6599\u6709\u95dc\uff1a\u5f71\u50cf\u5206\u985e\u7684\u95dc\u9375\u8cc7\u8a0a\u7de8\u78bc\u5728 VLM \u7684\u6f5b\u5728\u7a7a\u9593\u4e2d\uff0c\u4f46\u53ea\u6709\u5728\u6709\u8db3\u5920\u7684\u8a13\u7df4\u8cc7\u6599\u6642\u624d\u80fd\u6709\u6548\u89e3\u78bc\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cVLM \u8a13\u7df4\u671f\u9593\u985e\u5225\u66dd\u5149\u7684\u983b\u7387\u8207\u6307\u4ee4\u5fae\u8abf\uff0c\u4ee5\u53ca VLM \u5728\u9019\u4e9b\u985e\u5225\u4e2d\u7684\u8868\u73fe\u4e4b\u9593\u5b58\u5728\u5f88\u5f37\u7684\u76f8\u95dc\u6027\uff1b\u7576\u4f7f\u7528\u8db3\u5920\u7684\u8cc7\u6599\u8a13\u7df4\u6642\uff0cVLM \u53ef\u4ee5\u9054\u5230\u6700\u5148\u9032\u5206\u985e\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\u3002\u6839\u64da\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u900f\u904e\u5c07\u4ee5\u5206\u985e\u70ba\u91cd\u9ede\u7684\u8cc7\u6599\u96c6\u6574\u5408\u5230\u8a13\u7df4\u4e2d\u4f86\u589e\u5f37 VLM\uff0c\u4e26\u8b49\u660e VLM \u589e\u5f37\u7684\u5206\u985e\u6548\u80fd\u8f49\u79fb\u5230\u5176\u4e00\u822c\u80fd\u529b\uff0c\u9032\u800c\u4f7f\u65b0\u6536\u96c6\u7684 ImageWikiQA \u8cc7\u6599\u96c6\u63d0\u5347\u4e86 11.8%\u3002", "author": "Yuhui Zhang et.al.", "authors": "Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy", "id": "2405.18415v1", "paper_url": "http://arxiv.org/abs/2405.18415v1", "repo": "https://github.com/yuhui-zh15/vlmclassifier"}}