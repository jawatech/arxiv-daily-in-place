{"2410.23079": {"publish_time": "2024-10-30", "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference", "paper_summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e2d\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\n\u901a\u5e38\u96e3\u4ee5\u517c\u9867\u63a8\u8ad6\u901f\u5ea6\u548c\u904b\u7b97\u6548\u7387\uff0c\u9650\u5236\u4e86\n\u5373\u6642\u90e8\u7f72\u3002\u5feb\u53d6\u6a5f\u5236\u4e2d\u7684\u9375\u503c (KV) \u6e1b\u5c11\u4e86\u8f49\u63db\u5668\u6a21\u578b\u4e2d\u7684\u904b\u7b97\n\u958b\u92b7\uff0c\u4f46\u5728\u7dad\u8b77\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\nBUZZ\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684 KV \u5feb\u53d6\u6f14\u7b97\u6cd5\uff0c\u5229\u7528\u7d50\u69cb\u5316\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u4f86\n\u6700\u5c0f\u5316\u5feb\u53d6\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u540c\u6642\u63d0\u9ad8\u63a8\u8ad6\u901f\u5ea6\u3002BUZZ \u63a1\u7528\u8702\u5de2\u7d50\u69cb\n\u7a00\u758f\u5feb\u53d6\uff0c\u7d50\u5408\u6ed1\u52d5\u8996\u7a97\u4f86\u64f7\u53d6\u6700\u65b0\u8cc7\u8a0a\uff0c\u4e26\u52d5\u614b\u5c07\u6b77\u53f2\u4ee3\u5e63\u5206\u6bb5\u6210\n\u584a\uff0c\u4ee5\u512a\u5148\u8655\u7406\u9130\u8fd1\u5340\u57df\u4e2d\u7684\u91cd\u8981\u4ee3\u5e63\u3002\u6211\u5011\u5728\u56db\u500b\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\n\u8a55\u4f30 BUZZ\uff1aCNN/Daily Mail\u3001XSUM\u3001Wikitext \u548c 10-QA\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\nBUZZ (1) \u5728 LLM \u63a8\u8ad6\u4e2d\u5c07\u5feb\u53d6\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 $\\textbf{2.5}\\times$\uff0c\u540c\u6642\n\u5728\u9577\u6587\u672c\u6458\u8981\u4e2d\u7dad\u6301\u8d85\u904e 99% \u7684\u6e96\u78ba\u5ea6\uff0c\u4ee5\u53ca (2) \u5728\u76f8\u540c\u7684\u8a18\u61b6\u9ad4\u9650\u5236\u4e0b\uff0c\n\u8d85\u8d8a\u4e86\u591a\u6587\u4ef6\u554f\u7b54\u7684\u6700\u65b0\u6280\u8853\uff0c\u63d0\u5347\u4e86 $\\textbf{7.69%}$\uff0c\u800c\u5b8c\u6574\u5feb\u53d6\u65b9\u6cd5\n\u5247\u6703\u9047\u5230\u8a18\u61b6\u9ad4\u4e0d\u8db3\u7684\u554f\u984c\u3002\u6b64\u5916\uff0cBUZZ \u4ee5 $\\log{n}$ \u6642\u9593\u8907\u96dc\u5ea6\u5be6\u73fe\u4e86\n\u986f\u8457\u7684\u63a8\u8ad6\u901f\u5ea6\u63d0\u5347\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/JunqiZhao888/buzz-llm \u53d6\u5f97\u3002", "author": "Junqi Zhao et.al.", "authors": "Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, Shichao He", "id": "2410.23079v1", "paper_url": "http://arxiv.org/abs/2410.23079v1", "repo": "https://github.com/junqizhao888/buzz-llm"}}