{"2409.17539": {"publish_time": "2024-09-26", "title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but their performance in complex logical reasoning tasks remains\nunsatisfactory. Although some prompting methods, such as Chain-of-Thought, can\nimprove the reasoning ability of LLMs to some extent, they suffer from an\nunfaithful issue where derived conclusions may not align with the generated\nreasoning chain. To address this issue, some studies employ the approach of\npropositional logic to further enhance logical reasoning abilities of LLMs.\nHowever, the potential omissions in the extraction of logical expressions in\nthese methods can cause information loss in the logical reasoning process,\nthereby generating incorrect results. To this end, we propose Logic-of-Thought\n(LoT) prompting which employs propositional logic to generate expanded logical\ninformation from input context, and utilizes the generated logical information\nas an additional augmentation to the input prompts, thereby enhancing the\ncapability of logical reasoning. The LoT is orthogonal to existing prompting\nmethods and can be seamlessly integrated with them. Extensive experiments\ndemonstrate that LoT boosts the performance of various prompting methods with a\nstriking margin across five logical reasoning tasks. In particular, the LoT\nenhances Chain-of-Thought's performance on the ReClor dataset by +4.35%;\nmoreover, it improves Chain-of-Thought with Self-Consistency's performance on\nLogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on\nProofWriter dataset by +8%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u5728\u8907\u96dc\u908f\u8f2f\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u4ecd\u4e0d\u76e1\u7406\u60f3\u3002\u5118\u7ba1\u67d0\u4e9b\u63d0\u793a\u65b9\u6cd5\uff08\u4f8b\u5982\u601d\u60f3\u93c8\uff09\u53ef\u4ee5\u5728\u67d0\u7a2e\u7a0b\u5ea6\u4e0a\u6539\u5584 LLM \u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u5b58\u5728\u4e00\u500b\u4e0d\u5fe0\u5be6\u7684\u554f\u984c\uff0c\u5373\u63a8\u5c0e\u51fa\u7684\u7d50\u8ad6\u53ef\u80fd\u8207\u751f\u6210\u7684\u63a8\u7406\u93c8\u4e0d\u4e00\u81f4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4e00\u4e9b\u7814\u7a76\u63a1\u7528\u547d\u984c\u908f\u8f2f\u7684\u65b9\u6cd5\u4f86\u9032\u4e00\u6b65\u589e\u5f37 LLM \u7684\u908f\u8f2f\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u5728\u63d0\u53d6\u908f\u8f2f\u8868\u9054\u5f0f\u6642\u6f5b\u5728\u7684\u907a\u6f0f\u53ef\u80fd\u6703\u5c0e\u81f4\u908f\u8f2f\u63a8\u7406\u904e\u7a0b\u4e2d\u8cc7\u8a0a\u907a\u5931\uff0c\u5f9e\u800c\u7522\u751f\u4e0d\u6b63\u78ba\u7684\u7d50\u679c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u601d\u60f3\u908f\u8f2f (LoT) \u63d0\u793a\uff0c\u5b83\u63a1\u7528\u547d\u984c\u908f\u8f2f\u5f9e\u8f38\u5165\u4e0a\u4e0b\u6587\u751f\u6210\u64f4\u5145\u7684\u908f\u8f2f\u8cc7\u8a0a\uff0c\u4e26\u5c07\u751f\u6210\u7684\u908f\u8f2f\u8cc7\u8a0a\u4f5c\u70ba\u8f38\u5165\u63d0\u793a\u7684\u984d\u5916\u64f4\u5145\uff0c\u5f9e\u800c\u589e\u5f37\u908f\u8f2f\u63a8\u7406\u7684\u80fd\u529b\u3002LoT \u8207\u73fe\u6709\u7684\u63d0\u793a\u65b9\u6cd5\u6b63\u4ea4\uff0c\u4e26\u4e14\u53ef\u4ee5\u8207\u5b83\u5011\u7121\u7e2b\u6574\u5408\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8868\u660e\uff0cLoT \u4ee5\u986f\u8457\u7684\u5e45\u5ea6\u63d0\u5347\u4e86\u5404\u7a2e\u63d0\u793a\u65b9\u6cd5\u5728\u4e94\u9805\u908f\u8f2f\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002\u7279\u5225\u662f\uff0cLoT \u5c07\u601d\u60f3\u93c8\u5728 ReClor \u8cc7\u6599\u96c6\u4e0a\u7684\u8868\u73fe\u63d0\u5347\u4e86 +4.35%\uff1b\u6b64\u5916\uff0c\u5b83\u5c07\u5177\u6709\u81ea\u6d3d\u6027\u7684\u601d\u60f3\u93c8\u5728 LogiQA \u4e0a\u7684\u8868\u73fe\u63d0\u5347\u4e86 +5%\uff1b\u6b64\u5916\uff0c\u5b83\u5c07\u601d\u60f3\u6a39\u5728 ProofWriter \u8cc7\u6599\u96c6\u4e0a\u7684\u8868\u73fe\u63d0\u5347\u4e86 +8%\u3002", "author": "Tongxuan Liu et.al.", "authors": "Tongxuan Liu, Wenjiang Xu, Weizhe Huang, Xingyu Wang, Jiaxing Wang, Hailong Yang, Jing Li", "id": "2409.17539v1", "paper_url": "http://arxiv.org/abs/2409.17539v1", "repo": "null"}}