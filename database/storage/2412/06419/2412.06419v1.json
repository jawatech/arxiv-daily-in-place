{"2412.06419": {"publish_time": "2024-12-09", "title": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise Forward Importance Propagation", "paper_summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious language tasks, but their widespread deployment is impeded by their\nlarge size and high computational costs. Structural pruning is a prevailing\ntechnique used to introduce sparsity into pre-trained models and facilitate\ndirect hardware acceleration during inference by removing redundant connections\n(structurally-grouped parameters), such as channels and attention heads.\nExisting structural pruning approaches often employ either global or layer-wise\npruning criteria; however, they are hindered by ineffectiveness stemming from\ninaccurate evaluation of connection importance. Global pruning methods\ntypically assess component importance using near-zero and unreliable gradients,\nwhile layer-wise pruning approaches encounter significant pruning error\naccumulation issues. To this end, we propose a more accurate pruning metric\nbased on the block-wise importance score propagation, termed LLM-BIP.\nSpecifically, LLM-BIP precisely evaluates connection importance by gauging its\ninfluence on the respective transformer block output, which can be efficiently\napproximated in a single forward pass through an upper bound derived from the\nassumption of Lipschitz continuity. We evaluate the proposed method using\nLLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results\ndemonstrate that our approach achieves an average of 3.26% increase in accuracy\nfor common reasoning tasks compared to previous best baselines. It also reduces\nperplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB\ndataset, respectively.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u8a9e\u8a00\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u4f46\u5176\u5ee3\u6cdb\u90e8\u7f72\u53d7\u5230\u5176\u9f90\u5927\u898f\u6a21\u548c\u9ad8\u904b\u7b97\u6210\u672c\u7684\u963b\u7919\u3002\u7d50\u69cb\u5316\u526a\u679d\u662f\u4e00\u7a2e\u666e\u904d\u7528\u65bc\u5728\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u5f15\u5165\u7a00\u758f\u6027\uff0c\u4e26\u900f\u904e\u79fb\u9664\u591a\u9918\u9023\u63a5\uff08\u7d50\u69cb\u5316\u7fa4\u7d44\u53c3\u6578\uff09\uff0c\u4f8b\u5982\u901a\u9053\u548c\u6ce8\u610f\u529b\u982d\u90e8\uff0c\u4ee5\u5229\u65bc\u5728\u63a8\u8ad6\u671f\u9593\u76f4\u63a5\u786c\u9ad4\u52a0\u901f\u7684\u6280\u8853\u3002\u73fe\u6709\u7684\u7d50\u69cb\u5316\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u63a1\u7528\u5168\u57df\u6216\u9010\u5c64\u526a\u679d\u6e96\u5247\uff1b\u7136\u800c\uff0c\u5b83\u5011\u53d7\u5230\u6e90\u81ea\u65bc\u9023\u63a5\u91cd\u8981\u6027\u8a55\u4f30\u4e0d\u6e96\u78ba\u6240\u9020\u6210\u7684\u7121\u6548\u6027\u963b\u7919\u3002\u5168\u57df\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u63a5\u8fd1\u65bc\u96f6\u4e14\u4e0d\u53ef\u9760\u7684\u68af\u5ea6\u4f86\u8a55\u4f30\u7d44\u6210\u90e8\u5206\u7684\u91cd\u8981\u6027\uff0c\u800c\u9010\u5c64\u526a\u679d\u65b9\u6cd5\u5247\u6703\u906d\u9047\u986f\u8457\u7684\u526a\u679d\u8aa4\u5dee\u7d2f\u7a4d\u554f\u984c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u66f4\u7cbe\u78ba\u7684\u526a\u679d\u6307\u6a19\uff0c\u5176\u57fa\u65bc\u5340\u584a\u5f0f\u91cd\u8981\u6027\u5206\u6578\u50b3\u64ad\uff0c\u7a31\u70ba LLM-BIP\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cLLM-BIP \u7cbe\u78ba\u5730\u8a55\u4f30\u9023\u63a5\u7684\u91cd\u8981\u6027\uff0c\u65b9\u6cd5\u662f\u8a55\u4f30\u5176\u5c0d\u61c9Transformer\u5340\u584a\u8f38\u51fa\u7684\u5f71\u97ff\uff0c\u800c\u9019\u53ef\u4ee5\u7528\u4e00\u500b\u524d\u5411\u50b3\u905e\u4e2d\u7684\u55ae\u4e00\u4e0a\u9650\u4f86\u6709\u6548\u8fd1\u4f3c\uff0c\u8a72\u4e0a\u9650\u6e90\u81ea\u65bc Lipschitz \u9023\u7e8c\u6027\u7684\u5047\u8a2d\u3002\u6211\u5011\u4f7f\u7528 LLaMA-7B\u3001Vicuna-7B \u548c LLaMA-13B \u5728\u5e38\u898b\u7684\u96f6\u6b21\u4efb\u52d9\u4e2d\u8a55\u4f30\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u7d50\u679c\u8868\u660e\uff0c\u8207\u5148\u524d\u7684\u6700\u4f73\u57fa\u6e96\u7dda\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u5e38\u898b\u63a8\u7406\u4efb\u52d9\u4e2d\u5e73\u5747\u63d0\u9ad8\u4e86 3.26% \u7684\u6e96\u78ba\u5ea6\u3002\u5b83\u9084\u5206\u5225\u5c07 WikiText2 \u8cc7\u6599\u96c6\u548c PTB \u8cc7\u6599\u96c6\u7684\u56f0\u60d1\u5ea6\u5e73\u5747\u964d\u4f4e\u4e86 14.09 \u548c 68.76\u3002", "author": "Haihang Wu et.al.", "authors": "Haihang Wu", "id": "2412.06419v1", "paper_url": "http://arxiv.org/abs/2412.06419v1", "repo": "null"}}