{"2504.18400": {"publish_time": "2025-04-25", "title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography", "paper_summary": "Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis.", "paper_summary_zh": "<paragraph>\u5f62\u72c0\u6e2c\u91cf\u5df2\u6210\u70ba\u767d\u8cea\u7e96\u7dad\u675f\u6210\u50cf\u7684\u6709\u6f5b\u529b\u7684\u63cf\u8ff0\u7b26\uff0c\u70ba\u89e3\u5256\u8b8a\u7570\u6027\u4ee5\u53ca\u8207\u8a8d\u77e5\u548c\u81e8\u5e8a\u8868\u578b\u7684\u95dc\u806f\u63d0\u4f9b\u4e86\u88dc\u5145\u6027\u7684\u898b\u89e3\u3002\u7136\u800c\uff0c\u7531\u65bc\u4f9d\u8cf4\u57fa\u65bc\u9ad4\u7d20\u7684\u8868\u793a\uff0c\u7528\u65bc\u8a08\u7b97\u5f62\u72c0\u6e2c\u91cf\u7684\u50b3\u7d71\u65b9\u6cd5\u5c0d\u65bc\u5927\u898f\u6a21\u6578\u64da\u96c6\u800c\u8a00\u8a08\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u6642\u3002\u6211\u5011\u63d0\u51fa\u4e86 Tract2Shape\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5e7e\u4f55\uff08\u9ede\u96f2\uff09\u548c\u6a19\u91cf\uff08\u8868\u683c\uff09\u7279\u5fb5\u4f86\u9810\u6e2c\u5341\u500b\u767d\u8cea\u7e96\u7dad\u675f\u6210\u50cf\u5f62\u72c0\u6e2c\u91cf\u503c\u3002\u70ba\u4e86\u63d0\u9ad8\u6a21\u578b\u6548\u7387\uff0c\u6211\u5011\u5229\u7528\u964d\u7dad\u7b97\u6cd5\u8b93\u6a21\u578b\u9810\u6e2c\u4e94\u500b\u4e3b\u8981\u7684\u5f62\u72c0\u7d44\u6210\u90e8\u5206\u3002\u8a72\u6a21\u578b\u5728\u5169\u500b\u7368\u7acb\u7372\u53d6\u7684\u6578\u64da\u96c6 HCP-YA \u6578\u64da\u96c6\u548c PPMI \u6578\u64da\u96c6\u4e0a\u9032\u884c\u4e86\u8a13\u7df4\u548c\u8a55\u4f30\u3002\u6211\u5011\u901a\u904e\u5728 HCP-YA \u6578\u64da\u96c6\u4e0a\u8a13\u7df4\u548c\u6e2c\u8a66 Tract2Shape\uff0c\u4e26\u5c07\u7d50\u679c\u8207\u6700\u5148\u9032\u7684\u6a21\u578b\u9032\u884c\u6bd4\u8f03\u4f86\u8a55\u4f30\u5176\u6027\u80fd\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u8a55\u4f30\u5176\u7a69\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u5011\u9084\u5728\u672a\u898b\u904e\u7684 PPMI \u6578\u64da\u96c6\u4e0a\u6e2c\u8a66\u4e86 Tract2Shape\u3002Tract2Shape \u5728\u6240\u6709\u5341\u500b\u5f62\u72c0\u6e2c\u91cf\u65b9\u9762\u90fd\u512a\u65bc SOTA \u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff0c\u5728 HCP-YA \u6578\u64da\u96c6\u4e0a\u9054\u5230\u4e86\u6700\u9ad8\u7684\u5e73\u5747 Pearson's r \u548c\u6700\u4f4e\u7684 nMSE\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u614b\u8f38\u5165\u548c PCA \u90fd\u6709\u52a9\u65bc\u63d0\u9ad8\u6027\u80fd\u3002\u5728\u672a\u898b\u904e\u7684\u6e2c\u8a66 PPMI \u6578\u64da\u96c6\u4e0a\uff0cTract2Shape \u4fdd\u6301\u4e86\u9ad8 Pearson's r \u548c\u4f4e nMSE\uff0c\u8b49\u660e\u4e86\u5728\u8de8\u6578\u64da\u96c6\u8a55\u4f30\u4e2d\u7684\u5f37\u6cdb\u5316\u80fd\u529b\u3002Tract2Shape \u53ef\u4ee5\u6839\u64da\u7e96\u7dad\u675f\u6210\u50cf\u6578\u64da\u5feb\u901f\u3001\u6e96\u78ba\u4e14\u53ef\u6cdb\u5316\u5730\u9810\u6e2c\u767d\u8cea\u5f62\u72c0\u6e2c\u91cf\u503c\uff0c\u652f\u6301\u8de8\u6578\u64da\u96c6\u7684\u53ef\u64f4\u5c55\u5206\u6790\u3002\u8a72\u6846\u67b6\u70ba\u672a\u4f86\u5927\u898f\u6a21\u767d\u8cea\u5f62\u72c0\u5206\u6790\u5960\u5b9a\u4e86\u6709\u5e0c\u671b\u7684\u57fa\u790e\u3002</paragraph>\n", "author": "Yui Lo et.al.", "authors": "Yui Lo, Yuqian Chen, Dongnan Liu, Leo Zekelman, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Fan Zhang, Weidong Cai, Lauren J. O'Donnell", "id": "2504.18400v1", "paper_url": "http://arxiv.org/abs/2504.18400v1", "repo": "null"}}