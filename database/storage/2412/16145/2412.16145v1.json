{"2412.16145": {"publish_time": "2024-12-20", "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning", "paper_summary": "Improving the multi-step reasoning ability of large language models (LLMs)\nwith offline reinforcement learning (RL) is essential for quickly adapting them\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\nin aligning LLMs with human preferences, it is less suitable for multi-step\nreasoning tasks because (1) DPO relies on paired preference data, which is not\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\nuniformly, making it ineffective for credit assignment in multi-step reasoning\ntasks, which often come with sparse reward. In this work, we propose OREO\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\nmulti-step reasoning. Building on insights from previous works of maximum\nentropy reinforcement learning, it jointly learns a policy model and value\nfunction by optimizing the soft Bellman Equation. We show in principle that it\nreduces the need to collect pairwise data and enables better credit assignment.\nEmpirically, OREO surpasses existing offline learning methods on multi-step\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\nembodied agent control (ALFWorld). The approach can be extended to a\nmulti-iteration framework when additional resources are available. Furthermore,\nthe learned value function can be leveraged to guide the tree search for free,\nwhich can further boost performance during test time.", "paper_summary_zh": "<paragraph>\u900f\u904e\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2 (RL) \u4f86\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u591a\u6b65\u9a5f\u63a8\u7406\u80fd\u529b\uff0c\u5c0d\u65bc\u8b93 LLM \u80fd\u5feb\u901f\u9069\u61c9\u8907\u96dc\u7684\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u5df2\u5c55\u73fe\u51fa\u8b93 LLM \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u7684\u6f5b\u529b\uff0c\u4f46\u5b83\u8f03\u4e0d\u9069\u5408\u591a\u6b65\u9a5f\u63a8\u7406\u4efb\u52d9\uff0c\u539f\u56e0\u6709\uff1a(1) DPO \u4f9d\u8cf4\u914d\u5c0d\u504f\u597d\u8cc7\u6599\uff0c\u800c\u6b64\u985e\u8cc7\u6599\u4e0d\u6613\u53d6\u5f97\uff1b(2) DPO \u4e00\u8996\u540c\u4ec1\u5730\u5c0d\u5f85\u6240\u6709\u7b26\u865f\uff0c\u9019\u4f7f\u5f97\u5b83\u5728\u591a\u6b65\u9a5f\u63a8\u7406\u4efb\u52d9\u4e2d\u7121\u6cd5\u6709\u6548\u9032\u884c\u4fe1\u7528\u5206\u914d\uff0c\u800c\u591a\u6b65\u9a5f\u63a8\u7406\u4efb\u52d9\u901a\u5e38\u4f34\u96a8\u8457\u7a00\u758f\u7684\u734e\u52f5\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa OREO (\u96e2\u7dda\u63a8\u7406\u6700\u4f73\u5316)\uff0c\u9019\u662f\u4e00\u7a2e\u96e2\u7dda RL \u65b9\u6cd5\uff0c\u7528\u65bc\u589e\u5f37 LLM \u7684\u591a\u6b65\u9a5f\u63a8\u7406\u3002\u900f\u904e\u5efa\u69cb\u65bc\u6700\u5927\u71b5\u5f37\u5316\u5b78\u7fd2\u5148\u524d\u7684\u7814\u7a76\u898b\u89e3\uff0c\u5b83\u900f\u904e\u6700\u4f73\u5316\u8edf Bellman \u65b9\u7a0b\u5f0f\uff0c\u540c\u6642\u5b78\u7fd2\u653f\u7b56\u6a21\u578b\u548c\u50f9\u503c\u51fd\u6578\u3002\u6211\u5011\u5728\u539f\u7406\u4e0a\u8b49\u660e\uff0c\u5b83\u6e1b\u5c11\u4e86\u6536\u96c6\u6210\u5c0d\u8cc7\u6599\u7684\u9700\u6c42\uff0c\u4e26\u80fd\u9032\u884c\u66f4\u597d\u7684\u4fe1\u7528\u5206\u914d\u3002\u6839\u64da\u7d93\u9a57\uff0cOREO \u5728\u591a\u6b65\u9a5f\u63a8\u7406\u57fa\u6e96\u4e0a\u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u96e2\u7dda\u5b78\u7fd2\u65b9\u6cd5\uff0c\u5305\u62ec\u6578\u5b78\u63a8\u7406\u4efb\u52d9 (GSM8K\u3001MATH) \u548c\u5177\u8eab\u4ee3\u7406\u63a7\u5236 (ALFWorld)\u3002\u7576\u6709\u984d\u5916\u8cc7\u6e90\u6642\uff0c\u6b64\u65b9\u6cd5\u53ef\u4ee5\u5ef6\u4f38\u5230\u591a\u91cd\u53cd\u8986\u904b\u7b97\u67b6\u69cb\u3002\u6b64\u5916\uff0c\u5b78\u7fd2\u5230\u7684\u50f9\u503c\u51fd\u6578\u53ef\u4ee5\u88ab\u5229\u7528\u4f86\u514d\u8cbb\u5f15\u5c0e\u6a39\u72c0\u641c\u5c0b\uff0c\u9019\u53ef\u4ee5\u5728\u6e2c\u8a66\u671f\u9593\u9032\u4e00\u6b65\u63d0\u5347\u6548\u80fd\u3002</paragraph>", "author": "Huaijie Wang et.al.", "authors": "Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, Yi Wu", "id": "2412.16145v1", "paper_url": "http://arxiv.org/abs/2412.16145v1", "repo": "null"}}