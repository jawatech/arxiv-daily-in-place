{"2409.02228": {"publish_time": "2024-09-03", "title": "Unforgettable Generalization in Language Models", "paper_summary": "When language models (LMs) are trained to forget (or \"unlearn'') a skill, how\nprecisely does their behavior change? We study the behavior of transformer LMs\nin which tasks have been forgotten via fine-tuning on randomized labels. Such\nLMs learn to generate near-random predictions for individual examples in the\n\"training'' set used for forgetting. Across tasks, however, LMs exhibit extreme\nvariability in whether LM predictions change on examples outside the training\nset. In some tasks (like entailment classification), forgetting generalizes\nrobustly, and causes models to produce uninformative predictions on new task\ninstances; in other tasks (like physical commonsense reasoning and scientific\nquestion answering) forgetting affects only the training examples, and models\ncontinue to perform the \"forgotten'' task accurately even for examples very\nsimilar to those that appeared in the training set. Dataset difficulty is not\npredictive of whether a behavior can be forgotten; instead, generalization in\nforgetting is (weakly) predicted by the confidence of LMs' initial task\npredictions and the variability of LM representations of training data, with\nlow confidence and low variability both associated with greater generalization.\nPerhaps most surprisingly, random-label forgetting appears to be somewhat\ninsensitive to the contents of the training set: for example, models trained on\nscience questions with random labels continue to answer other science questions\naccurately, but begin to produce random labels on entailment classification\ntasks. Finally, we show that even generalizable forgetting is shallow: linear\nprobes trained on LMs' representations can still perform tasks reliably after\nforgetting. Our results highlight the difficulty and unpredictability of\nperforming targeted skill removal from models via fine-tuning.", "paper_summary_zh": "\u7576\u8a9e\u8a00\u6a21\u578b (LM) \u88ab\u8a13\u7df4\u53bb\u907a\u5fd8 (\u6216\u300c\u907a\u5931\u300d) \u4e00\u9805\u6280\u80fd\u6642\uff0c\u5b83\u5011\u7684\u884c\u70ba\u6703\u5982\u4f55\u7cbe\u6e96\u5730\u6539\u8b8a\uff1f\u6211\u5011\u7814\u7a76\u4e86Transformer LM \u7684\u884c\u70ba\uff0c\u5728\u9019\u4e9b\u884c\u70ba\u4e2d\uff0c\u4efb\u52d9\u5df2\u88ab\u907a\u5fd8\uff0c\u4e26\u900f\u904e\u5728\u96a8\u6a5f\u6a19\u7c64\u4e0a\u9032\u884c\u5fae\u8abf\u3002\u6b64\u985e LM \u5b78\u6703\u70ba\u300c\u8a13\u7df4\u300d\u96c6\u4e2d\u7528\u65bc\u907a\u5fd8\u7684\u500b\u5225\u7bc4\u4f8b\u7522\u751f\u8fd1\u4e4e\u96a8\u6a5f\u7684\u9810\u6e2c\u3002\u7136\u800c\uff0c\u5728\u5404\u9805\u4efb\u52d9\u4e2d\uff0cLM \u5728 LM \u9810\u6e2c\u662f\u5426\u6703\u6539\u8b8a\u8a13\u7df4\u96c6\u5916\u7684\u7bc4\u4f8b\u65b9\u9762\u8868\u73fe\u51fa\u6975\u5927\u7684\u8b8a\u7570\u6027\u3002\u5728\u67d0\u4e9b\u4efb\u52d9\u4e2d\uff08\u4f8b\u5982\u860a\u6db5\u5206\u985e\uff09\uff0c\u907a\u5fd8\u6703\u5f37\u5065\u5730\u6982\u5316\uff0c\u4e26\u5c0e\u81f4\u6a21\u578b\u5c0d\u65b0\u7684\u4efb\u52d9\u5be6\u4f8b\u7522\u751f\u7121\u610f\u7fa9\u7684\u9810\u6e2c\uff1b\u5728\u5176\u4ed6\u4efb\u52d9\u4e2d\uff08\u4f8b\u5982\u7269\u7406\u5e38\u8b58\u63a8\u7406\u548c\u79d1\u5b78\u554f\u984c\u89e3\u7b54\uff09\uff0c\u907a\u5fd8\u53ea\u6703\u5f71\u97ff\u8a13\u7df4\u7bc4\u4f8b\uff0c\u800c\u6a21\u578b\u4ecd\u80fd\u6e96\u78ba\u57f7\u884c\u300c\u88ab\u907a\u5fd8\u300d\u7684\u4efb\u52d9\uff0c\u5373\u4f7f\u5c0d\u65bc\u90a3\u4e9b\u51fa\u73fe\u5728\u8a13\u7df4\u96c6\u4e2d\u8207\u4e4b\u975e\u5e38\u985e\u4f3c\u7684\u7bc4\u4f8b\u4e5f\u662f\u5982\u6b64\u3002\u8cc7\u6599\u96c6\u7684\u96e3\u5ea6\u7121\u6cd5\u9810\u6e2c\u67d0\u7a2e\u884c\u70ba\u662f\u5426\u80fd\u88ab\u907a\u5fd8\uff1b\u76f8\u53cd\u5730\uff0c\u907a\u5fd8\u4e2d\u7684\u6982\u5316\u662f\u7531 LM \u521d\u59cb\u4efb\u52d9\u9810\u6e2c\u7684\u4fe1\u5fc3\u548c LM \u8a13\u7df4\u8cc7\u6599\u8868\u5fb5\u7684\u8b8a\u7570\u6027\uff08\u5f31\u5730\uff09\u9810\u6e2c\u7684\uff0c\u800c\u4f4e\u4fe1\u5fc3\u548c\u4f4e\u8b8a\u7570\u6027\u90fd\u8207\u66f4\u5927\u7684\u6982\u5316\u76f8\u95dc\u3002\u4e5f\u8a31\u6700\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u96a8\u6a5f\u6a19\u7c64\u907a\u5fd8\u4f3c\u4e4e\u5c0d\u8a13\u7df4\u96c6\u7684\u5167\u5bb9\u6709\u4e9b\u9072\u920d\uff1a\u4f8b\u5982\uff0c\u5728\u4f7f\u7528\u96a8\u6a5f\u6a19\u7c64\u8a13\u7df4\u79d1\u5b78\u554f\u984c\u7684\u6a21\u578b\u6703\u7e7c\u7e8c\u6e96\u78ba\u56de\u7b54\u5176\u4ed6\u79d1\u5b78\u554f\u984c\uff0c\u4f46\u6703\u958b\u59cb\u5728\u860a\u6db5\u5206\u985e\u4efb\u52d9\u4e2d\u7522\u751f\u96a8\u6a5f\u6a19\u7c64\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5373\u4f7f\u662f\u53ef\u6982\u5316\u7684\u907a\u5fd8\u4e5f\u662f\u819a\u6dfa\u7684\uff1a\u5728 LM \u8868\u5fb5\u4e0a\u8a13\u7df4\u7684\u7dda\u6027\u63a2\u6e2c\u4ecd\u53ef\u4ee5\u5728\u907a\u5fd8\u5f8c\u53ef\u9760\u5730\u57f7\u884c\u4efb\u52d9\u3002\u6211\u5011\u7684\u7d50\u679c\u7a81\u51fa\u4e86\u900f\u904e\u5fae\u8abf\u5f9e\u6a21\u578b\u4e2d\u57f7\u884c\u76ee\u6a19\u6280\u80fd\u79fb\u9664\u7684\u56f0\u96e3\u6027\u548c\u4e0d\u53ef\u9810\u6e2c\u6027\u3002", "author": "Eric Zhang et.al.", "authors": "Eric Zhang, Leshem Chosen, Jacob Andreas", "id": "2409.02228v1", "paper_url": "http://arxiv.org/abs/2409.02228v1", "repo": "null"}}