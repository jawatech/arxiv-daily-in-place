{"2409.09748": {"publish_time": "2024-09-15", "title": "Explore the Hallucination on Low-level Perception for MLLMs", "paper_summary": "The rapid development of Multi-modality Large Language Models (MLLMs) has\nsignificantly influenced various aspects of industry and daily life, showcasing\nimpressive capabilities in visual perception and understanding. However, these\nmodels also exhibit hallucinations, which limit their reliability as AI\nsystems, especially in tasks involving low-level visual perception and\nunderstanding. We believe that hallucinations stem from a lack of explicit\nself-awareness in these models, which directly impacts their overall\nperformance. In this paper, we aim to define and evaluate the self-awareness of\nMLLMs in low-level visual perception and understanding tasks. To this end, we\npresent QL-Bench, a benchmark settings to simulate human responses to low-level\nvision, investigating self-awareness in low-level visual perception through\nvisual question answering related to low-level attributes such as clarity and\nlighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990\nsingle images and 1,999 image pairs, each accompanied by an open-ended question\nabout its low-level features. Through the evaluation of 15 MLLMs, we\ndemonstrate that while some models exhibit robust low-level visual\ncapabilities, their self-awareness remains relatively underdeveloped. Notably,\nfor the same model, simpler questions are often answered more accurately than\ncomplex ones. However, self-awareness appears to improve when addressing more\nchallenging questions. We hope that our benchmark will motivate further\nresearch, particularly focused on enhancing the self-awareness of MLLMs in\ntasks involving low-level visual perception and understanding.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u5feb\u901f\u53d1\u5c55\u5df2\u663e\u7740\u5f71\u54cd\u4e86\u4ea7\u4e1a\u548c\u65e5\u5e38\u751f\u6d3b\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5c55\u793a\u4e86\u5728\u89c6\u89c9\u611f\u77e5\u548c\u7406\u89e3\u65b9\u9762\u7684\u60ca\u4eba\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e5f\u4f1a\u51fa\u73b0\u5e7b\u89c9\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4f4e\u7ea7\u89c6\u89c9\u611f\u77e5\u548c\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u3002\u6211\u4eec\u8ba4\u4e3a\u5e7b\u89c9\u6e90\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u81ea\u6211\u610f\u8bc6\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u4e86\u5b83\u4eec\u7684\u6574\u4f53\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u65e8\u5728\u5b9a\u4e49\u548c\u8bc4\u4f30 MLLM \u5728\u4f4e\u7ea7\u89c6\u89c9\u611f\u77e5\u548c\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u610f\u8bc6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 QL-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\u8bbe\u7f6e\uff0c\u7528\u4e8e\u6a21\u62df\u4eba\u7c7b\u5bf9\u4f4e\u7ea7\u89c6\u89c9\u7684\u53cd\u5e94\uff0c\u901a\u8fc7\u4e0e\u4f4e\u7ea7\u5c5e\u6027\uff08\u5982\u6e05\u6670\u5ea6\u548c\u5149\u7167\uff09\u76f8\u5173\u7684\u89c6\u89c9\u95ee\u9898\u89e3\u7b54\u6765\u8c03\u67e5\u4f4e\u7ea7\u89c6\u89c9\u611f\u77e5\u4e2d\u7684\u81ea\u6211\u610f\u8bc6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6784\u5efa\u4e86 LLSAVisionQA \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 2,990 \u5f20\u5355\u5f20\u56fe\u50cf\u548c 1,999 \u5f20\u56fe\u50cf\u5bf9\uff0c\u6bcf\u5f20\u56fe\u50cf\u6216\u56fe\u50cf\u5bf9\u90fd\u9644\u5e26\u4e00\u4e2a\u5173\u4e8e\u5176\u4f4e\u7ea7\u7279\u5f81\u7684\u5f00\u653e\u5f0f\u95ee\u9898\u3002\u901a\u8fc7\u5bf9 15 \u4e2a MLLM \u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u867d\u7136\u4e00\u4e9b\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u4f4e\u7ea7\u89c6\u89c9\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u7684\u81ea\u6211\u610f\u8bc6\u4ecd\u7136\u76f8\u5bf9\u6b20\u53d1\u8fbe\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5bf9\u4e8e\u540c\u4e00\u6a21\u578b\uff0c\u7b80\u5355\u7684\u95ee\u9898\u901a\u5e38\u6bd4\u590d\u6742\u7684\u95ee\u9898\u56de\u7b54\u5f97\u66f4\u51c6\u786e\u3002\u7136\u800c\uff0c\u5728\u89e3\u51b3\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u65f6\uff0c\u81ea\u6211\u610f\u8bc6\u4f3c\u4e4e\u6709\u6240\u63d0\u9ad8\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u57fa\u51c6\u5c06\u6fc0\u52b1\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u4e13\u6ce8\u4e8e\u589e\u5f3a MLLM \u5728\u6d89\u53ca\u4f4e\u7ea7\u89c6\u89c9\u611f\u77e5\u548c\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u610f\u8bc6\u3002", "author": "Yinan Sun et.al.", "authors": "Yinan Sun, Zicheng Zhang, Haoning Wu, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min", "id": "2409.09748v1", "paper_url": "http://arxiv.org/abs/2409.09748v1", "repo": "null"}}