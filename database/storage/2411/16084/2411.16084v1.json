{"2411.16084": {"publish_time": "2024-11-25", "title": "Deciphering genomic codes using advanced NLP techniques: a scoping review", "paper_summary": "Objectives: The vast and complex nature of human genomic sequencing data\npresents challenges for effective analysis. This review aims to investigate the\napplication of Natural Language Processing (NLP) techniques, particularly Large\nLanguage Models (LLMs) and transformer architectures, in deciphering genomic\ncodes, focusing on tokenization, transformer models, and regulatory annotation\nprediction. The goal of this review is to assess data and model accessibility\nin the most recent literature, gaining a better understanding of the existing\ncapabilities and constraints of these tools in processing genomic sequencing\ndata.\n  Methods: Following Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines, our scoping review was conducted across\nPubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library.\nStudies were included if they focused on NLP methodologies applied to genomic\nsequencing data analysis, without restrictions on publication date or article\ntype.\n  Results: A total of 26 studies published between 2021 and April 2024 were\nselected for review. The review highlights that tokenization and transformer\nmodels enhance the processing and understanding of genomic data, with\napplications in predicting regulatory annotations like transcription-factor\nbinding sites and chromatin accessibility.\n  Discussion: The application of NLP and LLMs to genomic sequencing data\ninterpretation is a promising field that can help streamline the processing of\nlarge-scale genomic data while also providing a better understanding of its\ncomplex structures. It has the potential to drive advancements in personalized\nmedicine by offering more efficient and scalable solutions for genomic\nanalysis. Further research is also needed to discuss and overcome current\nlimitations, enhancing model transparency and applicability.", "paper_summary_zh": "<paragraph>\u76ee\u6a19\uff1a\u4eba\u985e\u57fa\u56e0\u7d44\u5b9a\u5e8f\u8cc7\u6599\u7684\u5ee3\u6cdb\u4e14\u8907\u96dc\u7684\u6027\u8cea\u70ba\u6709\u6548\u5206\u6790\u5e36\u4f86\u6311\u6230\u3002\u672c\u7bc7\u8a55\u8ad6\u65e8\u5728\u63a2\u8a0e\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u6280\u8853\u7684\u61c9\u7528\uff0c\u7279\u5225\u662f\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u548cTransformer\u67b6\u69cb\uff0c\u5728\u7834\u8b6f\u57fa\u56e0\u7d44\u5bc6\u78bc\u4e2d\u7684\u61c9\u7528\uff0c\u91cd\u9ede\u95dc\u6ce8\u5206\u8a5e\u3001Transformer\u6a21\u578b\u548c\u8abf\u63a7\u8a3b\u91cb\u9810\u6e2c\u3002\u672c\u7bc7\u8a55\u8ad6\u7684\u76ee\u6a19\u662f\u8a55\u4f30\u6700\u65b0\u6587\u737b\u4e2d\u7684\u8cc7\u6599\u548c\u6a21\u578b\u53ef\u53ca\u6027\uff0c\u4ee5\u66f4\u6df1\u5165\u4e86\u89e3\u9019\u4e9b\u5de5\u5177\u5728\u8655\u7406\u57fa\u56e0\u7d44\u5b9a\u5e8f\u8cc7\u6599\u65b9\u9762\u7684\u73fe\u6709\u80fd\u529b\u548c\u9650\u5236\u3002\n\u65b9\u6cd5\uff1a\u9075\u5faa\u7cfb\u7d71\u6027\u56de\u9867\u548c\u5f8c\u8a2d\u5206\u6790\u7684\u9996\u9078\u5831\u544a\u9805\u76ee (PRISMA) \u6307\u5357\uff0c\u6211\u5011\u7684\u7bc4\u570d\u56de\u9867\u5728 PubMed\u3001Medline\u3001Scopus\u3001Web of Science\u3001Embase \u548c ACM \u6578\u4f4d\u5716\u66f8\u9928\u4e2d\u9032\u884c\u3002\u5982\u679c\u7814\u7a76\u91cd\u9ede\u662f\u61c9\u7528\u65bc\u57fa\u56e0\u7d44\u5b9a\u5e8f\u8cc7\u6599\u5206\u6790\u7684 NLP \u65b9\u6cd5\uff0c\u5247\u7d0d\u5165\u7814\u7a76\uff0c\u800c\u4e0d\u9650\u5236\u767c\u8868\u65e5\u671f\u6216\u6587\u7ae0\u985e\u578b\u3002\n\u7d50\u679c\uff1a\u5171\u9078\u51fa 2021 \u5e74\u81f3 2024 \u5e74 4 \u6708\u9593\u767c\u8868\u7684 26 \u7bc7\u7814\u7a76\u9032\u884c\u56de\u9867\u3002\u56de\u9867\u5f37\u8abf\uff0c\u5206\u8a5e\u548cTransformer\u6a21\u578b\u589e\u5f37\u4e86\u57fa\u56e0\u7d44\u8cc7\u6599\u7684\u8655\u7406\u548c\u7406\u89e3\uff0c\u4e26\u61c9\u7528\u65bc\u9810\u6e2c\u8f49\u9304\u56e0\u5b50\u7d50\u5408\u4f4d\u9ede\u548c\u67d3\u8272\u8cea\u53ef\u53ca\u6027\u7b49\u8abf\u63a7\u8a3b\u91cb\u3002\n\u8a0e\u8ad6\uff1a\u5c07 NLP \u548c LLM \u61c9\u7528\u65bc\u57fa\u56e0\u7d44\u5b9a\u5e8f\u8cc7\u6599\u89e3\u8b80\u662f\u4e00\u500b\u6709\u524d\u666f\u7684\u9818\u57df\uff0c\u6709\u52a9\u65bc\u7c21\u5316\u5927\u898f\u6a21\u57fa\u56e0\u7d44\u8cc7\u6599\u7684\u8655\u7406\uff0c\u540c\u6642\u4e5f\u66f4\u6df1\u5165\u4e86\u89e3\u5176\u8907\u96dc\u7d50\u69cb\u3002\u5b83\u6709\u6f5b\u529b\u900f\u904e\u63d0\u4f9b\u66f4\u6709\u6548\u7387\u4e14\u53ef\u64f4\u5145\u7684\u57fa\u56e0\u7d44\u5206\u6790\u89e3\u6c7a\u65b9\u6848\uff0c\u63a8\u52d5\u500b\u4eba\u5316\u91ab\u7642\u7684\u9032\u6b65\u3002\u9032\u4e00\u6b65\u7684\u7814\u7a76\u4e5f\u9700\u8981\u8a0e\u8ad6\u4e26\u514b\u670d\u76ee\u524d\u7684\u9650\u5236\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u9069\u7528\u6027\u3002</paragraph>", "author": "Shuyan Cheng et.al.", "authors": "Shuyan Cheng, Yishu Wei, Yiliang Zhou, Zihan Xu, Drew N Wright, Jinze Liu, Yifan Peng", "id": "2411.16084v1", "paper_url": "http://arxiv.org/abs/2411.16084v1", "repo": "null"}}