{"2410.22143": {"publish_time": "2024-10-29", "title": "AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts", "paper_summary": "Although large language models (LLMs) are typically aligned, they remain\nvulnerable to jailbreaking through either carefully crafted prompts in natural\nlanguage or, interestingly, gibberish adversarial suffixes. However, gibberish\ntokens have received relatively less attention despite their success in\nattacking aligned LLMs. Recent work, AmpleGCG~\\citep{liao2024amplegcg},\ndemonstrates that a generative model can quickly produce numerous customizable\ngibberish adversarial suffixes for any harmful query, exposing a range of\nalignment gaps in out-of-distribution (OOD) language spaces. To bring more\nattention to this area, we introduce AmpleGCG-Plus, an enhanced version that\nachieves better performance in fewer attempts. Through a series of exploratory\nexperiments, we identify several training strategies to improve the learning of\ngibberish suffixes. Our results, verified under a strict evaluation setting,\nshow that it outperforms AmpleGCG on both open-weight and closed-source models,\nachieving increases in attack success rate (ASR) of up to 17\\% in the white-box\nsetting against Llama-2-7B-chat, and more than tripling ASR in the black-box\nsetting against GPT-4. Notably, AmpleGCG-Plus jailbreaks the newer GPT-4o\nseries of models at similar rates to GPT-4, and, uncovers vulnerabilities\nagainst the recently proposed circuit breakers defense. We publicly release\nAmpleGCG-Plus along with our collected training datasets.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u662f\u5c0d\u9f4a\u7684\uff0c\u4f46\u5b83\u5011\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u7cbe\u5fc3\u8a2d\u8a08\u7684\u81ea\u7136\u8a9e\u8a00\u63d0\u793a\u6216\u6709\u8da3\u7684\u80e1\u8a00\u4e82\u8a9e\u5c0d\u6297\u5f8c\u7db4\u7684\u8d8a\u7344\u653b\u64ca\u3002\u7136\u800c\uff0c\u5118\u7ba1\u80e1\u8a00\u4e82\u8a9e\u6a19\u8a18\u5728\u653b\u64ca\u5c0d\u9f4a\u7684 LLM \u4e0a\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5b83\u5011\u53d7\u5230\u7684\u95dc\u6ce8\u76f8\u5c0d\u8f03\u5c11\u3002\u6700\u8fd1\u7684\u4e00\u9805\u5de5\u4f5c AmpleGCG~\\citep{liao2024amplegcg} \u8b49\u660e\uff0c\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u7522\u751f\u5927\u91cf\u53ef\u81ea\u8a02\u7684\u80e1\u8a00\u4e82\u8a9e\u5c0d\u6297\u5f8c\u7db4\u4ee5\u9032\u884c\u4efb\u4f55\u6709\u5bb3\u67e5\u8a62\uff0c\u5f9e\u800c\u66b4\u9732\u51fa\u5206\u5e03\u5916 (OOD) \u8a9e\u8a00\u7a7a\u9593\u4e2d\u7684\u4e00\u7cfb\u5217\u5c0d\u9f4a\u5dee\u8ddd\u3002\u70ba\u4e86\u8b93\u66f4\u591a\u4eba\u95dc\u6ce8\u9019\u500b\u9818\u57df\uff0c\u6211\u5011\u5f15\u5165\u4e86 AmpleGCG-Plus\uff0c\u9019\u662f\u4e00\u500b\u589e\u5f37\u7248\u672c\uff0c\u53ef\u4ee5\u5728\u66f4\u5c11\u7684\u5617\u8a66\u4e2d\u53d6\u5f97\u66f4\u597d\u7684\u8868\u73fe\u3002\u900f\u904e\u4e00\u7cfb\u5217\u63a2\u7d22\u6027\u5be6\u9a57\uff0c\u6211\u5011\u627e\u51fa\u5e7e\u7a2e\u8a13\u7df4\u7b56\u7565\u4f86\u6539\u5584\u80e1\u8a00\u4e82\u8a9e\u5f8c\u7db4\u7684\u5b78\u7fd2\u3002\u6211\u5011\u7684\u7d50\u679c\u5728\u56b4\u683c\u7684\u8a55\u4f30\u8a2d\u5b9a\u4e0b\u5f97\u5230\u9a57\u8b49\uff0c\u986f\u793a\u5b83\u5728\u958b\u653e\u6b0a\u91cd\u548c\u9589\u6e90\u6a21\u578b\u4e0a\u90fd\u512a\u65bc AmpleGCG\uff0c\u5728\u91dd\u5c0d Llama-2-7B-chat \u7684\u767d\u76d2\u8a2d\u5b9a\u4e2d\u653b\u64ca\u6210\u529f\u7387 (ASR) \u63d0\u9ad8\u4e86 17%\uff0c\u5728\u91dd\u5c0d GPT-4 \u7684\u9ed1\u76d2\u8a2d\u5b9a\u4e2d\uff0cASR \u589e\u52a0\u4e86\u4e09\u500d\u4ee5\u4e0a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cAmpleGCG-Plus \u4ee5\u8207 GPT-4 \u76f8\u4f3c\u7684\u901f\u7387\u8d8a\u7344\u4e86\u66f4\u65b0\u7684 GPT-4o \u7cfb\u5217\u6a21\u578b\uff0c\u4e26\u63ed\u9732\u4e86\u91dd\u5c0d\u6700\u8fd1\u63d0\u51fa\u7684\u65b7\u8def\u5668\u9632\u79a6\u7684\u6f0f\u6d1e\u3002\u6211\u5011\u516c\u958b\u767c\u5e03 AmpleGCG-Plus \u4ee5\u53ca\u6211\u5011\u6536\u96c6\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\u3002", "author": "Vishal Kumar et.al.", "authors": "Vishal Kumar, Zeyi Liao, Jaylen Jones, Huan Sun", "id": "2410.22143v1", "paper_url": "http://arxiv.org/abs/2410.22143v1", "repo": "null"}}