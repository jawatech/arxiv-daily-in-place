{"2410.08182": {"publish_time": "2024-10-10", "title": "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models", "paper_summary": "Existing multimodal retrieval benchmarks primarily focus on evaluating\nwhether models can retrieve and utilize external textual knowledge for question\nanswering. However, there are scenarios where retrieving visual information is\neither more beneficial or easier to access than textual data. In this paper, we\nintroduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in\nwhich we systematically identify and categorize scenarios where visually\naugmented knowledge is better than textual knowledge, for instance, more images\nfrom varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353\nhuman-annotated multiple-choice questions across 9 distinct scenarios. With\nMRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large\nvision-language models (LVLMs). Our results show that all LVLMs exhibit greater\nimprovements when augmented with images compared to textual knowledge,\nconfirming that MRAG-Bench is vision-centric. Additionally, we conduct\nextensive analysis with MRAG-Bench, which offers valuable insights into\nretrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces\nchallenges in effectively leveraging retrieved knowledge, achieving only a\n5.82% improvement with ground-truth information, in contrast to a 33.16%\nimprovement observed in human participants. These findings highlight the\nimportance of MRAG-Bench in encouraging the community to enhance LVLMs' ability\nto utilize retrieved visual knowledge more effectively.", "paper_summary_zh": "\u73fe\u6709\u7684\u591a\u6a21\u614b\u6aa2\u7d22\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u5728\u8a55\u4f30\u6a21\u578b\u662f\u5426\u80fd\u6aa2\u7d22\u548c\u5229\u7528\u5916\u90e8\u6587\u672c\u77e5\u8b58\u4f86\u56de\u7b54\u554f\u984c\u3002\u7136\u800c\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0c\u6aa2\u7d22\u8996\u89ba\u8cc7\u8a0a\u6bd4\u6aa2\u7d22\u6587\u5b57\u8cc7\u6599\u66f4\u6709\u5229\u6216\u66f4\u5bb9\u6613\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u9032\u4e86\u4e00\u500b\u591a\u6a21\u614b\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u57fa\u6e96 MRAG-Bench\uff0c\u5176\u4e2d\u6211\u5011\u7cfb\u7d71\u6027\u5730\u627e\u51fa\u4e26\u5206\u985e\u51fa\u8996\u89ba\u589e\u5f37\u77e5\u8b58\u512a\u65bc\u6587\u5b57\u77e5\u8b58\u7684\u60c5\u6cc1\uff0c\u4f8b\u5982\u66f4\u591a\u4f86\u81ea\u4e0d\u540c\u8996\u89d2\u7684\u5f71\u50cf\u3002MRAG-Bench \u5305\u542b 16,130 \u5f35\u5f71\u50cf\u548c 1,353 \u500b\u7531\u4eba\u985e\u8a3b\u89e3\u7684\u591a\u91cd\u9078\u64c7\u984c\uff0c\u6db5\u84cb 9 \u7a2e\u4e0d\u540c\u7684\u60c5\u6cc1\u3002\u900f\u904e MRAG-Bench\uff0c\u6211\u5011\u5c0d 10 \u500b\u958b\u6e90\u548c 4 \u500b\u5c08\u6709\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u9032\u884c\u8a55\u4f30\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u6240\u6709 LVLMs \u5728\u52a0\u5165\u5f71\u50cf\u5f8c\u90fd\u5c55\u73fe\u51fa\u6bd4\u52a0\u5165\u6587\u5b57\u77e5\u8b58\u6642\u66f4\u5927\u7684\u9032\u6b65\uff0c\u8b49\u5be6 MRAG-Bench \u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528 MRAG-Bench \u9032\u884c\u5ee3\u6cdb\u7684\u5206\u6790\uff0c\u9019\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\uff0c\u8b93\u6211\u5011\u6df1\u5165\u4e86\u89e3\u6aa2\u7d22\u589e\u5f37 LVLMs\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b GPT-4o \u5728\u6709\u6548\u5229\u7528\u6aa2\u7d22\u5230\u7684\u77e5\u8b58\u65b9\u9762\u9762\u81e8\u6311\u6230\uff0c\u50c5\u7372\u5f97 5.82% \u7684\u771f\u5be6\u8cc7\u8a0a\u9032\u6b65\uff0c\u8207\u4eba\u985e\u53c3\u8207\u8005\u89c0\u5bdf\u5230\u7684 33.16% \u9032\u6b65\u5f62\u6210\u5c0d\u6bd4\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86 MRAG-Bench \u7684\u91cd\u8981\u6027\uff0c\u5b83\u9f13\u52f5\u793e\u7fa4\u63d0\u5347 LVLMs \u66f4\u6709\u6548\u5730\u5229\u7528\u6aa2\u7d22\u5230\u7684\u8996\u89ba\u77e5\u8b58\u7684\u80fd\u529b\u3002", "author": "Wenbo Hu et.al.", "authors": "Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, Nanyun Peng", "id": "2410.08182v1", "paper_url": "http://arxiv.org/abs/2410.08182v1", "repo": "null"}}