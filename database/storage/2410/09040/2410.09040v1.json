{"2410.09040": {"publish_time": "2024-10-11", "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation", "paper_summary": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack.", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u57fa\u65bcTransformer\u7684 LLM\uff08\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\u5c0d\u8d8a\u7344\u653b\u64ca\u7684\u8106\u5f31\u6027\uff0c\u7279\u5225\u95dc\u6ce8\u57fa\u65bc\u6700\u4f73\u5316\u7684\u8caa\u5a6a\u5750\u6a19\u68af\u5ea6 (GCG) \u7b56\u7565\u3002\u6211\u5011\u9996\u5148\u89c0\u5bdf\u5230\u653b\u64ca\u7684\u6709\u6548\u6027\u8207\u6a21\u578b\u7684\u5167\u90e8\u884c\u70ba\u4e4b\u9593\u5b58\u5728\u6b63\u76f8\u95dc\u3002\u4f8b\u5982\uff0c\u7576\u6a21\u578b\u66f4\u95dc\u6ce8\u65e8\u5728\u78ba\u4fdd LLM \u5b89\u5168\u5c0d\u9f4a\u7684\u7cfb\u7d71\u63d0\u793a\u6642\uff0c\u653b\u64ca\u5f80\u5f80\u4e0d\u592a\u6709\u6548\u3002\u57fa\u65bc\u9019\u4e00\u767c\u73fe\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u589e\u5f37\u7684\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u64cd\u7e31\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5206\u6578\u4ee5\u4fc3\u9032 LLM \u8d8a\u7344\uff0c\u6211\u5011\u7a31\u4e4b\u70ba AttnGCG\u3002\u6839\u64da\u7d93\u9a57\uff0cAttnGCG \u5728\u4e0d\u540c\u7684 LLM \u4e2d\u986f\u793a\u51fa\u653b\u64ca\u6548\u7387\u7684\u6301\u7e8c\u6539\u9032\uff0c\u5728 Llama-2 \u7cfb\u5217\u4e2d\u5e73\u5747\u589e\u52a0\u4e86 ~7%\uff0c\u5728 Gemma \u7cfb\u5217\u4e2d\u589e\u52a0\u4e86 ~10%\u3002\u6211\u5011\u7684\u7b56\u7565\u9084\u5c55\u793a\u4e86\u5c0d\u672a\u898b\u7684\u6709\u5bb3\u76ee\u6a19\u548c\u9ed1\u76d2 LLM\uff08\u5982 GPT-3.5 \u548c GPT-4\uff09\u7684\u5f37\u5927\u7684\u653b\u64ca\u53ef\u50b3\u905e\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u6ce8\u610f\u5230\u6211\u5011\u7684\u6ce8\u610f\u529b\u5206\u6578\u53ef\u8996\u5316\u66f4\u5177\u53ef\u89e3\u91cb\u6027\uff0c\u4f7f\u6211\u5011\u80fd\u5920\u66f4\u597d\u5730\u4e86\u89e3\u6211\u5011\u6709\u91dd\u5c0d\u6027\u7684\u6ce8\u610f\u529b\u64cd\u7e31\u5982\u4f55\u4fc3\u9032\u66f4\u6709\u6548\u7684\u8d8a\u7344\u3002\u6211\u5011\u5728 https://github.com/UCSC-VLAA/AttnGCG-attack \u4e0a\u767c\u5e03\u4ee3\u78bc\u3002", "author": "Zijun Wang et.al.", "authors": "Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie", "id": "2410.09040v1", "paper_url": "http://arxiv.org/abs/2410.09040v1", "repo": "https://github.com/ucsc-vlaa/attngcg-attack"}}