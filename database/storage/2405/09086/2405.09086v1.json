{"2405.09086": {"publish_time": "2024-05-15", "title": "Chaos-based reinforcement learning with TD3", "paper_summary": "Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. This approach offers a model for\nconsidering how the biological brain can create variability in its behavior and\nlearn in an exploratory manner. At the same time, it is a learning model that\nhas the ability to automatically switch between exploration and exploitation\nmodes and the potential to realize higher explorations that reflect what it has\nlearned so far. However, the learning algorithms in CBRL have not been\nwell-established in previous studies and have yet to incorporate recent\nadvances in reinforcement learning. This study introduced Twin Delayed Deep\nDeterministic Policy Gradients (TD3), which is one of the state-of-the-art deep\nreinforcement learning algorithms that can treat deterministic and continuous\naction spaces, to CBRL. The validation results provide several insights. First,\nTD3 works as a learning algorithm for CBRL in a simple goal-reaching task.\nSecond, CBRL agents with TD3 can autonomously suppress their exploratory\nbehavior as learning progresses and resume exploration when the environment\nchanges. Finally, examining the effect of the agent's chaoticity on learning\nshows that extremely strong chaos negatively impacts the flexible switching\nbetween exploration and exploitation.", "paper_summary_zh": "<paragraph>\u57fa\u65bc\u6df7\u6c8c\u7684\u5f37\u5316\u5b78\u7fd2 (CBRL) \u662f\u4e00\u7a2e\u65b9\u6cd5\uff0c\u5176\u4e2d\u4ee3\u7406\u7684\u5167\u90e8\u6df7\u6c8c\u52d5\u529b\u9a45\u52d5\u63a2\u7d22\u3002\u6b64\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u6a21\u578b\uff0c\u7528\u65bc\u8003\u91cf\u751f\u7269\u5927\u8166\u5982\u4f55\u5728\u5176\u884c\u70ba\u4e2d\u5275\u9020\u8b8a\u7570\u6027\uff0c\u4e26\u4ee5\u63a2\u7d22\u65b9\u5f0f\u5b78\u7fd2\u3002\u540c\u6642\uff0c\u5b83\u662f\u4e00\u500b\u5b78\u7fd2\u6a21\u578b\uff0c\u5177\u6709\u5728\u63a2\u7d22\u548c\u5229\u7528\u6a21\u5f0f\u4e4b\u9593\u81ea\u52d5\u5207\u63db\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5be6\u73fe\u53cd\u6620\u5230\u76ee\u524d\u70ba\u6b62\u6240\u5b78\u5167\u5bb9\u7684\u66f4\u9ad8\u63a2\u7d22\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0cCBRL \u4e2d\u7684\u5b78\u7fd2\u6f14\u7b97\u6cd5\u5728\u5148\u524d\u7684\u7814\u7a76\u4e2d\u5c1a\u672a\u5efa\u7acb\u5b8c\u5584\uff0c\u4e26\u4e14\u5c1a\u672a\u7d0d\u5165\u5f37\u5316\u5b78\u7fd2\u7684\u6700\u65b0\u9032\u5c55\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u96d9\u91cd\u5ef6\u9072\u6df1\u5ea6\u78ba\u5b9a\u6027\u7b56\u7565\u68af\u5ea6 (TD3)\uff0c\u9019\u662f\u6700\u5148\u9032\u7684\u6df1\u5ea6\u5f37\u5316\u5b78\u7fd2\u6f14\u7b97\u6cd5\u4e4b\u4e00\uff0c\u53ef\u4ee5\u8655\u7406\u78ba\u5b9a\u6027\u548c\u9023\u7e8c\u52d5\u4f5c\u7a7a\u9593\uff0c\u5230 CBRL\u3002\u9a57\u8b49\u7d50\u679c\u63d0\u4f9b\u4e86\u591a\u9805\u898b\u89e3\u3002\u9996\u5148\uff0cTD3 \u53ef\u4f5c\u70ba CBRL \u7684\u5b78\u7fd2\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u7c21\u55ae\u7684\u76ee\u6a19\u9054\u6210\u4efb\u52d9\u3002\u5176\u6b21\uff0c\u5177\u6709 TD3 \u7684 CBRL \u4ee3\u7406\u53ef\u4ee5\u5728\u5b78\u7fd2\u9032\u5c55\u6642\u81ea\u4e3b\u6291\u5236\u5176\u63a2\u7d22\u884c\u70ba\uff0c\u4e26\u5728\u74b0\u5883\u767c\u751f\u8b8a\u5316\u6642\u6062\u5fa9\u63a2\u7d22\u3002\u6700\u5f8c\uff0c\u6aa2\u8996\u4ee3\u7406\u7684\u6df7\u6c8c\u6027\u5c0d\u5b78\u7fd2\u7684\u5f71\u97ff\u986f\u793a\uff0c\u6975\u5f37\u7684\u6df7\u6c8c\u6703\u5c0d\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u9593\u7684\u9748\u6d3b\u5207\u63db\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002</paragraph>", "author": "Toshitaka Matsuki et.al.", "authors": "Toshitaka Matsuki, Yusuke Sakemi, Kazuyuki Aihara", "id": "2405.09086v1", "paper_url": "http://arxiv.org/abs/2405.09086v1", "repo": "null"}}