{"2412.09601": {"publish_time": "2024-12-12", "title": "TimeRefine: Temporal Grounding with Time Refining Video LLM", "paper_summary": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.", "paper_summary_zh": "\u5f71\u7247\u6642\u5e8f\u5b9a\u4f4d\u65e8\u5728\u6839\u64da\u6587\u5b57\u63d0\u793a\u5728\u5f71\u7247\u4e2d\u627e\u51fa\u76f8\u95dc\u7684\u6642\u9593\u908a\u754c\u3002\u8fd1\u671f\u7814\u7a76\u5c08\u6ce8\u65bc\u8b93\u5f71\u7247\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u4e0b\u4e00\u500b\u6642\u5e8f\u6a19\u8a18\u9810\u6e2c\u4f86\u57f7\u884c\u5f71\u7247\u6642\u5e8f\u5b9a\u4f4d\u3002\u7136\u800c\uff0c\u5728\u50c5\u4f9d\u8cf4\u6642\u5e8f\u6a19\u8a18\u9810\u6e2c\u6642\uff0c\u5f71\u7247\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8981\u7cbe\u6e96\u5b9a\u4f4d\u5f71\u7247\u4e2d\u7684\u6642\u5e8f\u6a19\u8a18\u4ecd\u662f\u4e00\u5927\u6311\u6230\u3002\u6211\u5011\u63d0\u51fa\u7684 TimeRefine \u4ee5\u5169\u7a2e\u65b9\u5f0f\u89e3\u6c7a\u9019\u500b\u6311\u6230\u3002\u9996\u5148\uff0c\u6211\u5011\u4e26\u672a\u76f4\u63a5\u9810\u6e2c\u958b\u59cb\u548c\u7d50\u675f\u6642\u5e8f\u6a19\u8a18\uff0c\u800c\u662f\u5c07\u6642\u5e8f\u5b9a\u4f4d\u4efb\u52d9\u91cd\u65b0\u5b9a\u7fa9\u70ba\u6642\u5e8f\u7cbe\u7149\u4efb\u52d9\uff1a\u6a21\u578b\u5148\u505a\u51fa\u7c97\u7565\u9810\u6e2c\uff0c\u7136\u5f8c\u900f\u904e\u9810\u6e2c\u76ee\u6a19\u7247\u6bb5\u7684\u504f\u79fb\u91cf\u4f86\u7cbe\u7149\u9019\u4e9b\u9810\u6e2c\u3002\u9019\u500b\u7cbe\u7149\u7a0b\u5e8f\u6703\u91cd\u8907\u591a\u6b21\uff0c\u6a21\u578b\u900f\u904e\u6b64\u7a0b\u5e8f\u9010\u6b65\u81ea\u884c\u6539\u5584\u5176\u6642\u5e8f\u5b9a\u4f4d\u6e96\u78ba\u5ea6\u3002\u5176\u6b21\uff0c\u70ba\u4e86\u589e\u5f37\u6a21\u578b\u7684\u6642\u5e8f\u611f\u77e5\u80fd\u529b\uff0c\u6211\u5011\u6574\u5408\u4e86\u4e00\u500b\u8f14\u52a9\u9810\u6e2c\u982d\uff0c\u5982\u679c\u9810\u6e2c\u7247\u6bb5\u8207\u5be6\u969b\u60c5\u6cc1\u76f8\u5dee\u8d8a\u5927\uff0c\u9019\u500b\u8f14\u52a9\u9810\u6e2c\u982d\u5c31\u6703\u5c0d\u6a21\u578b\u65bd\u52a0\u66f4\u591a\u61f2\u7f70\uff0c\u56e0\u6b64\u9f13\u52f5\u6a21\u578b\u505a\u51fa\u66f4\u63a5\u8fd1\u4e14\u66f4\u7cbe\u6e96\u7684\u9810\u6e2c\u3002\u6211\u5011\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u53ef\u4ee5\u6574\u5408\u5230\u5927\u591a\u6578\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u6642\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\u4e2d\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cTimeRefine \u5728 ActivityNet \u548c Charades-STA \u8cc7\u6599\u96c6\u4e0a\u5206\u5225\u9054\u5230\u4e86 3.6% \u548c 5.0% \u7684 mIoU \u63d0\u5347\u3002\u7a0b\u5f0f\u78bc\u548c\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u5c07\u6703\u91cb\u51fa\u3002", "author": "Xizi Wang et.al.", "authors": "Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, David Crandall", "id": "2412.09601v1", "paper_url": "http://arxiv.org/abs/2412.09601v1", "repo": "null"}}