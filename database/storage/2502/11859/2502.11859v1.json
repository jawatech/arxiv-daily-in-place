{"2502.11859": {"publish_time": "2025-02-17", "title": "Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics", "paper_summary": "The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.", "paper_summary_zh": "\u591a\u5143\u667a\u80fd\u7406\u8ad6\u5f37\u8abf\u8a8d\u77e5\u80fd\u529b\u7684\u5c64\u6b21\u6027\u8cea\u3002\u70ba\u4e86\u63a8\u9032\u7a7a\u9593\u4eba\u5de5\u667a\u6167\uff0c\u6211\u5011\u958b\u5275\u4e86\u4e00\u500b\u5fc3\u7406\u6e2c\u91cf\u6846\u67b6\uff0c\u5728\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u5b9a\u7fa9\u4e86\u4e94\u7a2e\u57fa\u672c\u7a7a\u9593\u80fd\u529b (BSA)\uff1a\u7a7a\u9593\u77e5\u89ba\u3001\u7a7a\u9593\u95dc\u4fc2\u3001\u7a7a\u9593\u5b9a\u5411\u3001\u5fc3\u667a\u65cb\u8f49\u548c\u7a7a\u9593\u8996\u89ba\u5316\u3002\u901a\u904e\u4e5d\u9805\u7d93\u904e\u9a57\u8b49\u7684\u5fc3\u7406\u6e2c\u91cf\u5be6\u9a57\u5c0d 13 \u500b\u4e3b\u6d41 VLM \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u63ed\u793a\u4e86\u8207\u4eba\u985e\u76f8\u6bd4\u7684\u986f\u8457\u5dee\u8ddd\uff08\u5e73\u5747\u5206\u6578 24.95 \u5c0d 68.38\uff09\uff0c\u4e26\u5f97\u51fa\u4e09\u500b\u95dc\u9375\u767c\u73fe\uff1a1) VLM \u53cd\u6620\u4eba\u985e\u5c64\u6b21\u7d50\u69cb\uff082D \u5b9a\u5411\u6700\u5f37\uff0c3D \u65cb\u8f49\u6700\u5f31\uff09\u5177\u6709\u7368\u7acb\u7684 BSA\uff08Pearson's r<0.4\uff09\uff1b2) Qwen2-VL-7B \u7b49\u8f03\u5c0f\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u8f03\u5927\u7684\u6a21\u578b\uff0c\u5176\u4e2d Qwen \u9818\u5148\uff0830.82\uff09\uff0cInternVL2 \u843d\u5f8c\uff0819.6\uff09\uff1b3) \u601d\u60f3\u93c8\u7b49\u5e72\u9810\u63aa\u65bd\uff080.100  accuracy gain\uff09\u548c 5 \u6b21\u8a13\u7df4\uff080.259 \u63d0\u5347\uff09\u986f\u793a\u4e86\u67b6\u69cb\u7d04\u675f\u7684\u9650\u5236\u3002\u5df2\u8b58\u5225\u7684\u969c\u7919\u5305\u62ec\u5f31\u5e7e\u4f55\u7de8\u78bc\u548c\u7f3a\u5c11\u52d5\u614b\u6a21\u64ec\u3002\u901a\u904e\u5c07\u5fc3\u7406\u6e2c\u91cf BSA \u8207 VLM \u80fd\u529b\u806f\u7e6b\u8d77\u4f86\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u7528\u65bc\u7a7a\u9593\u667a\u80fd\u8a55\u4f30\u7684\u8a3a\u65b7\u5de5\u5177\u5305\u3001\u5177\u8eab AI \u958b\u767c\u7684\u65b9\u6cd5\u8ad6\u57fa\u790e\uff0c\u4ee5\u53ca\u5be6\u73fe\u985e\u4eba\u7a7a\u9593\u667a\u80fd\u7684\u8a8d\u77e5\u79d1\u5b78\u4fe1\u606f\u8def\u6a19\u3002", "author": "Wenrui Xu et.al.", "authors": "Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li", "id": "2502.11859v1", "paper_url": "http://arxiv.org/abs/2502.11859v1", "repo": "null"}}