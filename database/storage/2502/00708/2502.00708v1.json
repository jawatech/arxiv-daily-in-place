{"2502.00708": {"publish_time": "2025-02-02", "title": "PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation", "paper_summary": "Text-to-3D asset generation has achieved significant optimization under the\nsupervision of 2D diffusion priors. However, when dealing with compositional\nscenes, existing methods encounter several challenges: 1). failure to ensure\nthat composite scene layouts comply with physical laws; 2). difficulty in\naccurately capturing the assets and relationships described in complex scene\ndescriptions; 3). limited autonomous asset generation capabilities among layout\napproaches leveraging large language models (LLMs). To avoid these compromises,\nwe propose a novel framework for compositional scene generation, PhiP-G, which\nseamlessly integrates generation techniques with layout guidance based on a\nworld model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene\ndescription to generate a scene graph, and integrating a multimodal 2D\ngeneration agent and a 3D Gaussian generation method for targeted assets\ncreation. For the stage of layout, PhiP-G employs a physical pool with adhesion\ncapabilities and a visual supervision agent, forming a world model for layout\nprediction and planning. Extensive experiments demonstrate that PhiP-G\nsignificantly enhances the generation quality and physical rationality of the\ncompositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA)\nperformance in CLIP scores, achieves parity with the leading methods in\ngeneration quality as measured by the T$^3$Bench, and improves efficiency by\n24x.", "paper_summary_zh": "<paragraph>\u5728 2D \u64f4\u6563\u5148\u9a57\u7684\u76e3\u7763\u4e0b\uff0c\u6587\u5b57\u8f49 3D \u8cc7\u7522\u751f\u6210\u5df2\u53d6\u5f97\u986f\u8457\u7684\u6700\u4f73\u5316\u3002\u7136\u800c\uff0c\u5728\u8655\u7406\u5408\u6210\u5834\u666f\u6642\uff0c\u73fe\u6709\u65b9\u6cd5\u6703\u9047\u5230\u5e7e\u500b\u6311\u6230\uff1a1) \u7121\u6cd5\u78ba\u4fdd\u8907\u5408\u5834\u666f\u4f48\u5c40\u7b26\u5408\u7269\u7406\u5b9a\u5f8b\uff1b2) \u96e3\u4ee5\u6e96\u78ba\u6355\u6349\u8907\u96dc\u5834\u666f\u63cf\u8ff0\u4e2d\u6240\u63cf\u8ff0\u7684\u8cc7\u7522\u548c\u95dc\u4fc2\uff1b3) \u5728\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u4f48\u5c40\u65b9\u6cd5\u4e2d\uff0c\u81ea\u4e3b\u8cc7\u7522\u751f\u6210\u80fd\u529b\u6709\u9650\u3002\u70ba\u4e86\u907f\u514d\u9019\u4e9b\u6298\u8877\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5408\u6210\u5834\u666f\u751f\u6210\u7684\u65b0\u6846\u67b6 PhiP-G\uff0c\u5b83\u5c07\u751f\u6210\u6280\u8853\u8207\u57fa\u65bc\u4e16\u754c\u6a21\u578b\u7684\u4f48\u5c40\u6307\u5c0e\u7121\u7e2b\u6574\u5408\u3002\u5229\u7528\u57fa\u65bc LLM \u7684\u4ee3\u7406\uff0cPhiP-G \u5206\u6790\u8907\u96dc\u7684\u5834\u666f\u63cf\u8ff0\u4ee5\u751f\u6210\u5834\u666f\u5716\uff0c\u4e26\u6574\u5408\u591a\u6a21\u614b 2D \u751f\u6210\u4ee3\u7406\u548c 3D \u9ad8\u65af\u751f\u6210\u65b9\u6cd5\u4ee5\u9032\u884c\u76ee\u6a19\u8cc7\u7522\u5275\u5efa\u3002\u5c0d\u65bc\u4f48\u5c40\u968e\u6bb5\uff0cPhiP-G \u63a1\u7528\u5177\u6709\u9644\u8457\u80fd\u529b\u7684\u7269\u7406\u6c60\u548c\u8996\u89ba\u76e3\u7763\u4ee3\u7406\uff0c\u5f62\u6210\u7528\u65bc\u4f48\u5c40\u9810\u6e2c\u548c\u898f\u5283\u7684\u4e16\u754c\u6a21\u578b\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\uff0cPhiP-G \u5927\u5e45\u63d0\u5347\u4e86\u5408\u6210\u5834\u666f\u7684\u751f\u6210\u54c1\u8cea\u548c\u7269\u7406\u5408\u7406\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPhiP-G \u5728 CLIP \u5206\u6578\u4e2d\u7372\u5f97\u4e86\u6700\u5148\u9032 (SOTA) \u7684\u6548\u80fd\uff0c\u5728 T$^3$Bench \u6e2c\u91cf\u7684\u751f\u6210\u54c1\u8cea\u4e2d\u8207\u9818\u5148\u7684\u65b9\u6cd5\u9054\u5230\u540c\u7b49\u6c34\u6e96\uff0c\u4e26\u5c07\u6548\u7387\u63d0\u5347\u4e86 24 \u500d\u3002</paragraph>", "author": "Qixuan Li et.al.", "authors": "Qixuan Li, Chao Wang, Zongjin He, Yan Peng", "id": "2502.00708v1", "paper_url": "http://arxiv.org/abs/2502.00708v1", "repo": "null"}}