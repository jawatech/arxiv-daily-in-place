{"2412.15113": {"publish_time": "2024-12-19", "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture", "paper_summary": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8868\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u80fd\u5920\u5229\u7528\u5176\u8f38\u5165\u5e8f\u5217\u4e2d\u7684\u8cc7\u8a0a\uff0c\u9069\u7576\u5730\u56de\u61c9 LLM \u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u672a\u898b\u904e\u7684\u8cc7\u6599\u3002\u9019\u7a2e\u80fd\u529b\u7a31\u70ba\u8a9e\u5883\u4e2d\u5b78\u7fd2 (ICL)\u3002\u4eba\u985e\u548c\u975e\u4eba\u985e\u52d5\u7269\u8868\u73fe\u51fa\u985e\u4f3c\u7684\u80fd\u529b\uff0c\u4f46\u4ed6\u5011\u7684\u795e\u7d93\u67b6\u69cb\u8207 LLM \u6709\u5f88\u5927\u4e0d\u540c\u3002\u5118\u7ba1\u5982\u6b64\uff0cLLM \u4e2d\u7684\u4e00\u500b\u95dc\u9375\u7d44\u6210\u90e8\u5206\uff0c\u6ce8\u610f\u6a5f\u5236\uff0c\u985e\u4f3c\u65bc\u73fe\u4ee3\u806f\u60f3\u8a18\u61b6\u6a21\u578b\uff0c\u5728\u8a08\u7b97\u795e\u7d93\u79d1\u5b78\u793e\u7fa4\u4e2d\u5ee3\u6cdb\u4f7f\u7528\u4e26\u53d7\u5230\u5176\u5f71\u97ff\uff0c\u7528\u65bc\u5efa\u6a21\u751f\u7269\u8a18\u61b6\u7cfb\u7d71\u3002\u5229\u7528\u9019\u7a2e\u806f\u7e6b\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u80fd\u5920\u57f7\u884c ICL \u7684\u806f\u60f3\u8a18\u61b6\u6a21\u578b\u3002\u6211\u5011\u4ee5\u6b64\u70ba\u9748\u611f\uff0c\u6253\u9020\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6b98\u5dee\u4e32\u6d41\u67b6\u69cb\uff0c\u8b93\u8cc7\u8a0a\u80fd\u5920\u76f4\u63a5\u5728\u6ce8\u610f\u6b0a\u91cd\u4e4b\u9593\u6d41\u52d5\u3002\u6211\u5011\u5728\u4e00\u500b\u5169\u5c64 Transformer \u4e2d\u8a13\u7df4\u671f\u9593\u6e2c\u8a66\u9019\u500b\u67b6\u69cb\uff0c\u4e26\u5c55\u793a\u5176 ICL \u80fd\u529b\u6bd4\u6c92\u6709\u9019\u500b\u4fee\u6539\u6642\u8868\u73fe\u5f97\u66f4\u5feb\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u67b6\u69cb\u61c9\u7528\u65bc\u5177\u6709 800 \u842c\u500b\u53c3\u6578\u7684\u5c0f\u8a9e\u8a00\u6a21\u578b\uff0c\u5c08\u6ce8\u65bc\u6ce8\u610f\u6b0a\u91cd\u503c\uff0c\u7d50\u679c\u4e5f\u8868\u660e\u5728\u9019\u500b\u66f4\u5927\u4e14\u66f4\u81ea\u7136\u7684\u898f\u6a21\u4e0b\uff0cICL \u6548\u80fd\u6709\u6240\u63d0\u5347\u3002", "author": "Thomas F Burns et.al.", "authors": "Thomas F Burns, Tomoki Fukai, Christopher J Earls", "id": "2412.15113v1", "paper_url": "http://arxiv.org/abs/2412.15113v1", "repo": "https://github.com/tfburns/amicl-and-residual-attention-streams"}}