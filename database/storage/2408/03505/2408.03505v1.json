{"2408.03505": {"publish_time": "2024-08-07", "title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation", "paper_summary": "Multimodal large language models (MLLMs) have extended the success of large\nlanguage models (LLMs) to multiple data types, such as image, text and audio,\nachieving significant performance in various domains, including multimodal\ntranslation, visual question answering and content generation. Nonetheless,\nexisting systems are inefficient to train MLLMs due to substantial GPU bubbles\ncaused by the heterogeneous modality models and complex data dependencies in 3D\nparallelism. This paper proposes Optimus, a distributed MLLM training system\nthat reduces end-to-end MLLM training time. Optimus is based on our principled\nanalysis that scheduling the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling encoder computation\npossible for all GPUs, Optimus searches the separate parallel plans for encoder\nand LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM\nbubbles without breaking the original data dependencies in the MLLM model\narchitecture. We further decompose encoder layer computation into a series of\nkernels, and analyze the common bubble pattern of 3D parallelism to carefully\noptimize the sub-millisecond bubble scheduling, minimizing the overall training\ntime. Our experiments in a production cluster show that Optimus accelerates\nMLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.", "paper_summary_zh": "\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5df2\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6210\u529f\u64f4\u5c55\u5230\u591a\u7a2e\u8cc7\u6599\u985e\u578b\uff0c\u4f8b\u5982\u5f71\u50cf\u3001\u6587\u5b57\u548c\u97f3\u8a0a\uff0c\u5728\u591a\u6a21\u614b\u7ffb\u8b6f\u3001\u8996\u89ba\u554f\u7b54\u548c\u5167\u5bb9\u7522\u751f\u7b49\u5404\u7a2e\u9818\u57df\u4e2d\u5747\u53d6\u5f97\u986f\u8457\u7684\u8868\u73fe\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u73fe\u6709\u7684\u7cfb\u7d71\u7531\u65bc\u7570\u8cea\u6a21\u614b\u6a21\u578b\u548c 3D \u4e26\u884c\u8655\u7406\u4e2d\u8907\u96dc\u7684\u8cc7\u6599\u4f9d\u8cf4\u6027\u800c\u5c0e\u81f4\u5927\u91cf\u7684 GPU \u66ab\u505c\uff0c\u56e0\u6b64\u8a13\u7df4 MLLM \u7684\u6548\u7387\u5f88\u4f4e\u3002\u672c\u6587\u63d0\u51fa Optimus\uff0c\u9019\u662f\u4e00\u500b\u5206\u6563\u5f0f\u7684 MLLM \u8a13\u7df4\u7cfb\u7d71\uff0c\u53ef\u6e1b\u5c11\u7aef\u5c0d\u7aef\u7684 MLLM \u8a13\u7df4\u6642\u9593\u3002Optimus \u662f\u57fa\u65bc\u6211\u5011\u6709\u539f\u5247\u7684\u5206\u6790\uff0c\u5373\u5728 LLM \u66ab\u505c\u4e2d\u6392\u7a0b\u7de8\u78bc\u5668\u904b\u7b97\u53ef\u4ee5\u6e1b\u5c11 MLLM \u8a13\u7df4\u4e2d\u7684\u66ab\u505c\u3002\u70ba\u4f7f\u6240\u6709 GPU \u90fd\u80fd\u6392\u7a0b\u7de8\u78bc\u5668\u904b\u7b97\uff0cOptimus \u6703\u641c\u5c0b\u7de8\u78bc\u5668\u548c LLM \u7684\u7368\u7acb\u4e26\u884c\u8a08\u756b\uff0c\u4e26\u63a1\u7528\u66ab\u505c\u6392\u7a0b\u6f14\u7b97\u6cd5\uff0c\u4ee5\u5229\u7528 LLM \u66ab\u505c\uff0c\u800c\u4e0d\u6703\u4e2d\u65b7 MLLM \u6a21\u578b\u67b6\u69cb\u4e2d\u7684\u539f\u59cb\u8cc7\u6599\u4f9d\u8cf4\u6027\u3002\u6211\u5011\u9032\u4e00\u6b65\u5c07\u7de8\u78bc\u5668\u5c64\u904b\u7b97\u5206\u89e3\u6210\u4e00\u7cfb\u5217\u7684\u6838\uff0c\u4e26\u5206\u6790 3D \u4e26\u884c\u8655\u7406\u7684\u5e38\u898b\u66ab\u505c\u6a21\u5f0f\uff0c\u4ee5\u4ed4\u7d30\u6700\u4f73\u5316\u6b21\u6beb\u79d2\u7684\u66ab\u505c\u6392\u7a0b\uff0c\u5c07\u6574\u9ad4\u8a13\u7df4\u6642\u9593\u964d\u81f3\u6700\u4f4e\u3002\u6211\u5011\u5728\u751f\u7522\u53e2\u96c6\u4e2d\u7684\u5be6\u9a57\u986f\u793a\uff0c\u8207\u57fa\u6e96\u76f8\u6bd4\uff0cOptimus \u5728 3072 \u500b GPU \u4e0a\u4f7f\u7528 ViT-22B \u548c GPT-175B \u6a21\u578b\uff0c\u5c07 MLLM \u8a13\u7df4\u901f\u5ea6\u52a0\u5feb\u4e86 20.5%-21.3%\u3002", "author": "Weiqi Feng et.al.", "authors": "Weiqi Feng, Yangrui Chen, Shaoyu Wang, Yanghua Peng, Haibin Lin, Minlan Yu", "id": "2408.03505v1", "paper_url": "http://arxiv.org/abs/2408.03505v1", "repo": "null"}}