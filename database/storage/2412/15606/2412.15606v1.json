{"2412.15606": {"publish_time": "2024-12-20", "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage", "paper_summary": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9032\u6b65\u4fc3\u4f7f\u4e86\u591a\u6a21\u614b\u4ee3\u7406\u7684\u767c\u5c55\uff0c\u9019\u4e9b\u4ee3\u7406\u7528\u4f5c\u63a7\u5236\u5668\u4f86\u547c\u53eb\u5916\u90e8\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u89e3\u6c7a\u5be6\u969b\u4efb\u52d9\u7684\u53ef\u884c\u65b9\u5f0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u591a\u6a21\u614b\u4ee3\u7406\u8abf\u6574\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u81ea\u52d5\u751f\u6210\u591a\u6a21\u614b\u5de5\u5177\u4f7f\u7528\u6578\u64da\uff0c\u4e26\u8abf\u6574\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f5c\u70ba\u5f37\u5927\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u7684\u63a7\u5236\u5668\u3002\u70ba\u4e86\u4fdd\u7559\u6578\u64da\u8cea\u91cf\uff0c\u6211\u5011\u63d0\u793a GPT-4o \u8ff7\u4f60\u6a21\u578b\u751f\u6210\u67e5\u8a62\u3001\u6587\u4ef6\u548c\u8ecc\u8de1\uff0c\u7136\u5f8c\u518d\u4f7f\u7528\u67e5\u8a62\u6587\u4ef6\u548c\u8ecc\u8de1\u9a57\u8b49\u5668\u3002\u6839\u64da\u6578\u64da\u5408\u6210\u7ba1\u9053\uff0c\u6211\u5011\u6536\u96c6\u4e86 MM-Traj \u6578\u64da\u96c6\uff0c\u5176\u4e2d\u5305\u542b 20K \u500b\u5177\u6709\u5de5\u5177\u4f7f\u7528\u8ecc\u8de1\u7684\u4efb\u52d9\u3002\u7136\u5f8c\uff0c\u6211\u5011\u901a\u904e\u4f7f\u7528 MM-Traj \u5728 VLM \u4e0a\u9032\u884c\u8ecc\u8de1\u8abf\u6574\uff0c\u958b\u767c\u4e86 T3-Agent\uff0c\u7528\u65bc\u5de5\u5177\u4f7f\u7528\u3002\u5c0d GTA \u548c GAIA \u57fa\u6e96\u7684\u8a55\u4f30\u8868\u660e\uff0cT3-Agent \u5728\u5169\u500b\u6d41\u884c\u7684 VLM \u4e0a\u6301\u7e8c\u53d6\u5f97\u6539\u9032\uff1aMiniCPM-V-8.5B \u548c {Qwen2-VL-7B}\uff0c\u5176\u512a\u65bc\u672a\u8a13\u7df4\u7684 VLM 20%\uff0c\u986f\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6578\u64da\u5408\u6210\u7ba1\u9053\u7684\u6709\u6548\u6027\uff0c\u5f9e\u800c\u70ba\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u9ad8\u8cea\u91cf\u7684\u6578\u64da\u3002", "author": "Zhi Gao et.al.", "authors": "Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li", "id": "2412.15606v1", "paper_url": "http://arxiv.org/abs/2412.15606v1", "repo": "null"}}