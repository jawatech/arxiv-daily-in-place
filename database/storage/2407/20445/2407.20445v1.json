{"2407.20445": {"publish_time": "2024-07-29", "title": "Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation", "paper_summary": "Existing music captioning methods are limited to generating concise global\ndescriptions of short music clips, which fail to capture fine-grained musical\ncharacteristics and time-aware musical changes. To address these limitations,\nwe propose FUTGA, a model equipped with fined-grained music understanding\ncapabilities through learning from generative augmentation with temporal\ncompositions. We leverage existing music caption datasets and large language\nmodels (LLMs) to synthesize fine-grained music captions with structural\ndescriptions and time boundaries for full-length songs. Augmented by the\nproposed synthetic dataset, FUTGA is enabled to identify the music's temporal\nchanges at key transition points and their musical functions, as well as\ngenerate detailed descriptions for each music segment. We further introduce a\nfull-length music caption dataset generated by FUTGA, as the augmentation of\nthe MusicCaps and the Song Describer datasets. We evaluate the automatically\ngenerated captions on several downstream tasks, including music generation and\nretrieval. The experiments demonstrate the quality of the generated captions\nand the better performance in various downstream tasks achieved by the proposed\nmusic captioning approach. Our code and datasets can be found in\n\\href{https://huggingface.co/JoshuaW1997/FUTGA}{\\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}}.", "paper_summary_zh": "\u73fe\u6709\u7684\u97f3\u6a02\u6a19\u984c\u65b9\u6cd5\u50c5\u9650\u65bc\u7522\u751f\u7c21\u6f54\u7684\u5168\u5c40\u63cf\u8ff0\uff0c\u7528\u65bc\u63cf\u8ff0\u7c21\u77ed\u7684\u97f3\u6a02\u7247\u6bb5\uff0c\u7121\u6cd5\u6355\u6349\u7d30\u7dfb\u7684\u97f3\u6a02\u7279\u5fb5\u548c\u6642\u9593\u611f\u77e5\u7684\u97f3\u6a02\u8b8a\u5316\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa FUTGA\uff0c\u4e00\u7a2e\u5177\u5099\u7d30\u7dfb\u97f3\u6a02\u7406\u89e3\u80fd\u529b\u7684\u6a21\u578b\uff0c\u901a\u904e\u5f9e\u751f\u6210\u64f4\u5145\u548c\u6642\u9593\u7d44\u6210\u4e2d\u5b78\u7fd2\u4f86\u5be6\u73fe\u3002\u6211\u5011\u5229\u7528\u73fe\u6709\u7684\u97f3\u6a02\u6a19\u984c\u6578\u64da\u96c6\u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u5408\u6210\u5177\u6709\u7d50\u69cb\u63cf\u8ff0\u548c\u6642\u9593\u908a\u754c\u7684\u7d30\u7dfb\u97f3\u6a02\u6a19\u984c\uff0c\u7528\u65bc\u5168\u9577\u6b4c\u66f2\u3002\u901a\u904e\u63d0\u8b70\u7684\u5408\u6210\u6578\u64da\u96c6\u9032\u884c\u64f4\u5145\u5f8c\uff0cFUTGA \u80fd\u5920\u8b58\u5225\u97f3\u6a02\u5728\u95dc\u9375\u8f49\u63db\u9ede\u7684\u6642\u9593\u8b8a\u5316\u53ca\u5176\u97f3\u6a02\u529f\u80fd\uff0c\u4e26\u70ba\u6bcf\u500b\u97f3\u6a02\u7247\u6bb5\u751f\u6210\u8a73\u7d30\u7684\u63cf\u8ff0\u3002\u6211\u5011\u9032\u4e00\u6b65\u4ecb\u7d39\u4e86\u4e00\u500b\u7531 FUTGA \u751f\u6210\u7684\u5168\u9577\u97f3\u6a02\u6a19\u984c\u6578\u64da\u96c6\uff0c\u4f5c\u70ba MusicCaps \u548c Song Describer \u6578\u64da\u96c6\u7684\u64f4\u5145\u3002\u6211\u5011\u5728\u5e7e\u500b\u4e0b\u6e38\u4efb\u52d9\uff08\u5305\u62ec\u97f3\u6a02\u751f\u6210\u548c\u6aa2\u7d22\uff09\u4e0a\u8a55\u4f30\u81ea\u52d5\u751f\u6210\u7684\u6a19\u984c\u3002\u5be6\u9a57\u8b49\u660e\u4e86\u751f\u6210\u6a19\u984c\u7684\u54c1\u8cea\uff0c\u4ee5\u53ca\u63d0\u51fa\u7684\u97f3\u6a02\u6a19\u984c\u65b9\u6cd5\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u53d6\u5f97\u7684\u66f4\u597d\u6027\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6578\u64da\u96c6\u53ef\u4ee5\u5728 \\href{https://huggingface.co/JoshuaW1997/FUTGA}{\\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}} \u4e2d\u627e\u5230\u3002", "author": "Junda Wu et.al.", "authors": "Junda Wu, Zachary Novack, Amit Namburi, Jiaheng Dai, Hao-Wen Dong, Zhouhang Xie, Carol Chen, Julian McAuley", "id": "2407.20445v1", "paper_url": "http://arxiv.org/abs/2407.20445v1", "repo": "null"}}