{"2502.12947": {"publish_time": "2025-02-18", "title": "Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models", "paper_summary": "With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.", "paper_summary_zh": "\u96a8\u8457 Mixture-of-Experts (MoE) \u7684\u51fa\u73fe\uff0c\u6a21\u578b\u898f\u6a21\u7684\u6709\u6548\u64f4\u5c55\u52a0\u901f\u4e86\u8fd1\u5e74\u4f86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u767c\u5c55\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u9ad8\u8a18\u61b6\u9ad4\u9700\u6c42\u6703\u963b\u7919\u5b83\u5011\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u4f7f\u7528\u3002\u96d6\u7136\u77e5\u8b58\u84b8\u993e (KD) \u5df2\u88ab\u8b49\u660e\u662f\u4e00\u7a2e\u6a21\u578b\u58d3\u7e2e\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u5728 MoE \u6559\u5e2b\u6a21\u578b\u4e2d\u7684\u61c9\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u900f\u904e\u6211\u5011\u7684\u8abf\u67e5\uff0c\u6211\u5011\u767c\u73fe MoE \u6a21\u578b\u4e2d\u672a\u88ab\u555f\u7528\u7684\u5c08\u5bb6\u64c1\u6709\u6709\u50f9\u503c\u7684\u77e5\u8b58\uff0c\u9019\u4e9b\u77e5\u8b58\u5c0d\u5b78\u751f\u6a21\u578b\u6709\u76ca\u3002\u6211\u5011\u9032\u4e00\u6b65\u8b49\u660e\uff0c\u73fe\u6709\u7684 KD \u65b9\u6cd5\u4e26\u975e\u58d3\u7e2e MoE \u6a21\u578b\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u56e0\u70ba\u5b83\u5011\u7121\u6cd5\u6709\u6548\u5229\u7528\u9019\u4e9b\u77e5\u8b58\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u9996\u6b21\u63d0\u51fa\u5169\u7a2e\u76f4\u89c0\u7684 MoE \u5c08\u7528 KD \u65b9\u6cd5\uff1a\u77e5\u8b58\u64f4\u5145 (KA) \u548c\u5b78\u751f\u611f\u77e5\u8def\u7531\u5668 (SAR)\uff0c\u5169\u8005\u90fd\u65e8\u5728\u5f9e\u6240\u6709\u5c08\u5bb6\u6709\u6548\u63d0\u53d6\u77e5\u8b58\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cKA \u900f\u904e\u591a\u6b21\u62bd\u6a23\u5c08\u5bb6\u4f86\u64f4\u5145\u77e5\u8b58\uff0c\u800c SAR \u4f7f\u7528\u6240\u6709\u5c08\u5bb6\u4e26\u900f\u904e\u8def\u7531\u5668\u8a13\u7df4\u8abf\u6574\u5c08\u5bb6\u6b0a\u91cd\u4ee5\u63d0\u4f9b\u6700\u4f73\u77e5\u8b58\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u512a\u65bc\u50b3\u7d71\u7684 KD \u6a21\u578b\uff0c\u8b49\u660e\u4e86\u5b83\u5011\u5c0d MoE \u6559\u5e2b\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "author": "Gyeongman Kim et.al.", "authors": "Gyeongman Kim, Gyouk Chu, Eunho Yang", "id": "2502.12947v1", "paper_url": "http://arxiv.org/abs/2502.12947v1", "repo": "null"}}