{"2408.10914": {"publish_time": "2024-08-20", "title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training", "paper_summary": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts.", "paper_summary_zh": "\u5728\u9810\u8a13\u7df4\u8cc7\u6599\u6df7\u5408\u4e2d\u5305\u542b\u7a0b\u5f0f\u78bc\uff0c\u5373\u4f7f\u5c0d\u65bc\u4e26\u975e\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u7a0b\u5f0f\u78bc\u7684\u6a21\u578b\uff0c\u5df2\u6210\u70ba LLM \u9810\u8a13\u7df4\u7684\u5e38\u898b\u505a\u6cd5\u3002\u96d6\u7136\u5f9e\u696d\u8005\u4e4b\u9593\u6709\u5171\u8b58\uff0c\u8a8d\u70ba\u7a0b\u5f0f\u78bc\u8cc7\u6599\u5728\u4e00\u822c LLM \u7684\u6548\u80fd\u4e2d\u626e\u6f14\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4f46\u50c5\u6709\u5c11\u6578\u7814\u7a76\u5206\u6790\u7a0b\u5f0f\u78bc\u5c0d\u975e\u7a0b\u5f0f\u78bc\u4efb\u52d9\u7684\u7cbe\u78ba\u5f71\u97ff\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u7a0b\u5f0f\u78bc\u8cc7\u6599\u5c0d\u4e00\u822c\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6211\u5011\u8a62\u554f\u300c\u5728\u9810\u8a13\u7df4\u4e2d\u4f7f\u7528\u7684\u7a0b\u5f0f\u78bc\u8cc7\u6599\u5c0d\u7a0b\u5f0f\u78bc\u7522\u751f\u4e4b\u5916\u7684\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u6709\u4f55\u5f71\u97ff\u300d\u3002\u6211\u5011\u9032\u884c\u5ee3\u6cdb\u7684\u6d88\u878d\uff0c\u4e26\u8a55\u4f30\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u63a8\u7406\u4efb\u52d9\u3001\u4e16\u754c\u77e5\u8b58\u4efb\u52d9\u3001\u7a0b\u5f0f\u78bc\u57fa\u6e96\uff0c\u4ee5\u53ca\u5f9e 4.7 \u5104\u5230 28 \u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\u7684 LLM \u4f5c\u70ba\u8a55\u5be9\u7684\u7372\u52dd\u7387\u3002\u5728\u5404\u9805\u8a2d\u5b9a\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u4e00\u81f4\u7684\u7d50\u679c\uff0c\u7a0b\u5f0f\u78bc\u662f\u9060\u8d85\u51fa\u7de8\u78bc\u4efb\u52d9\u7684\u6982\u62ec\u6027\u95dc\u9375\u5efa\u69cb\u6a21\u7d44\uff0c\u800c\u7a0b\u5f0f\u78bc\u54c1\u8cea\u7684\u63d0\u5347\u5c0d\u6240\u6709\u4efb\u52d9\u90fd\u6709\u904e\u5927\u7684\u5f71\u97ff\u3002\u7279\u5225\u662f\uff0c\u8207\u50c5\u6587\u5b57\u7684\u9810\u8a13\u7df4\u76f8\u6bd4\uff0c\u52a0\u5165\u7a0b\u5f0f\u78bc\u53ef\u5206\u5225\u5728\u81ea\u7136\u8a9e\u8a00 (NL) \u63a8\u7406\u4e2d\u589e\u52a0\u9ad8\u9054 8.2%\u3001\u4e16\u754c\u77e5\u8b58\u4e2d\u589e\u52a0 4.2%\u3001\u751f\u6210\u7372\u52dd\u7387\u4e2d\u63d0\u5347 6.6%\uff0c\u4ee5\u53ca\u7a0b\u5f0f\u78bc\u6548\u80fd\u63d0\u5347 12 \u500d\u3002\u6211\u5011\u7684\u7814\u7a76\u5efa\u8b70\uff0c\u5728\u9810\u8a13\u7df4\u671f\u9593\u6295\u8cc7\u7a0b\u5f0f\u78bc\u54c1\u8cea\u548c\u4fdd\u7559\u7a0b\u5f0f\u78bc\u5177\u6709\u6b63\u9762\u7684\u5f71\u97ff\u3002", "author": "Viraat Aryabumi et.al.", "authors": "Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet \u00dcst\u00fcn, Sara Hooker", "id": "2408.10914v1", "paper_url": "http://arxiv.org/abs/2408.10914v1", "repo": "null"}}