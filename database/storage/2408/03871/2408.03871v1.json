{"2408.03871": {"publish_time": "2024-08-07", "title": "BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and Controllable Attributes for Improving Biomedical Text Readability", "paper_summary": "In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU", "paper_summary_zh": "<paragraph>\u5728\u9019\u500b\u7cfb\u7d71\u5831\u544a\u4e2d\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u6211\u5011\u5728 TAC 2023 \u8ecc\u9053\u7684\u4e00\u90e8\u5206\uff0cPLABA2023 \u751f\u7269\u91ab\u5b78\u6458\u8981\u7c21\u5316\u4efb\u52d9\u4e2d\u6240\u4f7f\u7528\u7684\u6a21\u578b\u548c\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u4ea4\u7684\u7cfb\u7d71\u8f38\u51fa\u4f86\u81ea\u4ee5\u4e0b\u4e09\u7a2e\u985e\u5225\uff1a1) \u9818\u57df\u5fae\u8abf\u7684 T5 \u985e\u4f3c\u6a21\u578b\uff0c\u5305\u62ec Biomedical-T5 \u548c Lay-SciFive\uff1b2) \u5fae\u8abf BARTLarge \u6a21\u578b\uff0c\u5177\u6709\u53ef\u63a7\u5c6c\u6027\uff08\u901a\u904e\u4ee3\u5e63\uff09BART-w-CTs\uff1b3) ChatGPT \u63d0\u793a\u3002\u6211\u5011\u9084\u5c55\u793a\u4e86\u6211\u5011\u5728 BioGPT \u5fae\u8abf\u4e2d\u70ba\u9019\u9805\u4efb\u52d9\u6240\u505a\u7684\u5de5\u4f5c\u3002\u5728\u4f7f\u7528 SARI \u5206\u6578\u7684\u5b98\u65b9\u81ea\u52d5\u8a55\u4f30\u4e2d\uff0cBeeManc \u5728\u6240\u6709\u5718\u968a\u4e2d\u6392\u540d\u7b2c 2\uff0c\u6211\u5011\u7684\u6a21\u578b LaySciFive \u5728\u6240\u6709 13 \u500b\u8a55\u4f30\u7cfb\u7d71\u4e2d\u6392\u540d\u7b2c 3\u3002\u5728\u5b98\u65b9\u4eba\u5de5\u8a55\u4f30\u4e2d\uff0c\u6211\u5011\u7684\u6a21\u578b BART-w-CTs \u5728\u53e5\u5b50\u7c21\u6f54\u6027\uff08\u5206\u6578 92.84\uff09\u4e2d\u6392\u540d\u7b2c 2\uff0c\u5728\u8853\u8a9e\u7c21\u6f54\u6027\uff08\u5206\u6578 82.33\uff09\u4e2d\u6392\u540d\u7b2c 3\uff0c\u5728\u6240\u6709 7 \u500b\u8a55\u4f30\u7cfb\u7d71\u4e2d\u6392\u540d\u7b2c 3\uff1b\u5b83\u9084\u7522\u751f\u4e86 91.57 \u7684\u9ad8\u6d41\u66a2\u5ea6\u5206\u6578\uff0c\u800c\u6700\u9ad8\u5206\u70ba 93.53\u3002\u5728\u7b2c\u4e8c\u8f2a\u63d0\u4ea4\u4e2d\uff0c\u6211\u5011\u4f7f\u7528 ChatGPT \u63d0\u793a\u7684\u5718\u968a\u5728\u5e7e\u500b\u985e\u5225\u4e2d\u6392\u540d\u7b2c 2\uff0c\u5305\u62ec\u7c21\u5316\u8853\u8a9e\u6e96\u78ba\u5ea6\u5206\u6578 92.26 \u548c\u5b8c\u6574\u6027\u5206\u6578 96.58\uff0c\u4ee5\u53ca\u5c0d PLABA-base-1\uff0895.73\uff09\u91cd\u65b0\u8a55\u4f30\u7684\u5fe0\u5be6\u5ea6\u5206\u6578 95.3 \u975e\u5e38\u76f8\u4f3c\u901a\u904e\u4eba\u5de5\u8a55\u4f30\u3002\u6211\u5011\u7684\u4ee3\u78bc\u3001\u5fae\u8abf\u6a21\u578b\u3001\u63d0\u793a\u548c\u7cfb\u7d71\u958b\u767c\u968e\u6bb5\u7684\u6578\u64da\u5206\u5272\u5c07\u5728 https://github.com/ HECTA-UoM/PLABA-MU \u4e2d\u63d0\u4f9b</paragraph>", "author": "Zihao Li et.al.", "authors": "Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, Goran Nenadic", "id": "2408.03871v1", "paper_url": "http://arxiv.org/abs/2408.03871v1", "repo": "https://github.com/hecta-uom/plaba-mu"}}