{"2409.17545": {"publish_time": "2024-09-26", "title": "Modulated Intervention Preference Optimization (MIPO): Keey the Easy, Refine the Difficult", "paper_summary": "Preference optimization methods typically begin training with a well-trained\nSFT model as a reference model. In RLHF and DPO, a regularization term is used\nduring the preference optimization process to prevent the policy model from\ndeviating too far from the reference model's distribution, thereby avoiding the\ngeneration of anomalous responses. When the reference model is already\nwell-aligned with the given data or only requires slight adjustments, this\napproach can produce a well-aligned model. However, if the reference model is\nnot aligned with the given data and requires significant deviation from its\ncurrent state, a regularization term may actually hinder the model alignment.\nIn this study, we propose \\textbf{Modulated Intervention Preference\nOptimization (MIPO)} to address this issue. MIPO modulates the degree of\nintervention from the reference model based on how well the given data is\naligned with it. If the data is well-aligned, the intervention is increased to\nprevent the policy model from diverging significantly from reference model.\nConversely, if the alignment is poor, the interference is reduced to facilitate\nmore extensive training. We compare the performance of MIPO and DPO using\nMistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental\nresults demonstrate that MIPO consistently outperforms DPO across various\nevaluation scenarios.", "paper_summary_zh": "\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u4ee5\u8bad\u7ec3\u6709\u7d20\u7684 SFT \u6a21\u578b\u4f5c\u4e3a\u53c2\u8003\u6a21\u578b\u6765\u5f00\u59cb\u8bad\u7ec3\u3002\u5728 RLHF \u548c DPO \u4e2d\uff0c\u5728\u504f\u597d\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6b63\u5219\u5316\u9879\u6765\u9632\u6b62\u7b56\u7565\u6a21\u578b\u504f\u79bb\u53c2\u8003\u6a21\u578b\u7684\u5206\u5e03\u592a\u8fdc\uff0c\u4ece\u800c\u907f\u514d\u751f\u6210\u5f02\u5e38\u54cd\u5e94\u3002\u5f53\u53c2\u8003\u6a21\u578b\u5df2\u7ecf\u4e0e\u7ed9\u5b9a\u6570\u636e\u5f88\u597d\u5730\u5bf9\u9f50\u6216\u53ea\u9700\u8981\u5fae\u8c03\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u4e00\u4e2a\u5f88\u597d\u5730\u5bf9\u9f50\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u5982\u679c\u53c2\u8003\u6a21\u578b\u4e0e\u7ed9\u5b9a\u6570\u636e\u4e0d\u4e00\u81f4\uff0c\u5e76\u4e14\u9700\u8981\u4e0e\u5176\u5f53\u524d\u72b6\u6001\u6709\u8f83\u5927\u504f\u5dee\uff0c\u90a3\u4e48\u6b63\u5219\u5316\u9879\u5b9e\u9645\u4e0a\u53ef\u80fd\u4f1a\u963b\u788d\u6a21\u578b\u5bf9\u9f50\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86**\u8c03\u5236\u5e72\u9884\u504f\u597d\u4f18\u5316 (MIPO)** \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002MIPO \u6839\u636e\u7ed9\u5b9a\u6570\u636e\u4e0e\u5176\u5bf9\u9f50\u7684\u7a0b\u5ea6\u6765\u8c03\u8282\u53c2\u8003\u6a21\u578b\u7684\u5e72\u9884\u7a0b\u5ea6\u3002\u5982\u679c\u6570\u636e\u5bf9\u9f50\u826f\u597d\uff0c\u5219\u589e\u52a0\u5e72\u9884\u4ee5\u9632\u6b62\u7b56\u7565\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u663e\u7740\u504f\u79bb\u3002\u76f8\u53cd\uff0c\u5982\u679c\u5bf9\u9f50\u8f83\u5dee\uff0c\u5219\u51cf\u5c11\u5e72\u6270\u4ee5\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u8bad\u7ec3\u3002\u6211\u4eec\u4f7f\u7528 Alpaca Eval 2.0 \u548c MT-Bench \u4e2d\u7684 Mistral-7B \u548c Llama3-8B \u6bd4\u8f83\u4e86 MIPO \u548c DPO \u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u8bc4\u4f30\u573a\u666f\u4e2d\uff0cMIPO \u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8e DPO\u3002", "author": "Cheolhun Jang et.al.", "authors": "Cheolhun Jang", "id": "2409.17545v1", "paper_url": "http://arxiv.org/abs/2409.17545v1", "repo": "null"}}