{"2409.17870": {"publish_time": "2024-09-26", "title": "Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores", "paper_summary": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 13\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5ee3\u6cdb\u61c9\u7528\uff0c\u4f46\u5728\u9ad8\u6548\u63a8\u7406\u65b9\u9762\u9762\u81e8\u6311\u6230\u3002\u96d6\u7136\u91cf\u5316\u65b9\u6cd5\u53ef\u4ee5\u6e1b\u5c11\u8a08\u7b97\u9700\u6c42\uff0c\u4f46\u4efb\u610f\u7cbe\u5ea6\u7684\u8d85\u4f4e\u4f4d\u5143\u91cf\u5316\u53d7\u5230 GPU Tensor Core \u652f\u63f4\u6709\u9650\u548c\u8a18\u61b6\u9ad4\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u963b\u7919\uff0c\u5c0e\u81f4\u52a0\u901f\u6548\u679c\u4e0d\u4f73\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u91dd\u5c0d\u4efb\u610f\u7cbe\u5ea6 LLM \u7684\u5168\u9762\u52a0\u901f\u65b9\u6848\u3002\u5176\u6838\u5fc3\u662f\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u65b0\u7a4e\u7684\u96d9\u6975 INT \u8cc7\u6599\u683c\u5f0f\uff0c\u5b83\u4fc3\u9032\u5e73\u884c\u904b\u7b97\u4e26\u652f\u63f4\u5c0d\u7a31\u91cf\u5316\uff0c\u6709\u6548\u5730\u6e1b\u5c11\u4e86\u8cc7\u6599\u5197\u9918\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u7a2e\u4efb\u610f\u7cbe\u5ea6\u77e9\u9663\u4e58\u6cd5\u65b9\u6848\uff0c\u5b83\u5728\u4f4d\u5143\u5c64\u7d1a\u5206\u89e3\u548c\u6062\u5fa9\u77e9\u9663\uff0c\u540c\u6642\u5be6\u73fe\u9748\u6d3b\u7684\u7cbe\u5ea6\u4e26\u6700\u5927\u5316 GPU Tensor Core \u7684\u5229\u7528\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u9ad8\u6548\u7684\u77e9\u9663\u9810\u8655\u7406\u65b9\u6cd5\uff0c\u5b83\u6703\u91dd\u5c0d\u5f8c\u7e8c\u904b\u7b97\u6700\u4f73\u5316\u8cc7\u6599\u914d\u7f6e\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u4ee5\u8cc7\u6599\u5fa9\u539f\u70ba\u5c0e\u5411\u7684\u8a18\u61b6\u9ad4\u7ba1\u7406\u7cfb\u7d71\uff0c\u5b83\u7b56\u7565\u6027\u5730\u5229\u7528\u5feb\u901f\u7684\u5171\u4eab\u8a18\u61b6\u9ad4\uff0c\u986f\u8457\u5730\u63d0\u9ad8\u4e86\u6838\u5fc3\u57f7\u884c\u901f\u5ea6\u4e26\u5c07\u8a18\u61b6\u9ad4\u5b58\u53d6\u5ef6\u9072\u964d\u5230\u6700\u4f4e\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8207 NVIDIA \u7684 CUTLASS \u76f8\u6bd4\uff0c\u77e9\u9663\u4e58\u6cd5\u901f\u5ea6\u63d0\u5347\u4e86 13 \u500d\u3002\u7576\u6574\u5408\u5230 LLM \u4e2d\u6642\uff0c\u6211\u5011\u5be6\u73fe\u4e86\u9ad8\u9054 6.7 \u500d\u7684\u63a8\u7406\u52a0\u901f\u3002\u9019\u4e9b\u6539\u9032\u986f\u8457\u63d0\u5347\u4e86 LLM \u63a8\u7406\u6548\u7387\uff0c\u8b93 LLM \u80fd\u6709\u66f4\u5ee3\u6cdb\u4e14\u66f4\u9748\u654f\u7684\u61c9\u7528\u3002", "author": "Shaobo Ma et.al.", "authors": "Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang", "id": "2409.17870v1", "paper_url": "http://arxiv.org/abs/2409.17870v1", "repo": "null"}}