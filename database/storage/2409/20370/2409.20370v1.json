{"2409.20370": {"publish_time": "2024-09-30", "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges", "paper_summary": "Reinforcement learning from human feedback (RLHF) has become the leading\napproach for fine-tuning large language models (LLM). However, RLHF has\nlimitations in multi-task learning (MTL) due to challenges of reward hacking\nand extreme multi-objective optimization (i.e., trade-off of multiple and/or\nsometimes conflicting objectives). Applying RLHF for MTL currently requires\ncareful tuning of the weights for reward model and data combinations. This is\noften done via human intuition and does not generalize. In this work, we\nintroduce a novel post-training paradigm which we called Constrained Generative\nPolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with\ncost-efficient constrained policy optimization with stratification, which can\nidentify the perfect blend in RLHF in a principled manner. It shows strong\nempirical results with theoretical guarantees, does not require extensive\nhyper-parameter tuning, and is plug-and-play in common post-training pipelines.\nTogether, this can detect and mitigate reward hacking behaviors while reaching\na pareto-optimal point across an extremely large number of objectives.\n  Our empirical evaluations demonstrate that CGPO significantly outperforms\nstandard RLHF algorithms like PPO and DPO across various tasks including\ngeneral chat, STEM questions, instruction following, and coding. Specifically,\nCGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in\nArena-Hard (STEM & reasoning), and consistent gains in other domains like math\nand coding. Notably, PPO, while commonly used, is prone to severe reward\nhacking in popular coding benchmarks, which CGPO successfully addresses. This\nbreakthrough in RLHF not only tackles reward hacking and extreme\nmulti-objective optimization challenges but also advances the state-of-the-art\nin aligning general-purpose LLMs for diverse applications.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u6210\u70ba\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9818\u5148\u65b9\u6cd5\u3002\u7136\u800c\uff0cRLHF \u5728\u591a\u4efb\u52d9\u5b78\u7fd2 (MTL) \u4e2d\u53d7\u5230\u734e\u52f5\u7834\u89e3\u548c\u6975\u7aef\u591a\u76ee\u6a19\u6700\u4f73\u5316\uff08\u4f8b\u5982\uff0c\u591a\u91cd\u548c/\u6216\u6709\u6642\u76f8\u4e92\u885d\u7a81\u7684\u76ee\u6a19\u4e4b\u9593\u7684\u53d6\u6368\uff09\u7684\u6311\u6230\u800c\u6709\u6240\u9650\u5236\u3002\u76ee\u524d\uff0c\u5c07 RLHF \u61c9\u7528\u65bc MTL \u9700\u8981\u4ed4\u7d30\u8abf\u6574\u734e\u52f5\u6a21\u578b\u548c\u8cc7\u6599\u7d44\u5408\u7684\u6b0a\u91cd\u3002\u9019\u901a\u5e38\u662f\u900f\u904e\u4eba\u985e\u76f4\u89ba\u4f86\u5b8c\u6210\uff0c\u800c\u4e14\u7121\u6cd5\u6982\u62ec\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u8a13\u7df4\u5f8c\u7bc4\u4f8b\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u53d7\u7d04\u675f\u751f\u6210\u7b56\u7565\u6700\u4f73\u5316 (CGPO)\u3002CGPO \u7684\u6838\u5fc3\u662f\u6cd5\u5b98\u6df7\u5408 (MoJ)\uff0c\u900f\u904e\u5206\u5c64\u9032\u884c\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u53d7\u7d04\u675f\u7b56\u7565\u6700\u4f73\u5316\uff0c\u5b83\u53ef\u4ee5\u4ee5\u539f\u5247\u6027\u7684\u65b9\u5f0f\u627e\u51fa RLHF \u4e2d\u7684\u5b8c\u7f8e\u878d\u5408\u3002\u5b83\u5728\u7406\u8ad6\u4fdd\u8b49\u4e0b\u5c55\u73fe\u5f37\u5927\u7684\u5be6\u8b49\u7d50\u679c\uff0c\u4e0d\u9700\u8981\u5ee3\u6cdb\u7684\u8d85\u53c3\u6578\u8abf\u6574\uff0c\u4e26\u4e14\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u65bc\u5e38\u898b\u7684\u8a13\u7df4\u5f8c\u7ba1\u9053\u3002\u7e3d\u4e4b\uff0c\u5b83\u53ef\u4ee5\u5728\u6975\u5927\u91cf\u7684\u76ee\u6a19\u4e2d\u5075\u6e2c\u548c\u6e1b\u8f15\u734e\u52f5\u7834\u89e3\u884c\u70ba\uff0c\u540c\u6642\u9054\u5230\u5e15\u96f7\u6258\u6700\u512a\u9ede\u3002\u6211\u5011\u7684\u5be6\u8b49\u8a55\u4f30\u8b49\u660e\uff0cCGPO \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u986f\u8457\u512a\u65bc\u6a19\u6e96 RLHF \u6f14\u7b97\u6cd5\uff0c\u4f8b\u5982\u4e00\u822c\u804a\u5929\u3001STEM \u554f\u984c\u3001\u6307\u4ee4\u9075\u5faa\u548c\u7de8\u78bc\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cCGPO \u5728 AlpacaEval-2\uff08\u4e00\u822c\u804a\u5929\uff09\u4e2d\u63d0\u5347\u4e86 7.4%\uff0c\u5728 Arena-Hard\uff08STEM \u548c\u63a8\u7406\uff09\u4e2d\u63d0\u5347\u4e86 12.5%\uff0c\u4e26\u4e14\u5728\u6578\u5b78\u548c\u7de8\u78bc\u7b49\u5176\u4ed6\u9818\u57df\u4e2d\u6301\u7e8c\u7372\u5f97\u6536\u76ca\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPPO \u96d6\u7136\u666e\u904d\u4f7f\u7528\uff0c\u4f46\u5728\u6d41\u884c\u7684\u7de8\u78bc\u57fa\u6e96\u4e2d\u5bb9\u6613\u53d7\u5230\u56b4\u91cd\u7684\u734e\u52f5\u7834\u89e3\uff0c\u800c CGPO \u6210\u529f\u5730\u89e3\u6c7a\u4e86\u9019\u500b\u554f\u984c\u3002RLHF \u7684\u9019\u9805\u7a81\u7834\u4e0d\u50c5\u89e3\u6c7a\u4e86\u734e\u52f5\u7834\u89e3\u548c\u6975\u7aef\u591a\u76ee\u6a19\u6700\u4f73\u5316\u7684\u6311\u6230\uff0c\u800c\u4e14\u9084\u63a8\u52d5\u4e86\u5c07\u901a\u7528 LLM \u8207\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u76f8\u7d50\u5408\u7684\u6700\u65b0\u6280\u8853\u3002", "author": "Tengyu Xu et.al.", "authors": "Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, Zhouhao Zeng, Yun He, Karishma Mandyam, Arya Talabzadeh, Madian Khabsa, Gabriel Cohen, Yuandong Tian, Hao Ma, Sinong Wang, Han Fang", "id": "2409.20370v1", "paper_url": "http://arxiv.org/abs/2409.20370v1", "repo": "null"}}