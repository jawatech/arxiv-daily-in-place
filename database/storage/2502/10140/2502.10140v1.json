{"2502.10140": {"publish_time": "2025-02-14", "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages", "paper_summary": "Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage.", "paper_summary_zh": "\u4f4e\u8cc7\u6e90\u8a9e\u8a00 (LRL) \u7531\u65bc\u8cc7\u6599\u6709\u9650\uff0c\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u9762\u81e8\u91cd\u5927\u6311\u6230\u3002\u96d6\u7136\u7576\u524d\u6700\u5148\u9032\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ecd\u96e3\u4ee5\u8655\u7406 LRL\uff0c\u4f46\u8f03\u5c0f\u7684\u591a\u8a9e\u8a00\u6a21\u578b (mLMS)\uff0c\u4f8b\u5982 mBERT \u548c XLM-R\uff0c\u7531\u65bc\u5176\u5bb9\u91cf\u66f4\u9069\u5408\u4f4e\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\uff0c\u56e0\u6b64\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u5e0c\u671b\u3002\u672c\u7814\u7a76\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u4e86\u57fa\u65bc\u53c3\u6578\u6548\u7387\u9069\u914d\u5668\u7684\u9069\u914d\u65b9\u6cd5\uff0c\u4ee5\u5c07 mLMS \u9069\u914d\u5230 LRL\uff0c\u8a55\u4f30\u4e86\u4e09\u7a2e\u67b6\u69cb\uff1a\u9806\u5e8f\u74f6\u9838\u3001\u53ef\u9006\u74f6\u9838\u548c\u4f4e\u79e9\u9069\u914d\u3002\u4f7f\u7528\u4f86\u81ea GlotCC \u7684\u975e\u7d50\u69cb\u5316\u6587\u672c\u548c\u4f86\u81ea ConceptNet \u7684\u7d50\u69cb\u5316\u77e5\u8b58\uff0c\u6211\u5011\u8868\u660e\u5c0f\u578b\u9069\u914d\u8cc7\u6599\u96c6\uff08\u4f8b\u5982\uff0c\u9ad8\u9054 1 GB \u7684\u81ea\u7531\u6587\u672c\u6216\u5e7e MB \u7684\u77e5\u8b58\u5716\u8b5c\u8cc7\u6599\uff09\u5728\u5167\u5728\uff08\u906e\u853d\u8a9e\u8a00\u6a21\u578b\uff09\u548c\u5916\u5728\u4efb\u52d9\uff08\u4e3b\u984c\u5206\u985e\u3001\u60c5\u7dd2\u5206\u6790\u548c\u547d\u540d\u5be6\u9ad4\u8b58\u5225\uff09\u4e2d\u7522\u751f\u589e\u76ca\u3002\u6211\u5011\u767c\u73fe\u9806\u5e8f\u74f6\u9838\u9069\u914d\u5668\u5728\u8a9e\u8a00\u6a21\u578b\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u800c\u53ef\u9006\u74f6\u9838\u9069\u914d\u5668\u7531\u65bc\u66f4\u597d\u7684\u5d4c\u5165\u5c0d\u9f4a\u548c\u66f4\u5927\u7684\u53c3\u6578\u6578\u91cf\uff0c\u5728\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7565\u52dd\u65bc\u5176\u4ed6\u65b9\u6cd5\u3002\u57fa\u65bc\u9069\u914d\u5668\u7684\u65b9\u6cd5\u5728\u4f7f\u7528\u66f4\u5c11\u53c3\u6578\u7684\u540c\u6642\uff0c\u53ef\u4ee5\u5339\u914d\u6216\u512a\u65bc\u5b8c\u5168\u5fae\u8abf\uff0c\u800c\u8f03\u5c0f\u7684 mLM \u88ab\u8b49\u660e\u6bd4 LLaMA-3\u3001GPT-4 \u548c\u57fa\u65bc DeepSeek-R1 \u7684\u84b8\u993e\u6a21\u578b\u7b49\u5927\u578b LLM \u66f4\u9069\u5408 LRL\u3002\u96d6\u7136\u9069\u914d\u53ef\u4ee5\u63d0\u9ad8\u6548\u80fd\uff0c\u4f46\u9810\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\u4ecd\u7136\u662f\u4e3b\u8981\u56e0\u7d20\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u9810\u8a13\u7df4\u8986\u84cb\u7bc4\u570d\u5ee3\u6cdb\u7684\u8a9e\u8a00\u3002", "author": "Daniil Gurgurov et.al.", "authors": "Daniil Gurgurov, Ivan Vykopal, Josef van Genabith, Simon Ostermann", "id": "2502.10140v1", "paper_url": "http://arxiv.org/abs/2502.10140v1", "repo": "null"}}