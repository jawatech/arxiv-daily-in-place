{"2411.04118": {"publish_time": "2024-11-06", "title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "paper_summary": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.", "paper_summary_zh": "<paragraph>\u591a\u7bc7\u8fd1\u671f\u4f5c\u54c1\u8a66\u5716\u5c08\u9580\u70ba\u91ab\u7642\u61c9\u7528\u958b\u767c\u57fa\u790e\u6a21\u578b\uff0c\u900f\u904e\u6301\u7e8c\u9810\u8a13\u7df4\u516c\u958b\u53ef\u7528\u7684\u751f\u7269\u91ab\u5b78\u8a9e\u6599\u5eab\uff0c\u8abf\u6574\u901a\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\u3002\u9019\u4e9b\u4f5c\u54c1\u901a\u5e38\u8072\u7a31\uff0c\u9019\u7a2e\u9818\u57df\u9069\u61c9\u5f0f\u9810\u8a13\u7df4 (DAPT) \u80fd\u63d0\u5347\u4e0b\u6e38\u91ab\u7642\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u4f8b\u5982\u56de\u7b54\u91ab\u5b78\u57f7\u7167\u8003\u8a66\u984c\u76ee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u4e03\u500b\u516c\u958b\u7684\u300c\u91ab\u7642\u300dLLM \u548c\u5169\u500b VLM \u8207\u5b83\u5011\u5c0d\u61c9\u7684\u57fa\u672c\u6a21\u578b\u9032\u884c\u6bd4\u8f03\uff0c\u4e26\u5f97\u51fa\u4e0d\u540c\u7684\u7d50\u8ad6\uff1a\u6240\u6709\u91ab\u7642 VLM \u548c\u5e7e\u4e4e\u6240\u6709\u91ab\u7642 LLM \u5728\u91ab\u7642\u554f\u7b54 (QA) \u4efb\u52d9\u7684\u96f6\u6b21/\u5c11\u6b21\u63d0\u793a\u6a21\u5f0f\u4e2d\uff0c\u90fd\u7121\u6cd5\u6301\u7e8c\u512a\u65bc\u5b83\u5011\u7684\u57fa\u672c\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5728\u6211\u5011\u65bc 3 \u6b21\u63d0\u793a\u8a2d\u5b9a\u4e2d\u8003\u91cf\u7684\u4efb\u52d9\u548c\u6a21\u578b\u914d\u5c0d\u4e2d\uff0c\u91ab\u7642 LLM \u50c5\u5728 12.1% \u7684\u60c5\u6cc1\u4e0b\u512a\u65bc\u5b83\u5011\u7684\u57fa\u672c\u6a21\u578b\uff0c\u5728 49.8% \u7684\u60c5\u6cc1\u4e0b\u9054\u5230\uff08\u7d71\u8a08\uff09\u5e73\u624b\uff0c\u800c\u5728\u5176\u9918 38.2% \u7684\u60c5\u6cc1\u4e0b\u5247\u986f\u8457\u4f4e\u65bc\u5b83\u5011\u7684\u57fa\u672c\u6a21\u578b\u3002\u6211\u5011\u7684\u7d50\u8ad6\u57fa\u65bc\uff1a(i) \u5c07\u6bcf\u500b\u91ab\u7642\u6a21\u578b\u8207\u5c0d\u61c9\u7684\u57fa\u672c\u6a21\u578b\u9032\u884c\u4e00\u5c0d\u4e00\u7684\u6bd4\u8f03\uff1b(ii) \u5206\u5225\u91dd\u5c0d\u6bcf\u500b\u6a21\u578b\u6700\u4f73\u5316\u63d0\u793a\uff1b\u4ee5\u53ca (iii) \u8003\u616e\u6bd4\u8f03\u4e2d\u7684\u7d71\u8a08\u4e0d\u78ba\u5b9a\u6027\u3002\u5118\u7ba1\u9019\u4e9b\u57fa\u672c\u505a\u6cd5\u4e26\u672a\u5728\u6587\u737b\u4e2d\u6301\u7e8c\u63a1\u7528\uff0c\u4f46\u6211\u5011\u7684\u6d88\u878d\u7814\u7a76\u986f\u793a\uff0c\u5b83\u5011\u6703\u5c0d\u7d50\u8ad6\u9020\u6210\u91cd\u5927\u5f71\u97ff\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u6700\u5148\u9032\u7684\u901a\u7528\u9818\u57df\u6a21\u578b\u53ef\u80fd\u5df2\u7d93\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u91ab\u7642\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u63d0\u51fa\u5efa\u8b70\u4ee5\u52a0\u5f37\u672a\u4f86\u7814\u7a76\u7684\u7d50\u8ad6\u3002</paragraph>", "author": "Daniel P. Jeong et.al.", "authors": "Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst", "id": "2411.04118v2", "paper_url": "http://arxiv.org/abs/2411.04118v2", "repo": "null"}}