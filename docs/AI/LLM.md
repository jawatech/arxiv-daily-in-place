
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-10**|**Linearizing Large Language Models**|Jean Mercat et.al.|[2405.06640v1](http://arxiv.org/abs/2405.06640v1)|[link](https://github.com/tri-ml/linear_open_lm)|
|**2024-05-10**|**Value Augmented Sampling for Language Model Alignment and Personalization**|Seungwook Han et.al.|[2405.06639v1](http://arxiv.org/abs/2405.06639v1)|[link](https://github.com/idanshen/Value-Augmented-Sampling)|
|**2024-05-10**|**Federated Document Visual Question Answering: A Pilot Study**|Khanh Nguyen et.al.|[2405.06636v1](http://arxiv.org/abs/2405.06636v1)|null|
|**2024-05-10**|**Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark**|Evan M. Williams et.al.|[2405.06634v1](http://arxiv.org/abs/2405.06634v1)|null|
|**2024-05-10**|**Characterizing the Accuracy - Efficiency Trade-off of Low-rank Decomposition in Language Models**|Chakshu Moar et.al.|[2405.06626v1](http://arxiv.org/abs/2405.06626v1)|null|
|**2024-05-10**|**Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems**|David "davidad" Dalrymple et.al.|[2405.06624v1](http://arxiv.org/abs/2405.06624v1)|null|
|**2024-05-10**|**Explaining Text Similarity in Transformer Models**|Alexandros Vasileiou et.al.|[2405.06604v1](http://arxiv.org/abs/2405.06604v1)|null|
|**2024-05-10**|**An Investigation of Incorporating Mamba for Speech Enhancement**|Rong Chao et.al.|[2405.06573v1](http://arxiv.org/abs/2405.06573v1)|[link](https://github.com/roychao19477/semamba)|
|**2024-05-10**|**What Can Natural Language Processing Do for Peer Review?**|Ilia Kuznetsov et.al.|[2405.06563v1](http://arxiv.org/abs/2405.06563v1)|null|
|**2024-05-10**|**Scalable Property Valuation Models via Graph-based Deep Learning**|Enrique Riveros et.al.|[2405.06553v1](http://arxiv.org/abs/2405.06553v1)|null|
|**2024-05-10**|**Sampling the Swadesh List to Identify Similar Languages with Tree Spaces**|Garett Ordway et.al.|[2405.06549v1](http://arxiv.org/abs/2405.06549v1)|null|
|**2024-05-10**|**Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval**|Mengjia Niu et.al.|[2405.06545v1](http://arxiv.org/abs/2405.06545v1)|null|
|**2024-05-10**|**ATSumm: Auxiliary information enhanced approach for abstractive disaster Tweet Summarization with sparse training data**|Piyush Kumar Garg et.al.|[2405.06541v1](http://arxiv.org/abs/2405.06541v1)|null|
|**2024-05-10**|**Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts**|Wenyu Huang et.al.|[2405.06524v1](http://arxiv.org/abs/2405.06524v1)|null|
|**2024-05-10**|**Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum Learning**|Yili Wang et.al.|[2405.06522v1](http://arxiv.org/abs/2405.06522v1)|[link](https://github.com/wangyili00/ldhgnn)|
|**2024-05-10**|**UniDM: A Unified Framework for Data Manipulation with Large Language Models**|Yichen Qian et.al.|[2405.06510v1](http://arxiv.org/abs/2405.06510v1)|null|
|**2024-05-10**|**Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based Method for Evaluating Chess Strategies from Textbooks**|Haifa Alrdahi et.al.|[2405.06499v1](http://arxiv.org/abs/2405.06499v1)|null|
|**2024-05-10**|**Solving Quantified Boolean Formulas with Few Existential Variables**|Leif Eriksson et.al.|[2405.06485v1](http://arxiv.org/abs/2405.06485v1)|null|
|**2024-05-10**|**LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing**|Ana Ezquerro et.al.|[2405.06483v1](http://arxiv.org/abs/2405.06483v1)|null|
|**2024-05-10**|**Attention is all they need: Cognitive science and the (techno)political economy of attention in humans and machines**|Pablo González de la Torre et.al.|[2405.06478v1](http://arxiv.org/abs/2405.06478v1)|null|
|**2024-05-10**|**Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification**|Yaoqin Ye et.al.|[2405.06468v1](http://arxiv.org/abs/2405.06468v1)|null|
|**2024-05-10**|**Are EEG-to-Text Models Working?**|Hyejeong Jo et.al.|[2405.06459v1](http://arxiv.org/abs/2405.06459v1)|null|
|**2024-05-10**|**Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation**|JoonHo Lee et.al.|[2405.06424v1](http://arxiv.org/abs/2405.06424v1)|[link](https://github.com/p-b-u/proxy_based_uncertainty)|
|**2024-05-10**|**Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting**|Tianxiang Zhan et.al.|[2405.06419v1](http://arxiv.org/abs/2405.06419v1)|[link](https://github.com/ztxtech/Time-Evidence-Fusion-Network)|
|**2024-05-10**|**PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning**|Jaejun Lee et.al.|[2405.06418v1](http://arxiv.org/abs/2405.06418v1)|null|
|**2024-05-10**|**Can Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?**|Hunter McNichols et.al.|[2405.06414v1](http://arxiv.org/abs/2405.06414v1)|null|
|**2024-05-10**|**Multi-level Personalized Federated Learning on Heterogeneous and Long-Tailed Data**|Rongyu Zhang et.al.|[2405.06413v1](http://arxiv.org/abs/2405.06413v1)|null|
|**2024-05-10**|**Potential and Limitations of LLMs in Capturing Structured Semantics: A Case Study on SRL**|Ning Cheng et.al.|[2405.06410v1](http://arxiv.org/abs/2405.06410v1)|null|
|**2024-05-10**|**Program Synthesis using Inductive Logic Programming for the Abstraction and Reasoning Corpus**|Filipe Marinho Rocha et.al.|[2405.06399v1](http://arxiv.org/abs/2405.06399v1)|null|
|**2024-05-10**|**Memory Mosaics**|Jianyu Zhang et.al.|[2405.06394v1](http://arxiv.org/abs/2405.06394v1)|null|
|**2024-05-10**|**LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play**|Li-Chun Lu et.al.|[2405.06373v1](http://arxiv.org/abs/2405.06373v1)|null|
|**2024-05-10**|**Intelligent Duty Cycling Management and Wake-up for Energy Harvesting IoT Networks with Correlated Activity**|David E. Ruíz-Guirola et.al.|[2405.06372v1](http://arxiv.org/abs/2405.06372v1)|null|
|**2024-05-10**|**Projection by Convolution: Optimal Sample Complexity for Reinforcement Learning in Continuous-Space MDPs**|Davide Maran et.al.|[2405.06363v1](http://arxiv.org/abs/2405.06363v1)|null|
|**2024-05-10**|**KeepOriginalAugment: Single Image-based Better Information-Preserving Data Augmentation Approach**|Teerath Kumar et.al.|[2405.06354v1](http://arxiv.org/abs/2405.06354v1)|null|
|**2024-05-10**|**Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology**|Rishav Hada et.al.|[2405.06346v1](http://arxiv.org/abs/2405.06346v1)|null|
|**2024-05-10**|**LMD3: Language Model Data Density Dependence**|John Kirchenbauer et.al.|[2405.06331v1](http://arxiv.org/abs/2405.06331v1)|null|
|**2024-05-10**|**ChatGPTest: opportunities and cautionary tales of utilizing AI for questionnaire pretesting**|Francisco Olivos et.al.|[2405.06329v1](http://arxiv.org/abs/2405.06329v1)|null|
|**2024-05-10**|**Correlation Dimension of Natural Language in a Statistical Manifold**|Xin Du et.al.|[2405.06321v1](http://arxiv.org/abs/2405.06321v1)|null|
|**2024-05-10**|**Decoding Emotions in Abstract Art: Cognitive Plausibility of CLIP in Recognizing Color-Emotion Associations**|Hanna-Sophia Widhoelzl et.al.|[2405.06319v1](http://arxiv.org/abs/2405.06319v1)|null|
|**2024-05-10**|**A NLP Approach to "Review Bombing" in Metacritic PC Videogames User Ratings**|Javier Coronado-Blázquez et.al.|[2405.06306v1](http://arxiv.org/abs/2405.06306v1)|null|
|**2024-05-10**|**Cross-domain Learning Framework for Tracking Users in RIS-aided Multi-band ISAC Systems with Sparse Labeled Data**|Jingzhi Hu et.al.|[2405.06299v1](http://arxiv.org/abs/2405.06299v1)|null|
|**2024-05-10**|**Aspect-oriented Consumer Health Answer Summarization**|Rochana Chaturvedi et.al.|[2405.06295v1](http://arxiv.org/abs/2405.06295v1)|null|
|**2024-05-10**|**Pruning as a Domain-specific LLM Extractor**|Nan Zhang et.al.|[2405.06275v1](http://arxiv.org/abs/2405.06275v1)|[link](https://github.com/psunlpgroup/d-pruner)|
|**2024-05-10**|**XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**|Fatemeh Nazary et.al.|[2405.06270v1](http://arxiv.org/abs/2405.06270v1)|null|
|**2024-05-10**|**A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting**|Jianli Xiao et.al.|[2405.06266v1](http://arxiv.org/abs/2405.06266v1)|null|
|**2024-05-10**|**Learning Latent Dynamic Robust Representations for World Models**|Ruixiang Sun et.al.|[2405.06263v1](http://arxiv.org/abs/2405.06263v1)|[link](https://github.com/bit1029public/hrssm)|
|**2024-05-10**|**Precise Apple Detection and Localization in Orchards using YOLOv5 for Robotic Harvesting Systems**|Jiang Ziyue et.al.|[2405.06260v1](http://arxiv.org/abs/2405.06260v1)|null|
|**2024-05-10**|**Automatic Generation of Model and Data Cards: A Step Towards Responsible AI**|Jiarui Liu et.al.|[2405.06258v1](http://arxiv.org/abs/2405.06258v1)|null|
|**2024-05-10**|**Disttack: Graph Adversarial Attacks Toward Distributed GNN Training**|Yuxiang Zhang et.al.|[2405.06247v1](http://arxiv.org/abs/2405.06247v1)|[link](https://github.com/zhangyxrepo/disttack)|
|**2024-05-10**|**SaudiBERT: A Large Language Model Pretrained on Saudi Dialect Corpora**|Faisal Qarah et.al.|[2405.06239v1](http://arxiv.org/abs/2405.06239v1)|null|
|**2024-05-10**|**Learning to Solve Geometry Problems via Simulating Human Dual-Reasoning Process**|Tong Xiao et.al.|[2405.06232v1](http://arxiv.org/abs/2405.06232v1)|null|
|**2024-05-10**|**For the Misgendered Chinese in Gender Bias Research: Multi-Task Learning with Knowledge Distillation for Pinyin Name-Gender Prediction**|Xiaocong Du et.al.|[2405.06221v1](http://arxiv.org/abs/2405.06221v1)|null|
|**2024-05-10**|**SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models**|Haojie Duanmu et.al.|[2405.06219v1](http://arxiv.org/abs/2405.06219v1)|null|
|**2024-05-10**|**Aligning Tutor Discourse Supporting Rigorous Thinking with Tutee Content Mastery for Predicting Math Achievement**|Mark Abdelshiheed et.al.|[2405.06218v1](http://arxiv.org/abs/2405.06218v1)|null|
|**2024-05-10**|**A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models**|Yujuan Ding et.al.|[2405.06211v1](http://arxiv.org/abs/2405.06211v1)|null|
|**2024-05-10**|**Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning**|Yujie Zhang et.al.|[2405.06206v1](http://arxiv.org/abs/2405.06206v1)|null|
|**2024-05-10**|**HC$^2$L: Hybrid and Cooperative Contrastive Learning for Cross-lingual Spoken Language Understanding**|Bowen Xing et.al.|[2405.06204v1](http://arxiv.org/abs/2405.06204v1)|null|
|**2024-05-10**|**VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks**|Manish Dhakal et.al.|[2405.06196v1](http://arxiv.org/abs/2405.06196v1)|null|
|**2024-05-10**|**Skeet: Towards a Lightweight Serverless Framework Supporting Modern AI-Driven App Development**|Kawasaki Fumitake et.al.|[2405.06164v1](http://arxiv.org/abs/2405.06164v1)|null|
|**2024-05-09**|**Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing Clinical and Social Effects of Substance Use Derived from Social Media**|Yao Ge et.al.|[2405.06145v1](http://arxiv.org/abs/2405.06145v1)|null|
|**2024-05-09**|**Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models**|Vyas Raina et.al.|[2405.06134v1](http://arxiv.org/abs/2405.06134v1)|null|
|**2024-05-09**|**Narrative to Trajectory (N2T+): Extracting Routes of Life or Death from Human Trafficking Text Corpora**|Saydeh N. Karabatis et.al.|[2405.06129v1](http://arxiv.org/abs/2405.06129v1)|null|
|**2024-05-09**|**Scalable Exact Verification of Optimization Proxies for Large-Scale Optimal Power Flow**|Rahul Nellikkath et.al.|[2405.06109v1](http://arxiv.org/abs/2405.06109v1)|null|
|**2024-05-09**|**Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?**|Yutong Hu et.al.|[2405.06105v1](http://arxiv.org/abs/2405.06105v1)|null|
|**2024-05-09**|**Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection**|Bhawesh Kumar et.al.|[2405.06093v1](http://arxiv.org/abs/2405.06093v1)|null|
|**2024-05-09**|**HMT: Hierarchical Memory Transformer for Long Context Language Processing**|Zifan He et.al.|[2405.06067v1](http://arxiv.org/abs/2405.06067v1)|[link](https://github.com/OswaldHe/HMT-pytorch)|
|**2024-05-09**|**LLMs for XAI: Future Directions for Explaining Explanations**|Alexandra Zytek et.al.|[2405.06064v1](http://arxiv.org/abs/2405.06064v1)|null|
|**2024-05-09**|**A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds**|Christopher Z. Cui et.al.|[2405.06059v1](http://arxiv.org/abs/2405.06059v1)|null|
|**2024-05-09**|**Large Language Models Show Human-like Social Desirability Biases in Survey Responses**|Aadesh Salecha et.al.|[2405.06058v1](http://arxiv.org/abs/2405.06058v1)|null|
|**2024-05-09**|**From Algorithm to Hardware: A Survey on Efficient and Safe Deployment of Deep Neural Networks**|Xue Geng et.al.|[2405.06038v1](http://arxiv.org/abs/2405.06038v1)|null|
|**2024-05-09**|**Natural Language Processing RELIES on Linguistics**|Juri Opitz et.al.|[2405.05966v1](http://arxiv.org/abs/2405.05966v1)|null|
|**2024-05-09**|**Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask**|Zineb Senane et.al.|[2405.05959v1](http://arxiv.org/abs/2405.05959v1)|[link](https://github.com/eqtpartners/tsde)|
|**2024-05-09**|**OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning**|Dan Qiao et.al.|[2405.05957v1](http://arxiv.org/abs/2405.05957v1)|[link](https://github.com/opennlg/openba-v2)|
|**2024-05-09**|**Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning**|Junzhi Chen et.al.|[2405.05955v1](http://arxiv.org/abs/2405.05955v1)|null|
|**2024-05-09**|**DOLOMITES: Domain-Specific Long-Form Methodical Tasks**|Chaitanya Malaviya et.al.|[2405.05938v1](http://arxiv.org/abs/2405.05938v1)|null|
|**2024-05-09**|**Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness**|Siyuan Li et.al.|[2405.05930v1](http://arxiv.org/abs/2405.05930v1)|null|
|**2024-05-09**|**FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting**|Xiaohui Zhong et.al.|[2405.05925v1](http://arxiv.org/abs/2405.05925v1)|null|
|**2024-05-09**|**Diag2Diag: Multi modal super resolution for physics discovery with application to fusion**|Azarakhsh Jalalvand et.al.|[2405.05908v1](http://arxiv.org/abs/2405.05908v1)|null|
|**2024-05-09**|**Truthful Aggregation of LLMs with an Application to Online Advertising**|Ermis Soumalias et.al.|[2405.05905v1](http://arxiv.org/abs/2405.05905v1)|null|
|**2024-05-09**|**Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?**|Zorik Gekhman et.al.|[2405.05904v1](http://arxiv.org/abs/2405.05904v1)|null|
|**2024-05-09**|**Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons**|Adian Liusie et.al.|[2405.05894v1](http://arxiv.org/abs/2405.05894v1)|null|
|**2024-05-09**|**Safe Exploration Using Bayesian World Models and Log-Barrier Optimization**|Yarden As et.al.|[2405.05890v1](http://arxiv.org/abs/2405.05890v1)|null|
|**2024-05-09**|**EWMoE: An effective model for global weather forecasting with mixture-of-experts**|Lihao Gan et.al.|[2405.06004v1](http://arxiv.org/abs/2405.06004v1)|null|
|**2024-05-09**|**Composable Part-Based Manipulation**|Weiyu Liu et.al.|[2405.05876v1](http://arxiv.org/abs/2405.05876v1)|null|
|**2024-05-09**|**ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers**|Liangliang Chen et.al.|[2405.05861v1](http://arxiv.org/abs/2405.05861v1)|null|
|**2024-05-09**|**Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control**|Gunshi Gupta et.al.|[2405.05852v1](http://arxiv.org/abs/2405.05852v1)|[link](https://github.com/ykarmesh/stable-control-representations)|
|**2024-05-09**|**Aequitas Flow: Streamlining Fair ML Experimentation**|Sérgio Jesus et.al.|[2405.05809v1](http://arxiv.org/abs/2405.05809v1)|null|
|**2024-05-09**|**Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference**|Zhihang Lin et.al.|[2405.05803v1](http://arxiv.org/abs/2405.05803v1)|[link](https://github.com/lzhxmu/vtw)|
|**2024-05-09**|**RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation**|Sourav Garg et.al.|[2405.05792v1](http://arxiv.org/abs/2405.05792v1)|null|
|**2024-05-09**|**A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties**|A. Noroozi et.al.|[2405.05790v1](http://arxiv.org/abs/2405.05790v1)|null|
|**2024-05-09**|**Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language**|Ronny Paul et.al.|[2405.05777v1](http://arxiv.org/abs/2405.05777v1)|null|
|**2024-05-09**|**Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions**|Polina Tsvilodub et.al.|[2405.05776v1](http://arxiv.org/abs/2405.05776v1)|null|
|**2024-05-09**|**To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**|Miquel Miró-Nicolau et.al.|[2405.05766v1](http://arxiv.org/abs/2405.05766v1)|null|
|**2024-05-09**|**DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models**|Mengxiao Geng et.al.|[2405.05763v1](http://arxiv.org/abs/2405.05763v1)|null|
|**2024-05-09**|**Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media**|Zhizhen Zhang et.al.|[2405.05760v1](http://arxiv.org/abs/2405.05760v1)|null|
|**2024-05-09**|**Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma**|Han Meng et.al.|[2405.05758v1](http://arxiv.org/abs/2405.05758v1)|null|
|**2024-05-09**|**CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks**|Nick et.al.|[2405.05755v1](http://arxiv.org/abs/2405.05755v1)|null|
|**2024-05-09**|**Can large language models understand uncommon meanings of common words?**|Jinyang Wu et.al.|[2405.05741v1](http://arxiv.org/abs/2405.05741v1)|null|
|**2024-05-09**|**Computational lexical analysis of Flamenco genres**|Pablo Rosillo-Rodes et.al.|[2405.05723v1](http://arxiv.org/abs/2405.05723v1)|null|
|**2024-05-09**|**Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution**|Sandrine Chausson et.al.|[2405.05705v1](http://arxiv.org/abs/2405.05705v1)|[link](https://github.com/s-l-chausson/easyclaimsdetection)|

#### Abstracts
##### **Linearizing Large Language Models**
2405.06640v1 by Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar

Linear transformers have emerged as a subquadratic-time alternative to
softmax attention and have garnered significant interest due to their
fixed-size recurrent state that lowers inference cost. However, their original
formulation suffers from poor scaling and underperforms compute-matched
transformers. Recent linear models such as RWKV and Mamba have attempted to
address these shortcomings by proposing novel time-mixing and gating
architectures, but pre-training large language models requires significant data
and compute investments. Thus, the search for subquadratic architectures is
limited by the availability of compute and quality pre-training datasets. As a
cost-effective alternative to pre-training linear transformers, we propose
Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to
uptrain existing large pre-trained transformers into Recurrent Neural Networks
(RNNs) with a modest compute budget. This allows us to leverage the strong
pre-training data and performance of existing transformer LLMs, while requiring
5% of the training cost. We find that our linearization technique leads to
competitive performance on standard benchmarks, but we identify persistent
in-context learning and long-context modeling shortfalls for even the largest
linear models. Our code and models can be found at
https://github.com/TRI-ML/linear_open_lm.

摘要：線性轉換器已成為 softmax 注意力的次二次時間替代方案，並因其降低了推論成本的固定大小遞迴狀態而備受關注。然而，它們的原始公式存在擴展性差且表現不如計算匹配轉換器的問題。最近的線性模型，例如 RWKV 和 Mamba，已嘗試通過提出新穎的時間混合和閘控架構來解決這些缺點，但預訓練大型語言模型需要大量的數據和計算投資。因此，對次二次架構的搜索受到計算和質量預訓練數據集可用性的限制。作為預訓練線性轉換器的經濟高效的替代方案，我們提出了遞迴注意力的可擴展上訓練 (SUPRA)。我們提出了一種將現有的預先訓練好的大型轉換器上訓練為具有適度計算預算的遞迴神經網路 (RNN) 的方法。這使我們能夠利用現有轉換器 LLM 的強大預訓練數據和性能，同時只需 5% 的訓練成本。我們發現我們的線性化技術可以在標準基準上帶來具有競爭力的性能，但我們發現即使是最大的線性模型也存在持續的語境學習和長語境建模的不足。我們的程式碼和模型可以在 https://github.com/TRI-ML/linear_open_lm 找到。

##### **Value Augmented Sampling for Language Model Alignment and Personalization**
2405.06639v1 by Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, Pulkit Agrawal

Aligning Large Language Models (LLMs) to cater to different human
preferences, learning new skills, and unlearning harmful behavior is an
important problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree
Search, are performant, but impractical for LLM adaptation due to their high
inference cost. On the other hand, using Reinforcement Learning (RL) for
adaptation is computationally efficient, but performs worse due to the
optimization challenges in co-training the value function and the policy. We
present a new framework for reward optimization, Value Augmented Sampling
(VAS), that can maximize different reward functions using data sampled from
only the initial, frozen LLM. VAS solves for the optimal reward-maximizing
policy without co-training the policy and the value function, making the
optimization stable, outperforming established baselines, such as PPO and DPO,
on standard benchmarks, and achieving comparable results to Best-of-128 with
lower inference cost. Unlike existing RL methods that require changing the
weights of the LLM, VAS does not require access to the weights of the
pre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are
available only as APIs. In addition, our algorithm unlocks the new capability
of composing several rewards and controlling the extent of each one during
deployment time, paving the road ahead for the future of aligned, personalized
LLMs.

摘要：調整大型語言模型 (LLM) 以迎合不同的人類偏好、學習新技能和取消有害行為是一個重要的問題。基於搜尋的方法，例如 Best-of-N 或蒙地卡羅樹搜尋，效能良好，但由於其推論成本高，因此不切實際用於 LLM 適應。另一方面，使用強化學習 (RL) 來適應在運算上很有效率，但由於在共同訓練價值函數和策略時有最佳化挑戰，因此表現較差。我們提出了一個新的獎勵最佳化架構，即價值增強取樣 (VAS)，它可以使用僅從初始凍結 LLM 取樣的資料來最大化不同的獎勵函數。VAS 解決了最佳化獎勵最大化的策略，而無需共同訓練策略和價值函數，使得最佳化穩定，在標準基準上優於已建立的基準，例如 PPO 和 DPO，並以較低的推論成本達到與 Best-of-128 相當的結果。與需要改變 LLM 權重的現有 RL 方法不同，VAS 不需要存取預先訓練的 LLM 的權重。因此，它甚至可以調整僅作為 API 可用的 LLM（例如 ChatGPT）。此外，我們的演算法解鎖了組合多個獎勵並在部署時控制每個獎勵程度的新功能，為未來對齊、個人化的 LLM 鋪平了道路。

##### **Federated Document Visual Question Answering: A Pilot Study**
2405.06636v1 by Khanh Nguyen, Dimosthenis Karatzas

An important handicap of document analysis research is that documents tend to
be copyrighted or contain private information, which prohibits their open
publication and the creation of centralised, large-scale document datasets.
Instead, documents are scattered in private data silos, making extensive
training over heterogeneous data a tedious task. In this work, we explore the
use of a federated learning (FL) scheme as a way to train a shared model on
decentralised private document data. We focus on the problem of Document VQA, a
task particularly suited to this approach, as the type of reasoning
capabilities required from the model can be quite different in diverse domains.
Enabling training over heterogeneous document datasets can thus substantially
enrich DocVQA models. We assemble existing DocVQA datasets from diverse domains
to reflect the data heterogeneity in real-world applications. We explore the
self-pretraining technique in this multi-modal setting, where the same data is
used for both pretraining and finetuning, making it relevant for privacy
preservation. We further propose combining self-pretraining with a Federated
DocVQA training method using centralized adaptive optimization that outperforms
the FedAvg baseline. With extensive experiments, we also present a
multi-faceted analysis on training DocVQA models with FL, which provides
insights for future research on this task. We show that our pretraining
strategies can effectively learn and scale up under federated training with
diverse DocVQA datasets and tuning hyperparameters is essential for practical
document tasks under federation.

摘要：文件分析研究的一項重要障礙在於文件往往受版權保護或包含私人資訊，這禁止它們公開發布和建立集中式、大型文件資料集。反之，文件分散在私人資料孤島中，讓異質資料上的廣泛訓練成為一項繁瑣的任務。在這項工作中，我們探討使用聯邦學習 (FL) 架構作為在分散式私人文件資料上訓練共享模型的方法。我們專注於文件 VQA 問題，這項任務特別適合此方法，因為模型所需的推理能力類型在不同的領域中可能大不相同。因此，在異質文件資料集上進行訓練能夠大幅豐富 DocVQA 模型。我們從不同的領域彙整現有的 DocVQA 資料集，以反映真實世界應用中的資料異質性。我們探討此多模式設定中的自我預訓練技術，其中相同的資料用於預訓練和微調，使其與隱私保護相關。我們進一步提出將自我預訓練與使用集中式自適應最佳化的聯邦 DocVQA 訓練方法結合，其優於 FedAvg 基準。透過廣泛的實驗，我們還針對使用 FL 訓練 DocVQA 模型提出多面向分析，這為未來針對此任務的研究提供見解。我們展示我們的預訓練策略能夠在使用不同的 DocVQA 資料集進行聯邦訓練的情況下有效學習和擴展，而且調整超參數對於聯邦下的實際文件任務至關重要。

##### **Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark**
2405.06634v1 by Evan M. Williams, Kathleen M. Carley

We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual
Network Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision
Language Models (VLMs) on 5 tasks related to three foundational network science
concepts: identifying nodes of maximal degree on a rendered graph, identifying
whether signed triads are balanced or unbalanced, and counting components. The
tasks are structured to be easy for a human who understands the underlying
graph theoretic concepts, and can all be solved by counting the appropriate
elements in graphs. We find that while GPT-4 consistently outperforms LLaVa,
both models struggle with every visual network analysis task we propose. We
publicly release the first benchmark for the evaluation of VLMs on foundational
VNA tasks.

摘要：我們評估 GPT-4 和 LLaVa 的零次學習能力，以執行小規模圖形上的簡單視覺網路分析 (VNA) 任務。我們評估視覺語言模型 (VLM) 在 5 個與三個基礎網路科學概念相關的任務上：識別渲染圖形上最大程度的節點、識別帶正負號的三元組是平衡還是不平衡，以及計算組成部分。這些任務的結構對於理解基礎圖形理論概念的人類來說很容易，並且都可以透過計算圖形中的適當元素來解決。我們發現，雖然 GPT-4 持續優於 LLaVa，但這兩個模型都難以處理我們提出的每個視覺網路分析任務。我們公開發布了第一個用於評估 VLM 在基礎 VNA 任務上的基準。

##### **Characterizing the Accuracy - Efficiency Trade-off of Low-rank Decomposition in Language Models**
2405.06626v1 by Chakshu Moar, Michael Pellauer, Hyoukjun Kwon

Large language models (LLMs) have emerged and presented their general
problem-solving capabilities with one model. However, the model size has
increased dramatically with billions of parameters to enable such broad
problem-solving capabilities. In addition, due to the dominance of
matrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model
size ratio is significantly lower than that of CNNs. This shift pushes LLMs
from a computation-bound regime to a memory-bound regime. Therefore, optimizing
the memory footprint and traffic is an important optimization direction for
LLMs today.
  Model compression methods such as quantization and parameter pruning have
been actively explored for achieving the memory footprint and traffic
optimization. However, the accuracy-efficiency trade-off of rank pruning for
LLMs is not well-understood yet. Therefore, we characterize the
accuracy-efficiency trade-off of a low-rank decomposition method, specifically
Tucker decomposition, on recent language models, including an open-source LLM,
Llama 2.
  We formalize the low-rank decomposition design space and show that the
decomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To
navigate such a vast design space, we formulate the design space and perform
thorough case studies of accuracy-efficiency trade-offs using six widely used
LLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve
a 9\% model size reduction with minimal accuracy drops, which range from 4\%p
to 10\%p, depending on the difficulty of the benchmark, without any retraining
to recover accuracy after decomposition. The results show that low-rank
decomposition can be a promising direction for LLM-based applications that
require real-time service in scale (e.g., AI agent assist and real-time coding
assistant), where the latency is as important as the model accuracy.

摘要：大型語言模型 (LLM) 已浮現，並以單一模型展示其一般問題解決能力。然而，模型大小已大幅增加，擁有數十億個參數，以實現如此廣泛的問題解決能力。此外，由於 LLM 中矩陣-矩陣和矩陣-向量乘法的優勢，計算與模型大小的比率顯著低於 CNN。此轉變將 LLM 從計算受限的模式推向記憶受限的模式。因此，優化記憶體使用量和流量是當今 LLM 的重要優化方向。
模型壓縮方法（例如量化和參數剪枝）已被積極探索，以實現記憶體使用量和流量優化。然而，LLM 的秩剪枝的準確度-效率權衡尚未被充分理解。因此，我們描述了低秩分解方法（特別是 Tucker 分解）在近期語言模型（包括開源 LLM，Llama 2）上的準確度-效率權衡。
我們形式化了低秩分解設計空間，並表明分解設計空間非常龐大（例如，Llama2-7B 的 O($2^{37}$)。為了導航如此廣闊的設計空間，我們制定了設計空間，並使用 BERT 和 Llama 2 模型上的六個廣泛使用的 LLM 基準，對準確度-效率權衡進行了徹底的案例研究。我們的結果表明，我們可以在不重新訓練以在分解後恢復準確度的情況下，實現 9% 的模型大小減小，準確度下降最小，範圍從 4%p 到 10%p，具體取決於基準的難度。結果表明，低秩分解對於需要大規模即時服務的基於 LLM 的應用（例如，AI 代理協助和即時編碼助理）來說，可能是一個有前途的方向，其中延遲與模型準確度一樣重要。

##### **Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems**
2405.06624v1 by David "davidad" Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barrett, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, Joshua Tenenbaum

Ensuring that AI systems reliably and robustly avoid harmful or dangerous
behaviours is a crucial challenge, especially for AI systems with a high degree
of autonomy and general intelligence, or systems used in safety-critical
contexts. In this paper, we will introduce and define a family of approaches to
AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature
of these approaches is that they aim to produce AI systems which are equipped
with high-assurance quantitative safety guarantees. This is achieved by the
interplay of three core components: a world model (which provides a
mathematical description of how the AI system affects the outside world), a
safety specification (which is a mathematical description of what effects are
acceptable), and a verifier (which provides an auditable proof certificate that
the AI satisfies the safety specification relative to the world model). We
outline a number of approaches for creating each of these three core
components, describe the main technical challenges, and suggest a number of
potential solutions to them. We also argue for the necessity of this approach
to AI safety, and for the inadequacy of the main alternative approaches.

摘要：確保 AI 系統可靠且穩健地避免有害或危險的行為是一項至關重要的挑戰，特別是對於高度自主和具備一般智慧的 AI 系統，或用於安全關鍵情境中的系統。在本文中，我們將介紹和定義一系列 AI 安全方法，我們將其稱為保證安全 (GS) AI。這些方法的核心特徵是它們旨在產生具備高保證定量安全保證的 AI 系統。這是透過三個核心組成的交互作用來實現的：世界模型（提供 AI 系統如何影響外部世界的數學描述）、安全規範（對可接受影響的數學描述）和驗證器（提供可稽核的證明證明，證明 AI 滿足相對於世界模型的安全規範）。我們概述了建立這三個核心組成中每一個的許多方法，描述了主要的技術挑戰，並提出了一些可能的解決方案。我們也主張這種方法對於 AI 安全的必要性，以及主要替代方法的不足。

##### **Explaining Text Similarity in Transformer Models**
2405.06604v1 by Alexandros Vasileiou, Oliver Eberle

As Transformers have become state-of-the-art models for natural language
processing (NLP) tasks, the need to understand and explain their predictions is
increasingly apparent. Especially in unsupervised applications, such as
information retrieval tasks, similarity models built on top of foundation model
representations have been widely applied. However, their inner prediction
mechanisms have mostly remained opaque. Recent advances in explainable AI have
made it possible to mitigate these limitations by leveraging improved
explanations for Transformers through layer-wise relevance propagation (LRP).
Using BiLRP, an extension developed for computing second-order explanations in
bilinear similarity models, we investigate which feature interactions drive
similarity in NLP models. We validate the resulting explanations and
demonstrate their utility in three corpus-level use cases, analyzing
grammatical interactions, multilingual semantics, and biomedical text
retrieval. Our findings contribute to a deeper understanding of different
semantic similarity tasks and models, highlighting how novel explainable AI
methods enable in-depth analyses and corpus-level insights.

摘要：隨著 Transformer 成為自然語言處理 (NLP) 任務的最新技術，了解和解釋其預測的需求日益明顯。特別是在無監督應用程式中，例如資訊檢索任務，建立在基礎模型表示上的相似性模型已被廣泛應用。然而，它們的內部預測機制大多仍然不透明。可解釋 AI 的最新進展使得透過分層相關性傳播 (LRP) 為 Transformer 提供更好的解釋，從而減輕這些限制。使用 BiLRP，一種為計算雙線性相似性模型中的二階解釋而開發的延伸，我們探討了哪些特徵互動驅動了 NLP 模型中的相似性。我們驗證了產生的解釋，並展示了它們在三個語料庫級別用例中的效用，分析語法互動、多語言語義和生物醫學文字檢索。我們的發現有助於更深入地了解不同的語義相似性任務和模型，強調了新穎的可解釋 AI 方法如何實現深入分析和語料庫級別見解。

##### **An Investigation of Incorporating Mamba for Speech Enhancement**
2405.06573v1 by Rong Chao, Wen-Huang Cheng, Moreno La Quatra, Sabato Marco Siniscalchi, Chao-Han Huck Yang, Szu-Wei Fu, Yu Tsao

This work aims to study a scalable state-space model (SSM), Mamba, for the
speech enhancement (SE) task. We exploit a Mamba-based regression model to
characterize speech signals and build an SE system upon Mamba, termed SEMamba.
We explore the properties of Mamba by integrating it as the core model in both
basic and advanced SE systems, along with utilizing signal-level distances as
well as metric-oriented loss functions. SEMamba demonstrates promising results
and attains a PESQ score of 3.55 on the VoiceBank-DEMAND dataset. When combined
with the perceptual contrast stretching technique, the proposed SEMamba yields
a new state-of-the-art PESQ score of 3.69.

摘要：本研究旨在針對語音增強 (SE) 任務研究一種可擴充的狀態空間模型 (SSM) Mamba。我們利用基於 Mamba 的回歸模型來描述語音訊號，並在 Mamba 上建立一個 SE 系統，稱為 SEMamba。我們透過將 Mamba 整合為基礎和進階 SE 系統中的核心模型，以及利用訊號層級距離和指標導向損失函數來探索 Mamba 的屬性。SEMamba 展示了令人滿意的結果，並在 VoiceBank-DEMAND 資料集上獲得 3.55 的 PESQ 分數。當與感知對比拉伸技術結合使用時，提出的 SEMamba 可產生 3.69 的新最先進 PESQ 分數。

##### **What Can Natural Language Processing Do for Peer Review?**
2405.06563v1 by Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K. Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Mausam, Margot Mieskes, Aurélie Névéol, Danish Pruthi, Lizhen Qu, Roy Schwartz, Noah A. Smith, Thamar Solorio, Jingyan Wang, Xiaodan Zhu, Anna Rogers, Nihar B. Shah, Iryna Gurevych

The number of scientific articles produced every year is growing rapidly.
Providing quality control over them is crucial for scientists and, ultimately,
for the public good. In modern science, this process is largely delegated to
peer review -- a distributed procedure in which each submission is evaluated by
several independent experts in the field. Peer review is widely used, yet it is
hard, time-consuming, and prone to error. Since the artifacts involved in peer
review -- manuscripts, reviews, discussions -- are largely text-based, Natural
Language Processing has great potential to improve reviewing. As the emergence
of large language models (LLMs) has enabled NLP assistance for many new tasks,
the discussion on machine-assisted peer review is picking up the pace. Yet,
where exactly is help needed, where can NLP help, and where should it stand
aside? The goal of our paper is to provide a foundation for the future efforts
in NLP for peer-reviewing assistance. We discuss peer review as a general
process, exemplified by reviewing at AI conferences. We detail each step of the
process from manuscript submission to camera-ready revision, and discuss the
associated challenges and opportunities for NLP assistance, illustrated by
existing work. We then turn to the big challenges in NLP for peer review as a
whole, including data acquisition and licensing, operationalization and
experimentation, and ethical issues. To help consolidate community efforts, we
create a companion repository that aggregates key datasets pertaining to peer
review. Finally, we issue a detailed call for action for the scientific
community, NLP and AI researchers, policymakers, and funding bodies to help
bring the research in NLP for peer review forward. We hope that our work will
help set the agenda for research in machine-assisted scientific quality control
in the age of AI, within the NLP community and beyond.

摘要：<paragraph>每年產出的科學文章數量正快速增長。對其進行品質控管對於科學家而言至關重要，最終也攸關公眾利益。在現代科學中，此程序在很大程度上委任給同儕審查，這是一個分散的程序，其中每個提交都會由該領域的數位獨立專家進行評估。同儕審查被廣泛使用，但它很困難、耗時且容易出錯。由於同儕審查中涉及的成品（手稿、評論、討論）在很大程度上是基於文字，自然語言處理在改善審查方面有很大的潛力。隨著大型語言模型（LLM）的出現讓 NLP 協助執行許多新任務，機器輔助同儕審查的討論正逐漸升溫。然而，究竟在哪些地方需要協助，NLP 能在哪些地方提供協助，又應該在哪些地方退居一旁？我們論文的目標是為未來在 NLP 中提供同儕審查協助的努力奠定基礎。我們將同儕審查視為一個通用的程序，並以 AI 會議中的審查為例。我們詳細說明從手稿提交到定稿修訂的每個步驟，並討論 NLP 協助相關的挑戰和機會，並以現有工作為例說明。接著我們探討 NLP 在同儕審查中面臨的重大挑戰，包括資料取得和授權、操作化和實驗，以及倫理問題。為了幫助鞏固社群的努力，我們建立了一個伴隨儲存庫，彙集與同儕審查相關的主要資料集。最後，我們向科學社群、NLP 和 AI 研究人員、政策制定者和資助機構發布一項詳細的行動呼籲，以協助推動 NLP 在同儕審查中的研究。我們希望我們的作品有助於設定在 AI 時代中，機器輔助科學品質控管研究的議程，並擴及到 NLP 社群以外。</paragraph>

##### **Scalable Property Valuation Models via Graph-based Deep Learning**
2405.06553v1 by Enrique Riveros, Carla Vairetti, Christian Wegmann, Santiago Truffa, Sebastián Maldonado

This paper aims to enrich the capabilities of existing deep learning-based
automated valuation models through an efficient graph representation of peer
dependencies, thus capturing intricate spatial relationships. In particular, we
develop two novel graph neural network models that effectively identify
sequences of neighboring houses with similar features, employing different
message passing algorithms. The first strategy consider standard spatial graph
convolutions, while the second one utilizes transformer graph convolutions.
This approach confers scalability to the modeling process. The experimental
evaluation is conducted using a proprietary dataset comprising approximately
200,000 houses located in Santiago, Chile. We show that employing tailored
graph neural networks significantly improves the accuracy of house price
prediction, especially when utilizing transformer convolutional message passing
layers.

摘要：本文旨在通过有效表示对等依赖关系的图形，从而捕捉复杂的 spatial 关系，来丰富基于深度学习的现有自动估值模型的能力。具体来说，我们开发了两个新颖的图神经网络模型，它们可以有效地识别具有相似特征的相邻房屋序列，并采用不同的消息传递算法。第一种策略考虑标准 spatial 图形卷积，而第二种策略利用 transformer 图形卷积。这种方法赋予了建模过程可扩展性。实验评估是使用一个专有数据集进行的，该数据集包含大约 200,000 所位于智利圣地亚哥的房屋。我们表明，采用定制图神经网络可以显著提高房价预测的准确性，尤其是在利用 transformer 卷积消息传递层时。

##### **Sampling the Swadesh List to Identify Similar Languages with Tree Spaces**
2405.06549v1 by Garett Ordway, Vic Patrangenaru

Communication plays a vital role in human interaction. Studying language is a
worthwhile task and more recently has become quantitative in nature with
developments of fields like quantitative comparative linguistics and
lexicostatistics. With respect to the authors own native languages, the
ancestry of the English language and the Latin alphabet are of the primary
interest. The Indo-European Tree traces many modern languages back to the
Proto-Indo-European root. Swadesh's cognates played a large role in developing
that historical perspective where some of the primary branches are Germanic,
Celtic, Italic, and Balto-Slavic. This paper will use data analysis on open
books where the simplest singular space is the 3-spider - a union T3 of three
rays with their endpoints glued at a point 0 - which can represent these tree
spaces for language clustering. These trees are built using a single linkage
method for clustering based on distances between samples from languages which
use the Latin Script. Taking three languages at a time, the barycenter is
determined. Some initial results have found both non-sticky and sticky sample
means. If the mean exhibits non-sticky properties, then one language may come
from a different ancestor than the other two. If the mean is considered sticky,
then the languages may share a common ancestor or all languages may have
different ancestry.

摘要：溝通在人類互動中扮演著至關重要的角色。研究語言是一項有價值的任務，最近隨著量化比較語言學和詞彙統計學等領域的發展而變得具有量化性質。關於作者自己的母語，英語和拉丁字母的祖先是最主要的興趣。印歐語系樹追溯許多現代語言回到原始印歐語根。斯瓦迪士的同源詞在發展歷史觀點方面發揮了重要作用，其中一些主要分支是日耳曼語、凱爾特語、義大利語和巴爾托-斯拉夫語。本文將使用開放書籍上的數據分析，其中最簡單的單一空間是 3 隻蜘蛛 - 三條射線的聯合 T3，其端點粘合在點 0 - 可以表示這些樹狀空間以進行語言聚類。這些樹是使用單一連結方法構建的，用於基於來自使用拉丁文字的語言的樣本之間的距離進行聚類。一次取三種語言，確定質心。一些初步結果發現了非黏性和黏性樣本平均值。如果平均值表現出非黏性屬性，則一種語言可能來自與其他兩種語言不同的祖先。如果平均值被認為是黏性的，則這些語言可能共享一個共同的祖先，或者所有語言可能具有不同的祖先。

##### **Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval**
2405.06545v1 by Mengjia Niu, Hao Li, Jie Shi, Hamed Haddadi, Fan Mo

Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, although their susceptibility to hallucination poses
significant challenges for their deployment in critical areas such as
healthcare. To address this issue, retrieving relevant facts from knowledge
graphs (KGs) is considered a promising method. Existing KG-augmented approaches
tend to be resource-intensive, requiring multiple rounds of retrieval and
verification for each factoid, which impedes their application in real-world
scenarios.
  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval
(Re-KGR) to augment the factuality of LLMs' responses with less retrieval
efforts in the medical field. Our approach leverages the attribution of
next-token predictive probability distributions across different tokens, and
various model layers to primarily identify tokens with a high potential for
hallucination, reducing verification rounds by refining knowledge triples
associated with these tokens. Moreover, we rectify inaccurate content using
retrieved knowledge in the post-processing stage, which improves the
truthfulness of generated responses. Experimental results on a medical dataset
demonstrate that our approach can enhance the factual capability of LLMs across
various foundational models as evidenced by the highest scores on truthfulness.

摘要：大型語言模型 (LLM) 在各個領域展現出卓越的能力，儘管它們容易出現幻覺，對其在醫療保健等關鍵領域的部署構成重大挑戰。為了解決這個問題，從知識圖譜 (KG) 中擷取相關事實被認為是一種有前途的方法。現有的 KG 增強方法往往需要大量資源，需要對每個事實進行多輪檢索和驗證，這阻礙了它們在實際場景中的應用。
在這項研究中，我們提出了自修正增強知識圖譜檢索 (Re-KGR)，以減少醫療領域中 LLM 回應的事實性檢索工作。我們的做法利用了不同標記之間下一個標記預測概率分佈的歸因，以及各種模型層，以主要識別具有高幻覺潛力的標記，通過修正與這些標記相關的知識三元組來減少驗證輪次。此外，我們在後處理階段使用檢索到的知識來修正不準確的內容，這提高了生成回應的真實性。在醫療數據集上的實驗結果表明，我們的做法可以增強 LLM 在各種基礎模型上的事實能力，最高真實性分數證明了這一點。

##### **ATSumm: Auxiliary information enhanced approach for abstractive disaster Tweet Summarization with sparse training data**
2405.06541v1 by Piyush Kumar Garg, Roshni Chakraborty, Sourav Kumar Dandapat

The abundance of situational information on Twitter poses a challenge for
users to manually discern vital and relevant information during disasters. A
concise and human-interpretable overview of this information helps
decision-makers in implementing efficient and quick disaster response. Existing
abstractive summarization approaches can be categorized as sentence-based or
key-phrase-based approaches. This paper focuses on sentence-based approach,
which is typically implemented as a dual-phase procedure in literature. The
initial phase, known as the extractive phase, involves identifying the most
relevant tweets. The subsequent phase, referred to as the abstractive phase,
entails generating a more human-interpretable summary. In this study, we adopt
the methodology from prior research for the extractive phase. For the
abstractive phase of summarization, most existing approaches employ deep
learning-based frameworks, which can either be pre-trained or require training
from scratch. However, to achieve the appropriate level of performance, it is
imperative to have substantial training data for both methods, which is not
readily available. This work presents an Abstractive Tweet Summarizer (ATSumm)
that effectively addresses the issue of data sparsity by using auxiliary
information. We introduced the Auxiliary Pointer Generator Network (AuxPGN)
model, which utilizes a unique attention mechanism called Key-phrase attention.
This attention mechanism incorporates auxiliary information in the form of
key-phrases and their corresponding importance scores from the input tweets. We
evaluate the proposed approach by comparing it with 10 state-of-the-art
approaches across 13 disaster datasets. The evaluation results indicate that
ATSumm achieves superior performance compared to state-of-the-art approaches,
with improvement of 4-80% in ROUGE-N F1-score.

摘要：<paragraph>Twitter 上豐富的情境資訊對使用者來說，在災害期間手動辨別重要且相關資訊是一項挑戰。簡潔且人類可解讀的資訊總覽有助於決策者執行有效且快速的災害應變。現有的抽象摘要方法可以分類為基於句子或基於關鍵字組的方法。本文專注於基於句子的方法，這通常在文獻中實作為雙階段程序。第一階段，稱為萃取階段，涉及識別最相關的推文。後續階段，稱為抽象階段，需要產生更具人類可解讀性的摘要。在本研究中，我們採用先前的研究方法進行萃取階段。對於摘要階段，大多數現有方法採用基於深度學習的架構，這些架構可以經過預先訓練或需要從頭開始訓練。然而，為了達到適當的效能水準，這兩種方法都需要大量的訓練資料，而這並不容易取得。這項工作提出了一個抽象推文摘要器 (ATSumm)，它透過使用輔助資訊有效地解決資料稀疏的問題。我們引入了輔助指標產生器網路 (AuxPGN) 模型，它利用一種稱為關鍵字組注意力的獨特注意力機制。這種注意力機制以關鍵字組的形式納入輔助資訊，以及來自輸入推文的對應重要性分數。我們透過將提出的方法與 13 個災害資料集中的 10 種最先進的方法進行比較來評估。評估結果表明，與最先進的方法相比，ATSumm 取得了卓越的效能，ROUGE-N F1 分數提高了 4-80%。</paragraph>

##### **Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts**
2405.06524v1 by Wenyu Huang, Guancheng Zhou, Mirella Lapata, Pavlos Vougiouklis, Sebastien Montella, Jeff Z. Pan

Although Large Language Models (LLMs) are effective in performing various NLP
tasks, they still struggle to handle tasks that require extensive, real-world
knowledge, especially when dealing with long-tail facts (facts related to
long-tail entities). This limitation highlights the need to supplement LLMs
with non-parametric knowledge. To address this issue, we analysed the effects
of different types of non-parametric knowledge, including textual passage and
knowledge graphs (KGs). Since LLMs have probably seen the majority of factual
question-answering datasets already, to facilitate our analysis, we proposed a
fully automatic pipeline for creating a benchmark that requires knowledge of
long-tail facts for answering the involved questions. Using this pipeline, we
introduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different
knowledge settings using the proposed benchmark. Our experiments show that LLMs
alone struggle with answering these questions, especially when the long-tail
level is high or rich knowledge is required. Nonetheless, the performance of
the same models improved significantly when they were prompted with
non-parametric knowledge. We observed that, in most cases, prompting LLMs with
KG triples surpasses passage-based prompting using a state-of-the-art
retriever. In addition, while prompting LLMs with both KG triples and documents
does not consistently improve knowledge coverage, it can dramatically reduce
hallucinations in the generated content.

摘要：儘管大型語言模型 (LLM) 在執行各種自然語言處理 (NLP) 任務時很有效，它們在處理需要廣泛的真實世界知識的任務時仍有困難，特別是在處理長尾事實（與長尾實體相關的事實）時。此限制突顯了補充 LLM 以獲得非參數知識的必要性。為了解決此問題，我們分析了不同類型非參數知識的影響，包括文字段落和知識圖 (KG)。由於 LLM 可能已經看過大多數事實性的問答資料集，為了方便我們的分析，我們提出了一個全自動管道，用於建立一個基準，需要了解長尾事實才能回答相關問題。使用此管道，我們引入了 LTGen 基準。我們使用所提出的基準，在不同的知識設定中評估最先進的 LLM。我們的實驗表明，LLM 單獨難以回答這些問題，特別是在長尾層級很高或需要豐富的知識時。儘管如此，當使用非參數知識提示這些模型時，它們的效能顯著提升。我們觀察到，在大多數情況下，使用最先進的檢索器提示 LLM 使用 KG 三元組會優於基於段落的提示。此外，儘管同時提示 LLM 使用 KG 三元組和文件並不會持續改善知識涵蓋範圍，但它可以大幅減少產生的內容中的幻覺。

##### **Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum Learning**
2405.06522v1 by Yili Wang

In recent years, heterogeneous graph neural networks (HGNNs) have achieved
excellent performance in handling heterogeneous information networks (HINs).
Curriculum learning is a machine learning strategy where training examples are
presented to a model in a structured order, starting with easy examples and
gradually increasing difficulty, aiming to improve learning efficiency and
generalization. To better exploit the rich information in HINs, previous
methods have started to explore the use of curriculum learning strategy to
train HGNNs. Specifically, these works utilize the absolute value of the loss
at each training epoch to evaluate the learning difficulty of each training
sample. However, the relative loss, rather than the absolute value of loss,
reveals the learning difficulty. Therefore, we propose a novel
loss-decrease-aware training schedule (LDTS). LDTS uses the trend of loss
decrease between each training epoch to better evaluating the difficulty of
training samples, thereby enhancing the curriculum learning of HGNNs for
downstream tasks. Additionally, we propose a sampling strategy to alleviate
training imbalance issues. Our method further demonstrate the efficacy of
curriculum learning in enhancing HGNNs capabilities. We call our method
Loss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN). The code is
public at https://github.com/wangyili00/LDHGNN.

摘要：近年来，异构图神经网络（HGNN）在处理异构信息网络（HIN）方面取得了优异的性能。课程学习是一种机器学习策略，其中训练示例以结构化顺序呈现给模型，从简单的示例开始，逐渐增加难度，旨在提高学习效率和泛化能力。为了更好地利用 HIN 中丰富的的信息，以前的方法已经开始探索使用课程学习策略来训练 HGNN。具体来说，这些工作利用每个训练 epoch 的损失的绝对值来评估每个训练样本的学习难度。然而，相对损失而不是损失的绝对值揭示了学习难度。因此，我们提出了一种新颖的损失减少感知训练计划（LDTS）。LDTS 使用每个训练 epoch 之间的损失减少趋势来更好地评估训练样本的难度，从而增强 HGNN 对下游任务的课程学习。此外，我们提出了一种采样策略来缓解训练不平衡问题。我们的方法进一步证明了课程学习在增强 HGNN 能力方面的有效性。我们称我们的方法为损失减少感知异构图神经网络（LDHGNN）。代码在 https://github.com/wangyili00/LDHGNN 公开。

##### **UniDM: A Unified Framework for Data Manipulation with Large Language Models**
2405.06510v1 by Yichen Qian, Yongyi He, Rong Zhu, Jintao Huang, Zhijian Ma, Haibin Wang, Yaohua Wang, Xiuyu Sun, Defu Lian, Bolin Ding, Jingren Zhou

Designing effective data manipulation methods is a long standing problem in
data lakes. Traditional methods, which rely on rules or machine learning
models, require extensive human efforts on training data collection and tuning
models. Recent methods apply Large Language Models (LLMs) to resolve multiple
data manipulation tasks. They exhibit bright benefits in terms of performance
but still require customized designs to fit each specific task. This is very
costly and can not catch up with the requirements of big data lake platforms.
In this paper, inspired by the cross-task generality of LLMs on NLP tasks, we
pave the first step to design an automatic and general solution to tackle with
data manipulation tasks. We propose UniDM, a unified framework which
establishes a new paradigm to process data manipulation tasks using LLMs. UniDM
formalizes a number of data manipulation tasks in a unified form and abstracts
three main general steps to solve each task. We develop an automatic context
retrieval to allow the LLMs to retrieve data from data lakes, potentially
containing evidence and factual information. For each step, we design effective
prompts to guide LLMs to produce high quality results. By our comprehensive
evaluation on a variety of benchmarks, our UniDM exhibits great generality and
state-of-the-art performance on a wide variety of data manipulation tasks.

摘要：<paragraph>在資料湖中設計有效的資料處理方法是一個長久以來的問題。傳統的方法依賴於規則或機器學習模型，需要大量的人力在訓練資料收集和調整模型上。最近的方法將大型語言模型 (LLM) 應用於解決多種資料處理任務。它們在效能方面表現出顯著的優點，但仍然需要客製化設計以符合每個特定任務。這非常昂貴，且無法趕上大資料湖平台的需求。在本文中，受 LLMs 在 NLP 任務中跨任務通用的啟發，我們踏出第一步，設計一個自動化且通用的解決方案來處理資料處理任務。我們提出 UniDM，一個統一的框架，它建立了一個新的典範，使用 LLMs 來處理資料處理任務。UniDM 以統一的形式形式化許多資料處理任務，並抽象出三個主要的步驟來解決每個任務。我們開發了一個自動化內容擷取，以允許 LLMs 從資料湖中擷取資料，其中可能包含證據和事實資訊。對於每個步驟，我們設計了有效的提示來引導 LLMs 產生高品質的結果。透過我們在各種基準上的全面評估，我們的 UniDM 在各種資料處理任務上表現出極大的通用性和最先進的效能。</paragraph>

##### **Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based Method for Evaluating Chess Strategies from Textbooks**
2405.06499v1 by Haifa Alrdahi, Riza Batista-Navarro

The chess domain is well-suited for creating an artificial intelligence (AI)
system that mimics real-world challenges, including decision-making. Throughout
the years, minimal attention has been paid to investigating insights derived
from unstructured chess data sources. In this study, we examine the complicated
relationships between multiple referenced moves in a chess-teaching textbook,
and propose a novel method designed to encapsulate chess knowledge derived from
move-action phrases. This study investigates the feasibility of using a
modified sentiment analysis method as a means for evaluating chess moves based
on text. Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents
an advancement in evaluating the sentiment associated with referenced chess
moves. By extracting insights from move-action phrases, our approach aims to
provide a more fine-grained and contextually aware `chess move'-based sentiment
classification. Through empirical experiments and analysis, we evaluate the
performance of our fine-tuned ABSA model, presenting results that confirm the
efficiency of our approach in advancing aspect-based sentiment classification
within the chess domain. This research contributes to the area of game-playing
by machines and shows the practical applicability of leveraging NLP techniques
to understand the context of strategic games.

摘要：西洋棋領域非常適合建立人工智慧 (AI) 系統，模擬包括決策制定在內的真實世界挑戰。多年來，對於從非結構化西洋棋數據來源中衍生的見解的研究一直很少。在這項研究中，我們探討了西洋棋教學教科書中多個參考著法之間的複雜關係，並提出了一種新穎的方法，旨在封裝從著法短語中衍生的西洋棋知識。本研究探討了使用改良的情感分析方法作為評估基於文字的西洋棋著法的可行性。我們提出的基於面向切面的情感分析 (ABSA) 方法代表了評估與參考西洋棋著法相關的情感的進步。通過從著法短語中提取見解，我們的目標是提供更細緻且具有情境感知的「西洋棋著法」為基礎的情感分類。透過實證實驗和分析，我們評估了我們微調後的 ABSA 模型的效能，呈現的結果證實了我們的方法在推進西洋棋領域中基於面向切面的情感分類的有效性。這項研究有助於機器遊戲領域，並展示了利用自然語言處理技術來理解策略遊戲背景的實際應用性。

##### **Solving Quantified Boolean Formulas with Few Existential Variables**
2405.06485v1 by Leif Eriksson, Victor Lagerkvist, George Osipov, Sebastian Ordyniak, Fahad Panolan, Mateusz Rychlicki

The quantified Boolean formula (QBF) problem is an important decision problem
generally viewed as the archetype for PSPACE-completeness. Many problems of
central interest in AI are in general not included in NP, e.g., planning, model
checking, and non-monotonic reasoning, and for such problems QBF has
successfully been used as a modelling tool. However, solvers for QBF are not as
advanced as state of the art SAT solvers, which has prevented QBF from becoming
a universal modelling language for PSPACE-complete problems. A theoretical
explanation is that QBF (as well as many other PSPACE-complete problems) lacks
natural parameters} guaranteeing fixed-parameter tractability (FPT).
  In this paper we tackle this problem and consider a simple but overlooked
parameter: the number of existentially quantified variables. This natural
parameter is virtually unexplored in the literature which one might find
surprising given the general scarcity of FPT algorithms for QBF. Via this
parameterization we then develop a novel FPT algorithm applicable to QBF
instances in conjunctive normal form (CNF) of bounded clause length. We
complement this by a W[1]-hardness result for QBF in CNF of unbounded clause
length as well as sharper lower bounds for the bounded arity case under the
(strong) exponential-time hypothesis.

摘要：量化布林公式 (QBF) 問題是一個重要的決策問題，通常被視為 PSPACE 完整性的原型。許多在 AI 中廣受關注的問題通常不包含在 NP 中，例如規劃、模型檢查和非單調推理，而對於此類問題，QBF 已成功地被用作建模工具。然而，QBF 的求解器不如最先進的 SAT 求解器先進，這使得 QBF 無法成為 PSPACE 完整問題的通用建模語言。一個理論解釋是 QBF（以及許多其他 PSPACE 完整問題）缺乏保證固定參數易處理性 (FPT) 的自然參數。在本文中，我們解決了這個問題，並考慮了一個簡單但被忽視的參數：存在量化變數的數量。這個自然參數在文獻中幾乎沒有被探討，這可能會讓人感到驚訝，因為 QBF 的 FPT 演算法普遍稀少。透過這個參數化，我們接著開發了一個新穎的 FPT 演算法，適用於聯集範式 (CNF) 中從句長度受限的 QBF 實例。我們透過一個 W[1]-hardness 結果對 CNF 中從句長度不受限的 QBF 進行補充，以及在（強）指數時間假設下對有界元數情況進行更嚴格的下界。

##### **LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing**
2405.06483v1 by Ana Ezquerro, David Vilares

This paper describes our participation in SemEval 2024 Task 3, which focused
on Multimodal Emotion Cause Analysis in Conversations. We developed an early
prototype for an end-to-end system that uses graph-based methods from
dependency parsing to identify causal emotion relations in multi-party
conversations. Our model comprises a neural transformer-based encoder for
contextualizing multimodal conversation data and a graph-based decoder for
generating the adjacency matrix scores of the causal graph. We ranked 7th out
of 15 valid and official submissions for Subtask 1, using textual inputs only.
We also discuss our participation in Subtask 2 during post-evaluation using
multi-modal inputs.

摘要：本文描述了我們參與 SemEval 2024 任務 3 的情況，該任務專注於對話中的多模態情緒原因分析。我們開發了一個端到端系統的早期原型，該系統使用基於依賴關係分析的圖形化方法來識別多方對話中的因果情緒關係。我們的模型包含一個用於將多模態對話數據語境化的神經轉換器編碼器和一個用於生成因果圖的鄰接矩陣分數的基於圖形的解碼器。在僅使用文本輸入的情況下，我們在 15 個有效且官方的提交中排名第 7。我們還在評估後討論了我們在子任務 2 中使用多模態輸入的參與情況。

##### **Attention is all they need: Cognitive science and the (techno)political economy of attention in humans and machines**
2405.06478v1 by Pablo González de la Torre, Marta Pérez-Verdugo, Xabier E. Barandiaran

This paper critically analyses the "attention economy" within the framework
of cognitive science and techno-political economics, as applied to both human
and machine interactions. We explore how current business models, particularly
in digital platform capitalism, harness user engagement by strategically
shaping attentional patterns. These platforms utilize advanced AI and massive
data analytics to enhance user engagement, creating a cycle of attention
capture and data extraction. We review contemporary (neuro)cognitive theories
of attention and platform engagement design techniques and criticize classical
cognitivist and behaviourist theories for their inadequacies in addressing the
potential harms of such engagement on user autonomy and wellbeing. 4E
approaches to cognitive science, instead, emphasizing the embodied, extended,
enactive, and ecological aspects of cognition, offer us an intrinsic normative
standpoint and a more integrated understanding of how attentional patterns are
actively constituted by adaptive digital environments. By examining the
precarious nature of habit formation in digital contexts, we reveal the
techno-economic underpinnings that threaten personal autonomy by disaggregating
habits away from the individual, into an AI managed collection of behavioural
patterns. Our current predicament suggests the necessity of a paradigm shift
towards an ecology of attention. This shift aims to foster environments that
respect and preserve human cognitive and social capacities, countering the
exploitative tendencies of cognitive capitalism.

摘要：本文在認知科學和技術政治經濟學的框架內批判性地分析「注意力經濟」，並將其應用於人類和機器互動。我們探討當前商業模式，特別是在數位平台資本主義中，如何透過策略性地塑造注意力模式來利用使用者參與。這些平台利用先進的人工智慧和大量資料分析來提升使用者參與，創造一個注意力捕捉和資料萃取的循環。我們回顧當代（神經）認知理論、注意力和平台參與設計技術，並批評古典認知主義和行為主義理論在處理此類參與對使用者自主性和福祉的潛在危害時存在不足。相反地，認知科學的 4E 方法強調認知的具身、延伸、能動和生態面向，為我們提供了一個內在的規範觀點，以及對注意力模式如何被適應性數位環境積極構成的更整合理解。透過檢視數位環境中習慣養成的脆弱本質，我們揭露了威脅個人自主的技術經濟基礎，將習慣從個人中分解成由人工智慧管理的行為模式集合。我們目前的困境表明，有必要朝向注意力生態系統轉變。這種轉變旨在培養尊重和維護人類認知和社會能力的環境，以對抗認知資本主義的剝削傾向。

##### **Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification**
2405.06468v1 by Yaoqin Ye, Junjie Zhang, Hongwei Shi

The task of medical image recognition is notably complicated by the presence
of varied and multiple pathological indications, presenting a unique challenge
in multi-label classification with unseen labels. This complexity underlines
the need for computer-aided diagnosis methods employing multi-label zero-shot
learning. Recent advancements in pre-trained vision-language models (VLMs) have
showcased notable zero-shot classification abilities on medical images.
However, these methods have limitations on leveraging extensive pre-trained
knowledge from broader image datasets, and often depend on manual prompt
construction by expert radiologists. By automating the process of prompt
tuning, prompt learning techniques have emerged as an efficient way to adapt
VLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in
performing class-specific prompts on unseen categories, limiting
generalizability in fine-grained scenarios. To overcome these constraints, we
introduce a novel prompt generation approach inspirited by text generation in
natural language processing (NLP). Our method, named Pseudo-Prompt Generating
(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring
a RNN-based decoder, PsPG autoregressively generates class-tailored embedding
vectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label
chest radiograph datasets affirm the superiority of our approach against
leading medical vision-language and multi-label prompt learning methods. The
source code is available at https://github.com/fallingnight/PsPG

摘要：醫療影像辨識的任務因各種病理徵兆的存在而變得複雜，在多標籤分類中呈現獨特挑戰，且標籤未曾見過。此複雜性強調了採用多標籤零次學習的電腦輔助診斷方法之必要性。預訓練視覺語言模型 (VLM) 的近期進展已在醫療影像中展示出顯著的零次分類能力。然而，這些方法在利用更廣泛影像資料集中的廣泛預訓練知識方面有其限制，且通常依賴放射科專家手動建立提示。透過自動化提示調整的過程，提示學習技術已成為將 VLM 適應至下游任務的有效方法。然而，現有的基於 CoOp 的策略無法對未見類別執行特定類別的提示，限制了細粒度場景中的泛化能力。為了克服這些限制，我們引入了一種新穎的提示生成方法，其靈感來自自然語言處理 (NLP) 中的文字生成。我們的方法稱為偽提示生成 (PsPG)，它利用多模式特徵的先驗知識。PsPG 採用基於 RNN 的解碼器，自迴歸地生成針對類別量身打造的嵌入向量，即偽提示。在各種多標籤胸部 X 光影像資料集上的比較評估證實了我們的方法優於領先的醫療視覺語言和多標籤提示學習方法。原始碼可在 https://github.com/fallingnight/PsPG 取得

##### **Are EEG-to-Text Models Working?**
2405.06459v1 by Hyejeong Jo, Yiqian Yang, Juhyeok Han, Yiqun Duan, Hui Xiong, Won Hee Lee

This work critically analyzes existing models for open-vocabulary EEG-to-Text
translation. We identify a crucial limitation: previous studies often employed
implicit teacher-forcing during evaluation, artificially inflating performance
metrics. Additionally, they lacked a critical benchmark - comparing model
performance on pure noise inputs. We propose a methodology to differentiate
between models that truly learn from EEG signals and those that simply memorize
training data. Our analysis reveals that model performance on noise data can be
comparable to that on EEG data. These findings highlight the need for stricter
evaluation practices in EEG-to-Text research, emphasizing transparent reporting
and rigorous benchmarking with noise inputs. This approach will lead to more
reliable assessments of model capabilities and pave the way for robust
EEG-to-Text communication systems.

摘要：這項研究批判性地分析了現有的開放式詞彙 EEG 轉文字翻譯模型。我們發現了一個關鍵限制：先前的研究在評估時經常使用隱含的教師強制，人為地提高了效能指標。此外，他們缺乏一個關鍵的基準 - 比較模型在純雜訊輸入上的效能。我們提出了一種方法來區分真正從 EEG 訊號中學習的模型和僅僅記憶訓練資料的模型。我們的分析顯示，模型在雜訊資料上的效能可以與在 EEG 資料上的效能相媲美。這些發現突顯了 EEG 轉文字研究中需要更嚴格的評估實務，強調透明的報告和嚴謹的雜訊輸入基準測試。這種方法將導致對模型功能更可靠的評估，並為強健的 EEG 轉文字通訊系統鋪路。

##### **Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation**
2405.06424v1 by JoonHo Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, JuYoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, Seungjai Min

Assessing response quality to instructions in language models is vital but
challenging due to the complexity of human language across different contexts.
This complexity often results in ambiguous or inconsistent interpretations,
making accurate assessment difficult. To address this issue, we propose a novel
Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty
estimation for the quality of paired responses based on Bayesian approximation.
Trained with preference datasets, our uncertainty-enabled proxy not only scores
rewards for responses but also evaluates their inherent uncertainty. Empirical
results demonstrate significant benefits of incorporating the proposed proxy
into language model training. Our method boosts the instruction following
capability of language models by refining data curation for training and
improving policy optimization objectives, thereby surpassing existing methods
by a large margin on benchmarks such as Vicuna and MT-bench. These findings
highlight that our proposed approach substantially advances language model
training and paves a new way of harnessing uncertainty within language models.

摘要：評估語言模型對指令的回應品質至關重要，但由於人類語言在不同語境中的複雜性，這是一個挑戰。這種複雜性通常會導致模稜兩可或不一致的解讀，使得準確評估變得困難。為了解決這個問題，我們提出了一個創新的不確定性感知獎勵模型 (URM)，它基於貝氏近似為配對回應的品質引入了穩健的不確定性估計。透過偏好資料集的訓練，我們的不確定性啟用代理不僅為回應評分，還評估它們固有的不確定性。實證結果證明了將建議的代理納入語言模型訓練的顯著好處。我們的模型透過改善訓練資料整理和優化策略最佳化目標，提升了語言模型遵循指令的能力，從而大幅超越了現有方法，例如 Vicuna 和 MT-bench 等基準。這些發現強調了我們提出的方法大幅推動了語言模型訓練，並為利用語言模型中的不確定性開闢了一條新途徑。

##### **Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting**
2405.06419v1 by Tianxiang Zhan, Yuanpeng He, Zhen Li, Yong Deng

In real-world scenarios, time series forecasting often demands timeliness,
making research on model backbones a perennially hot topic. To meet these
performance demands, we propose a novel backbone from the perspective of
information fusion. Introducing the Basic Probability Assignment (BPA) Module
and the Time Evidence Fusion Network (TEFN), based on evidence theory, allows
us to achieve superior performance. On the other hand, the perspective of
multi-source information fusion effectively improves the accuracy of
forecasting. Due to the fact that BPA is generated by fuzzy theory, TEFN also
has considerable interpretability. In real data experiments, the TEFN partially
achieved state-of-the-art, with low errors comparable to PatchTST, and
operating efficiency surpass performance models such as Dlinear. Meanwhile,
TEFN has high robustness and small error fluctuations in the random
hyperparameter selection. TEFN is not a model that achieves the ultimate in
single aspect, but a model that balances performance, accuracy, stability, and
interpretability.

摘要：在現實世界的場景中，時間序列預測通常需要及時性，這使得模型主幹的研究成為一個長盛不衰的熱門話題。為了滿足這些效能需求，我們從資訊融合的角度提出了一個新穎的主幹。引入基於證據理論的基本機率分配（BPA）模組和時間證據融合網路（TEFN），讓我們能夠實現卓越的效能。另一方面，多來源資訊融合的觀點有效地提高了預測的準確性。由於 BPA 是由模糊理論產生的，因此 TEFN 也具有相當的可解釋性。在真實資料實驗中，TEFN 部分達到最先進的技術，其誤差低於 PatchTST，且運作效率優於 Dlinear 等效能模型。同時，TEFN 在隨機超參數選擇中具有很高的穩健性和小的誤差波動。TEFN 不是一個在單一層面達到極致的模型，而是一個平衡效能、準確性、穩定性和可解釋性的模型。

##### **PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning**
2405.06418v1 by Jaejun Lee, Minsung Hwang, Joyce Jiyoung Whang

While a number of knowledge graph representation learning (KGRL) methods have
been proposed over the past decade, very few theoretical analyses have been
conducted on them. In this paper, we present the first PAC-Bayesian
generalization bounds for KGRL methods. To analyze a broad class of KGRL
models, we propose a generic framework named ReED (Relation-aware
Encoder-Decoder), which consists of a relation-aware message passing encoder
and a triplet classification decoder. Our ReED framework can express at least
15 different existing KGRL models, including not only graph neural
network-based models such as R-GCN and CompGCN but also shallow-architecture
models such as RotatE and ANALOGY. Our generalization bounds for the ReED
framework provide theoretical grounds for the commonly used tricks in KGRL,
e.g., parameter-sharing and weight normalization schemes, and guide desirable
design choices for practical KGRL methods. We empirically show that the
critical factors in our generalization bounds can explain actual generalization
errors on three real-world knowledge graphs.

摘要：在过去十年中，虽然提出了许多知识图表示学习 (KGRL) 方法，但对它们进行的理论分析却很少。在本文中，我们提出了 KGRL 方法的第一个 PAC 贝叶斯泛化界。为了分析广泛的 KGRL 模型，我们提出了一个名为 ReED（关系感知编码器解码器）的通用框架，它由一个关系感知消息传递编码器和一个三元组分类解码器组成。我们的 ReED 框架可以表达至少 15 个不同的现有 KGRL 模型，不仅包括基于图神经网络的模型，如 R-GCN 和 CompGCN，还包括浅层架构模型，如 RotatE 和 ANALOGY。我们对 ReED 框架的泛化界为 KGRL 中常用的技巧提供了理论依据，例如参数共享和权重归一化方案，并指导了实用 KGRL 方法的理想设计选择。我们通过实验证明，我们的泛化界中的关键因素可以解释三个真实世界知识图上的实际泛化误差。

##### **Can Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?**
2405.06414v1 by Hunter McNichols, Jaewook Lee, Stephen Fancsali, Steve Ritter, Andrew Lan

Intelligent Tutoring Systems (ITSs) often contain an automated feedback
component, which provides a predefined feedback message to students when they
detect a predefined error. To such a feedback component, we often resort to
template-based approaches. These approaches require significant effort from
human experts to detect a limited number of possible student errors and provide
corresponding feedback. This limitation is exemplified in open-ended math
questions, where there can be a large number of different incorrect errors. In
our work, we examine the capabilities of large language models (LLMs) to
generate feedback for open-ended math questions, similar to that of an
established ITS that uses a template-based approach. We fine-tune both
open-source and proprietary LLMs on real student responses and corresponding
ITS-provided feedback. We measure the quality of the generated feedback using
text similarity metrics. We find that open-source and proprietary models both
show promise in replicating the feedback they see during training, but do not
generalize well to previously unseen student errors. These results suggest that
despite being able to learn the formatting of feedback, LLMs are not able to
fully understand mathematical errors made by students.

摘要：智能教學系統 (ITS) 通常包含一個自動化回饋組件，當偵測到預先定義的錯誤時，會向學生提供預先定義的回饋訊息。對於這樣的回饋組件，我們通常會採用基於範本的方法。這些方法需要人類專家付出大量的精力來偵測有限數量的可能的學生錯誤，並提供相應的回饋。這種限制在開放式數學問題中得到體現，在開放式數學問題中可能存在大量不同的錯誤。在我們的研究中，我們檢視了大型語言模型 (LLM) 為開放式數學問題產生回饋的能力，類似於使用基於範本方法的既有 ITS。我們對開源和專有 LLM 進行微調，並使用真實學生的回應和相應的 ITS 提供的回饋。我們使用文字相似度量度量產生的回饋的品質。我們發現開源和專有模型在複製訓練期間看到的回饋方面都顯示出前景，但無法很好地推廣到以前未見過的學生錯誤。這些結果表明，儘管能夠學習回饋的格式，但 LLM 無法完全理解學生所犯的數學錯誤。

##### **Multi-level Personalized Federated Learning on Heterogeneous and Long-Tailed Data**
2405.06413v1 by Rongyu Zhang, Yun Chen, Chenrui Wu, Fangxin Wang, Bo Li

Federated learning (FL) offers a privacy-centric distributed learning
framework, enabling model training on individual clients and central
aggregation without necessitating data exchange. Nonetheless, FL
implementations often suffer from non-i.i.d. and long-tailed class
distributions across mobile applications, e.g., autonomous vehicles, which
leads models to overfitting as local training may converge to sub-optimal. In
our study, we explore the impact of data heterogeneity on model bias and
introduce an innovative personalized FL framework, Multi-level Personalized
Federated Learning (MuPFL), which leverages the hierarchical architecture of FL
to fully harness computational resources at various levels. This framework
integrates three pivotal modules: Biased Activation Value Dropout (BAVD) to
mitigate overfitting and accelerate training; Adaptive Cluster-based Model
Update (ACMU) to refine local models ensuring coherent global aggregation; and
Prior Knowledge-assisted Classifier Fine-tuning (PKCF) to bolster
classification and personalize models in accord with skewed local data with
shared knowledge. Extensive experiments on diverse real-world datasets for
image classification and semantic segmentation validate that MuPFL consistently
outperforms state-of-the-art baselines, even under extreme non-i.i.d. and
long-tail conditions, which enhances accuracy by as much as 7.39% and
accelerates training by up to 80% at most, marking significant advancements in
both efficiency and effectiveness.

摘要：聯邦學習 (FL) 提供以隱私為中心的分布式學習框架，可在個別用戶端上進行模型訓練和集中聚合，而無需進行資料交換。儘管如此，FL 實作通常會受到非獨立同分布 (non-i.i.d.) 和長尾類別分佈在行動應用程式中的影響，例如自動駕駛汽車，這會導致模型過度擬合，因為局部訓練可能會收斂到次佳值。在我們的研究中，我們探討了資料異質性對模型偏差的影響，並引入了創新的個人化 FL 框架，多層級個人化聯邦學習 (MuPFL)，它利用 FL 的階層式架構來充分利用各層級的運算資源。此框架整合了三個關鍵模組：有偏激活值中斷 (BAVD) 可減輕過度擬合並加速訓練；基於適應性叢集的模型更新 (ACMU) 可精煉局部模型，確保一致的全球聚合；以及先驗知識輔助分類器微調 (PKCF) 可加強分類並根據具有共享知識的偏斜局部資料個人化模型。在用於影像分類和語意分割的各種真實世界資料集上進行的廣泛實驗驗證，MuPFL 始終優於最先進的基準，即使在極端的非獨立同分布 (non-i.i.d.) 和長尾條件下，準確度最高可提高 7.39%，而訓練速度最多可加快 80%，這標誌著效率和效能方面都有顯著進步。

##### **Potential and Limitations of LLMs in Capturing Structured Semantics: A Case Study on SRL**
2405.06410v1 by Ning Cheng, Zhaohui Yan, Ziming Wang, Zhijie Li, Jiaming Yu, Zilong Zheng, Kewei Tu, Jinan Xu, Wenjuan Han

Large Language Models (LLMs) play a crucial role in capturing structured
semantics to enhance language understanding, improve interpretability, and
reduce bias. Nevertheless, an ongoing controversy exists over the extent to
which LLMs can grasp structured semantics. To assess this, we propose using
Semantic Role Labeling (SRL) as a fundamental task to explore LLMs' ability to
extract structured semantics. In our assessment, we employ the prompting
approach, which leads to the creation of our few-shot SRL parser, called
PromptSRL. PromptSRL enables LLMs to map natural languages to explicit semantic
structures, which provides an interpretable window into the properties of LLMs.
We find interesting potential: LLMs can indeed capture semantic structures, and
scaling-up doesn't always mirror potential. Additionally, limitations of LLMs
are observed in C-arguments, etc. Lastly, we are surprised to discover that
significant overlap in the errors is made by both LLMs and untrained humans,
accounting for almost 30% of all errors.

摘要：大型語言模型 (LLM) 在捕捉結構化語義以增強語言理解、提升可解釋性及減少偏誤方面扮演著至關重要的角色。儘管如此，LLM 在何種程度上能夠掌握結構化語義仍存在著持續的爭議。為了評估這一點，我們建議使用語義角色標註 (SRL) 作為一項基本任務，以探索 LLM 提取結構化語義的能力。在我們的評估中，我們採用提示方法，這導致了我們少樣本 SRL 解析器的建立，稱為 PromptSRL。PromptSRL 能讓 LLM 將自然語言對應到明確的語義結構，這提供了可解釋的視窗，讓我們得以一窺 LLM 的特性。我們發現了有趣的潛力：LLM 確實可以捕捉語義結構，而且擴充並不總是反映潛力。此外，我們在 C 參數等方面觀察到了 LLM 的限制。最後，我們驚訝地發現，LLM 和未受過訓練的人類在錯誤方面有顯著的重疊，約占所有錯誤的 30%。

##### **Program Synthesis using Inductive Logic Programming for the Abstraction and Reasoning Corpus**
2405.06399v1 by Filipe Marinho Rocha, Inês Dutra, Vítor Santos Costa

The Abstraction and Reasoning Corpus (ARC) is a general artificial
intelligence benchmark that is currently unsolvable by any Machine Learning
method, including Large Language Models (LLMs). It demands strong
generalization and reasoning capabilities which are known to be weaknesses of
Neural Network based systems. In this work, we propose a Program Synthesis
system that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to
solve ARC. We have manually defined a simple Domain Specific Language (DSL)
that corresponds to a small set of object-centric abstractions relevant to ARC.
This is the Background Knowledge used by ILP to create Logic Programs that
provide reasoning capabilities to our system. The full system is capable of
generalize to unseen tasks, since ILP can create Logic Program(s) from few
examples, in the case of ARC: pairs of Input-Output grids examples for each
task. These Logic Programs are able to generate Objects present in the Output
grid and the combination of these can form a complete program that transforms
an Input grid into an Output grid. We randomly chose some tasks from ARC that
dont require more than the small number of the Object primitives we implemented
and show that given only these, our system can solve tasks that require each,
such different reasoning.

摘要：抽象推理语料库 (ARC) 是一种通用人工智能基准，目前任何机器学习方法（包括大型语言模型 (LLM)）都无法解决。它要求很强的泛化和推理能力，而众所周知，基于神经网络的系统在这方面存在弱点。在这项工作中，我们提出了一种程序合成系统，它使用归纳逻辑规划 (ILP)（符号 AI 的一个分支）来解决 ARC。我们手动定义了一种简单的特定领域语言 (DSL)，它对应于与 ARC 相关的一组小的以对象为中心的概念。这是 ILP 用于创建逻辑程序的背景知识，为我们的系统提供推理能力。完整的系统能够泛化到看不见的任务，因为 ILP 可以根据少数示例创建逻辑程序，在 ARC 的情况下：每个任务的输入-输出网格示例对。这些逻辑程序能够生成输出网格中存在的对象，这些对象的组合可以形成一个完整的程序，将输入网格转换为输出网格。我们从 ARC 中随机选择了某些任务，这些任务不需要我们实现的少量对象基元，并表明仅给定这些，我们的系统就可以解决需要每种任务的不同推理的任务。

##### **Memory Mosaics**
2405.06394v1 by Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, Léon Bottou

Memory Mosaics are networks of associative memories working in concert to
achieve a prediction task of interest. Like transformers, memory mosaics
possess compositional capabilities and in-context learning capabilities. Unlike
transformers, memory mosaics achieve these capabilities in comparatively
transparent ways. We demonstrate these capabilities on toy examples and we also
show that memory mosaics perform as well or better than transformers on
medium-scale language modeling tasks.

摘要：記憶馬賽克是聯想記憶的網絡，協同工作以達成有興趣的預測任務。與Transformer一樣，記憶馬賽克具備組合能力和情境學習能力。與Transformer不同的是，記憶馬賽克能以相對透明的方式達成這些能力。我們在玩具範例中展示這些能力，我們也顯示記憶馬賽克在中規模語言建模任務中表現得和Transformer一樣好，甚至更好。

##### **LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play**
2405.06373v1 by Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, Shao-Hua Sun

Large language models (LLMs) have shown exceptional proficiency in natural
language processing but often fall short of generating creative and original
responses to open-ended questions. To enhance LLM creativity, our key insight
is to emulate the human process of inducing collective creativity through
engaging discussions with participants from diverse backgrounds and
perspectives. To this end, we propose LLM Discussion, a three-phase discussion
framework that facilitates vigorous and diverging idea exchanges and ensures
convergence to creative answers. Moreover, we adopt a role-playing technique by
assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate
the efficacy of the proposed framework with the Alternative Uses Test,
Similarities Test, Instances Test, and Scientific Creativity Test through both
LLM evaluation and human study. Our proposed framework outperforms single-LLM
approaches and existing multi-LLM frameworks across various creativity metrics.

摘要：大型語言模型 (LLM) 在自然語言處理方面表現出非凡的能力，但通常無法對開放式問題產生有創意和原創的回應。為了增強 LLM 的創造力，我們的關鍵見解是模擬人類透過與來自不同背景和觀點的參與者進行引人入勝的討論來激發集體創造力的過程。為此，我們提出了 LLM 討論，一個三階段討論架構，促進激烈的思想交流和分歧，並確保收斂到有創意的答案。此外，我們採用角色扮演技術，賦予 LLM 不同的角色，以對抗 LLM 的同質性。我們透過 LLM 評估和人類研究，使用替代用途測試、相似性測試、實例測試和科學創造力測試來評估所提出架構的效能。我們提出的架構在各種創造力指標上優於單一 LLM 方法和現有的多 LLM 架構。

##### **Intelligent Duty Cycling Management and Wake-up for Energy Harvesting IoT Networks with Correlated Activity**
2405.06372v1 by David E. Ruíz-Guirola, Onel L. A. López, Samuel Montejo-Sánchez, Israel Leyva Mayorga, Zhu Han, Petar Popovski

This paper presents an approach for energy-neutral Internet of Things (IoT)
scenarios where the IoT devices (IoTDs) rely entirely on their energy
harvesting capabilities to sustain operation. We use a Markov chain to
represent the operation and transmission states of the IoTDs, a modulated
Poisson process to model their energy harvesting process, and a discrete-time
Markov chain to model their battery state. The aim is to efficiently manage the
duty cycling of the IoTDs, so as to prolong their battery life and reduce
instances of low-energy availability. We propose a duty-cycling management
based on K- nearest neighbors, aiming to strike a trade-off between energy
efficiency and detection accuracy. This is done by incorporating spatial and
temporal correlations among IoTDs' activity, as well as their energy harvesting
capabilities. We also allow the base station to wake up specific IoTDs if more
information about an event is needed upon initial detection. Our proposed
scheme shows significant improvements in energy savings and performance, with
up to 11 times lower misdetection probability and 50\% lower energy consumption
for high-density scenarios compared to a random duty cycling benchmark.

摘要：本文提出了一種能源中立物聯網 (IoT) 場景的方法，其中物聯網設備 (IoTD) 完全依賴其能量收集能力來維持運作。我們使用馬可夫鏈來表示 IoTD 的運作和傳輸狀態，調製泊松過程來模擬它們的能量收集過程，以及離散時間馬可夫鏈來模擬它們的電池狀態。目標是有效管理 IoTD 的工作週期，以延長其電池壽命並減少低能量可用性的情況。我們提出了一種基於 K 最近鄰居的占空比管理，旨在平衡能源效率和檢測準確性。這是通過整合 IoTD 活動之間的空間和時間關聯性以及它們的能量收集能力來完成的。我們還允許基站喚醒特定的 IoTD，如果在初始檢測後需要更多有關事件的信息。我們提出的方案在節能和性能方面顯示出顯著的改進，與隨機占空比基準相比，高密度場景的誤檢測概率降低了 11 倍，能耗降低了 50%。

##### **Projection by Convolution: Optimal Sample Complexity for Reinforcement Learning in Continuous-Space MDPs**
2405.06363v1 by Davide Maran, Alberto Maria Metelli, Matteo Papini, Marcello Restelli

We consider the problem of learning an $\varepsilon$-optimal policy in a
general class of continuous-space Markov decision processes (MDPs) having
smooth Bellman operators. Given access to a generative model, we achieve
rate-optimal sample complexity by performing a simple, \emph{perturbed} version
of least-squares value iteration with orthogonal trigonometric polynomials as
features. Key to our solution is a novel projection technique based on ideas
from harmonic analysis. Our~$\widetilde{\mathcal{O}}(\epsilon^{-2-d/(\nu+1)})$
sample complexity, where $d$ is the dimension of the state-action space and
$\nu$ the order of smoothness, recovers the state-of-the-art result of
discretization approaches for the special case of Lipschitz MDPs $(\nu=0)$. At
the same time, for $\nu\to\infty$, it recovers and greatly generalizes the
$\mathcal{O}(\epsilon^{-2})$ rate of low-rank MDPs, which are more amenable to
regression approaches. In this sense, our result bridges the gap between two
popular but conflicting perspectives on continuous-space MDPs.

摘要：我們考慮在具有平滑 Bellman 算子的連續空間馬可夫決策過程 (MDP) 的一般類別中學習 $\varepsilon$-最優策略的問題。透過取得生成模型，我們透過執行一個簡單的、正交三角多項式作為特徵的「攝動」版本最小平方值迭代來達成速率最優樣本複雜度。我們解決方案的關鍵在於一種基於調和分析概念的新穎投影技術。我們的 $\widetilde{\mathcal{O}}(\epsilon^{-2-d/(\nu+1)})$ 樣本複雜度，其中 $d$ 是狀態動作空間的維度，而 $\nu$ 是平滑度順序，會在 Lipschitz MDP（$\nu=0$）的特例中恢復離散化方法的最新結果。同時，對於 $\nu\to\infty$，它會恢復並大幅概括低秩 MDP 的 $\mathcal{O}(\epsilon^{-2})$ 速率，而低秩 MDP 更適用於回歸方法。在這個意義上，我們的結果彌合了兩種流行但相互矛盾的連續空間 MDP 觀點之間的差距。

##### **KeepOriginalAugment: Single Image-based Better Information-Preserving Data Augmentation Approach**
2405.06354v1 by Teerath Kumar, Alessandra Mileo, Malika Bendechache

Advanced image data augmentation techniques play a pivotal role in enhancing
the training of models for diverse computer vision tasks. Notably, SalfMix and
KeepAugment have emerged as popular strategies, showcasing their efficacy in
boosting model performance. However, SalfMix reliance on duplicating salient
features poses a risk of overfitting, potentially compromising the model's
generalization capabilities. Conversely, KeepAugment, which selectively
preserves salient regions and augments non-salient ones, introduces a domain
shift that hinders the exchange of crucial contextual information, impeding
overall model understanding. In response to these challenges, we introduce
KeepOriginalAugment, a novel data augmentation approach. This method
intelligently incorporates the most salient region within the non-salient area,
allowing augmentation to be applied to either region. Striking a balance
between data diversity and information preservation, KeepOriginalAugment
enables models to leverage both diverse salient and non-salient regions,
leading to enhanced performance. We explore three strategies for determining
the placement of the salient region minimum, maximum, or random and investigate
swapping perspective strategies to decide which part (salient or non-salient)
undergoes augmentation. Our experimental evaluations, conducted on
classification datasets such as CIFAR-10, CIFAR-100, and TinyImageNet,
demonstrate the superior performance of KeepOriginalAugment compared to
existing state-of-the-art techniques.

摘要：先進的影像資料擴充技術在增強模型訓練以執行各種電腦視覺任務方面扮演著關鍵角色。值得注意的是，SalfMix 和 KeepAugment 已成為廣受歡迎的策略，展示了它們在提升模型效能方面的效力。然而，SalfMix 依賴於複製顯著特徵，這會造成過度擬合的風險，潛在會損害模型的概化能力。相反地，KeepAugment 選擇性地保留顯著區域並擴充非顯著區域，會造成領域轉移，這會阻礙關鍵脈絡資訊的交換，並妨礙整體模型理解。為了應對這些挑戰，我們引進 KeepOriginalAugment，這是一種新穎的資料擴充方法。此方法在非顯著區域內智慧地整合最顯著的區域，允許將擴充套用至任一區域。KeepOriginalAugment 在資料多樣性和資訊保留之間取得平衡，使模型能同時利用多樣化的顯著和非顯著區域，進而提升效能。我們探討了三種策略，以決定顯著區域最小值、最大值或隨機放置的位置，並研究交換觀點策略，以決定哪個部分（顯著或非顯著）會進行擴充。我們在 CIFAR-10、CIFAR-100 和 TinyImageNet 等分類資料集上進行的實驗評估，證明 KeepOriginalAugment 的效能優於現有的最先進技術。

##### **Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology**
2405.06346v1 by Rishav Hada, Safiya Husain, Varun Gumma, Harshita Diddee, Aditya Yadavalli, Agrima Seth, Nidhi Kulkarni, Ujwal Gadiraju, Aditya Vashistha, Vivek Seshadri, Kalika Bali

Existing research in measuring and mitigating gender bias predominantly
centers on English, overlooking the intricate challenges posed by non-English
languages and the Global South. This paper presents the first comprehensive
study delving into the nuanced landscape of gender bias in Hindi, the third
most spoken language globally. Our study employs diverse mining techniques,
computational models, field studies and sheds light on the limitations of
current methodologies. Given the challenges faced with mining gender biased
statements in Hindi using existing methods, we conducted field studies to
bootstrap the collection of such sentences. Through field studies involving
rural and low-income community women, we uncover diverse perceptions of gender
bias, underscoring the necessity for context-specific approaches. This paper
advocates for a community-centric research design, amplifying voices often
marginalized in previous studies. Our findings not only contribute to the
understanding of gender bias in Hindi but also establish a foundation for
further exploration of Indic languages. By exploring the intricacies of this
understudied context, we call for thoughtful engagement with gender bias,
promoting inclusivity and equity in linguistic and cultural contexts beyond the
Global North.

摘要：現有的性別偏見測量和緩解研究主要集中在英語，忽視了非英語語言和全球南方所面臨的複雜挑戰。本文提出了第一個全面研究，探討印地語中性別偏見的細微差別，印地語是全球第三大語言。我們的研究採用了多種挖掘技術、計算模型、實地研究，並闡明了當前方法的局限性。由於使用現有方法挖掘印地語中帶有性別偏見的陳述時面臨挑戰，我們進行了實地研究，以引導此類句子的收集。通過涉及農村和低收入社區婦女的實地研究，我們發現了對性別偏見的不同看法，強調了對特定情境方法的必要性。本文提倡以社區為中心的研究所設計，擴大在以前的研究中經常被邊緣化的聲音。我們的研究結果不僅有助於理解印地語中的性別偏見，而且還為進一步探索印度語言奠定了基礎。通過探索這個研究不足的背景的複雜性，我們呼籲對性別偏見進行深思熟慮的參與，在全球北方以外的語言和文化背景中促進包容性和公平性。

##### **LMD3: Language Model Data Density Dependence**
2405.06331v1 by John Kirchenbauer, Garrett Honke, Gowthami Somepalli, Jonas Geiping, Daphne Ippolito, Katherine Lee, Tom Goldstein, David Andre

We develop a methodology for analyzing language model task performance at the
individual example level based on training data density estimation. Experiments
with paraphrasing as a controlled intervention on finetuning data demonstrate
that increasing the support in the training distribution for specific test
queries results in a measurable increase in density, which is also a
significant predictor of the performance increase caused by the intervention.
Experiments with pretraining data demonstrate that we can explain a significant
fraction of the variance in model perplexity via density measurements. We
conclude that our framework can provide statistical evidence of the dependence
of a target model's predictions on subsets of its training data, and can more
generally be used to characterize the support (or lack thereof) in the training
data for a given test task.

摘要：我們開發了一種基於訓練資料密度估計來分析語言模型任務表現的層級方法。以改寫作為對微調資料進行控制干預的實驗，證明增加訓練分佈中特定測試查詢的支持會導致密度可測量地增加，這也是由干預造成的效能增加的重要預測指標。預訓練資料的實驗證明，我們能夠透過密度測量來解釋模型困惑度中顯著的部分差異。我們得出結論，我們的架構可以提供統計證據，證明目標模型預測依賴於其訓練資料的子集，並且更普遍地用於描述給定測試任務在訓練資料中的支援（或缺乏支援）。

##### **ChatGPTest: opportunities and cautionary tales of utilizing AI for questionnaire pretesting**
2405.06329v1 by Francisco Olivos, Minhui Liu

The rapid advancements in generative artificial intelligence have opened up
new avenues for enhancing various aspects of research, including the design and
evaluation of survey questionnaires. However, the recent pioneering
applications have not considered questionnaire pretesting. This article
explores the use of GPT models as a useful tool for pretesting survey
questionnaires, particularly in the early stages of survey design. Illustrated
with two applications, the article suggests incorporating GPT feedback as an
additional stage before human pretesting, potentially reducing successive
iterations. The article also emphasizes the indispensable role of researchers'
judgment in interpreting and implementing AI-generated feedback.

摘要：生成式人工智能的快速发展为增强研究的各个方面开辟了新的途径，包括调查问卷的设计和评估。然而，最近的开创性应用并未考虑问卷预测试。本文探讨了将 GPT 模型用作预测试调查问卷的实用工具，特别是在调查设计的早期阶段。本文通过两个应用来说明，建议将 GPT 反馈作为人工预测试前的附加阶段，从而可能减少连续的迭代。本文还强调了研究人员在解释和实施人工智能生成的反馈中的不可或缺的作用。

##### **Correlation Dimension of Natural Language in a Statistical Manifold**
2405.06321v1 by Xin Du, Kumiko Tanaka-Ishii

The correlation dimension of natural language is measured by applying the
Grassberger-Procaccia algorithm to high-dimensional sequences produced by a
large-scale language model. This method, previously studied only in a Euclidean
space, is reformulated in a statistical manifold via the Fisher-Rao distance.
Language exhibits a multifractal, with global self-similarity and a universal
dimension around 6.5, which is smaller than those of simple discrete random
sequences and larger than that of a Barab\'asi-Albert process. Long memory is
the key to producing self-similarity. Our method is applicable to any
probabilistic model of real-world discrete sequences, and we show an
application to music data.

摘要：通過將 Grassberger-Procaccia 演算法應用於大型語言模型產生的高維序列，來測量自然語言的關聯維度。此方法先前僅在歐幾里得空間中研究，透過 Fisher-Rao 距離在統計流形中重新制定。語言表現出多重分形，具有整體自相似性和約 6.5 的通用維度，這小於簡單離散隨機序列的維度，而大於 Barabási-Albert 程序的維度。長時記憶是產生自相似性的關鍵。我們的模型適用於任何實際世界離散序列的機率模型，我們展示了音樂資料的應用。

##### **Decoding Emotions in Abstract Art: Cognitive Plausibility of CLIP in Recognizing Color-Emotion Associations**
2405.06319v1 by Hanna-Sophia Widhoelzl, Ece Takmaz

This study investigates the cognitive plausibility of a pretrained multimodal
model, CLIP, in recognizing emotions evoked by abstract visual art. We employ a
dataset comprising images with associated emotion labels and textual rationales
of these labels provided by human annotators. We perform linguistic analyses of
rationales, zero-shot emotion classification of images and rationales, apply
similarity-based prediction of emotion, and investigate color-emotion
associations. The relatively low, yet above baseline, accuracy in recognizing
emotion for abstract images and rationales suggests that CLIP decodes emotional
complexities in a manner not well aligned with human cognitive processes.
Furthermore, we explore color-emotion interactions in images and rationales.
Expected color-emotion associations, such as red relating to anger, are
identified in images and texts annotated with emotion labels by both humans and
CLIP, with the latter showing even stronger interactions. Our results highlight
the disparity between human processing and machine processing when connecting
image features and emotions.

摘要：本研究探討預訓練多模態模型 CLIP 在辨識抽象視覺藝術所引發的情緒時，認知上的可信度。我們採用包含圖像、相關情緒標籤以及由人類標註者提供的這些標籤的文字依據之資料集。我們執行依據的語言分析、圖像和依據的零次情緒分類，套用基於相似性的情緒預測，並探討色彩與情緒的關聯性。辨識抽象圖像和依據時，相對較低但高於基準的準確度，顯示 CLIP 以一種與人類認知過程不太一致的方式解碼情緒的複雜性。此外，我們探討圖像和依據中的色彩與情緒互動。預期的色彩與情緒關聯性，例如紅色與憤怒相關，在由人類和 CLIP 標註為情緒標籤的圖像和文字中被辨識出來，後者顯示出更強的互動性。我們的結果突顯了在連結圖像特徵和情緒時，人類處理和機器處理之間的差異。

##### **A NLP Approach to "Review Bombing" in Metacritic PC Videogames User Ratings**
2405.06306v1 by Javier Coronado-Blázquez

Many videogames suffer "review bombing" -a large volume of unusually low
scores that in many cases do not reflect the real quality of the product- when
rated by users. By taking Metacritic's 50,000+ user score aggregations for PC
games in English language, we use a Natural Language Processing (NLP) approach
to try to understand the main words and concepts appearing in such cases,
reaching a 0.88 accuracy on a validation set when distinguishing between just
bad ratings and review bombings. By uncovering and analyzing the patterns
driving this phenomenon, these results could be used to further mitigate these
situations.

摘要：許多電玩遊戲會遭受「評論轟炸」-大量異常低的分數，在許多情況下無法反映產品的真實品質-由使用者評分時。透過使用 Metacritic 的 50,000 多個 PC 遊戲英文使用者評分彙總，我們使用自然語言處理 (NLP) 方法，嘗試了解在這種情況下出現的主要字詞和概念，在區分不良評分和評論轟炸時，驗證集的準確度達到 0.88。透過揭露和分析驅動此現象的模式，這些結果可用於進一步緩解這些情況。

##### **Cross-domain Learning Framework for Tracking Users in RIS-aided Multi-band ISAC Systems with Sparse Labeled Data**
2405.06299v1 by Jingzhi Hu, Dusit Niyato, Jun Luo

Integrated sensing and communications (ISAC) is pivotal for 6G communications
and is boosted by the rapid development of reconfigurable intelligent surfaces
(RISs). Using the channel state information (CSI) across multiple frequency
bands, RIS-aided multi-band ISAC systems can potentially track users' positions
with high precision. Though tracking with CSI is desirable as no communication
overheads are incurred, it faces challenges due to the multi-modalities of CSI
samples, irregular and asynchronous data traffic, and sparse labeled data for
learning the tracking function. This paper proposes the X2Track framework,
where we model the tracking function by a hierarchical architecture, jointly
utilizing multi-modal CSI indicators across multiple bands, and optimize it in
a cross-domain manner, tackling the sparsity of labeled data for the target
deployment environment (namely, target domain) by adapting the knowledge
learned from another environment (namely, source domain). Under X2Track, we
design an efficient deep learning algorithm to minimize tracking errors, based
on transformer neural networks and adversarial learning techniques. Simulation
results verify that X2Track achieves decimeter-level axial tracking errors even
under scarce UL data traffic and strong interference conditions and can adapt
to diverse deployment environments with fewer than 5% training data, or
equivalently, 5 minutes of UE tracks, being labeled.

摘要：整合感測與通訊 (ISAC) 是 6G 通訊的關鍵，並受到可重構智慧表面 (RIS) 的快速發展所推動。透過使用多個頻率頻段的頻道狀態資訊 (CSI)，RIS 輔助的多頻段 ISAC 系統有可能以高精確度追蹤使用者的位置。儘管透過 CSI 追蹤很理想，因為不會產生通訊負擔，但由於 CSI 樣本的多模態、不規則且非同步的資料傳輸，以及用於學習追蹤功能的稀疏標籤資料，因此面臨挑戰。本文提出 X2Track 架構，其中我們透過階層式架構為追蹤功能建模，共同利用多個頻段中的多模式 CSI 指標，並以跨領域的方式進行最佳化，透過調整從另一個環境（即來源網域）學到的知識來解決目標部署環境（即目標網域）中標籤資料的稀疏性。在 X2Track 下，我們設計一種有效率的深度學習演算法，以最小化追蹤誤差，並基於Transformer神經網路和對抗式學習技術。模擬結果驗證 X2Track 即使在稀少的 UL 資料傳輸和強干擾條件下，也能達成分米等級的軸向追蹤誤差，並能以少於 5% 的訓練資料，或等效於 5 分鐘的 UE 追蹤被標籤，適應多樣的部署環境。

##### **Aspect-oriented Consumer Health Answer Summarization**
2405.06295v1 by Rochana Chaturvedi, Abari Bhattacharya, Shweta Yadav

Community Question-Answering (CQA) forums have revolutionized how people seek
information, especially those related to their healthcare needs, placing their
trust in the collective wisdom of the public. However, there can be several
answers in response to a single query, which makes it hard to grasp the key
information related to the specific health concern. Typically, CQA forums
feature a single top-voted answer as a representative summary for each query.
However, a single answer overlooks the alternative solutions and other
information frequently offered in other responses. Our research focuses on
aspect-based summarization of health answers to address this limitation.
Summarization of responses under different aspects such as suggestions,
information, personal experiences, and questions can enhance the usability of
the platforms. We formalize a multi-stage annotation guideline and contribute a
unique dataset comprising aspect-based human-written health answer summaries.
We build an automated multi-faceted answer summarization pipeline with this
dataset based on task-specific fine-tuning of several state-of-the-art models.
The pipeline leverages question similarity to retrieve relevant answer
sentences, subsequently classifying them into the appropriate aspect type.
Following this, we employ several recent abstractive summarization models to
generate aspect-based summaries. Finally, we present a comprehensive human
analysis and find that our summaries rank high in capturing relevant content
and a wide range of solutions.

摘要：社群問答 (CQA) 論壇徹底改變了人們尋求資訊的方式，特別是與其醫療保健需求相關的資訊，將其信任寄託在公眾的集體智慧上。然而，針對單一查詢可能會有好幾個答案，這使得難以掌握與特定健康問題相關的關鍵資訊。通常，CQA 論壇會針對每個查詢，將單一最高票答案列為代表性摘要。然而，單一答案會忽略其他回應中經常提供的其他解決方案與資訊。我們的研究著重於基於面向的健康答案摘要，以解決此項限制。針對不同面向的回應摘要，例如建議、資訊、個人經驗和問題，可以提升平台的可使用性。我們形式化一個多階段註解指南，並貢獻一個獨特的資料集，包含基於面向的人工撰寫健康答案摘要。我們根據多個最先進模型的特定任務微調，使用此資料集建立一個自動化多面向答案摘要管道。此管道利用問題相似性來擷取相關答案句子，隨後將其分類至適當的面向類型。在此之後，我們採用幾個最近的抽象摘要模型來產生基於面向的摘要。最後，我們提出一個全面的人工分析，並發現我們的摘要在擷取相關內容和廣泛的解決方案方面排名很高。

##### **Pruning as a Domain-specific LLM Extractor**
2405.06275v1 by Nan Zhang, Yanchi Liu, Xujiang Zhao, Wei Cheng, Runxue Bao, Rui Zhang, Prasenjit Mitra, Haifeng Chen

Large Language Models (LLMs) have exhibited remarkable proficiency across a
wide array of NLP tasks. However, the escalation in model size also engenders
substantial deployment costs. While few efforts have explored model pruning
techniques to reduce the size of LLMs, they mainly center on general or
task-specific weights. This leads to suboptimal performance due to lacking
specificity on the target domain or generality on different tasks when applied
to domain-specific challenges. This work introduces an innovative unstructured
dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It
extracts a compressed, domain-specific, and task-agnostic LLM by identifying
LLM weights that are pivotal for general capabilities, like linguistic
capability and multi-task solving, and domain-specific knowledge. More
specifically, we first assess general weight importance by quantifying the
error incurred upon their removal with the help of an open-domain calibration
dataset. Then, we utilize this general weight importance to refine the training
loss, so that it preserves generality when fitting into a specific domain.
Moreover, by efficiently approximating weight importance with the refined
training loss on a domain-specific calibration dataset, we obtain a pruned
model emphasizing generality and specificity. Our comprehensive experiments
across various tasks in healthcare and legal domains show the effectiveness of
D-Pruner in domain-specific compression. Our code is available at
https://github.com/psunlpgroup/D-Pruner.

摘要：大型語言模型 (LLM) 在廣泛的自然語言處理任務中展現出卓越的能力。然而，模型規模的擴大也帶來顯著的部署成本。儘管很少有研究探索模型剪枝技術以縮小 LLM 的規模，但它們主要集中於一般或特定於任務的權重。這會導致次佳效能，因為當應用於特定於領域的挑戰時，缺乏針對目標領域的特殊性或不同任務的一般性。這項工作介紹了一個創新的非結構化雙重剪枝方法 D-Pruner，用於 LLM 的特定於領域的壓縮。它通過識別對一般能力（例如語言能力和多任務解決）和特定於領域的知識至關重要的 LLM 權重，來提取一個壓縮的、特定於領域的和與任務無關的 LLM。更具體地說，我們首先通過使用開放領域校準資料集來量化移除它們時產生的錯誤，來評估一般權重重要性。然後，我們利用這個一般權重重要性來改善訓練損失，以便在適應特定領域時保持一般性。此外，通過在特定於領域的校準資料集上使用改善的訓練損失來有效近似權重重要性，我們獲得了一個強調一般性和特殊性的剪枝模型。我們在醫療保健和法律領域的各種任務中進行的全面實驗顯示了 D-Pruner 在特定於領域的壓縮中的有效性。我們的程式碼可在 https://github.com/psunlpgroup/D-Pruner 取得。

##### **XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**
2405.06270v1 by Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio

The integration of Large Language Models (LLMs) into healthcare diagnostics
offers a promising avenue for clinical decision-making. This study outlines the
development of a novel method for zero-shot/few-shot in-context learning (ICL)
by integrating medical domain knowledge using a multi-layered structured
prompt. We also explore the efficacy of two communication styles between the
user and LLMs: the Numerical Conversational (NC) style, which processes data
incrementally, and the Natural Language Single-Turn (NL-ST) style, which
employs long narrative prompts. Our study systematically evaluates the
diagnostic accuracy and risk factors, including gender bias and false negative
rates, using a dataset of 920 patient records in various few-shot scenarios.
Results indicate that traditional clinical machine learning (ML) models
generally outperform LLMs in zero-shot and few-shot settings. However, the
performance gap narrows significantly when employing few-shot examples
alongside effective explainable AI (XAI) methods as sources of domain
knowledge. Moreover, with sufficient time and an increased number of examples,
the conversational style (NC) nearly matches the performance of ML models. Most
notably, LLMs demonstrate comparable or superior cost-sensitive accuracy
relative to ML models. This research confirms that, with appropriate domain
knowledge and tailored communication strategies, LLMs can significantly enhance
diagnostic processes. The findings highlight the importance of optimizing the
number of training examples and communication styles to improve accuracy and
reduce biases in LLM applications.

摘要：大型語言模型 (LLM) 整合到醫療診斷中，為臨床決策制定提供了有前景的方法。本研究概述了一種通過使用多層結構提示整合醫學領域知識來進行零次學習/少次學習情境學習 (ICL) 的新方法的開發。我們還探討了使用者與 LLM 之間兩種溝通方式的效能：以增量方式處理資料的數值對話 (NC) 方式，以及採用長篇敘事提示的自然語言單輪 (NL-ST) 方式。我們的研究系統性地評估了診斷準確性和風險因素，包括性別偏見和假陰性率，並使用 920 個患者記錄的資料集在各種少次學習情境中進行評估。結果表明，傳統的臨床機器學習 (ML) 模型通常在零次學習和少次學習設定中優於 LLM。然而，當採用少次學習範例以及有效的可解釋 AI (XAI) 方法作為領域知識來源時，效能差距會顯著縮小。此外，在有充足的時間和增加範例數量的情況下，對話方式 (NC) 幾乎可以與 ML 模型的效能相匹配。最值得注意的是，LLM 相對於 ML 模型展現出可比或優越的成本敏感度準確性。這項研究證實，透過適當的領域知識和量身打造的溝通策略，LLM 可以顯著增強診斷程序。這些發現強調了最佳化訓練範例數量和溝通方式以提高準確性並減少 LLM 應用中偏見的重要性。

##### **A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting**
2405.06266v1 by Jianli Xiao, Baichao Long

Traffic flow forecasting is a crucial task in transportation management and
planning. The main challenges for traffic flow forecasting are that (1) as the
length of prediction time increases, the accuracy of prediction will decrease;
(2) the predicted results greatly rely on the extraction of temporal and
spatial dependencies from the road networks. To overcome the challenges
mentioned above, we propose a multi-channel spatial-temporal transformer model
for traffic flow forecasting, which improves the accuracy of the prediction by
fusing results from different channels of traffic data. Our approach leverages
graph convolutional network to extract spatial features from each channel while
using a transformer-based architecture to capture temporal dependencies across
channels. We introduce an adaptive adjacency matrix to overcome limitations in
feature extraction from fixed topological structures. Experimental results on
six real-world datasets demonstrate that introducing a multi-channel mechanism
into the temporal model enhances performance and our proposed model outperforms
state-of-the-art models in terms of accuracy.

摘要：交通流量預測是運輸管理和規劃中的重要任務。交通流量預測的主要挑戰在於：(1) 隨著預測時間的增加，預測的準確度會下降；(2) 預測結果極度依賴從道路網路中擷取時間和空間依賴性。為了克服上述挑戰，我們提出一個多通道時空轉換器模型用於交通流量預測，這透過融合來自不同交通資料通道的結果來提高預測準確度。我們的做法利用圖形卷積網路從每個通道擷取空間特徵，同時使用基於轉換器的架構來擷取跨通道的時間依賴性。我們引入一個自適應鄰接矩陣，以克服從固定拓撲結構中擷取特徵的限制。在六個真實世界資料集上的實驗結果證明，將多通道機制引入時間模型會增強效能，而我們提出的模型在準確度方面優於最先進的模型。

##### **Learning Latent Dynamic Robust Representations for World Models**
2405.06263v1 by Ruixiang Sun, Hongyu Zang, Xin Li, Riashat Islam

Visual Model-Based Reinforcement Learning (MBRL) promises to encapsulate
agent's knowledge about the underlying dynamics of the environment, enabling
learning a world model as a useful planner. However, top MBRL agents such as
Dreamer often struggle with visual pixel-based inputs in the presence of
exogenous or irrelevant noise in the observation space, due to failure to
capture task-specific features while filtering out irrelevant spatio-temporal
details. To tackle this problem, we apply a spatio-temporal masking strategy, a
bisimulation principle, combined with latent reconstruction, to capture
endogenous task-specific aspects of the environment for world models,
effectively eliminating non-essential information. Joint training of
representations, dynamics, and policy often leads to instabilities. To further
address this issue, we develop a Hybrid Recurrent State-Space Model (HRSSM)
structure, enhancing state representation robustness for effective policy
learning. Our empirical evaluation demonstrates significant performance
improvements over existing methods in a range of visually complex control tasks
such as Maniskill \cite{gu2023maniskill2} with exogenous distractors from the
Matterport environment. Our code is avaliable at
https://github.com/bit1029public/HRSSM.

摘要：視覺模型基礎強化學習 (MBRL) 承諾封裝代理對環境基礎動態的知識，使學習世界模型成為有用的規劃器。然而，頂尖的 MBRL 代理，例如 Dreamer，在觀察空間中存在外生或無關雜訊時，通常會與基於視覺像素的輸入奮戰，這是因為無法在過濾無關的時空細節的同時擷取特定於任務的功能。為了解決這個問題，我們應用時空遮罩策略、雙模擬原理，結合潛在重建，以擷取世界模型的內生特定於任務的環境面向，有效消除非必要的資訊。表徵、動態和政策的聯合訓練通常會導致不穩定。為進一步解決這個問題，我們開發了混合遞迴狀態空間模型 (HRSSM) 結構，增強狀態表徵的穩健性，以進行有效的政策學習。我們的實證評估證明，在視覺上複雜的控制任務中，例如 Maniskill \cite{gu2023maniskill2}，與來自 Matterport 環境的外生干擾物相比，我們的方法有顯著的效能提升。我們的程式碼可在 https://github.com/bit1029public/HRSSM 取得。

##### **Precise Apple Detection and Localization in Orchards using YOLOv5 for Robotic Harvesting Systems**
2405.06260v1 by Jiang Ziyue, Yin Bo, Lu Boyun

The advancement of agricultural robotics holds immense promise for
transforming fruit harvesting practices, particularly within the apple
industry. The accurate detection and localization of fruits are pivotal for the
successful implementation of robotic harvesting systems. In this paper, we
propose a novel approach to apple detection and position estimation utilizing
an object detection model, YOLOv5. Our primary objective is to develop a robust
system capable of identifying apples in complex orchard environments and
providing precise location information. To achieve this, we curated an
autonomously labeled dataset comprising diverse apple tree images, which was
utilized for both training and evaluation purposes. Through rigorous
experimentation, we compared the performance of our YOLOv5-based system with
other popular object detection models, including SSD. Our results demonstrate
that the YOLOv5 model outperforms its counterparts, achieving an impressive
apple detection accuracy of approximately 85%. We believe that our proposed
system's accurate apple detection and position estimation capabilities
represent a significant advancement in agricultural robotics, laying the
groundwork for more efficient and sustainable fruit harvesting practices.

摘要：農業機器人的進步為轉型水果收穫實務帶來巨大的希望，特別是在蘋果產業中。準確地偵測並定位水果對於機器人收穫系統的成功實施至關重要。在本文中，我們提出了一種利用物件偵測模型 YOLOv5 來進行蘋果偵測和位置估計的新方法。我們的首要目標是開發一個強健的系統，能夠在複雜的果園環境中識別蘋果並提供精確的位置資訊。為了達成此目標，我們整理了一個由多樣蘋果樹影像組成的自動標註資料集，並將其用於訓練和評估目的。透過嚴謹的實驗，我們將我們基於 YOLOv5 的系統的效能與其他流行的物件偵測模型（包括 SSD）進行比較。我們的結果證明了 YOLOv5 模型優於其他模型，達到了大約 85% 的驚人蘋果偵測準確率。我們相信我們提出的系統準確的蘋果偵測和位置估計能力代表了農業機器人的重大進步，為更有效率且永續的水果收穫實務奠定了基礎。

##### **Automatic Generation of Model and Data Cards: A Step Towards Responsible AI**
2405.06258v1 by Jiarui Liu, Wenkai Li, Zhijing Jin, Mona Diab

In an era of model and data proliferation in machine learning/AI especially
marked by the rapid advancement of open-sourced technologies, there arises a
critical need for standardized consistent documentation. Our work addresses the
information incompleteness in current human-generated model and data cards. We
propose an automated generation approach using Large Language Models (LLMs).
Our key contributions include the establishment of CardBench, a comprehensive
dataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with
the development of the CardGen pipeline comprising a two-step retrieval
process. Our approach exhibits enhanced completeness, objectivity, and
faithfulness in generated model and data cards, a significant step in
responsible AI documentation practices ensuring better accountability and
traceability.

摘要：在機器學習/AI 中，模型和資料激增的時代，特別是標誌著開源技術的快速進步，出現了對標準化一致文件編寫的關鍵需求。我們的研究解決了當前人類產生的模型和資料卡中資訊不完整的問題。我們提出使用大型語言模型 (LLM) 的自動化生成方法。我們的關鍵貢獻包括建立 CardBench，一個從超過 4.8k 個模型卡和 1.4k 個資料卡彙總而成的綜合資料集，並結合 CardGen 管道的開發，其中包含兩步驟檢索程序。我們的做法在產生的模型和資料卡中展現出增強的完整性、客觀性和忠實度，這是負責任的 AI 文件編寫實務中的一個重要步驟，可確保更好的問責制和可追溯性。

##### **Disttack: Graph Adversarial Attacks Toward Distributed GNN Training**
2405.06247v1 by Yuxiang Zhang, Xin Liu, Meng Wu, Wei Yan, Mingyu Yan, Xiaochun Ye, Dongrui Fan

Graph Neural Networks (GNNs) have emerged as potent models for graph
learning. Distributing the training process across multiple computing nodes is
the most promising solution to address the challenges of ever-growing
real-world graphs. However, current adversarial attack methods on GNNs neglect
the characteristics and applications of the distributed scenario, leading to
suboptimal performance and inefficiency in attacking distributed GNN training.
  In this study, we introduce Disttack, the first framework of adversarial
attacks for distributed GNN training that leverages the characteristics of
frequent gradient updates in a distributed system. Specifically, Disttack
corrupts distributed GNN training by injecting adversarial attacks into one
single computing node. The attacked subgraphs are precisely perturbed to induce
an abnormal gradient ascent in backpropagation, disrupting gradient
synchronization between computing nodes and thus leading to a significant
performance decline of the trained GNN. We evaluate Disttack on four large
real-world graphs by attacking five widely adopted GNNs. Compared with the
state-of-the-art attack method, experimental results demonstrate that Disttack
amplifies the model accuracy degradation by 2.75$\times$ and achieves speedup
by 17.33$\times$ on average while maintaining unnoticeability.

摘要：圖形神經網路 (GNN) 已成為圖形學習的強大模型。將訓練流程分佈在多個運算節點上是應對不斷成長的真實世界圖形挑戰的最有希望的解決方案。然而，目前針對 GNN 的對抗攻擊方法忽略了分佈式場景的特徵和應用，導致在攻擊分佈式 GNN 訓練時性能不佳且效率低下。在本研究中，我們介紹了 Disttack，這是第一個針對分佈式 GNN 訓練的對抗攻擊框架，它利用了分佈式系統中頻繁梯度更新的特徵。具體來說，Disttack 通過向單一運算節點注入對抗攻擊來破壞分佈式 GNN 訓練。被攻擊的子圖被精確地擾動以在反向傳播中誘發異常梯度上升，破壞運算節點之間的梯度同步，從而導致訓練好的 GNN 的性能顯著下降。我們通過攻擊五個廣泛採用的 GNN 在四個大型真實世界圖形上評估了 Disttack。與最先進的攻擊方法相比，實驗結果表明，Disttack 將模型準確度下降幅度放大了 2.75 倍，同時在保持不可察覺性的情況下平均提速 17.33 倍。

##### **SaudiBERT: A Large Language Model Pretrained on Saudi Dialect Corpora**
2405.06239v1 by Faisal Qarah

In this paper, we introduce SaudiBERT, a monodialect Arabic language model
pretrained exclusively on Saudi dialectal text. To demonstrate the model's
effectiveness, we compared SaudiBERT with six different multidialect Arabic
language models across 11 evaluation datasets, which are divided into two
groups: sentiment analysis and text classification. SaudiBERT achieved average
F1-scores of 86.15\% and 87.86\% in these groups respectively, significantly
outperforming all other comparative models. Additionally, we present two novel
Saudi dialectal corpora: the Saudi Tweets Mega Corpus (STMC), which contains
over 141 million tweets in Saudi dialect, and the Saudi Forums Corpus (SFC),
which includes 15.2 GB of text collected from five Saudi online forums. Both
corpora are used in pretraining the proposed model, and they are the largest
Saudi dialectal corpora ever reported in the literature. The results confirm
the effectiveness of SaudiBERT in understanding and analyzing Arabic text
expressed in Saudi dialect, achieving state-of-the-art results in most tasks
and surpassing other language models included in the study. SaudiBERT model is
publicly available on \url{https://huggingface.co/faisalq/SaudiBERT}.

摘要：<paragraph>在本文中，我們介紹 SaudiBERT，這是一個單方言阿拉伯語語言模型，並專門針對沙烏地方言文本進行預訓練。為了展示模型的有效性，我們將 SaudiBERT 與六個不同的多方言阿拉伯語語言模型進行比較，比較了 11 個評估資料集，這些資料集分為兩組：情緒分析和文本分類。在這些組別中，SaudiBERT 分別達到了 86.15% 和 87.86% 的平均 F1 分數，顯著優於所有其他比較模型。此外，我們還提出了兩個新穎的沙烏地方言語料庫：沙烏地推文巨量語料庫 (STMC)，其中包含超過 1.41 億條沙烏地方言推文，以及沙烏地論壇語料庫 (SFC)，其中包含從五個沙烏地線上論壇收集的 15.2 GB 文字。這兩個語料庫都用於預訓練所提出的模型，而且它們是有史以來在文獻中報告過最大的沙烏地方言語料庫。結果證實了 SaudiBERT 在理解和分析以沙烏地方言表達的阿拉伯語文本方面的有效性，在大部分任務中都達到了最先進的結果，並超越了研究中包含的其他語言模型。SaudiBERT 模型已公開發布於 \url{https://huggingface.co/faisalq/SaudiBERT}。</paragraph>

##### **Learning to Solve Geometry Problems via Simulating Human Dual-Reasoning Process**
2405.06232v1 by Tong Xiao, Jiayu Liu, Zhenya Huang, Jinze Wu, Jing Sha, Shijin Wang, Enhong Chen

Geometry Problem Solving (GPS), which is a classic and challenging math
problem, has attracted much attention in recent years. It requires a solver to
comprehensively understand both text and diagram, master essential geometry
knowledge, and appropriately apply it in reasoning. However, existing works
follow a paradigm of neural machine translation and only focus on enhancing the
capability of encoders, which neglects the essential characteristics of human
geometry reasoning. In this paper, inspired by dual-process theory, we propose
a Dual-Reasoning Geometry Solver (DualGeoSolver) to simulate the dual-reasoning
process of humans for GPS. Specifically, we construct two systems in
DualGeoSolver, namely Knowledge System and Inference System. Knowledge System
controls an implicit reasoning process, which is responsible for providing
diagram information and geometry knowledge according to a step-wise reasoning
goal generated by Inference System. Inference System conducts an explicit
reasoning process, which specifies the goal in each reasoning step and applies
the knowledge to generate program tokens for resolving it. The two systems
carry out the above process iteratively, which behaves more in line with human
cognition. We conduct extensive experiments on two benchmark datasets, GeoQA
and GeoQA+. The results demonstrate the superiority of DualGeoSolver in both
solving accuracy and robustness from explicitly modeling human reasoning
process and knowledge application.

摘要：幾何問題求解 (GPS) 是一個經典且具有挑戰性的數學問題，在近年來備受關注。它要求求解者全面理解文字和圖表，掌握基本的幾何知識，並適當地將其應用於推理中。然而，現有的作品遵循神經機器翻譯的範例，僅專注於增強編碼器的能力，而忽略了人類幾何推理的基本特徵。在本文中，受到雙重過程理論的啟發，我們提出了一個雙重推理幾何求解器 (DualGeoSolver)，以模擬人類對 GPS 的雙重推理過程。具體來說，我們在 DualGeoSolver 中構建了兩個系統，即知識系統和推理系統。知識系統控制一個隱式推理過程，負責根據推理系統生成的逐步推理目標提供圖表信息和幾何知識。推理系統進行一個明確的推理過程，它指定每個推理步驟中的目標，並應用知識來生成用於解決它的程序令牌。這兩個系統迭代地執行上述過程，這更符合人類認知。我們在兩個基準數據集 GeoQA 和 GeoQA+ 上進行了廣泛的實驗。結果證明了 DualGeoSolver 在解決準確性和魯棒性方面都優於明確建模人類推理過程和知識應用。

##### **For the Misgendered Chinese in Gender Bias Research: Multi-Task Learning with Knowledge Distillation for Pinyin Name-Gender Prediction**
2405.06221v1 by Xiaocong Du, Haipeng Zhang

Achieving gender equality is a pivotal factor in realizing the UN's Global
Goals for Sustainable Development. Gender bias studies work towards this and
rely on name-based gender inference tools to assign individual gender labels
when gender information is unavailable. However, these tools often inaccurately
predict gender for Chinese Pinyin names, leading to potential bias in such
studies. With the growing participation of Chinese in international activities,
this situation is becoming more severe. Specifically, current tools focus on
pronunciation (Pinyin) information, neglecting the fact that the latent
connections between Pinyin and Chinese characters (Hanzi) behind convey
critical information. As a first effort, we formulate the Pinyin name-gender
guessing problem and design a Multi-Task Learning Network assisted by Knowledge
Distillation that enables the Pinyin embeddings in the model to possess
semantic features of Chinese characters and to learn gender information from
Chinese character names. Our open-sourced method surpasses commercial
name-gender guessing tools by 9.70\% to 20.08\% relatively, and also
outperforms the state-of-the-art algorithms.

摘要：實現性別平等是實現聯合國永續發展目標的關鍵因素。性別偏見研究朝此方向努力，並依賴基於名稱的性別推論工具，在沒有性別資訊時指派個人性別標籤。然而，這些工具經常無法準確預測中文拼音名稱的性別，導致此類研究潛在的偏見。隨著中國人參與國際活動日益增加，這種情況正變得更加嚴重。具體而言，目前的工具專注於發音（拼音）資訊，忽視了拼音和漢字之間的潛在關聯傳達了關鍵資訊。作為第一步，我們制定了拼音名稱性別猜測問題，並設計了一個由知識蒸餾輔助的多任務學習網路，使模型中的拼音嵌入具備漢字的語義特徵，並從漢字名稱中學習性別資訊。我們的開源方法相對於商業名稱性別猜測工具高出 9.70% 至 20.08%，並且也優於最先進的演算法。

##### **SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models**
2405.06219v1 by Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin

Large language models (LLMs) can now handle longer sequences of tokens,
enabling complex tasks like book understanding and generating lengthy novels.
However, the key-value (KV) cache required for LLMs consumes substantial memory
as context length increasing, becoming the bottleneck for deployment. In this
paper, we present a strategy called SKVQ, which stands for sliding-window KV
cache quantization, to address the issue of extremely low bitwidth KV cache
quantization. To achieve this, SKVQ rearranges the channels of the KV cache in
order to improve the similarity of channels in quantization groups, and applies
clipped dynamic quantization at the group level. Additionally, SKVQ ensures
that the most recent window tokens in the KV cache are preserved with high
precision. This helps maintain the accuracy of a small but important portion of
the KV cache.SKVQ achieves high compression ratios while maintaining accuracy.
Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization
approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit
values with minimal loss of accuracy. With SKVQ, it is possible to process
context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7
times faster decoding.

摘要：大型語言模型 (LLM) 現在可以處理更長的符號序列，
執行複雜的任務，例如理解書籍和產生冗長的章節。
然而，LLM 所需的鍵值 (KV) 快取會隨著內容長度的增加而消耗大量的記憶體，
成為部署的瓶頸。在本文中，我們提出了一種稱為 SKVQ 的策略，代表滑動視窗 KV 快取量化，
以解決極低位元寬度 KV 快取量化的問題。為此，SKVQ 重新排列 KV 快取的通道，
以提高量化組中通道的相似性，並在組級別套用裁剪動態量化。此外，SKVQ 確保
KV 快取中最新的視窗符號以高精度保留。這有助於維護 KV 快取中一小部分但重要的部分的準確性。
SKVQ 在維持準確性的同時實現了高壓縮率。我們對 LLM 的評估表明，SKVQ 超越了先前的量化方法，
允許將 KV 快取量化為 2 位元鍵和 1.5 位元值，準確度損失極小。使用 SKVQ，可以在 80GB 記憶體 GPU 上處理長達 1M 的內容長度，適用於 7b 模型，且解碼速度最高可提升 7 倍。

##### **Aligning Tutor Discourse Supporting Rigorous Thinking with Tutee Content Mastery for Predicting Math Achievement**
2405.06218v1 by Mark Abdelshiheed, Jennifer K. Jacobs, Sidney K. D'Mello

This work investigates how tutoring discourse interacts with students'
proximal knowledge to explain and predict students' learning outcomes. Our work
is conducted in the context of high-dosage human tutoring where 9th-grade
students (N= 1080) attended small group tutorials and individually practiced
problems on an Intelligent Tutoring System (ITS). We analyzed whether tutors'
talk moves and students' performance on the ITS predicted scores on math
learning assessments. We trained Random Forest Classifiers (RFCs) to
distinguish high and low assessment scores based on tutor talk moves, student's
ITS performance metrics, and their combination. A decision tree was extracted
from each RFC to yield an interpretable model. We found AUCs of 0.63 for talk
moves, 0.66 for ITS, and 0.77 for their combination, suggesting interactivity
among the two feature sources. Specifically, the best decision tree emerged
from combining the tutor talk moves that encouraged rigorous thinking and
students' ITS mastery. In essence, tutor talk that encouraged mathematical
reasoning predicted achievement for students who demonstrated high mastery on
the ITS, whereas tutors' revoicing of students' mathematical ideas and
contributions was predictive for students with low ITS mastery. Implications
for practice are discussed.

摘要：本研究探討家教話語如何與學生的近端知識互動，以解釋和預測學生的學習成果。我們的工作是在高劑量人類家教的背景下進行的，其中 9 年級學生 (N= 1080) 參加了小組教學，並在智能家教系統 (ITS) 上個別練習題目。我們分析了家教的談話舉動和學生在 ITS 上的表現是否預測了數學學習評估的得分。我們訓練隨機森林分類器 (RFC) 根據家教談話舉動、學生的 ITS 表現指標及其組合來區分高分和低分。從每個 RFC 中提取決策樹以產生可解釋的模型。我們發現談話舉動的 AUC 為 0.63，ITS 為 0.66，它們的組合為 0.77，這表明兩個特徵來源之間存在交互性。具體來說，最好的決策樹來自結合鼓勵嚴謹思維的家教談話舉動和學生的 ITS 掌握度。實質上，鼓勵數學推理的家教談話預測了在 ITS 上表現出高掌握度的學生的成就，而家教對學生的數學思想和貢獻的重述則預測了 ITS 掌握度低的學生的成就。討論了對實踐的影響。

##### **A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models**
2405.06211v1 by Yujuan Ding, Wenqi Fan, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li

As one of the most advanced techniques in AI, Retrieval-Augmented Generation
(RAG) techniques can offer reliable and up-to-date external knowledge,
providing huge convenience for numerous tasks. Particularly in the era of
AI-generated content (AIGC), the powerful capacity of retrieval in RAG in
providing additional knowledge enables retrieval-augmented generation to assist
existing generative AI in producing high-quality outputs. Recently, large
Language Models (LLMs) have demonstrated revolutionary abilities in language
understanding and generation, while still facing inherent limitations, such as
hallucinations and out-of-date internal knowledge. Given the powerful abilities
of RAG in providing the latest and helpful auxiliary information,
retrieval-augmented large language models have emerged to harness external and
authoritative knowledge bases, rather than solely relying on the model's
internal knowledge, to augment the generation quality of LLMs. In this survey,
we comprehensively review existing research studies in retrieval-augmented
large language models (RA-LLMs), covering three primary technical perspectives:
architectures, training strategies, and applications. As the preliminary
knowledge, we briefly introduce the foundations and recent advances of LLMs.
Then, to illustrate the practical significance of RAG for LLMs, we categorize
mainstream relevant work by application areas, detailing specifically the
challenges of each and the corresponding capabilities of RA-LLMs. Finally, to
deliver deeper insights, we discuss current limitations and several promising
directions for future research.

摘要：作為 AI 中最先進的技術之一，檢索增強生成 (RAG) 技術可以提供可靠且最新的外部知識，為眾多任務提供了極大的便利。特別是在 AI 生成的內容 (AIGC) 時代，RAG 中檢索的強大能力在提供額外知識方面，使檢索增強生成能夠協助現有的生成式 AI 產生高品質的輸出。最近，大型語言模型 (LLM) 在語言理解和生成方面展示了革命性的能力，但仍面臨固有的限制，例如幻覺和過時的內部知識。鑑於 RAG 在提供最新且有用的輔助資訊方面的強大能力，檢索增強大型語言模型已經出現，以利用外部和權威的知識庫，而不是僅依賴模型的內部知識，來增強 LLM 的生成品質。在本次調查中，我們全面回顧了檢索增強大型語言模型 (RA-LLM) 中現有的研究，涵蓋了三個主要的技術觀點：架構、訓練策略和應用。作為初步知識，我們簡要介紹 LLM 的基礎和最新進展。然後，為了說明 RAG 對 LLM 的實際意義，我們按應用領域對主流相關工作進行分類，詳細說明每個領域的挑戰和 RA-LLM 相應的能力。最後，為了提供更深入的見解，我們討論了當前的限制和未來研究的幾個有希望的方向。

##### **Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning**
2405.06206v1 by Yujie Zhang, Neil Gong, Michael K. Reiter

Federated Learning (FL) is a decentralized machine learning method that
enables participants to collaboratively train a model without sharing their
private data. Despite its privacy and scalability benefits, FL is susceptible
to backdoor attacks, where adversaries poison the local training data of a
subset of clients using a backdoor trigger, aiming to make the aggregated model
produce malicious results when the same backdoor condition is met by an
inference-time input. Existing backdoor attacks in FL suffer from common
deficiencies: fixed trigger patterns and reliance on the assistance of model
poisoning. State-of-the-art defenses based on Byzantine-robust aggregation
exhibit a good defense performance on these attacks because of the significant
divergence between malicious and benign model updates. To effectively conceal
malicious model updates among benign ones, we propose DPOT, a backdoor attack
strategy in FL that dynamically constructs backdoor objectives by optimizing a
backdoor trigger, making backdoor data have minimal effect on model updates. We
provide theoretical justifications for DPOT's attacking principle and display
experimental results showing that DPOT, via only a data-poisoning attack,
effectively undermines state-of-the-art defenses and outperforms existing
backdoor attack techniques on various datasets.

摘要：聯邦學習 (FL) 是一種分散式機器學習方法，使參與者能夠在不分享其私有數據的情況下協作訓練模型。儘管具有隱私和可擴充性優勢，但 FL 容易受到後門攻擊，其中對手使用後門觸發器毒害部分客戶的本地訓練數據，目的是在推理時間輸入滿足相同後門條件時讓聚合模型產生惡意結果。現有的 FL 中的後門攻擊存在常見的缺陷：固定的觸發模式和依賴模型毒化的協助。基於拜占庭魯棒聚合的最新防禦在這些攻擊中表現出良好的防禦性能，因為惡意和良性模型更新之間存在顯著差異。為了在良性更新中有效隱藏惡意模型更新，我們提出了 DPOT，這是一種 FL 中的後門攻擊策略，通過優化後門觸發器動態構建後門目標，使後門數據對模型更新的影響最小。我們為 DPOT 的攻擊原理提供了理論依據，並展示了實驗結果，表明 DPOT 僅通過數據毒化攻擊，就能有效破壞最先進的防禦，並且在各種數據集上優於現有的後門攻擊技術。

##### **HC$^2$L: Hybrid and Cooperative Contrastive Learning for Cross-lingual Spoken Language Understanding**
2405.06204v1 by Bowen Xing, Ivor W. Tsang

State-of-the-art model for zero-shot cross-lingual spoken language
understanding performs cross-lingual unsupervised contrastive learning to
achieve the label-agnostic semantic alignment between each utterance and its
code-switched data. However, it ignores the precious intent/slot labels, whose
label information is promising to help capture the label-aware semantics
structure and then leverage supervised contrastive learning to improve both
source and target languages' semantics. In this paper, we propose Hybrid and
Cooperative Contrastive Learning to address this problem. Apart from
cross-lingual unsupervised contrastive learning, we design a holistic approach
that exploits source language supervised contrastive learning, cross-lingual
supervised contrastive learning and multilingual supervised contrastive
learning to perform label-aware semantics alignments in a comprehensive manner.
Each kind of supervised contrastive learning mechanism includes both
single-task and joint-task scenarios. In our model, one contrastive learning
mechanism's input is enhanced by others. Thus the total four contrastive
learning mechanisms are cooperative to learn more consistent and discriminative
representations in the virtuous cycle during the training process. Experiments
show that our model obtains consistent improvements over 9 languages, achieving
new state-of-the-art performance.

摘要：最先進的零次學習跨語言口語語言理解模型執行跨語言無監督對比學習，以實現每個語句及其代碼轉換數據之間的標籤不可知語義對齊。然而，它忽略了寶貴的意圖/插槽標籤，其標籤信息有望幫助捕捉標籤感知語義結構，然後利用監督對比學習來改善源語言和目標語言的語義。在本文中，我們提出混合和協作對比學習來解決這個問題。除了跨語言無監督對比學習之外，我們設計了一種整體方法，利用源語言監督對比學習、跨語言監督對比學習和多語言監督對比學習以全面方式執行標籤感知語義對齊。每種類型的監督對比學習機制都包括單任務和聯合任務場景。在我們的模型中，一種對比學習機制的輸入由其他機制增強。因此，在訓練過程中，總共四種對比學習機制相互協作，以學習更一致且具有區別性的表徵。實驗表明，我們的模型在 9 種語言中獲得了一致的改進，達到了新的最先進性能。

##### **VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks**
2405.06196v1 by Manish Dhakal, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal

Foundation Vision-Language Models (VLMs) trained using large-scale
open-domain images and text pairs have recently been adapted to develop
Vision-Language Segmentation Models (VLSMs) that allow providing text prompts
during inference to guide image segmentation. If robust and powerful VLSMs can
be built for medical images, it could aid medical professionals in many
clinical tasks where they must spend substantial time delineating the target
structure of interest. VLSMs for medical images resort to fine-tuning base VLM
or VLSM pretrained on open-domain natural image datasets due to fewer annotated
medical image datasets; this fine-tuning is resource-consuming and expensive as
it usually requires updating all or a significant fraction of the pretrained
parameters. Recently, lightweight blocks called adapters have been proposed in
VLMs that keep the pretrained model frozen and only train adapters during
fine-tuning, substantially reducing the computing resources required. We
introduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained
vision-language segmentation models using transformer encoders. Our experiments
in widely used CLIP-based segmentation models show that with only 3 million
trainable parameters, the VLSM-Adapter outperforms state-of-the-art and is
comparable to the upper bound end-to-end fine-tuning. The source code is
available at: https://github.com/naamiinepal/vlsm-adapter.

摘要：<paragraph>基礎視覺語言模型 (VLM) 使用大規模開放領域影像和文字對進行訓練，最近已調整用於開發視覺語言分割模型 (VLSM)，允許在推論期間提供文字提示，以引導影像分割。如果可以為醫學影像建立強大且強大的 VLSM，它可以幫助醫療專業人員執行許多臨床任務，在這些任務中，他們必須花費大量時間來描繪目標結構。針對醫學影像的 VLSM 會使用開放領域自然影像資料集預訓練的基礎 VLM 或 VLSM 進行微調，因為標註醫學影像資料集較少；這種微調會消耗資源且昂貴，因為它通常需要更新所有或大部分預訓練參數。最近，在 VLM 中提出了稱為適配器的輕量級區塊，它會保持預訓練模型凍結，並且只在微調期間訓練適配器，大幅減少所需的運算資源。我們引進一種新穎的適配器，VLSM-Adapter，它可以使用Transformer編碼器微調預訓練的視覺語言分割模型。我們在廣泛使用的 CLIP 基礎分割模型中進行的實驗顯示，VLSM-Adapter 只有 300 萬個可訓練參數，其效能優於現有技術，並且可與上限端到端微調相媲美。原始碼可在 https://github.com/naamiinepal/vlsm-adapter 取得。</paragraph>

##### **Skeet: Towards a Lightweight Serverless Framework Supporting Modern AI-Driven App Development**
2405.06164v1 by Kawasaki Fumitake, Shota Kishi, James Neve

The field of web and mobile software frameworks is relatively mature, with a
large variety of tools in different languages that facilitate traditional app
development where data in a relational database is displayed and modified. Our
position is that many current frameworks became popular during single server
deployment of MVC architecture apps, and do not facilitate modern aspects of
app development such as cloud computing and the incorporation of emerging
technologies such as AI. We present a novel framework which accomplishes these
purposes, Skeet, which was recently released to general use, alongside an
initial evaluation. Skeet provides an app structure that reflects current
trends in architecture, and tool suites that allow developers with minimal
knowledge of AI internals to easily incorporate such technologies into their
apps and deploy them.

摘要：網頁和行動軟體架構領域已經相對成熟，有各種不同語言的工具，可用於顯示和修改關聯式資料庫中的資料的傳統應用程式開發。我們的立場是，許多當前的架構在 MVC 架構應用程式的單一伺服器部署期間變得流行，並且無法促進應用程式開發的現代化面向，例如雲端運算和整合新興技術，例如 AI。我們提出了一個實現這些目的的新穎架構 Skeet，它最近已發布供一般使用，並附有初步評估。Skeet 提供了一個反映當前架構趨勢的應用程式結構，以及允許開發人員在對 AI 內部結構知之甚少的情況下，輕鬆地將這些技術整合到他們的應用程式中並部署它們的工具組。

##### **Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing Clinical and Social Effects of Substance Use Derived from Social Media**
2405.06145v1 by Yao Ge, Sudeshna Das, Karen O'Connor, Mohammed Ali Al-Garadi, Graciela Gonzalez-Hernandez, Abeed Sarker

Substance use disorders (SUDs) are a growing concern globally, necessitating
enhanced understanding of the problem and its trends through data-driven
research. Social media are unique and important sources of information about
SUDs, particularly since the data in such sources are often generated by people
with lived experiences. In this paper, we introduce Reddit-Impacts, a
challenging Named Entity Recognition (NER) dataset curated from subreddits
dedicated to discussions on prescription and illicit opioids, as well as
medications for opioid use disorder. The dataset specifically concentrates on
the lesser-studied, yet critically important, aspects of substance use--its
clinical and social impacts. We collected data from chosen subreddits using the
publicly available Application Programming Interface for Reddit. We manually
annotated text spans representing clinical and social impacts reported by
people who also reported personal nonmedical use of substances including but
not limited to opioids, stimulants and benzodiazepines. Our objective is to
create a resource that can enable the development of systems that can
automatically detect clinical and social impacts of substance use from
text-based social media data. The successful development of such systems may
enable us to better understand how nonmedical use of substances affects
individual health and societal dynamics, aiding the development of effective
public health strategies. In addition to creating the annotated data set, we
applied several machine learning models to establish baseline performances.
Specifically, we experimented with transformer models like BERT, and RoBERTa,
one few-shot learning model DANN by leveraging the full training dataset, and
GPT-3.5 by using one-shot learning, for automatic NER of clinical and social
impacts. The dataset has been made available through the 2024 SMM4H shared
tasks.

摘要：物質使用疾患 (SUD) 在全球範圍內日益受到關注，這需要通過數據驅動的研究增強對問題及其趨勢的理解。社交媒體是關於 SUD 的獨特而重要的信息來源，特別是因為此類來源中的數據通常是由有親身經歷的人生成。在本文中，我們介紹了 Reddit-Impacts，這是一個具有挑戰性的命名實體識別 (NER) 數據集，它從專門討論處方和非法類鴉片藥物以及類鴉片藥物使用障礙藥物的子版塊中整理而成。該數據集特別關注物質使用中研究較少但至關重要的方面——其臨床和社會影響。我們使用 Reddit 的公開應用程式介面從選定的子版塊收集數據。我們手動註解了文本跨度，代表了那些報告了個人非醫療用途物質（包括但不限於類鴉片藥物、興奮劑和苯二氮卓類藥物）的人報告的臨床和社會影響。我們的目標是創建一個資源，它可以使系統的開發成為可能，該系統可以從基於文本的社交媒體數據中自動檢測物質使用的臨床和社會影響。此類系統的成功開發可能會使我們更好地了解非醫療用途物質如何影響個人健康和社會動態，從而幫助制定有效的公共衛生策略。除了創建註解數據集之外，我們還應用多個機器學習模型來建立基準性能。具體來說，我們對Transformer模型（如 BERT 和 RoBERTa）進行了試驗，一個通過利用完整訓練數據集的少樣本學習模型 DANN，以及通過使用一次性學習的 GPT-3.5，用於臨床和社會影響的自動 NER。該數據集已通過 2024 SMM4H 共享任務提供。

##### **Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models**
2405.06134v1 by Vyas Raina, Rao Ma, Charles McGhee, Kate Knill, Mark Gales

Recent developments in large speech foundation models like Whisper have led
to their widespread use in many automatic speech recognition (ASR)
applications. These systems incorporate `special tokens' in their vocabulary,
such as $\texttt{<endoftext>}$, to guide their language generation process.
However, we demonstrate that these tokens can be exploited by adversarial
attacks to manipulate the model's behavior. We propose a simple yet effective
method to learn a universal acoustic realization of Whisper's
$\texttt{<endoftext>}$ token, which, when prepended to any speech signal,
encourages the model to ignore the speech and only transcribe the special
token, effectively `muting' the model. Our experiments demonstrate that the
same, universal 0.64-second adversarial audio segment can successfully mute a
target Whisper ASR model for over 97\% of speech samples. Moreover, we find
that this universal adversarial audio segment often transfers to new datasets
and tasks. Overall this work demonstrates the vulnerability of Whisper models
to `muting' adversarial attacks, where such attacks can pose both risks and
potential benefits in real-world settings: for example the attack can be used
to bypass speech moderation systems, or conversely the attack can also be used
to protect private speech data.

摘要：近期大型語音基礎模型（例如 Whisper）的發展，已廣泛運用於許多自動語音辨識 (ASR) 應用程式中。這些系統在其詞彙中加入「特殊符號」，例如 $\texttt{<endoftext>}$，以引導其語言產生流程。然而，我們證明這些符號可被對抗性攻擊利用，以操控模型的行為。我們提出一個簡單但有效的方法，以學習 Whisper 的 $\texttt{<endoftext>}$ 符號的通用音響實現，當它被附加到任何語音訊號時，會鼓勵模型忽略語音並僅轉錄特殊符號，有效地「靜音」模型。我們的實驗證明，相同的通用 0.64 秒對抗性音訊片段可以成功靜音目標 Whisper ASR 模型，適用於超過 97% 的語音範例。此外，我們發現這個通用對抗性音訊片段通常可以轉移到新的資料集和任務中。整體而言，這項工作證明了 Whisper 模型容易受到「靜音」對抗性攻擊，此類攻擊在現實世界中可能帶來風險和潛在好處：例如，攻擊可用於繞過語音審核系統，反之，攻擊也可被用於保護私人語音資料。

##### **Narrative to Trajectory (N2T+): Extracting Routes of Life or Death from Human Trafficking Text Corpora**
2405.06129v1 by Saydeh N. Karabatis, Vandana P. Janeja

Climate change and political unrest in certain regions of the world are
imposing extreme hardship on many communities and are forcing millions of
vulnerable populations to abandon their homelands and seek refuge in safer
lands. As international laws are not fully set to deal with the migration
crisis, people are relying on networks of exploiting smugglers to escape the
devastation in order to live in stability. During the smuggling journey,
migrants can become victims of human trafficking if they fail to pay the
smuggler and may be forced into coerced labor. Government agencies and
anti-trafficking organizations try to identify the trafficking routes based on
stories of survivors in order to gain knowledge and help prevent such crimes.
In this paper, we propose a system called Narrative to Trajectory (N2T+), which
extracts trajectories of trafficking routes. N2T+ uses Data Science and Natural
Language Processing techniques to analyze trafficking narratives, automatically
extract relevant location names, disambiguate possible name ambiguities, and
plot the trafficking route on a map. In a comparative evaluation we show that
the proposed multi-dimensional approach offers significantly higher geolocation
detection than other state of the art techniques.

摘要：氣候變遷和世界上某些地區的政治動盪
給許多社區帶來極端的困難，並迫使數百萬弱勢人口放棄家園，在更安全的土地上尋求庇護。由於國際法尚未完全制定應對移民危機的措施，人們依賴剝削走私者的網路逃離毀滅，以求生活穩定。在走私過程中，如果移民無法支付走私者的費用，他們可能會成為人口販運的受害者，並可能被迫從事強制勞動。政府機構和反人口販運組織試圖根據倖存者的故事找出人口販運路線，以獲得知識並幫助預防此類犯罪。在本文中，我們提出一個名為敘述到軌跡 (N2T+) 的系統，它會擷取人口販運路線的軌跡。N2T+ 使用資料科學和自然語言處理技術來分析人口販運敘述，自動擷取相關的地名，消除可能的同名歧義，並在地圖上繪製人口販運路線。在比較評估中，我們證明所提出的多維度方法提供的地理定位偵測顯著高於其他現有技術。

##### **Scalable Exact Verification of Optimization Proxies for Large-Scale Optimal Power Flow**
2405.06109v1 by Rahul Nellikkath, Mathieu Tanneau, Pascal Van Hentenryck, Spyros Chatzivasileiadis

Optimal Power Flow (OPF) is a valuable tool for power system operators, but
it is a difficult problem to solve for large systems.
  Machine Learning (ML) algorithms, especially Neural Networks-based (NN)
optimization proxies, have emerged as a promising new tool for solving OPF, by
estimating the OPF solution much faster than traditional methods.
  However, these ML algorithms act as black boxes, and it is hard to assess
their worst-case performance across the entire range of possible inputs than an
OPF can have.
  Previous work has proposed a mixed-integer programming-based methodology to
quantify the worst-case violations caused by a NN trained to estimate the OPF
solution, throughout the entire input domain.
  This approach, however, does not scale well to large power systems and more
complex NN models.
  This paper addresses these issues by proposing a scalable algorithm to
compute worst-case violations of NN proxies used for approximating large power
systems within a reasonable time limit.
  This will help build trust in ML models to be deployed in large
industry-scale power grids.

摘要：最佳電力流 (OPF) 是電力系統操作員的寶貴工具，但對於大型系統而言，這是一個難以解決的問題。
機器學習 (ML) 演算法，尤其是基於神經網路 (NN) 的最佳化代理，已成為一種有前途的新工具，用於解決 OPF，其估計 OPF 解決方案的速度遠快於傳統方法。
然而，這些 ML 演算法就像黑盒子，很難評估它們在 OPF 可能擁有的所有輸入範圍內的最悪情況效能。
先前的研究提出了一個基於混合整數規劃的方法，用於量化由訓練用於估計 OPF 解決方案的 NN 造成的最悪情況違規，遍及整個輸入網域。
然而，此方法無法很好地擴展到大型電力系統和更複雜的 NN 模型。
本文透過提出一個可擴充演算法來解決這些問題，以計算用於近似大型電力系統的 NN 代理的最悪情況違規，並在合理的時限內。
這將有助於建立對 ML 模型的信任，以便部署在大型產業規模的電網中。

##### **Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?**
2405.06105v1 by Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, Yansong Feng

Recent studies have shown that Large Language Models (LLMs) have the
potential to process extremely long text. Many works only evaluate LLMs'
long-text processing ability on the language modeling task, with perplexity
(PPL) as the evaluation metric. However, in our study, we find that there is no
correlation between PPL and LLMs' long-text understanding ability. Besides, PPL
may only reflect the model's ability to model local information instead of
catching long-range dependency. Therefore, only using PPL to prove the model
could process long text is inappropriate. The local focus feature of PPL could
also explain some existing phenomena, such as the great extrapolation ability
of the position method ALiBi. When evaluating a model's ability in long text,
we might pay more attention to PPL's limitation and avoid overly relying on it.

摘要：最近的研究表明，大型語言模型 (LLM) 具有處理極長文本的潛力。許多作品僅在語言建模任務上評估 LLM 的長文本處理能力，並以困惑度 (PPL) 作為評估指標。然而，在我們的研究中，我們發現 PPL 與 LLM 的長文本理解能力之間沒有相關性。此外，PPL 可能只反映模型對局部資訊建模的能力，而不是捕捉長距離依賴關係。因此，僅使用 PPL 來證明模型可以處理長文本是不適當的。PPL 的局部關注特徵也可以解釋一些現有現象，例如位置方法 ALiBi 的強大外推能力。在評估模型在長文本中的能力時，我們可能更關注 PPL 的限制，並避免過度依賴它。

##### **Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection**
2405.06093v1 by Bhawesh Kumar, Jonathan Amar, Eric Yang, Nan Li, Yugang Jia

Large Language Models (LLMs) have demonstrated their efficacy across a broad
spectrum of tasks in healthcare applications. However, often LLMs need to be
fine-tuned on task-specific expert annotated data to achieve optimal
performance, which can be expensive and time consuming. In this study, we
fine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels
obtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)
tables, which specify care plan in clinical trial protocols. We introduce a
filtering mechanism to select high-confidence labels for this table
classification task, thereby reducing the noise in the auto-generated labels.
We show that fine-tuned PaLM-2 with those labels achieves performance that
exceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is
close to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our
results show that leveraging LLM-generated labels through powerful models like
gemini-pro can potentially serve as a viable strategy for improving LLM
performance through fine-tuning in specialized tasks, particularly in domains
where expert annotations are scarce, expensive, or time-consuming to obtain.

摘要：大型語言模型 (LLM) 已證明其在醫療保健應用中廣泛任務的功效。然而，LLM 通常需要針對特定任務的專家註釋資料進行微調才能達到最佳效能，這可能會很昂貴且耗時。在這項研究中，我們使用從 gemini-pro 1.0 取得的雜訊標籤，以參數有效微調 (PEFT) 微調 PaLM-2，以偵測臨床試驗協定中的照護計畫規範的事件時程表 (SoE) 表格。我們引入一個過濾機制，以選擇此表格分類任務的高信心標籤，從而降低自動產生標籤中的雜訊。我們證明微調後的 PaLM-2 使用這些標籤，其效能超越 gemini-pro 1.0 和其他 LLM。此外，其效能接近根據非專家註釋者取得的標籤微調的 PaLM-2。我們的結果顯示，透過功能強大的模型（例如 gemini-pro）利用 LLM 產生的標籤，有可能成為透過微調改善 LLM 效能的可行策略，特別是在取得專家註釋稀少、昂貴或耗時的領域中。

##### **HMT: Hierarchical Memory Transformer for Long Context Language Processing**
2405.06067v1 by Zifan He, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong

Transformer-based large language models (LLM) have been widely used in
language processing applications. However, most of them restrict the context
window that permits the model to attend to every token in the inputs. Previous
works in recurrent models can memorize past tokens to enable unlimited context
and maintain effectiveness. However, they have "flat" memory architectures,
which have limitations in selecting and filtering information. Since humans are
good at learning and self-adjustment, we speculate that imitating brain memory
hierarchy is beneficial for model memorization. We propose the Hierarchical
Memory Transformer (HMT), a novel framework that enables and improves models'
long-context processing ability by imitating human memorization behavior.
Leveraging memory-augmented segment-level recurrence, we organize the memory
hierarchy by preserving tokens from early input token segments, passing memory
embeddings along the sequence, and recalling relevant information from history.
Evaluating general language modeling (Wikitext-103, PG-19) and
question-answering tasks (PubMedQA), we show that HMT steadily improves the
long-context processing ability of context-constrained and long-context models.
With an additional 0.5% - 2% of parameters, HMT can easily plug in and augment
future LLMs to handle long context effectively. Our code is open-sourced on
Github: https://github.com/OswaldHe/HMT-pytorch.

摘要：<paragraph>基於 Transformer 的大型語言模型 (LLM) 已廣泛用於語言處理應用程式中。然而，它們大多限制了允許模型關注輸入中每個符號的上下文視窗。以往在遞迴模型中的工作可以記住過去的符號，以啟用無限的上下文並維持效能。然而，它們有「扁平」的記憶體架構，在選擇和過濾資訊方面有其限制。由於人類擅長學習和自我調整，我們推測模仿大腦記憶體階層對於模型記憶化是有益的。我們提出階層式記憶Transformer (HMT)，這是一個新穎的架構，透過模仿人類的記憶行為，啟用並提升模型的長上下文處理能力。利用記憶增強的區段層級遞迴，我們透過保留早期輸入符號區段的符號，沿著序列傳遞記憶嵌入，並從歷史中提取相關資訊，來組織記憶體階層。評估一般語言模型（Wikitext-103、PG-19）和問答任務（PubMedQA），我們展示 HMT 穩定地提升了受限於上下文的模型和長上下文模型的長上下文處理能力。透過額外的 0.5% - 2% 參數，HMT 可以輕鬆地插入和擴充未來的 LLM，以有效地處理長上下文。我們的程式碼在 Github 上開放原始碼：https://github.com/OswaldHe/HMT-pytorch。</paragraph>

##### **LLMs for XAI: Future Directions for Explaining Explanations**
2405.06064v1 by Alexandra Zytek, Sara Pidò, Kalyan Veeramachaneni

In response to the demand for Explainable Artificial Intelligence (XAI), we
investigate the use of Large Language Models (LLMs) to transform ML
explanations into natural, human-readable narratives. Rather than directly
explaining ML models using LLMs, we focus on refining explanations computed
using existing XAI algorithms. We outline several research directions,
including defining evaluation metrics, prompt design, comparing LLM models,
exploring further training methods, and integrating external data. Initial
experiments and user study suggest that LLMs offer a promising way to enhance
the interpretability and usability of XAI.

摘要：為了回應可解釋人工智慧 (XAI) 的需求，我們探討使用大型語言模型 (LLM) 將機器學習解釋轉換為自然的人類可讀敘述。我們專注於使用現有 XAI 演算法計算出的解釋，而不是直接使用 LLM 來解釋機器學習模型。我們概述了幾個研究方向，包括定義評估指標、提示設計、比較 LLM 模型、探索進一步的訓練方法，以及整合外部資料。初步的實驗和使用者研究表明，LLM 提供了一個有希望的方法來增強 XAI 的可解釋性和可用性。

##### **A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds**
2405.06059v1 by Christopher Z. Cui, Xiangyu Peng, Mark O. Riedl

Open-ended worlds are those in which there are no pre-specified goals or
environmental reward signal. As a consequence, an agent must know how to
perform a multitude of tasks. However, when a new task is presented to an
agent, we expect it to be able to reuse some of what it knows from previous
tasks to rapidly learn that new task. We introduce a novel technique whereby
policies for different a priori known tasks are combined into a
Mixture-of-Experts model with an attention mechanism across a mix of frozen and
unfrozen experts. The model learns when to attend to frozen task-specific
experts when appropriate and learns new experts to handle novel situations. We
work in an open-ended text-based environment in which the agent is tasked with
behaving like different types of character roles and must rapidly learn
behaviors associated with new character role types. We show that our agent both
obtains more rewards in the zero-shot setting, and discovers these rewards with
greater sample efficiency in the few-shot learning settings.

摘要：開放式世界是指其中沒有預先指定的目標或環境獎勵信號的世界。因此，代理必須知道如何執行多項任務。但是，當一個新任務呈現給代理時，我們希望它能夠重複使用它從以前任務中所知道的一些知識，以便快速學習那個新任務。我們引入了一種新技術，藉此將不同先驗已知任務的策略組合成一個專家混合模型，並在凍結和未凍結專家的組合中使用注意力機制。該模型學習在適當時機專注於凍結的特定任務專家，並學習新的專家來處理新情況。我們在一個開放式基於文本的環境中工作，在該環境中，代理被賦予扮演不同類型角色的任務，並且必須快速學習與新角色類型相關的行為。我們展示了我們的代理在零次學習設置中獲得了更多獎勵，並且在少次學習設置中以更高的樣本效率發現了這些獎勵。

##### **Large Language Models Show Human-like Social Desirability Biases in Survey Responses**
2405.06058v1 by Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, João Sedoc, Lyle H. Ungar, Johannes C. Eichstaedt

As Large Language Models (LLMs) become widely used to model and simulate
human behavior, understanding their biases becomes critical. We developed an
experimental framework using Big Five personality surveys and uncovered a
previously undetected social desirability bias in a wide range of LLMs. By
systematically varying the number of questions LLMs were exposed to, we
demonstrate their ability to infer when they are being evaluated. When
personality evaluation is inferred, LLMs skew their scores towards the
desirable ends of trait dimensions (i.e., increased extraversion, decreased
neuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,
Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent
models, with GPT-4's survey responses changing by 1.20 (human) standard
deviations and Llama 3's by 0.98 standard deviations-very large effects. This
bias is robust to randomization of question order and paraphrasing.
Reverse-coding all the questions decreases bias levels but does not eliminate
them, suggesting that this effect cannot be attributed to acquiescence bias.
Our findings reveal an emergent social desirability bias and suggest
constraints on profiling LLMs with psychometric tests and on using LLMs as
proxies for human participants.

摘要：隨著大型語言模型 (LLM) 被廣泛用於模擬和模擬
人類行為，了解其偏見變得至關重要。我們開發了一個
使用大五人格測驗的實驗框架，並發現了廣泛的 LLM 中以前未檢測到的社會期望偏差。通過
系統地改變 LLM 接觸到的問題數量，我們
證明了他們推斷自己何時被評估的能力。當
人格評估被推斷出來時，LLM 會將其得分偏向特質維度的理想終點（即，外向性增加，神經質減少
等）。這種偏差存在於所有測試模型中，包括 GPT-4/3.5、
Claude 3、Llama 3 和 PaLM-2。偏差水平似乎在更新的
模型中有所增加，GPT-4 的調查回應改變了 1.20（人類）標準
偏差和 Llama 3 的 0.98 標準偏差 - 非常大的影響。這種
偏差對問題順序和同義詞的隨機化具有魯棒性。
對所有問題進行反向編碼會降低偏差水平，但並不能消除
它們，表明這種效應不能歸因於順從偏差。
我們的發現揭示了一個新出現的社會期望偏差，並暗示
對使用心理測驗對 LLM 進行分析和使用 LLM 作為
人類參與者的代理存在限制。

##### **From Algorithm to Hardware: A Survey on Efficient and Safe Deployment of Deep Neural Networks**
2405.06038v1 by Xue Geng, Zhe Wang, Chunyun Chen, Qing Xu, Kaixin Xu, Chao Jin, Manas Gupta, Xulei Yang, Zhenghua Chen, Mohamed M. Sabry Aly, Jie Lin, Min Wu, Xiaoli Li

Deep neural networks (DNNs) have been widely used in many artificial
intelligence (AI) tasks. However, deploying them brings significant challenges
due to the huge cost of memory, energy, and computation. To address these
challenges, researchers have developed various model compression techniques
such as model quantization and model pruning. Recently, there has been a surge
in research of compression methods to achieve model efficiency while retaining
the performance. Furthermore, more and more works focus on customizing the DNN
hardware accelerators to better leverage the model compression techniques. In
addition to efficiency, preserving security and privacy is critical for
deploying DNNs. However, the vast and diverse body of related works can be
overwhelming. This inspires us to conduct a comprehensive survey on recent
research toward the goal of high-performance, cost-efficient, and safe
deployment of DNNs. Our survey first covers the mainstream model compression
techniques such as model quantization, model pruning, knowledge distillation,
and optimizations of non-linear operations. We then introduce recent advances
in designing hardware accelerators that can adapt to efficient model
compression approaches. Additionally, we discuss how homomorphic encryption can
be integrated to secure DNN deployment. Finally, we discuss several issues,
such as hardware evaluation, generalization, and integration of various
compression approaches. Overall, we aim to provide a big picture of efficient
DNNs, from algorithm to hardware accelerators and security perspectives.

摘要：深度神经網路 (DNN) 已廣泛用於許多人工智慧 (AI) 任務。然而，由於記憶體、能源和運算的龐大成本，部署它們帶來重大挑戰。為了應對這些挑戰，研究人員開發了各種模型壓縮技術，例如模型量化和模型剪枝。最近，針對壓縮方法的研究激增，以在保持效能的同時提升模型效率。此外，越來越多的工作專注於自訂 DNN 硬體加速器，以更好地利用模型壓縮技術。除了效率之外，維護安全性與隱私對於部署 DNN 至關重要。然而，龐大且多元的相關工作可能會令人不知所措。這激勵我們對近期研究進行全面調查，目標為高性能、具成本效益且安全的 DNN 部署。我們的調查首先涵蓋主流模型壓縮技術，例如模型量化、模型剪枝、知識萃取和非線性運算最佳化。然後，我們介紹設計硬體加速器的最新進展，這些加速器可以適應高效的模型壓縮方法。此外，我們討論如何整合同態加密來保護 DNN 部署。最後，我們討論幾個問題，例如硬體評估、概化和各種壓縮方法的整合。總的來說，我們旨在從演算法到硬體加速器和安全觀點提供高效 DNN 的全貌。

##### **Natural Language Processing RELIES on Linguistics**
2405.05966v1 by Juri Opitz, Shira Wein, Nathan Schneider

Large Language Models (LLMs) have become capable of generating highly fluent
text in certain languages, without modules specially designed to capture
grammar or semantic coherence. What does this mean for the future of linguistic
expertise in NLP? We highlight several aspects in which NLP (still) relies on
linguistics, or where linguistic thinking can illuminate new directions. We
argue our case around the acronym $RELIES$ that encapsulates six major facets
where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource
settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. This
list is not exhaustive, nor is linguistics the main point of reference for
every effort under these themes; but at a macro level, these facets highlight
the enduring importance of studying machine systems vis-a-vis systems of human
language.

摘要：大型語言模型 (LLM) 已具備產生高度流暢的文本的能力，而無需特別設計來捕捉語法或語義連貫性的模組。這對 NLP 中語言專業的未來有何意義？我們重點介紹了 NLP（仍然）依賴語言學的幾個方面，或語言思維可以照亮新方向的地方。我們圍繞縮寫詞 $RELIES$ 來論證我們的論點，它概括了語言學對 NLP 有所貢獻的六個主要方面：$R$esources（資源）、$E$valuation（評估）、$L$ow-resource settings（低資源設定）、$I$nterpretability（可解釋性）、$E$xplanation（說明）和 $S$tudy of language（語言研究）。此清單並非詳盡無遺，語言學也不是這些主題下每項工作的參考重點；但從巨觀層面來看，這些方面突顯了研究機器系統與人類語言系統之間的持久重要性。

##### **Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask**
2405.05959v1 by Zineb Senane, Lele Cao, Valentin Leonhard Buchner, Yusuke Tashiro, Lei You, Pawel Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim

Time Series Representation Learning (TSRL) focuses on generating informative
representations for various Time Series (TS) modeling tasks. Traditional
Self-Supervised Learning (SSL) methods in TSRL fall into four main categories:
reconstructive, adversarial, contrastive, and predictive, each with a common
challenge of sensitivity to noise and intricate data nuances. Recently,
diffusion-based methods have shown advanced generative capabilities. However,
they primarily target specific application scenarios like imputation and
forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our
work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first
diffusion-based SSL TSRL approach. TSDE segments TS data into observed and
masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It
applies a trainable embedding function, featuring dual-orthogonal Transformer
encoders with a crossover mechanism, to the observed part. We train a reverse
diffusion process conditioned on the embeddings, designed to predict noise
added to the masked part. Extensive experiments demonstrate TSDE's superiority
in imputation, interpolation, forecasting, anomaly detection, classification,
and clustering. We also conduct an ablation study, present embedding
visualizations, and compare inference speed, further substantiating TSDE's
efficiency and validity in learning representations of TS data.

摘要：時間序列表示學習 (TSRL) 專注於為各種時間序列 (TS) 建模任務產生有意義的表示。TSRL 中的傳統自監督學習 (SSL) 方法分為四種類型：重建、對抗、對比和預測，每種類型都面臨對雜訊和複雜資料細微差別敏感的共同挑戰。最近，基於擴散的方法已展現出先進的生成能力。然而，它們主要針對特定應用場景，例如填補和預測，在利用擴散模型進行通用 TSRL 方面存在差距。我們的作品時間序列擴散嵌入 (TSDE) 作為第一個基於擴散的 SSL TSRL 方法，彌補了這一差距。TSDE 使用填補-插值-預測 (IIF) 蒙版將 TS 資料分割為觀察部分和遮蔽部分。它將可訓練的嵌入函數應用到觀察部分，該函數具有帶有交叉機制的雙正交 Transformer 編碼器。我們訓練一個反向擴散過程，以嵌入為條件，旨在預測添加到遮蔽部分的雜訊。廣泛的實驗證明了 TSDE 在填補、插值、預測、異常偵測、分類和聚類方面的優越性。我們還進行了消融研究，展示了嵌入視覺化，並比較了推理速度，進一步證實了 TSDE 在學習 TS 資料表示方面的效率和有效性。

##### **OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning**
2405.05957v1 by Dan Qiao, Yi Su, Pinzheng Wang, Jing Ye, Wenjing Xie, Yuechi Zhou, Yuyang Ding, Zecheng Tang, Jikai Wang, Yixin Ji, Yue Wang, Pei Guo, Zechen Sun, Zikang Zhang, Juntao Li, Pingfu Chao, Wenliang Chen, Guohong Fu, Guodong Zhou, Qiaoming Zhu, Min Zhang

Large Language Models (LLMs) have played an important role in many fields due
to their powerful capabilities.However, their massive number of parameters
leads to high deployment requirements and incurs significant inference costs,
which impedes their practical applications. Training smaller models is an
effective way to address this problem. Therefore, we introduce OpenBA-V2, a
3.4B model derived from multi-stage compression and continual pre-training from
the original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible
training objectives, and techniques such as layer pruning, neural pruning, and
vocabulary pruning to achieve a compression rate of 77.3\% with minimal
performance loss. OpenBA-V2 demonstrates competitive performance compared to
other open-source models of similar size, achieving results close to or on par
with the 15B OpenBA model in downstream tasks such as common sense reasoning
and Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be
compressed into smaller ones with minimal performance loss by employing
advanced training objectives and data strategies, which may help deploy LLMs in
resource-limited scenarios.

摘要：大型語言模型 (LLM) 由於其強大的功能，在許多領域中扮演著重要的角色。然而，其龐大的參數量導致了高部署需求，並產生顯著的推論成本，這阻礙了其實際應用。訓練較小的模型是解決此問題的有效方法。因此，我們引入了 OpenBA-V2，一個從原始 15B OpenBA 模型的多階段壓縮和持續預訓練衍生的 3.4B 模型。OpenBA-V2 利用更多數據、更靈活的訓練目標，以及層修剪、神經修剪和詞彙修剪等技術，以最小的效能損失實現 77.3% 的壓縮率。與其他類似規模的開源模型相比，OpenBA-V2 表現出競爭力的效能，在常識推理和命名實體識別 (NER) 等下游任務中，獲得接近或等同於 15B OpenBA 模型的結果。OpenBA-V2 說明了 LLM 可以透過採用進階訓練目標和數據策略，壓縮成較小的模型，且效能損失最小，這有助於在資源受限的情況下部署 LLM。

##### **Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning**
2405.05955v1 by Junzhi Chen, Juhao Liang, Benyou Wang

The emergence of large language models (LLMs) has opened up unprecedented
possibilities for automating complex tasks that are often comparable to human
performance. Despite their capabilities, LLMs still encounter difficulties in
completing tasks that require high levels of accuracy and complexity due to
their inherent limitations in handling multifaceted problems single-handedly.
This paper introduces "Smurfs", a cutting-edge multi-agent framework designed
to revolutionize the application of LLMs. By transforming a conventional LLM
into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and
execution without necessitating extra training. This is achieved through
innovative prompting strategies that allocate distinct roles within the model,
thereby facilitating collaboration among specialized agents. The framework
gives access to external tools to efficiently solve complex tasks. Our
empirical investigation, featuring the mistral-7b-instruct model as a case
study, showcases Smurfs' superior capability in intricate tool utilization
scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and
I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded
performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive
ablation studies, we dissect the contribution of the core components of the
multi-agent framework to its overall efficacy. This not only verifies the
effectiveness of the framework, but also sets a route for future exploration of
multi-agent LLM systems.

摘要：大型語言模型 (LLM) 的出現為自動化複雜任務開闢了前所未有的可能性，這些任務通常與人類的表現相當。儘管有這些能力，但 LLM 在完成需要高準確度和複雜性的任務時仍會遇到困難，因為它們在單獨處理多方面的問題時有其固有的限制。本文介紹了「Smurfs」，這是一個旨在革新 LLM 應用程序的尖端多代理架構。透過將傳統的 LLM 轉變為協同的多代理整體，Smurfs 增強了任務分解和執行，而無需額外的訓練。這是透過創新的提示策略實現的，這些策略在模型中分配不同的角色，從而促進專業代理之間的協作。該框架允許使用外部工具來有效地解決複雜任務。我們的實證調查以 mistral-7b-instruct 模型為案例研究，展示了 Smurfs 在複雜工具使用場景中的卓越能力。值得注意的是，Smurfs 在 ToolBench I2 和 I3 基準測試中以 84.4% 的勝率擊敗 ChatGPT-ReACT，超越了 GPT-4 模型創下的 73.5% 的最高記錄。此外，透過全面的消融研究，我們剖析了多代理架構的核心組成部分對其整體效能的貢獻。這不僅驗證了該架構的有效性，也為未來探索多代理 LLM 系統設定了一個路線。

##### **DOLOMITES: Domain-Specific Long-Form Methodical Tasks**
2405.05938v1 by Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, Chris Alberti

Experts in various fields routinely perform methodical writing tasks to plan,
organize, and report their work. From a clinician writing a differential
diagnosis for a patient, to a teacher writing a lesson plan for students, these
tasks are pervasive, requiring to methodically generate structured long-form
output for a given input. We develop a typology of methodical tasks structured
in the form of a task objective, procedure, input, and output, and introduce
DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited
from hundreds of experts from across 25 fields. Our benchmark further contains
specific instantiations of methodical tasks with concrete input and output
examples (1,857 in total) which we obtain by collecting expert revisions of up
to 10 model-generated examples of each task. We use these examples to evaluate
contemporary language models highlighting that automating methodical tasks is a
challenging long-form generation problem, as it requires performing complex
inferences, while drawing upon the given context as well as domain knowledge.

摘要：各領域的專家例行執行有條理的寫作任務，以規劃、組織和報告其工作。從臨床醫生為患者撰寫鑑別診斷，到教師為學生撰寫課程計畫，這些任務無所不在，需要有條理地為給定的輸入產生結構化的長篇輸出。我們開發了一種方法任務類型學，以任務目標、程序、輸入和輸出的形式進行結構化，並介紹 DoLoMiTes，這是一個新的基準，其中包含從 25 個領域的數百名專家獲得的 519 項此類任務的規格。我們的基準進一步包含具體方法任務的具體實例，以及具體的輸入和輸出範例（總計 1,857 個），我們透過收集每個任務最多 10 個模型生成範例的專家修訂來取得這些範例。我們使用這些範例來評估當代語言模型，強調自動化方法任務是一個具有挑戰性的長篇生成問題，因為它需要執行複雜的推論，同時利用給定的背景以及領域知識。

##### **Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness**
2405.05930v1 by Siyuan Li, Xi Lin, Yaju Liu, Jianhua Li

AI-generated content (AIGC) models, represented by large language models
(LLM), have brought revolutionary changes to the content generation fields. The
high-speed and extensive 6G technology is an ideal platform for providing
powerful AIGC mobile service applications, while future 6G mobile networks also
need to support intelligent and personalized mobile generation services.
However, the significant ethical and security issues of current AIGC models,
such as adversarial attacks, privacy, and fairness, greatly affect the
credibility of 6G intelligent networks, especially in ensuring secure, private,
and fair AIGC applications. In this paper, we propose TrustGAIN, a novel
paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale
AIGC services in future 6G networks. We first discuss the adversarial attacks
and privacy threats faced by AIGC systems in 6G networks, as well as the
corresponding protection issues. Subsequently, we emphasize the importance of
ensuring the unbiasedness and fairness of the mobile generative service in
future intelligent networks. In particular, we conduct a use case to
demonstrate that TrustGAIN can effectively guide the resistance against
malicious or generated false information. We believe that TrustGAIN is a
necessary paradigm for intelligent and trustworthy 6G networks to support AIGC
services, ensuring the security, privacy, and fairness of AIGC network
services.

摘要：由大型語言模型 (LLM) 所代表的人工智慧生成內容 (AIGC) 模型，為內容生成領域帶來了革命性的變化。高速且廣泛的 6G 技術是提供強大 AIGC 行動服務應用程式的理想平台，而未來的 6G 行動網路也需要支援智慧且個人化的行動生成服務。然而，當前 AIGC 模型的重大道德和安全性問題，例如對抗性攻擊、隱私和公平性，極大地影響了 6G 智慧網路的可信度，特別是在確保安全的、私密的和公平的 AIGC 應用程式方面。在本文中，我們提出了 TrustGAIN，這是一種在 6G 網路中值得信賴的 AIGC 新典範，以確保未來 6G 網路中值得信賴的大規模 AIGC 服務。我們首先討論了 6G 網路中 AIGC 系統面臨的對抗性攻擊和隱私威脅，以及相應的保護問題。隨後，我們強調了確保未來智慧網路中行動生成服務的公正性和公平性的重要性。特別是，我們進行了一個用例，以展示 TrustGAIN 可以有效地指導對抗惡意或生成的虛假資訊。我們相信 TrustGAIN 是智慧且值得信賴的 6G 網路支援 AIGC 服務的必要典範，確保 AIGC 網路服務的安全性、隱私和公平性。

##### **FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting**
2405.05925v1 by Xiaohui Zhong, Lei Chen, Hao Li, Jie Feng, Bo Lu

Ensemble weather forecasting is essential for weather predictions and
mitigating the impacts of extreme weather events. Constructing an ensemble
prediction system (EPS) based on conventional numerical weather prediction
(NWP) models is highly computationally expensive. Machine learning (ML) models
have emerged as valuable tools for deterministic weather forecasts, providing
forecasts with significantly reduced computational requirements and even
surpassing the forecast performance of traditional NWP models. However,
challenges arise when applying ML models to ensemble forecasting. Recent ML
models, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of Data
Assimilations (EDA) or two operational NWP ensemble members for forecast
generation. The spatial resolution of 1{\deg} or 2{\deg} in these models is
often considered too coarse for many applications. To overcome these
limitations, we introduce FuXi-ENS, an advanced ML model designed to deliver
6-hourly global ensemble weather forecasts up to 15 days. This model runs at a
significantly improved spatial resolution of 0.25{\deg}, incorporating 5
upper-air atmospheric variables at 13 pressure levels, along with 13 surface
variables. By leveraging the inherent probabilistic nature of Variational
AutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines the
continuous ranked probability score (CRPS) and the KL divergence between the
predicted and target distribution. This innovative approach represents an
advancement over the traditional use of L1 loss combined with the KL loss in
standard VAE models when VAE for ensemble weather forecasts. Evaluation results
demonstrate that FuXi-ENS outperforms ensemble forecasts from the European
Centre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model,
on 98.1% of 360 variable and forecast lead time combinations on CRPS.

摘要：集合天氣預測對於天氣預測和減輕極端天氣事件的影響至關重要。基於傳統數值天氣預測 (NWP) 模型構建集合預測系統 (EPS) 在計算上非常昂貴。機器學習 (ML) 模型已成為確定性天氣預測的寶貴工具，提供計算需求大幅降低的預測，甚至超越傳統 NWP 模型的預測性能。然而，將 ML 模型應用於集合預測時會出現挑戰。最近的 ML 模型，例如 GenCast 和 SEEDS 模型，依賴於 ERA5 資料同化集合 (EDA) 或兩個運算 NWP 集合成員進行預測生成。這些模型中 1{\deg} 或 2{\deg} 的空間解析度通常被認為對於許多應用而言過於粗糙。為了克服這些限制，我們引入了 FuXi-ENS，這是一個先進的 ML 模型，旨在提供長達 15 天的 6 小時全球集合天氣預測。此模型以顯著改善的 0.25{\deg} 空間解析度運行，結合了 13 個壓力層的 5 個高空大氣變數，以及 13 個地表變數。通過利用變分自動編碼器 (VAE) 固有的機率性質，FuXi-ENS 最佳化了結合連續等級機率分數 (CRPS) 和預測分佈與目標分佈之間的 KL 散度的損失函數。這種創新方法代表了在 VAE 用於集合天氣預測時，對標準 VAE 模型中 L1 損失與 KL 損失的傳統使用的一種進步。評估結果表明，FuXi-ENS 在 CRPS 上勝過歐洲中期天氣預報中心 (ECMWF) 的集合預測，後者是世界領先的 NWP 模型，在 360 個變數和預測提前時間組合中的 98.1%。

##### **Diag2Diag: Multi modal super resolution for physics discovery with application to fusion**
2405.05908v1 by Azarakhsh Jalalvand, Max Curie, SangKyeun Kim, Peter Steiner, Jaemin Seo, Qiming Hu, Andrew Oakleigh Nelson, Egemen Kolemen

This paper introduces a groundbreaking multi-modal neural network model
designed for resolution enhancement, which innovatively leverages
inter-diagnostic correlations within a system. Traditional approaches have
primarily focused on uni-modal enhancement strategies, such as pixel-based
image enhancement or heuristic signal interpolation. In contrast, our model
employs a novel methodology by harnessing the diagnostic relationships within
the physics of fusion plasma. Initially, we establish the correlation among
diagnostics within the tokamak. Subsequently, we utilize these correlations to
substantially enhance the temporal resolution of the Thomson Scattering
diagnostic, which assesses plasma density and temperature. By increasing its
resolution from conventional 200Hz to 500kHz, we facilitate a new level of
insight into plasma behavior, previously attainable only through
computationally intensive simulations. This enhancement goes beyond simple
interpolation, offering novel perspectives on the underlying physical phenomena
governing plasma dynamics.

摘要：本文介紹了一種創新的多模態神經網路模型，專為解析度提升而設計，它創新地利用系統內的診斷間相關性。傳統方法主要集中於單模態增強策略，例如基於像素的影像增強或啟發式訊號插值。相反地，我們的模型採用一種新的方法，利用融合電漿物理學中的診斷關係。最初，我們在托卡馬克內建立診斷之間的相關性。隨後，我們利用這些相關性來大幅提升湯姆森散射診斷的時間解析度，該診斷評估電漿密度和溫度。透過將其解析度從傳統的 200Hz 提升至 500kHz，我們促成對電漿行為的新層次見解，以前只能透過計算密集型模擬才能達成。這種增強超越了單純的插值，提供了對支配電漿動態的基礎物理現象的新觀點。

##### **Truthful Aggregation of LLMs with an Application to Online Advertising**
2405.05905v1 by Ermis Soumalias, Michael J. Curry, Sven Seuken

We address the challenge of aggregating the preferences of multiple agents
over LLM-generated replies to user queries, where agents might modify or
exaggerate their preferences. New agents may participate for each new query,
making fine-tuning LLMs on these preferences impractical. To overcome these
challenges, we propose an auction mechanism that operates without fine-tuning
or access to model weights. This mechanism is designed to provably converge to
the ouput of the optimally fine-tuned LLM as computational resources are
increased. The mechanism can also incorporate contextual information about the
agents when avaiable, which significantly accelerates its convergence. A
well-designed payment rule ensures that truthful reporting is the optimal
strategy for all agents, while also promoting an equity property by aligning
each agent's utility with her contribution to social welfare - an essential
feature for the mechanism's long-term viability. While our approach can be
applied whenever monetary transactions are permissible, our flagship
application is in online advertising. In this context, advertisers try to steer
LLM-generated responses towards their brand interests, while the platform aims
to maximize advertiser value and ensure user satisfaction. Experimental results
confirm that our mechanism not only converges efficiently to the optimally
fine-tuned LLM but also significantly boosts advertiser value and platform
revenue, all with minimal computational overhead.

摘要：<paragraph>我們解決了匯總多個代理對 LLM 生成的使用者查詢回覆的偏好的挑戰，其中代理可能會修改或誇大其偏好。新的代理可能會參與每個新的查詢，這使得對這些偏好進行微調 LLM 變得不切實際。為了克服這些挑戰，我們提出了一種拍賣機制，它在不微調或訪問模型權重的條件下運作。此機制旨在隨著計算資源的增加，可證明地收斂到經過最佳微調的 LLM 的輸出。該機制還可以納入有關代理的上下文訊息（如果可用），這會顯著加速其收斂。設計良好的支付規則確保如實報告是所有代理的最佳策略，同時還通過將每個代理的效用與其對社會福利的貢獻相一致來促進公平屬性——這是該機制長期可行性的基本特徵。儘管我們的方法可以在允許金錢交易時應用，但我們的旗艦應用是在線上廣告中。在此背景下，廣告客戶試圖將 LLM 生成的回應引導至其品牌利益，而平台旨在最大化廣告客戶價值並確保使用者滿意度。實驗結果證實，我們的機制不僅有效地收斂到經過最佳微調的 LLM，而且還顯著提升了廣告客戶價值和平台收益，所有這些都以最小的計算開銷實現。</paragraph>

##### **Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?**
2405.05904v1 by Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig

When large language models are aligned via supervised fine-tuning, they may
encounter new factual information that was not acquired through pre-training.
It is often conjectured that this can teach the model the behavior of
hallucinating factually incorrect responses, as the model is trained to
generate facts that are not grounded in its pre-existing knowledge. In this
work, we study the impact of such exposure to new knowledge on the capability
of the fine-tuned model to utilize its pre-existing knowledge. To this end, we
design a controlled setup, focused on closed-book QA, where we vary the
proportion of the fine-tuning examples that introduce new knowledge. We
demonstrate that large language models struggle to acquire new factual
knowledge through fine-tuning, as fine-tuning examples that introduce new
knowledge are learned significantly slower than those consistent with the
model's knowledge. However, we also find that as the examples with new
knowledge are eventually learned, they linearly increase the model's tendency
to hallucinate. Taken together, our results highlight the risk in introducing
new factual knowledge through fine-tuning, and support the view that large
language models mostly acquire factual knowledge through pre-training, whereas
fine-tuning teaches them to use it more efficiently.

摘要：當大型語言模型透過監督式微調來對齊時，可能會遇到在預訓練中未獲得的新事實資訊。人們經常猜測，這可能會教導模型虛構事實上不正確的回應的行為，因為該模型經過訓練可以產生不基於其既有知識的事實。在這項工作中，我們研究了這種接觸新知識對微調模型利用其既有知識的能力的影響。為此，我們設計了一個受控的設定，專注於閉卷問答，其中我們改變了引入新知識的微調範例的比例。我們證明了大型語言模型難以透過微調來獲得新的事實知識，因為引入新知識的微調範例的學習速度顯著低於與模型知識一致的範例。然而，我們也發現，隨著具有新知識的範例最終被學習，它們會線性增加模型虛構的趨勢。綜合起來，我們的結果突顯了透過微調引入新的事實知識的風險，並支持這樣一種觀點，即大型語言模型主要透過預訓練來獲得事實知識，而微調則教導它們更有效地使用它。

##### **Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons**
2405.05894v1 by Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales

LLM-as-a-judge approaches are a practical and effective way of assessing a
range of text tasks, aligning with human judgements especially when applied in
a comparative assessment fashion. However, when using pairwise comparisons to
rank a set of candidates the computational costs scale quadratically with the
number of candidates, which can have practical limitations. This paper
introduces a Product of Expert (PoE) framework for efficient LLM Comparative
Assessment. Here individual comparisons are considered experts that provide
information on a pair's score difference. The PoE framework combines the
information from these experts to yield an expression that can be maximized
with respect to the underlying set of candidates, and is highly flexible where
any form of expert can be assumed. When Gaussian experts are used one can
derive simple closed-form solutions for the optimal candidate ranking, as well
as expressions for selecting which comparisons should be made to maximize the
probability of this ranking. Our approach enables efficient comparative
assessment, where by using only a small subset of the possible comparisons, one
can generate score predictions that correlate as well to human judgements as
the predictions when all comparisons are used. We evaluate the approach on
multiple NLG tasks and demonstrate that our framework can yield considerable
computational savings when performing pairwise comparative assessment. When N
is large, with as few as 2% of comparisons the PoE solution can achieve similar
performance to when all comparisons are used.

摘要：LLM-as-a-judge 方法是一種評估各種文本任務的實用且有效的方法，特別是在以比較評估方式應用時，與人類判斷一致。然而，當使用成對比較對一組候選者進行排名時，計算成本會隨著候選者數量呈二次方擴展，這可能會帶來實際限制。本文介紹了一個專家產品 (PoE) 框架，用於高效的 LLM 比較評估。在此，個別比較被視為專家，提供有關一對分數差異的資訊。PoE 框架結合了這些專家的資訊，產生了一個表達式，可以針對基礎候選者集合進行最大化，並且非常靈活，可以假設任何形式的專家。當使用高斯專家時，可以為最佳候選者排名推導出簡單的閉合形式解，以及用於選擇哪些比較以最大化此排名的機率的表達式。我們的做法可以進行有效的比較評估，只需使用可能比較的一小部分子集，就可以產生與人類判斷相關的評分預測，就像使用所有比較時的預測一樣。我們在多個 NLG 任務上評估了這種方法，並證明了我們的框架在執行成對比較評估時可以產生大量的計算節省。當 N 很大的時候，PoE 解決方案只需 2% 的比較次數，就能達到與使用所有比較次數時類似的效能。

##### **Safe Exploration Using Bayesian World Models and Log-Barrier Optimization**
2405.05890v1 by Yarden As, Bhavya Sukhija, Andreas Krause

A major challenge in deploying reinforcement learning in online tasks is
ensuring that safety is maintained throughout the learning process. In this
work, we propose CERL, a new method for solving constrained Markov decision
processes while keeping the policy safe during learning. Our method leverages
Bayesian world models and suggests policies that are pessimistic w.r.t. the
model's epistemic uncertainty. This makes CERL robust towards model
inaccuracies and leads to safe exploration during learning. In our experiments,
we demonstrate that CERL outperforms the current state-of-the-art in terms of
safety and optimality in solving CMDPs from image observations.

摘要：在線上任務中部署強化學習的一項重大挑戰是確保在整個學習過程中維持安全性。在此工作中，我們提出 CERL，一種在學習過程中保持策略安全性的約束馬可夫決策過程求解新方法。我們的模型利用貝氏世界模型，並提出對模型認識不確定性持悲觀態度的策略。這使得 CERL 對模型不準確性具有魯棒性，並在學習過程中進行安全探索。在我們的實驗中，我們證明 CERL 在從影像觀察中解決 CMDP 的安全性和最佳性方面優於當前的最新技術。

##### **EWMoE: An effective model for global weather forecasting with mixture-of-experts**
2405.06004v1 by Lihao Gan, Xin Man, Chenghong Zhang, Jie Shao

Weather forecasting is a crucial task for meteorologic research, with direct
social and economic impacts. Recently, data-driven weather forecasting models
based on deep learning have shown great potential, achieving superior
performance compared with traditional numerical weather prediction methods.
However, these models often require massive training data and computational
resources. In this paper, we propose EWMoE, an effective model for accurate
global weather forecasting, which requires significantly less training data and
computational resources. Our model incorporates three key components to enhance
prediction accuracy: meteorology-specific embedding, a core Mixture-of-Experts
(MoE) layer, and two specific loss functions. We conduct our evaluation on the
ERA5 dataset using only two years of training data. Extensive experiments
demonstrate that EWMoE outperforms current models such as FourCastNet and
ClimaX at all forecast time, achieving competitive performance compared with
the state-of-the-art Pangu-Weather model in evaluation metrics such as Anomaly
Correlation Coefficient (ACC) and Root Mean Square Error (RMSE). Additionally,
ablation studies indicate that applying the MoE architecture to weather
forecasting offers significant advantages in improving accuracy and resource
efficiency.

摘要：天氣預測是氣象研究中的關鍵任務，具有直接的社會和經濟影響。最近，基於深度學習的數據驅動天氣預測模型展現出巨大的潛力，與傳統數值天氣預測方法相比，取得了卓越的性能。然而，這些模型通常需要大量的訓練數據和計算資源。在本文中，我們提出 EWMoE，一個用於準確全球天氣預測的有效模型，它需要顯著更少的訓練數據和計算資源。我們的模型包含三個關鍵組成部分以增強預測準確性：特定於氣象學的嵌入、核心混合專家 (MoE) 層和兩個特定損失函數。我們僅使用兩年的訓練數據在 ERA5 數據集上進行評估。大量的實驗表明，EWMoE 在所有預測時間都優於 FourCastNet 和 ClimaX 等當前模型，在異常相關係數 (ACC) 和均方根誤差 (RMSE) 等評估指標中取得了與最先進的 Pangu-Weather 模型相當的性能。此外，消融研究表明，將 MoE 架構應用於天氣預測在提高準確性和資源效率方面提供了顯著優勢。

##### **Composable Part-Based Manipulation**
2405.05876v1 by Weiyu Liu, Jiayuan Mao, Joy Hsu, Tucker Hermans, Animesh Garg, Jiajun Wu

In this paper, we propose composable part-based manipulation (CPM), a novel
approach that leverages object-part decomposition and part-part correspondences
to improve learning and generalization of robotic manipulation skills. By
considering the functional correspondences between object parts, we
conceptualize functional actions, such as pouring and constrained placing, as
combinations of different correspondence constraints. CPM comprises a
collection of composable diffusion models, where each model captures a
different inter-object correspondence. These diffusion models can generate
parameters for manipulation skills based on the specific object parts.
Leveraging part-based correspondences coupled with the task decomposition into
distinct constraints enables strong generalization to novel objects and object
categories. We validate our approach in both simulated and real-world
scenarios, demonstrating its effectiveness in achieving robust and generalized
manipulation capabilities.

摘要：在本文中，我們提出可組成部分式操作 (CPM)，這是一種新穎的方法，它利用物件部分分解和部分對應來改善機器人操作技能的學習和概化。透過考量物件部分之間的功能對應，我們將功能動作（例如倒水和受限放置）概念化為不同對應約束的組合。CPM 包含一組可組成的擴散模型，其中每個模型都擷取不同的物件間對應。這些擴散模型可根據特定的物件部分產生操作技能的參數。利用部分對應以及將任務分解為不同約束，能對新物件和物件類別進行強大的概化。我們在模擬和真實世界的場景中驗證了我們的做法，證明了其在達成穩健且概化的操作能力方面的有效性。

##### **ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers**
2405.05861v1 by Liangliang Chen, Shiyu Jin, Haoyu Wang, Liangjun Zhang

Excavators are crucial for diverse tasks such as construction and mining,
while autonomous excavator systems enhance safety and efficiency, address labor
shortages, and improve human working conditions. Different from the existing
modularized approaches, this paper introduces ExACT, an end-to-end autonomous
excavator system that processes raw LiDAR, camera data, and joint positions to
control excavator valves directly. Utilizing the Action Chunking with
Transformers (ACT) architecture, ExACT employs imitation learning to take
observations from multi-modal sensors as inputs and generate actionable
sequences. In our experiment, we build a simulator based on the captured
real-world data to model the relations between excavator valve states and joint
velocities. With a few human-operated demonstration data trajectories, ExACT
demonstrates the capability of completing different excavation tasks, including
reaching, digging and dumping through imitation learning in validations with
the simulator. To the best of our knowledge, ExACT represents the first
instance towards building an end-to-end autonomous excavator system via
imitation learning methods with a minimal set of human demonstrations. The
video about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.

摘要：挖掘機對於建築和採礦等各種任務至關重要，而自動化挖掘機系統則可增進安全性與效率，解決勞動力短缺問題，並改善人類的工作條件。與現有的模組化方法不同，本文介紹了 ExACT，這是一個端對端的自動化挖掘機系統，可處理原始 LiDAR、相機資料和接合位置，以直接控制挖掘機閥門。ExACT 利用動作分塊與 Transformer (ACT) 架構，採用模仿學習，將來自多模式感測器的觀察結果作為輸入，並產生可操作的序列。在我們的實驗中，我們根據擷取的真實世界資料建立了一個模擬器，以模擬挖掘機閥門狀態與接合速度之間的關係。透過一些由人類操作的示範資料軌跡，ExACT 證明了其完成不同挖掘任務的能力，包括透過模擬器中的模仿學習來進行觸及、挖掘和傾倒。據我們所知，ExACT 代表了透過模仿學習方法，使用最少的人類示範來建立端對端自動化挖掘機系統的第一個實例。可以在 https://youtu.be/NmzR_Rf-aEk 上觀看有關這項工作的影片。

##### **Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control**
2405.05852v1 by Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner

Embodied AI agents require a fine-grained understanding of the physical world
mediated through visual and language inputs. Such capabilities are difficult to
learn solely from task-specific data. This has led to the emergence of
pre-trained vision-language models as a tool for transferring representations
learned from internet-scale data to downstream tasks and new domains. However,
commonly used contrastively trained representations such as in CLIP have been
shown to fail at enabling embodied agents to gain a sufficiently fine-grained
scene understanding -- a capability vital for control. To address this
shortcoming, we consider representations from pre-trained text-to-image
diffusion models, which are explicitly optimized to generate images from text
prompts and as such, contain text-conditioned representations that reflect
highly fine-grained visuo-spatial information. Using pre-trained text-to-image
diffusion models, we construct Stable Control Representations which allow
learning downstream control policies that generalize to complex, open-ended
environments. We show that policies learned using Stable Control
Representations are competitive with state-of-the-art representation learning
approaches across a broad range of simulated control settings, encompassing
challenging manipulation and navigation tasks. Most notably, we show that
Stable Control Representations enable learning policies that exhibit
state-of-the-art performance on OVMM, a difficult open-vocabulary navigation
benchmark.

摘要：具身人工智慧代理需要透過視覺和語言輸入，對物理世界有細緻的理解。這種能力很難僅從特定任務的資料中學習。這導致了預先訓練的視覺語言模型的出現，作為將從網路規模資料中學習到的表徵轉移到下游任務和新領域的工具。然而，已顯示出像 CLIP 中常用的對比訓練表徵，無法讓具身代理獲得足夠細緻的場景理解，而這對於控制來說是至關重要的能力。為了解決這個缺點，我們考慮了預先訓練的文字到影像擴散模型的表徵，這些表徵經過明確最佳化，以便從文字提示中產生影像，因此包含反映高度細緻視覺空間資訊的文字條件表徵。使用預先訓練的文字到影像擴散模型，我們建構了穩定的控制表徵，允許學習下游控制政策，並推廣到複雜的、開放式的環境。我們展示了使用穩定的控制表徵學習的政策，在廣泛的模擬控制設定中與最先進的表徵學習方法競爭，包括具有挑戰性的操作和導航任務。最值得注意的是，我們展示了穩定的控制表徵能讓學習政策在 OVMM 上展現最先進的效能，OVMM 是一個困難的開放式詞彙導航基準。

##### **Aequitas Flow: Streamlining Fair ML Experimentation**
2405.05809v1 by Sérgio Jesus, Pedro Saleiro, Inês Oliveira e Silva, Beatriz M. Jorge, Rita P. Ribeiro, João Gama, Pedro Bizarro, Rayid Ghani

Aequitas Flow is an open-source framework for end-to-end Fair Machine
Learning (ML) experimentation in Python. This package fills the existing
integration gaps in other Fair ML packages of complete and accessible
experimentation. It provides a pipeline for fairness-aware model training,
hyperparameter optimization, and evaluation, enabling rapid and simple
experiments and result analysis. Aimed at ML practitioners and researchers, the
framework offers implementations of methods, datasets, metrics, and standard
interfaces for these components to improve extensibility. By facilitating the
development of fair ML practices, Aequitas Flow seeks to enhance the adoption
of these concepts in AI technologies.

摘要：Aequitas Flow 是一個開放原始碼架構，用於端對端的公平機器學習 (ML) 實驗，以 Python 編寫。此套件填補了其他公平 ML 套件中現有的完整且可存取實驗整合差距。它提供了一個公平模型訓練、超參數最佳化和評估的管道，能進行快速且簡單的實驗和結果分析。此架構專為 ML 實務工作者和研究人員設計，提供方法、資料集、指標和這些元件的標準介面的實作，以改善可擴充性。透過促進公平 ML 實務的發展，Aequitas Flow 旨在提升這些概念在 AI 技術中的採用率。

##### **Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference**
2405.05803v1 by Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji

Multimodal large language models (MLLMs) demand considerable computations for
inference due to the extensive parameters and the additional input tokens
needed for visual information representation. Herein, we introduce Visual
Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid
inference. Our approach is inspired by two intriguing phenomena we have
observed: (1) the attention sink phenomenon that is prevalent in LLMs also
persists in MLLMs, suggesting that initial tokens and nearest tokens receive
the majority of attention, while middle vision tokens garner minimal attention
in deep layers; (2) the presence of information migration, which implies that
visual information is transferred to subsequent text tokens within the first
few layers of MLLMs. As per our findings, we conclude that vision tokens are
not necessary in the deep layers of MLLMs. Thus, we strategically withdraw them
at a certain layer, enabling only text tokens to engage in subsequent layers.
To pinpoint the ideal layer for vision tokens withdrawal, we initially analyze
a limited set of tiny datasets and choose the first layer that meets the
Kullback-Leibler divergence criterion. Our VTW approach can cut computational
overhead by over 40\% across diverse multimodal tasks while maintaining
performance. Our code is released at https://github.com/lzhxmu/VTW.

摘要：多模态大型语言模型 (MLLM) 由于广泛的参数和视觉信息表示所需的附加输入标记，因此需要大量计算才能进行推理。在此，我们介绍视觉标记撤回 (VTW)，这是一个即插即用模块，用于提升 MLLM 以进行快速推理。我们的方法受到我们观察到的两个有趣的现象的启发：(1) 在 LLM 中普遍存在的注意力汇集现象也存在于 MLLM 中，表明初始标记和最近标记接收了大部分注意力，而中间视觉标记在深层中获得的注意力最少；(2) 信息迁移的存在，这意味着视觉信息在 MLLM 的前几层内被传输到后续文本标记。根据我们的发现，我们得出结论，视觉标记在 MLLM 的深层中是不必要的。因此，我们在某一层对其进行战略性撤回，仅允许文本标记参与后续层。为了精确定位视觉标记撤回的理想层，我们最初分析了一组有限的小型数据集，并选择了满足 Kullback-Leibler 散度准则的第一层。我们的 VTW 方法可以在保持性能的同时，在各种多模态任务中将计算开销减少 40% 以上。我们的代码已在 https://github.com/lzhxmu/VTW 中发布。

##### **RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation**
2405.05792v1 by Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Sünderhauf, Feras Dayoub, Ian Reid

Mapping is crucial for spatial reasoning, planning and robot navigation.
Existing approaches range from metric, which require precise geometry-based
optimization, to purely topological, where image-as-node based graphs lack
explicit object-level reasoning and interconnectivity. In this paper, we
propose a novel topological representation of an environment based on "image
segments", which are semantically meaningful and open-vocabulary queryable,
conferring several advantages over previous works based on pixel-level
features. Unlike 3D scene graphs, we create a purely topological graph with
segments as nodes, where edges are formed by a) associating segment-level
descriptors between pairs of consecutive images and b) connecting neighboring
segments within an image using their pixel centroids. This unveils a
"continuous sense of a place", defined by inter-image persistence of segments
along with their intra-image neighbours. It further enables us to represent and
update segment-level descriptors through neighborhood aggregation using graph
convolution layers, which improves robot localization based on segment-level
retrieval. Using real-world data, we show how our proposed map representation
can be used to i) generate navigation plans in the form of "hops over segments"
and ii) search for target objects using natural language queries describing
spatial relations of objects. Furthermore, we quantitatively analyze data
association at the segment level, which underpins inter-image connectivity
during mapping and segment-level localization when revisiting the same place.
Finally, we show preliminary trials on segment-level `hopping' based zero-shot
real-world navigation. Project page with supplementary details:
oravus.github.io/RoboHop/

摘要：<paragraph>繪製地圖對於空間推理、規劃和機器人導航至關重要。
現有方法從度量（需要精確的基於幾何的最佳化）到純拓撲（其中以影像為節點的圖形缺乏明確的物件層級推理和互連性）不等。在本文中，我們提出一個基於「影像區段」的新穎拓撲表示，這些區段在語意上是有意義的，並且可以進行開放式詞彙查詢，與基於像素層級特徵的先前工作相比，具有多項優點。與 3D 場景圖形不同，我們使用區段作為節點建立一個純拓撲圖形，其中邊緣是由 a) 將區段層級描述符與連續兩張影像之間的配對關聯起來，以及 b) 使用其像素質心連接影像中的鄰近區段所形成的。這揭示了一個「連續的地方感」，它是由區段在影像之間的持續性及其在影像內的鄰近元素所定義的。它進一步使我們能夠使用圖形卷積層透過鄰域聚合來表示和更新區段層級描述符，這改進了基於區段層級擷取的機器人定位。使用真實世界資料，我們展示了我們提出的地圖表示如何用於 i) 以「區段跳躍」的形式產生導航計畫，以及 ii) 使用描述物件空間關係的自然語言查詢來搜尋目標物件。此外，我們定量分析了區段層級的資料關聯性，這在繪製地圖期間支撐了影像之間的連接性，以及在再次造訪同一個地方時的區段層級定位。最後，我們展示了基於區段層級「跳躍」的零次學習真實世界導航的初步試驗。包含補充詳細資訊的專案頁面：
oravus.github.io/RoboHop/</paragraph>

##### **A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties**
2405.05790v1 by A. Noroozi, M. Ravan, B. Razavi, R. S. Fisher, Y. Law, M. S. Hasan

In this paper, we present a robust version of the well-known exact
low-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA,
to localize brain sources in the presence of different forward model
uncertainties. Methods: We first assume that the true lead field matrix is a
transformation of the existing lead field matrix distorted by uncertainties and
propose an iterative approach to estimate this transformation accurately. Major
sources of the forward model uncertainties, including differences in geometry,
conductivity, and source space resolution between the real and simulated head
models, and misaligned electrode positions, are then simulated to test the
proposed method. Results: ReLORETA and eLORETA are applied to simulated focal
sources in different regions of the brain and the presence of various noise
levels as well as real data from a patient with focal epilepsy. The results
show that ReLORETA is considerably more robust and accurate than eLORETA in all
cases. Conclusion: Having successfully dealt with the forward model
uncertainties, ReLORETA proved to be a promising method for real-world clinical
applications. Significance: eLORETA is one of the localization techniques that
could be used to study brain activity for medical applications such as
determining the epileptogenic zone in patients with medically refractory
epilepsy. However, the major limitation of eLORETA is sensitivity to the
uncertainties in the forward model. Since this problem can substantially
undermine its performance in real-world applications where the exact lead field
matrix is unknown, developing a more robust method capable of dealing with
these uncertainties is of significant interest.

摘要：<paragraph>在本文中，我們提出了一種強健版的著名精確低解析度電磁斷層掃描 (eLORETA) 技術，名為 ReLORETA，用於定位存在不同正向模型不確定性的腦部來源。方法：我們首先假設真實導線場矩陣是不確定性扭曲的現有導線場矩陣的轉換，並提出了一種反覆運算方法來準確估計此轉換。然後模擬正向模型不確定性的主要來源，包括真實和模擬頭部模型之間的幾何形狀、電導率和源空間解析度的差異，以及電極位置未對齊，以測試所提出的方法。結果：ReLORETA 和 eLORETA 應用於腦部不同區域的模擬焦點來源，以及各種噪聲級別以及來自癲癇患者的真實數據。結果表明，在所有情況下，ReLORETA 都比 eLORETA 更強健且更準確。結論：在成功處理了正向模型不確定性後，ReLORETA 被證明是一種很有前景的真實世界臨床應用方法。意義：eLORETA 是一種定位技術，可用於研究腦部活動，用於醫療應用，例如確定藥物難治性癲癇患者的癲癇發作區。然而，eLORETA 的主要限制是對正向模型的不確定性敏感。由於此問題可能會嚴重損害其在未知確切導線場矩陣的真實世界應用中的性能，因此開發一種能夠應對這些不確定性的更強健的方法具有重大意義。</paragraph>

##### **Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language**
2405.05777v1 by Ronny Paul, Himanshu Buckchash, Shantipriya Parida, Dilip K. Prasad

S\'ami, an indigenous language group comprising multiple languages, faces
digital marginalization due to the limited availability of data and
sophisticated language models designed for its linguistic intricacies. This
work focuses on increasing technological participation for the S\'ami language.
We draw the attention of the ML community towards the language modeling problem
of Ultra Low Resource (ULR) languages. ULR languages are those for which the
amount of available textual resources is very low, and the speaker count for
them is also very low. ULRLs are also not supported by mainstream Large
Language Models (LLMs) like ChatGPT, due to which gathering artificial training
data for them becomes even more challenging. Mainstream AI foundational model
development has given less attention to this category of languages. Generally,
these languages have very few speakers, making it hard to find them. However,
it is important to develop foundational models for these ULR languages to
promote inclusion and the tangible abilities and impact of LLMs. To this end,
we have compiled the available S\'ami language resources from the web to create
a clean dataset for training language models. In order to study the behavior of
modern LLM models with ULR languages (S\'ami), we have experimented with
different kinds of LLMs, mainly at the order of $\sim$ seven billion
parameters. We have also explored the effect of multilingual LLM training for
ULRLs. We found that the decoder-only models under a sequential multilingual
training scenario perform better than joint multilingual training, whereas
multilingual training with high semantic overlap, in general, performs better
than training from scratch.This is the first study on the S\'ami language for
adapting non-statistical language models that use the latest developments in
the field of natural language processing (NLP).

摘要：S'ami 是一種包含多種語言的原住民語言群，由於缺乏資料和針對其語言複雜性而設計的精緻語言模型，面臨數位邊緣化。這項工作專注於增加 S'ami 語言的科技參與。我們將機器學習社群的注意力引導至極低資源 (ULR) 語言的語言建模問題。ULR 語言是指可用文字資源非常少，且說話者數量也很少的語言。ULRL 也不受主流大型語言模型 (LLM)（例如 ChatGPT）支援，因此為其收集人工訓練資料變得更具挑戰性。主流 AI 基礎模型開發較少關注這類語言。一般來說，這些語言的使用者很少，因此很難找到他們。然而，開發這些 ULR 語言的基礎模型非常重要，以促進包容性以及 LLM 的實際能力和影響力。為此，我們已從網路上編譯可用的 S'ami 語言資源，以建立一個用於訓練語言模型的乾淨資料集。為了研究現代 LLM 模型在 ULR 語言（S'ami）中的行為，我們已針對不同類型的 LLM 進行實驗，主要約為 70 億個參數。我們也探討了多語言 LLM 訓練對 ULRL 的影響。我們發現，在順序多語言訓練場景下的僅解碼器模型表現優於聯合多語言訓練，而一般而言，語意重疊度高的多語言訓練表現優於從頭開始訓練。這是第一個針對 S'ami 語言的研究，用於調整非統計語言模型，這些模型使用自然語言處理 (NLP) 領域的最新發展。

##### **Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions**
2405.05776v1 by Polina Tsvilodub, Paul Marty, Sonia Ramotowska, Jacopo Romoli, Michael Franke

Human communication is based on a variety of inferences that we draw from
sentences, often going beyond what is literally said. While there is wide
agreement on the basic distinction between entailment, implicature, and
presupposition, the status of many inferences remains controversial. In this
paper, we focus on three inferences of plain and embedded disjunctions, and
compare them with regular scalar implicatures. We investigate this comparison
from the novel perspective of the predictions of state-of-the-art large
language models, using the same experimental paradigms as recent studies
investigating the same inferences with humans. The results of our best
performing models mostly align with those of humans, both in the large
differences we find between those inferences and implicatures, as well as in
fine-grained distinctions among different aspects of those inferences.

摘要：人類溝通建立在我們從句子中得出的各種推論上，通常超越了字面上的意思。雖然在蘊涵、含義和預設的基本區別上達成廣泛共識，但許多推論的狀態仍然存在爭議。在本文中，我們專注於普通和嵌入式析取的三個推論，並將它們與常規標量含義比較。我們從最先進的大語言模型預測的新穎角度調查了這種比較，使用與最近研究使用人類調查相同推論的相同實驗範例。我們表現最好的模型的結果大多與人類的結果一致，無論是在我們發現的這些推論和含義之間的巨大差異中，還是在這些推論的不同方面的細微區別中。

##### **To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**
2405.05766v1 by Miquel Miró-Nicolau, Gabriel Moyà-Alcover, Antoni Jaume-i-Capó, Manuel González-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho

The increasing reliance on Deep Learning models, combined with their inherent
lack of transparency, has spurred the development of a novel field of study
known as eXplainable AI (XAI) methods. These methods seek to enhance the trust
of end-users in automated systems by providing insights into the rationale
behind their decisions. This paper presents a novel approach for measuring user
trust in XAI systems, allowing their refinement. Our proposed metric combines
both performance metrics and trust indicators from an objective perspective. To
validate this novel methodology, we conducted a case study in a realistic
medical scenario: the usage of XAI system for the detection of pneumonia from
x-ray images.

摘要：隨著對深度學習模型依賴性的增加，加上其固有的透明度不足，促使一個新的研究領域發展，稱為可解釋 AI (XAI) 方法。這些方法旨在透過深入了解決策背後的原理，來提升最終使用者對自動化系統的信賴。本文提出了一種衡量使用者對 XAI 系統信賴度的新穎方法，允許對其進行改進。我們提出的指標結合了客觀觀點下的效能指標和信賴指標。為了驗證這個新穎的方法，我們在一個真實的醫療場景中進行了一個案例研究：使用 XAI 系統從 X 光影像中偵測肺炎。

##### **DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models**
2405.05763v1 by Mengxiao Geng, Jiahao Zhu, Xiaolin Zhu, Qiqing Liu, Dong Liang, Qiegen Liu

Detail features of magnetic resonance images play a cru-cial role in accurate
medical diagnosis and treatment, as they capture subtle changes that pose
challenges for doc-tors when performing precise judgments. However, the widely
utilized naive diffusion model has limitations, as it fails to accurately
capture more intricate details. To en-hance the quality of MRI reconstruction,
we propose a comprehensive detail-preserving reconstruction method using
multiple diffusion models to extract structure and detail features in k-space
domain instead of image do-main. Moreover, virtual binary modal masks are
utilized to refine the range of values in k-space data through highly adaptive
center windows, which allows the model to focus its attention more efficiently.
Last but not least, an inverted pyramid structure is employed, where the
top-down image information gradually decreases, ena-bling a cascade
representation. The framework effective-ly represents multi-scale sampled data,
taking into ac-count the sparsity of the inverted pyramid architecture, and
utilizes cascade training data distribution to repre-sent multi-scale data.
Through a step-by-step refinement approach, the method refines the
approximation of de-tails. Finally, the proposed method was evaluated by
con-ducting experiments on clinical and public datasets. The results
demonstrate that the proposed method outper-forms other methods.

摘要：磁共振影像的細節特徵在準確的醫療診斷和治療中扮演著至關重要的角色，因為它們捕捉到細微的變化，對醫生執行精確判斷時構成挑戰。然而，廣泛使用的樸素擴散模型有其局限性，因為它無法準確捕捉到更複雜的細節。為了增強 MRI 重建的品質，我們提出了一種使用多個擴散模型的全面細節保留重建方法，在 k 空間域中提取結構和細節特徵，而不是影像域。此外，虛擬二元模態遮罩被用於透過高度自適應中心視窗來改善 k 空間資料中的值域，這使得模型可以更有效率地集中其注意力。最後但並非最不重要的是，採用了一個倒金字塔結構，其中自上而下的影像資訊逐漸減少，實現了串聯表示。該架構有效地表示多尺度取樣資料，考慮了倒金字塔架構的稀疏性，並利用串聯訓練資料分佈來表示多尺度資料。透過逐步細化的方式，該方法改善了細節的近似值。最後，透過在臨床和公共資料集上進行實驗，對所提出的方法進行了評估。結果表明，所提出的方法優於其他方法。

##### **Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media**
2405.05760v1 by Zhizhen Zhang, Ning Wang, Haojie Li, Zhihui Wang

The purpose of semantic location prediction is to extract relevant semantic
location information from multimodal social media posts, offering a more
contextual understanding of daily activities compared to GPS coordinates.
However, this task becomes challenging due to the presence of noise and
irrelevant information in "text-image" pairs. Existing methods suffer from
insufficient feature representations and fail to consider the comprehensive
integration of similarity at different granularities, making it difficult to
filter out noise and irrelevant information. To address these challenges, we
propose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for
predicting social users' semantic locations. First, we utilize a pre-trained
large-scale vision-language model to extract high-quality feature
representations from social media posts. Then, we introduce a Similarity-Guided
Interaction Module (SIM) to alleviate modality heterogeneity and noise
interference by incorporating coarse-grained and fine-grained similarity
guidance for modality interactions. Specifically, we propose a novel
similarity-aware feature interpolation attention mechanism at the coarse level,
leveraging modality-wise similarity to mitigate heterogeneity and reduce noise
within each modality. Meanwhile, we employ a similarity-aware feed-forward
block at the fine level, utilizing element-wise similarity to further mitigate
the impact of modality heterogeneity. Building upon pre-processed features with
minimal noise and modal interference, we propose a Similarity-aware Feature
Fusion Module (SFM) to fuse two modalities with cross-attention mechanism.
Comprehensive experimental results demonstrate the superior performance of our
proposed method in handling modality imbalance while maintaining efficient
fusion effectiveness.

摘要：语义位置预测的目的是从多模态社交媒体帖子中提取相关的语义位置信息，与 GPS 坐标相比，提供对日常活动的更具背景意义的理解。然而，由于“文本-图像”对中存在噪声和无关信息，这项任务变得具有挑战性。现有方法存在特征表示不足的问题，并且无法考虑在不同粒度上的相似性的综合集成，这使得难以滤除噪声和无关信息。为了应对这些挑战，我们提出了一种用于预测社交用户语义位置的相似性引导多模态融合 Transformer (SG-MFT)。首先，我们利用预训练的大规模视觉语言模型从社交媒体帖子中提取高质量的特征表示。然后，我们引入了一个相似性引导交互模块 (SIM)，通过为模态交互纳入粗粒度和细粒度相似性指导来缓解模态异构性和噪声干扰。具体来说，我们在粗粒度上提出了一种新颖的相似性感知特征插值注意力机制，利用模态相似性来减轻异构性并降低每个模态内的噪声。同时，我们在细粒度上采用相似性感知前馈块，利用元素级相似性进一步减轻模态异构性的影响。在经过预处理的、噪声和模态干扰最小的特征之上，我们提出了一种相似性感知特征融合模块 (SFM)，以交叉注意力机制融合两种模态。全面的实验结果证明了我们提出的方法在处理模态不平衡的同时保持高效融合有效性方面的卓越性能。

##### **Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma**
2405.05758v1 by Han Meng, Yitian Yang, Yunan Li, Jungup Lee, Yi-Chieh Lee

Qualitative analysis is a challenging, yet crucial aspect of advancing
research in the field of Human-Computer Interaction (HCI). Recent studies show
that large language models (LLMs) can perform qualitative coding within
existing schemes, but their potential for collaborative human-LLM discovery and
new insight generation in qualitative analysis is still underexplored. To
bridge this gap and advance qualitative analysis by harnessing the power of
LLMs, we propose CHALET, a novel methodology that leverages the human-LLM
collaboration paradigm to facilitate conceptualization and empower qualitative
research. The CHALET approach involves LLM-supported data collection,
performing both human and LLM deductive coding to identify disagreements, and
performing collaborative inductive coding on these disagreement cases to derive
new conceptual insights. We validated the effectiveness of CHALET through its
application to the attribution model of mental-illness stigma, uncovering
implicit stigmatization themes on cognitive, emotional and behavioral
dimensions. We discuss the implications for future research, methodology, and
the transdisciplinary opportunities CHALET presents for the HCI community and
beyond.

摘要：質性分析是推進人機互動 (HCI) 領域研究的挑戰性但至關重要的方面。最近的研究表明，大型語言模型 (LLM) 能在現有架構中執行質性編碼，但它們在質性分析中進行協作式人機發現和產生新見解的潛力仍未得到充分探索。為了彌合這一差距，並透過利用 LLM 的力量推進質性分析，我們提出了 CHALET，一種創新的方法論，它利用人機協作範例來促進概念化並賦能質性研究。CHALET 方法涉及 LLM 支持的資料收集，執行人類和 LLM 演繹編碼以找出分歧，並對這些分歧案例執行協作式歸納編碼以得出新的概念見解。我們透過將 CHALET 應用於心理疾病污名歸因模型，驗證了其有效性，揭示了認知、情緒和行為面向的隱性污名化主題。我們討論了 CHALET 為 HCI 社群及其他領域帶來的未來研究、方法論和跨學科機會的影響。

##### **CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks**
2405.05755v1 by Nick, Nikzad, Yongsheng Gao, Jun Zhou

In recent years, convolutional neural networks (CNNs) with channel-wise
feature refining mechanisms have brought noticeable benefits to modelling
channel dependencies. However, current attention paradigms fail to infer an
optimal channel descriptor capable of simultaneously exploiting statistical and
spatial relationships among feature maps. In this paper, to overcome this
shortcoming, we present a novel channel-wise spatially autocorrelated (CSA)
attention mechanism. Inspired by geographical analysis, the proposed CSA
exploits the spatial relationships between channels of feature maps to produce
an effective channel descriptor. To the best of our knowledge, this is the f
irst time that the concept of geographical spatial analysis is utilized in deep
CNNs. The proposed CSA imposes negligible learning parameters and light
computational overhead to the deep model, making it a powerful yet efficient
attention module of choice. We validate the effectiveness of the proposed CSA
networks (CSA-Nets) through extensive experiments and analysis on ImageNet, and
MS COCO benchmark datasets for image classification, object detection, and
instance segmentation. The experimental results demonstrate that CSA-Nets are
able to consistently achieve competitive performance and superior
generalization than several state-of-the-art attention-based CNNs over
different benchmark tasks and datasets.

摘要：近年來，具有通道級特徵精煉機制的卷積神經網路 (CNN) 為通道依賴關係建模帶來了顯著的好處。然而，當前的注意力範例無法推斷出同時利用特徵圖中統計和空間關係的最佳通道描述符。在本文中，為了克服這個缺點，我們提出了一種新的通道級空間自相關 (CSA) 注意力機制。受地理分析的啟發，所提出的 CSA 利用特徵圖通道之間的空間關係來產生有效的通道描述符。據我們所知，這是地理空間分析的概念首次用於深度 CNN。所提出的 CSA 對深度模型施加了可忽略的學習參數和輕量級的運算開銷，使其成為功能強大且高效的注意力模組。我們透過在 ImageNet 和 MS COCO 基準資料集上進行廣泛的實驗和分析，驗證了所提出的 CSA 網路 (CSA-Nets) 的有效性，用於影像分類、物件偵測和實例分割。實驗結果表明，與多種現有注意力式 CNN 相比，CSA-Nets 能在不同的基準任務和資料集上持續達成具競爭力的效能和優越的泛化能力。

##### **Can large language models understand uncommon meanings of common words?**
2405.05741v1 by Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao

Large language models (LLMs) like ChatGPT have shown significant advancements
across diverse natural language understanding (NLU) tasks, including
intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged
testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely
comprehend the world' remains unclear, fostering numerous studies and sparking
heated debates. Prevailing research mainly focuses on surface-level NLU,
neglecting fine-grained explorations. However, such explorations are crucial
for understanding their unique comprehension mechanisms, aligning with human
cognition, and finally enhancing LLMs' general NLU capacities. To address this
gap, our study delves into LLMs' nuanced semantic comprehension capabilities,
particularly regarding common words with uncommon meanings. The idea stems from
foundational principles of human communication within psychology, which
underscore accurate shared understandings of word semantics. Specifically, this
paper presents the innovative construction of a Lexical Semantic Comprehension
(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing
both fine-grained and cross-lingual dimensions. Introducing models of both
open-source and closed-source, varied scales and architectures, our extensive
empirical experiments demonstrate the inferior performance of existing models
in this basic lexical-meaning understanding task. Notably, even the
state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%
and 22.3%, respectively. Additionally, multiple advanced prompting techniques
and retrieval-augmented generation are also introduced to help alleviate this
trouble, yet limitations persist. By highlighting the above critical
shortcomings, this research motivates further investigation and offers novel
insights for developing more intelligent LLMs.

摘要：大型語言模型 (LLM)，例如 ChatGPT，在各種自然語言理解 (NLU) 任務中已展現出顯著的進展，包括智能對話和自主代理。然而，由於缺乏廣泛認可的測試機制，回答「LLM 是隨機鸚鵡還是真正理解世界」的問題仍然不明確，這也促成了許多研究和引發激烈的辯論。現行的研究主要關注於表面層的 NLU，忽略了細緻的探討。然而，此類探討對於理解其獨特的理解機制、與人類認知保持一致，並最終增強 LLM 的一般 NLU 能力至關重要。為了解決這個差距，我們的研究深入探討了 LLM 的細微語義理解能力，特別是關於具有不尋常含義的常見詞彙。這個想法源自心理學中人類溝通的基本原理，它強調準確共享對詞彙語義的理解。具體來說，本文提出了創新的詞彙語義理解 (LeSC) 資料集建構，並採用新穎的評估指標，這是第一個涵蓋細緻和跨語言維度的基準。透過引入開源和閉源、不同規模和架構的模型，我們的廣泛實證實驗證明了現有模型在這個基本的詞彙含義理解任務中的表現較差。值得注意的是，即使是現今最先進的 LLM GPT-4 和 GPT-3.5 也分別落後於 16 歲的人類 3.9% 和 22.3%。此外，也引入了多種進階提示技術和檢索增強生成，以幫助緩解這個問題，但限制仍然存在。透過強調上述關鍵缺點，本研究激勵了進一步的調查，並為開發更智能的 LLM 提供了新穎的見解。

##### **Computational lexical analysis of Flamenco genres**
2405.05723v1 by Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez

Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of
Humanity, is a profound expression of cultural identity rooted in Andalusia,
Spain. However, there is a lack of quantitative studies that help identify
characteristic patterns in this long-lived music tradition. In this work, we
present a computational analysis of Flamenco lyrics, employing natural language
processing and machine learning to categorize over 2000 lyrics into their
respective Flamenco genres, termed as $\textit{palos}$. Using a Multinomial
Naive Bayes classifier, we find that lexical variation across styles enables to
accurately identify distinct $\textit{palos}$. More importantly, from an
automatic method of word usage, we obtain the semantic fields that characterize
each style. Further, applying a metric that quantifies the inter-genre distance
we perform a network analysis that sheds light on the relationship between
Flamenco styles. Remarkably, our results suggest historical connections and
$\textit{palo}$ evolutions. Overall, our work illuminates the intricate
relationships and cultural significance embedded within Flamenco lyrics,
complementing previous qualitative discussions with quantitative analyses and
sparking new discussions on the origin and development of traditional music
genres.

摘要：弗拉門戈舞，被聯合國教科文組織認定為人類非物質文化遺產的一部分，是根植於西班牙安達盧西亞的文化認同的深刻表達。然而，缺乏定量的研究來幫助識別這一悠久的音樂傳統中的特徵模式。在這項工作中，我們提出了弗拉門戈歌詞的計算分析，採用自然語言處理和機器學習將 2000 多首歌詞分類到各自的弗拉門戈流派中，稱為$\textit{palos}$。使用多項式樸素貝葉斯分類器，我們發現不同風格的詞彙變異能夠準確識別不同的$\textit{palos}$。更重要的是，從一種自動化的詞彙使用方式中，我們獲得了表徵每種風格的語義場。此外，應用量化流派間距離的度量標準，我們進行了一項網路分析，闡明了弗拉門戈風格之間的關係。值得注意的是，我們的結果表明了歷史聯繫和$\textit{palo}$的演變。總的來說，我們的作品闡明了弗拉門戈歌詞中嵌入的複雜關係和文化意義，用定量分析補充了先前的定性討論，並引發了對傳統音樂流派起源和發展的新討論。

##### **Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution**
2405.05705v1 by Sandrine Chausson, Björn Ross

Many tasks related to Computational Social Science and Web Content Analysis
involve classifying pieces of text based on the claims they contain.
State-of-the-art approaches usually involve fine-tuning models on large
annotated datasets, which are costly to produce. In light of this, we propose
and release a qualitative and versatile few-shot learning methodology as a
common paradigm for any claim-based textual classification task. This
methodology involves defining the classes as arbitrarily sophisticated
taxonomies of claims, and using Natural Language Inference models to obtain the
textual entailment between these and a corpus of interest. The performance of
these models is then boosted by annotating a minimal sample of data points,
dynamically sampled using the well-established statistical heuristic of
Probabilistic Bisection. We illustrate this methodology in the context of three
tasks: climate change contrarianism detection, topic/stance classification and
depression-relates symptoms detection. This approach rivals traditional
pre-train/fine-tune approaches while drastically reducing the need for data
annotation.

摘要：許多與運算社會科學和網路內容分析相關的任務，涉及根據文字中包含的論點對文字片段進行分類。最先進的方法通常涉及對大型註解資料集進行模型微調，而這些資料集的製作成本很高。有鑑於此，我們提出並發布一種定性且通用的少樣本學習方法論，作為基於論點的文字分類任務的通用範例。此方法論涉及將類別定義為任意複雜的論點分類法，並使用自然語言推論模型來取得這些論點與感興趣的語料庫之間的文字蘊涵。然後，透過註解最少量的資料點來提升這些模型的效能，並使用已建立良好的機率二分統計啟發法動態取樣。我們在以下三個任務的脈絡中說明此方法論：氣候變遷反對論檢測、主題/立場分類和憂鬱相關症狀檢測。此方法與傳統的預訓練/微調方法競爭，同時大幅減少了資料註解的需求。

