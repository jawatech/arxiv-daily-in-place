{"2410.08245": {"publish_time": "2024-10-10", "title": "Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts", "paper_summary": "Multimodal learning has gained increasing importance across various fields,\noffering the ability to integrate data from diverse sources such as images,\ntext, and personalized records, which are frequently observed in medical\ndomains. However, in scenarios where some modalities are missing, many existing\nframeworks struggle to accommodate arbitrary modality combinations, often\nrelying heavily on a single modality or complete data. This oversight of\npotential modality combinations limits their applicability in real-world\nsituations. To address this challenge, we propose Flex-MoE (Flexible\nMixture-of-Experts), a new framework designed to flexibly incorporate arbitrary\nmodality combinations while maintaining robustness to missing data. The core\nidea of Flex-MoE is to first address missing modalities using a new missing\nmodality bank that integrates observed modality combinations with the\ncorresponding missing ones. This is followed by a uniquely designed Sparse MoE\nframework. Specifically, Flex-MoE first trains experts using samples with all\nmodalities to inject generalized knowledge through the generalized router\n($\\mathcal{G}$-Router). The $\\mathcal{S}$-Router then specializes in handling\nfewer modality combinations by assigning the top-1 gate to the expert\ncorresponding to the observed modality combination. We evaluate Flex-MoE on the\nADNI dataset, which encompasses four modalities in the Alzheimer's Disease\ndomain, as well as on the MIMIC-IV dataset. The results demonstrate the\neffectiveness of Flex-MoE highlighting its ability to model arbitrary modality\ncombinations in diverse missing modality scenarios. Code is available at\nhttps://github.com/UNITES-Lab/flex-moe.", "paper_summary_zh": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5404\u4e2a\u9886\u57df\u4e2d\u83b7\u5f97\u8d8a\u6765\u8d8a\u591a\u7684\u91cd\u89c6\uff0c\n\u5b83\u63d0\u4f9b\u4e86\u6574\u5408\u6765\u81ea\u56fe\u50cf\u3001\n\u6587\u672c\u548c\u4e2a\u6027\u5316\u8bb0\u5f55\u7b49\u4e0d\u540c\u6765\u6e90\u7684\u6570\u636e\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u6570\u636e\u901a\u5e38\u5728\u533b\u5b66\n\u9886\u57df\u4e2d\u89c2\u5bdf\u5230\u3002\u7136\u800c\uff0c\u5728\u67d0\u4e9b\u6a21\u6001\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u8bb8\u591a\u73b0\u6709\u7684\n\u6846\u67b6\u96be\u4ee5\u9002\u5e94\u4efb\u610f\u6a21\u6001\u7ec4\u5408\uff0c\u901a\u5e38\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5355\u4e00\u6a21\u6001\u6216\u5b8c\u6574\u6570\u636e\u3002\u8fd9\u79cd\u5bf9\n\u6f5c\u5728\u6a21\u6001\u7ec4\u5408\u7684\u5ffd\u89c6\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\n\u60c5\u51b5\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Flex-MoE\uff08\u7075\u6d3b\n\u4e13\u5bb6\u6df7\u5408\uff09\uff0c\u4e00\u4e2a\u65e8\u5728\u7075\u6d3b\u5730\u7eb3\u5165\u4efb\u610f\n\u6a21\u6001\u7ec4\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u7f3a\u5931\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002Flex-MoE \u7684\u6838\u5fc3\n\u601d\u60f3\u662f\u9996\u5148\u4f7f\u7528\u65b0\u7684\u7f3a\u5931\u6a21\u6001\u5e93\u6765\u89e3\u51b3\u7f3a\u5931\u6a21\u6001\uff0c\u8be5\u5e93\u5c06\u89c2\u5bdf\u5230\u7684\u6a21\u6001\u7ec4\u5408\u4e0e\n\u76f8\u5e94\u7684\u7f3a\u5931\u6a21\u6001\u76f8\u7ed3\u5408\u3002\u63a5\u4e0b\u6765\u662f\u4e00\u4e2a\u72ec\u7279\u8bbe\u8ba1\u7684\u7a00\u758f MoE\n\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0cFlex-MoE \u9996\u5148\u4f7f\u7528\u5177\u6709\u6240\u6709\n\u6a21\u6001\u7684\u6837\u672c\u8bad\u7ec3\u4e13\u5bb6\uff0c\u4ee5\u901a\u8fc7\u5e7f\u4e49\u8def\u7531\u5668\u6ce8\u5165\u5e7f\u4e49\u77e5\u8bc6\n\uff08$\\mathcal{G}$-Router\uff09\u3002\u7136\u540e\uff0c$\\mathcal{S}$-Router \u901a\u8fc7\u5c06 top-1 \u95e8\u5206\u914d\u7ed9\u4e13\u5bb6\u6765\u4e13\u95e8\u5904\u7406\u8f83\u5c11\u7684\u6a21\u6001\u7ec4\u5408\n\u5bf9\u5e94\u4e8e\u89c2\u5bdf\u5230\u7684\u6a21\u6001\u7ec4\u5408\u3002\u6211\u4eec\u5728\nADNI \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30 Flex-MoE\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e2d\u7684\u56db\u79cd\u6a21\u6001\n\u9886\u57df\uff0c\u4ee5\u53ca MIMIC-IV \u6570\u636e\u96c6\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\nFlex-MoE \u7684\u6709\u6548\u6027\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u4e0d\u540c\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e2d\u5bf9\u4efb\u610f\u6a21\u6001\u7ec4\u5408\u8fdb\u884c\u5efa\u6a21\u7684\u80fd\u529b\u3002\u4ee3\u7801\u53ef\u5728\nhttps://github.com/UNITES-Lab/flex-moe \u83b7\u5f97\u3002", "author": "Sukwon Yun et.al.", "authors": "Sukwon Yun, Inyoung Choi, Jie Peng, Yangfan Wu, Jingxuan Bao, Qiyiwen Zhang, Jiayi Xin, Qi Long, Tianlong Chen", "id": "2410.08245v1", "paper_url": "http://arxiv.org/abs/2410.08245v1", "repo": "https://github.com/unites-lab/flex-moe"}}