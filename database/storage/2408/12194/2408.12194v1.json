{"2408.12194": {"publish_time": "2024-08-22", "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "paper_summary": "Pretrained language models like BERT and T5 serve as crucial backbone\nencoders for dense retrieval. However, these models often exhibit limited\ngeneralization capabilities and face challenges in improving in domain\naccuracy. Recent research has explored using large language models (LLMs) as\nretrievers, achieving SOTA performance across various tasks. Despite these\nadvancements, the specific benefits of LLMs over traditional retrievers and the\nimpact of different LLM configurations, such as parameter sizes, pretraining\nduration, and alignment processes on retrieval tasks remain unclear. In this\nwork, we conduct a comprehensive empirical study on a wide range of retrieval\ntasks, including in domain accuracy, data efficiency, zero shot generalization,\nlengthy retrieval, instruction based retrieval, and multi task learning. We\nevaluate over 15 different backbone LLMs and non LLMs. Our findings reveal that\nlarger models and extensive pretraining consistently enhance in domain accuracy\nand data efficiency. Additionally, larger models demonstrate significant\npotential in zero shot generalization, lengthy retrieval, instruction based\nretrieval, and multi task learning. These results underscore the advantages of\nLLMs as versatile and effective backbone encoders in dense retrieval, providing\nvaluable insights for future research and development in this field.", "paper_summary_zh": "\u9810\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u4f8b\u5982 BERT \u548c T5\uff0c\u7528\u4f5c\u5bc6\u96c6\u6aa2\u7d22\u7684\u95dc\u9375\u9aa8\u5e79\u7de8\u78bc\u5668\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u8868\u73fe\u51fa\u6709\u9650\u7684\u6982\u62ec\u80fd\u529b\uff0c\u4e26\u4e14\u5728\u63d0\u9ad8\u9818\u57df\u6e96\u78ba\u6027\u65b9\u9762\u9762\u81e8\u6311\u6230\u3002\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u6aa2\u7d22\u5668\uff0c\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5be6\u73fe\u4e86 SOTA \u6548\u80fd\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u9032\u5c55\uff0cLLM \u76f8\u8f03\u65bc\u50b3\u7d71\u6aa2\u7d22\u5668\u7684\u5177\u9ad4\u512a\u9ede\u4ee5\u53ca\u4e0d\u540c LLM \u7d44\u614b\uff08\u4f8b\u5982\u53c3\u6578\u5927\u5c0f\u3001\u9810\u8a13\u7df4\u6301\u7e8c\u6642\u9593\u548c\u5c0d\u9f4a\u7a0b\u5e8f\uff09\u5c0d\u6aa2\u7d22\u4efb\u52d9\u7684\u5f71\u97ff\u4ecd\u4e0d\u6e05\u695a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c0d\u5ee3\u6cdb\u7684\u6aa2\u7d22\u4efb\u52d9\u9032\u884c\u4e86\u5168\u9762\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u5305\u62ec\u9818\u57df\u6e96\u78ba\u6027\u3001\u8cc7\u6599\u6548\u7387\u3001\u96f6\u6b21\u6982\u62ec\u3001\u9577\u7bc7\u6aa2\u7d22\u3001\u57fa\u65bc\u6307\u4ee4\u7684\u6aa2\u7d22\u548c\u591a\u4efb\u52d9\u5b78\u7fd2\u3002\u6211\u5011\u8a55\u4f30\u4e86 15 \u500b\u4e0d\u540c\u7684\u9aa8\u5e79 LLM \u548c\u975e LLM\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u8f03\u5927\u7684\u6a21\u578b\u548c\u5ee3\u6cdb\u7684\u9810\u8a13\u7df4\u6301\u7e8c\u589e\u5f37\u9818\u57df\u6e96\u78ba\u6027\u548c\u8cc7\u6599\u6548\u7387\u3002\u6b64\u5916\uff0c\u8f03\u5927\u7684\u6a21\u578b\u5728\u96f6\u6b21\u6982\u62ec\u3001\u9577\u7bc7\u6aa2\u7d22\u3001\u57fa\u65bc\u6307\u4ee4\u7684\u6aa2\u7d22\u548c\u591a\u4efb\u52d9\u5b78\u7fd2\u4e2d\u5c55\u73fe\u4e86\u986f\u8457\u7684\u6f5b\u529b\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86 LLM \u4f5c\u70ba\u5bc6\u96c6\u6aa2\u7d22\u4e2d\u591a\u529f\u80fd\u4e14\u6709\u6548\u7684\u9aa8\u5e79\u7de8\u78bc\u5668\u7684\u512a\u9ede\uff0c\u70ba\u8a72\u9818\u57df\u672a\u4f86\u7684\u7814\u7a76\u548c\u958b\u767c\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002", "author": "Kun Luo et.al.", "authors": "Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, Kang Liu", "id": "2408.12194v1", "paper_url": "http://arxiv.org/abs/2408.12194v1", "repo": "null"}}