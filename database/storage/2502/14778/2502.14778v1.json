{"2502.14778": {"publish_time": "2025-02-20", "title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models", "paper_summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make the source code and data publicly\navailable upon acceptance.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u5df2\u5728\u82f1\u8a9e\u4e2d\u8868\u73fe\u51fa\u5f37\u52c1\u7684\u6548\u80fd\uff0c\u4f46\u7531\u65bc\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u5b83\u5011\u5728\u65e5\u8a9e\u4e2d\u7684\u6548\u80fd\u4ecd\u7136\u6709\u9650\u3002\u76ee\u524d\u7684\u65e5\u8a9e LMM \u901a\u5e38\u4f9d\u8cf4\u65bc\u7ffb\u8b6f\u5f8c\u7684\u82f1\u8a9e\u8cc7\u6599\u96c6\uff0c\u9650\u5236\u4e86\u5b83\u5011\u64f7\u53d6\u7279\u5b9a\u65bc\u65e5\u672c\u7684\u6587\u5316\u77e5\u8b58\u7684\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63a2\u7d22\u4e86\u65e5\u8a9e PDF \u8cc7\u6599\u4f5c\u70ba\u8a13\u7df4\u8cc7\u6e90\u7684\u6f5b\u529b\uff0c\u9019\u500b\u9818\u57df\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5168\u81ea\u52d5\u7684\u7ba1\u9053\uff0c\u5229\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u6a21\u578b\u900f\u904e\u7248\u9762\u5206\u6790\u3001\u5149\u5b78\u5b57\u5143\u8fa8\u8b58\u548c\u8996\u89ba\u8a9e\u8a00\u914d\u5c0d\u5f9e PDF \u4e2d\u64f7\u53d6\u5f71\u50cf\u6587\u5b57\u5c0d\uff0c\u6d88\u9664\u4e86\u624b\u52d5\u8a3b\u89e3\u7684\u9700\u8981\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f9e\u64f7\u53d6\u7684\u5f71\u50cf\u6587\u5b57\u5c0d\u4e2d\u5efa\u69cb\u8aaa\u660e\u8cc7\u6599\uff0c\u4ee5\u8c50\u5bcc\u8a13\u7df4\u8cc7\u6599\u3002\u70ba\u4e86\u8a55\u4f30 PDF \u884d\u751f\u8cc7\u6599\u7684\u6548\u80fd\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u65e5\u8a9e LMM\uff0c\u4e26\u5728\u65e5\u8a9e LMM \u57fa\u6e96\u4e0a\u8a55\u4f30\u5b83\u5011\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\u4e86\u986f\u8457\u7684\u9032\u6b65\uff0c\u5728 Heron-Bench \u4e0a\u7684\u6548\u80fd\u63d0\u5347\u5e45\u5ea6\u5f9e 3.9% \u5230 13.8%\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u91cd\u9ede\u8aaa\u660e\u4e86 PDF \u884d\u751f\u8cc7\u6599\u5c0d\u5404\u7a2e\u56e0\u7d20\u7684\u5f71\u97ff\uff0c\u4f8b\u5982\u6a21\u578b\u5927\u5c0f\u548c\u8a9e\u8a00\u6a21\u578b\uff0c\u52a0\u5f37\u4e86\u5176\u4f5c\u70ba\u65e5\u8a9e LMM \u7684\u591a\u6a21\u614b\u8cc7\u6e90\u7684\u50f9\u503c\u3002\u6211\u5011\u8a08\u756b\u5728\u63a5\u53d7\u5f8c\u516c\u958b\u539f\u59cb\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u3002", "author": "Jeonghun Baek et.al.", "authors": "Jeonghun Baek, Akiko Aizawa, Kiyoharu Aizawa", "id": "2502.14778v1", "paper_url": "http://arxiv.org/abs/2502.14778v1", "repo": "null"}}