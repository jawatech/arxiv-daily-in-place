{"2405.08134": {"publish_time": "2024-05-13", "title": "Many-Shot Regurgitation (MSR) Prompting", "paper_summary": "We introduce Many-Shot Regurgitation (MSR) prompting, a new black-box\nmembership inference attack framework for examining verbatim content\nreproduction in large language models (LLMs). MSR prompting involves dividing\nthe input text into multiple segments and creating a single prompt that\nincludes a series of faux conversation rounds between a user and a language\nmodel to elicit verbatim regurgitation. We apply MSR prompting to diverse text\nsources, including Wikipedia articles and open educational resources (OER)\ntextbooks, which provide high-quality, factual content and are continuously\nupdated over time. For each source, we curate two dataset types: one that LLMs\nwere likely exposed to during training ($D_{\\rm pre}$) and another consisting\nof documents published after the models' training cutoff dates ($D_{\\rm\npost}$). To quantify the occurrence of verbatim matches, we employ the Longest\nCommon Substring algorithm and count the frequency of matches at different\nlength thresholds. We then use statistical measures such as Cliff's delta,\nKolmogorov-Smirnov (KS) distance, and Kruskal-Wallis H test to determine\nwhether the distribution of verbatim matches differs significantly between\n$D_{\\rm pre}$ and $D_{\\rm post}$. Our findings reveal a striking difference in\nthe distribution of verbatim matches between $D_{\\rm pre}$ and $D_{\\rm post}$,\nwith the frequency of verbatim reproduction being significantly higher when\nLLMs (e.g. GPT models and LLaMAs) are prompted with text from datasets they\nwere likely trained on. For instance, when using GPT-3.5 on Wikipedia articles,\nwe observe a substantial effect size (Cliff's delta $= -0.984$) and a large KS\ndistance ($0.875$) between the distributions of $D_{\\rm pre}$ and $D_{\\rm\npost}$. Our results provide compelling evidence that LLMs are more prone to\nreproducing verbatim content when the input text is likely sourced from their\ntraining data.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5f15\u5165\u4e86\u591a\u91cd\u63d0\u793a\u5f0f\u56de\u994b (MSR)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u9ed1\u76d2\u6703\u54e1\u63a8\u8ad6\u653b\u64ca\u67b6\u69cb\uff0c\u7528\u65bc\u6aa2\u67e5\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u9010\u5b57\u5167\u5bb9\u8907\u88fd\u3002MSR \u63d0\u793a\u5f0f\u6d89\u53ca\u5c07\u8f38\u5165\u6587\u5b57\u5206\u6210\u591a\u500b\u5340\u6bb5\uff0c\u4e26\u5efa\u7acb\u4e00\u500b\u55ae\u4e00\u63d0\u793a\u5f0f\uff0c\u5176\u4e2d\u5305\u542b\u4f7f\u7528\u8005\u8207\u8a9e\u8a00\u6a21\u578b\u4e4b\u9593\u4e00\u7cfb\u5217\u865b\u5047\u7684\u5c0d\u8a71\u56de\u5408\uff0c\u4ee5\u5f15\u767c\u9010\u5b57\u56de\u994b\u3002\u6211\u5011\u5c07 MSR \u63d0\u793a\u5f0f\u61c9\u7528\u65bc\u4e0d\u540c\u7684\u6587\u5b57\u4f86\u6e90\uff0c\u5305\u62ec\u7dad\u57fa\u767e\u79d1\u6587\u7ae0\u548c\u958b\u653e\u6559\u80b2\u8cc7\u6e90 (OER) \u6559\u79d1\u66f8\uff0c\u9019\u4e9b\u4f86\u6e90\u63d0\u4f9b\u4e86\u9ad8\u54c1\u8cea\u7684\u4e8b\u5be6\u5167\u5bb9\uff0c\u4e26\u6703\u6301\u7e8c\u66f4\u65b0\u3002\u5c0d\u65bc\u6bcf\u500b\u4f86\u6e90\uff0c\u6211\u5011\u7b56\u5283\u4e86\u5169\u7a2e\u8cc7\u6599\u96c6\u985e\u578b\uff1a\u4e00\u7a2e\u662f LLM \u5728\u8a13\u7df4\u671f\u9593\u53ef\u80fd\u63a5\u89f8\u5230\u7684 ($D_{\\rm pre}$)\uff0c\u53e6\u4e00\u7a2e\u5247\u5305\u542b\u5728\u6a21\u578b\u8a13\u7df4\u622a\u6b62\u65e5\u671f\u5f8c\u767c\u5e03\u7684\u6587\u4ef6 ($D_{\\rm post}$)\u3002\u70ba\u4e86\u91cf\u5316\u9010\u5b57\u5339\u914d\u7684\u767c\u751f\uff0c\u6211\u5011\u63a1\u7528\u6700\u9577\u516c\u5171\u5b50\u5b57\u4e32\u6f14\u7b97\u6cd5\uff0c\u4e26\u8a08\u7b97\u4e0d\u540c\u9577\u5ea6\u95be\u503c\u7684\u5339\u914d\u983b\u7387\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u7d71\u8a08\u91cf\u5ea6\uff0c\u4f8b\u5982 Cliff's delta\u3001Kolmogorov-Smirnov (KS) \u8ddd\u96e2\u548c Kruskal-Wallis H \u6aa2\u5b9a\uff0c\u4f86\u78ba\u5b9a\u9010\u5b57\u5339\u914d\u7684\u5206\u5e03\u662f\u5426\u5728 $D_{\\rm pre}$ \u548c $D_{\\rm post}$ \u4e4b\u9593\u6709\u986f\u8457\u5dee\u7570\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86 $D_{\\rm pre}$ \u548c $D_{\\rm post}$ \u4e4b\u9593\u9010\u5b57\u5339\u914d\u5206\u5e03\u7684\u986f\u8457\u5dee\u7570\uff0c\u7576\u4f7f\u7528 LLM\uff08\u4f8b\u5982 GPT \u6a21\u578b\u548c LLaMA\uff09\u63d0\u793a\u4f86\u81ea\u4ed6\u5011\u53ef\u80fd\u53d7\u904e\u8a13\u7df4\u7684\u8cc7\u6599\u96c6\u7684\u6587\u5b57\u6642\uff0c\u9010\u5b57\u8907\u88fd\u7684\u983b\u7387\u6703\u986f\u8457\u63d0\u9ad8\u3002\u4f8b\u5982\uff0c\u5728\u7dad\u57fa\u767e\u79d1\u6587\u7ae0\u4e2d\u4f7f\u7528 GPT-3.5 \u6642\uff0c\u6211\u5011\u89c0\u5bdf\u5230 $D_{\\rm pre}$ \u548c $D_{\\rm post}$ \u7684\u5206\u5e03\u4e4b\u9593\u6709\u986f\u8457\u7684\u6548\u679c\u5927\u5c0f\uff08Cliff's delta $= -0.984$\uff09\u548c\u8f03\u5927\u7684 KS \u8ddd\u96e2 ($0.875$\uff09\u3002\u6211\u5011\u7684\u7d50\u679c\u63d0\u4f9b\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u8b49\u64da\uff0c\u8b49\u660e\u7576\u8f38\u5165\u6587\u5b57\u53ef\u80fd\u4f86\u81ea\u5176\u8a13\u7df4\u8cc7\u6599\u6642\uff0cLLM \u66f4\u5bb9\u6613\u8907\u88fd\u9010\u5b57\u5167\u5bb9\u3002</paragraph>", "author": "Shashank Sonkar et.al.", "authors": "Shashank Sonkar, Richard G. Baraniuk", "id": "2405.08134v1", "paper_url": "http://arxiv.org/abs/2405.08134v1", "repo": "https://github.com/luffycodes/Many-Shot-Regurgitation-MIA"}}