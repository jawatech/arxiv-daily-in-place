{"2409.16024": {"publish_time": "2024-09-24", "title": "Bridging Environments and Language with Rendering Functions and Vision-Language Models", "paper_summary": "Vision-language models (VLMs) have tremendous potential for grounding\nlanguage, and thus enabling language-conditioned agents (LCAs) to perform\ndiverse tasks specified with text. This has motivated the study of LCAs based\non reinforcement learning (RL) with rewards given by rendering images of an\nenvironment and evaluating those images with VLMs. If single-task RL is\nemployed, such approaches are limited by the cost and time required to train a\npolicy for each new task. Multi-task RL (MTRL) is a natural alternative, but\nrequires a carefully designed corpus of training tasks and does not always\ngeneralize reliably to new tasks. Therefore, this paper introduces a novel\ndecomposition of the problem of building an LCA: first find an environment\nconfiguration that has a high VLM score for text describing a task; then use a\n(pretrained) goal-conditioned policy to reach that configuration. We also\nexplore several enhancements to the speed and quality of VLM-based LCAs,\nnotably, the use of distilled models, and the evaluation of configurations from\nmultiple viewpoints to resolve the ambiguities inherent in a single 2D view. We\ndemonstrate our approach on the Humanoid environment, showing that it results\nin LCAs that outperform MTRL baselines in zero-shot generalization, without\nrequiring any textual task descriptions or other forms of environment-specific\nannotation during training.\n  Videos and an interactive demo can be found at\nhttps://europe.naverlabs.com/text2control", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5177\u6709\u5c07\u8a9e\u8a00\u57fa\u790e\u5316\u7684\u5de8\u5927\u6f5b\u529b\uff0c\u5f9e\u800c\u4f7f\u8a9e\u8a00\u689d\u4ef6\u4ee3\u7406 (LCA) \u80fd\u5920\u57f7\u884c\u7528\u6587\u5b57\u6307\u5b9a\u7684\u5404\u7a2e\u4efb\u52d9\u3002\u9019\u4fc3\u4f7f\u4e86\u57fa\u65bc\u5f37\u5316\u5b78\u7fd2 (RL) \u7684 LCA \u7684\u7814\u7a76\uff0c\u5176\u734e\u52f5\u662f\u901a\u904e\u6e32\u67d3\u74b0\u5883\u7684\u5f71\u50cf\u4e26\u4f7f\u7528 VLM \u8a55\u4f30\u9019\u4e9b\u5f71\u50cf\u800c\u7d66\u4e88\u7684\u3002\u5982\u679c\u63a1\u7528\u55ae\u4e00\u4efb\u52d9 RL\uff0c\u6b64\u985e\u65b9\u6cd5\u6703\u53d7\u5230\u8a13\u7df4\u6bcf\u500b\u65b0\u4efb\u52d9\u7684\u7b56\u7565\u6240\u9700\u7684\u6210\u672c\u548c\u6642\u9593\u7684\u9650\u5236\u3002\u591a\u4efb\u52d9 RL (MTRL) \u662f\u4e00\u7a2e\u81ea\u7136\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u4ed4\u7d30\u8a2d\u8a08\u7684\u8a13\u7df4\u4efb\u52d9\u8a9e\u6599\u5eab\uff0c\u4e26\u4e14\u4e26\u4e0d\u7e3d\u662f\u80fd\u53ef\u9760\u5730\u63a8\u5ee3\u5230\u65b0\u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u65b0\u7a4e\u7684 LCA \u5efa\u69cb\u554f\u984c\u5206\u89e3\uff1a\u9996\u5148\u627e\u5230\u4e00\u500b\u74b0\u5883\u914d\u7f6e\uff0c\u5176\u5177\u6709\u63cf\u8ff0\u4efb\u52d9\u6587\u5b57\u7684\u9ad8 VLM \u5206\u6578\uff1b\u7136\u5f8c\u4f7f\u7528\uff08\u9810\u8a13\u7df4\u7684\uff09\u76ee\u6a19\u689d\u4ef6\u7b56\u7565\u4f86\u9054\u6210\u8a72\u914d\u7f6e\u3002\u6211\u5011\u9084\u63a2\u7d22\u4e86\u57fa\u65bc VLM \u7684 LCA \u7684\u901f\u5ea6\u548c\u54c1\u8cea\u7684\u5e7e\u9805\u5f37\u5316\uff0c\u7279\u5225\u662f\u84b8\u993e\u6a21\u578b\u7684\u4f7f\u7528\uff0c\u4ee5\u53ca\u5f9e\u591a\u500b\u8996\u9ede\u8a55\u4f30\u914d\u7f6e\u4ee5\u89e3\u6c7a\u55ae\u4e00 2D \u8996\u5716\u4e2d\u56fa\u6709\u7684\u6a21\u7cca\u6027\u3002\u6211\u5011\u5728\u985e\u4eba\u74b0\u5883\u4e2d\u5c55\u793a\u4e86\u6211\u5011\u7684\u505a\u6cd5\uff0c\u8868\u660e\u5b83\u7522\u751f\u7684 LCA \u5728\u96f6\u6b21\u5b78\u7fd2\u6cdb\u5316\u4e2d\u512a\u65bc MTRL \u57fa\u6e96\uff0c\u800c\u7121\u9700\u5728\u8a13\u7df4\u671f\u9593\u8981\u6c42\u4efb\u4f55\u6587\u5b57\u4efb\u52d9\u63cf\u8ff0\u6216\u5176\u4ed6\u5f62\u5f0f\u7684\u74b0\u5883\u7279\u5b9a\u8a3b\u89e3\u3002\u53ef\u4ee5\u5728 https://europe.naverlabs.com/text2control \u627e\u5230\u5f71\u7247\u548c\u4e92\u52d5\u5f0f\u793a\u7bc4\u3002", "author": "Theo Cachet et.al.", "authors": "Theo Cachet, Christopher R. Dance, Olivier Sigaud", "id": "2409.16024v1", "paper_url": "http://arxiv.org/abs/2409.16024v1", "repo": "null"}}