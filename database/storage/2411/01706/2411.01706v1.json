{"2411.01706": {"publish_time": "2024-11-03", "title": "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "paper_summary": "Complex Word Identification (CWI) is an essential step in the lexical\nsimplification task and has recently become a task on its own. Some variations\nof this binary classification task have emerged, such as lexical complexity\nprediction (LCP) and complexity evaluation of multi-word expressions (MWE).\nLarge language models (LLMs) recently became popular in the Natural Language\nProcessing community because of their versatility and capability to solve\nunseen tasks in zero/few-shot settings. Our work investigates LLM usage,\nspecifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and\nclosed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE\nsettings. We evaluate zero-shot, few-shot, and fine-tuning settings and show\nthat LLMs struggle in certain conditions or achieve comparable results against\nexisting methods. In addition, we provide some views on meta-learning combined\nwith prompt learning. In the end, we conclude that the current state of LLMs\ncannot or barely outperform existing methods, which are usually much smaller.", "paper_summary_zh": "\u8907\u96dc\u8a5e\u5f59\u8b58\u5225 (CWI) \u662f\u8a5e\u5f59\u7c21\u5316\u4efb\u52d9\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u6b65\u9a5f\uff0c\u6700\u8fd1\u5df2\u6210\u70ba\u4e00\u9805\u7368\u7acb\u7684\u4efb\u52d9\u3002\u6b64\u4e8c\u5143\u5206\u985e\u4efb\u52d9\u51fa\u73fe\u4e86\u4e00\u4e9b\u8b8a\u5316\uff0c\u4f8b\u5982\u8a5e\u5f59\u8907\u96dc\u5ea6\u9810\u6e2c (LCP) \u548c\u591a\u8a5e\u8868\u9054 (MWE) \u7684\u8907\u96dc\u5ea6\u8a55\u4f30\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u671f\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u793e\u7fa4\u4e2d\u5ee3\u53d7\u6b61\u8fce\uff0c\u56e0\u70ba\u5b83\u5011\u5177\u6709\u591a\u529f\u80fd\u6027\uff0c\u4e26\u6709\u80fd\u529b\u5728\u96f6\u6b21/\u5c11\u6b21\u5617\u8a66\u7684\u8a2d\u5b9a\u4e2d\u89e3\u6c7a\u672a\u898b\u904e\u7684\u554f\u984c\u3002\u6211\u5011\u7684\u7814\u7a76\u63a2\u8a0e\u4e86 LLM \u7684\u4f7f\u7528\uff0c\u7279\u5225\u662f\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\uff0c\u4f8b\u5982 Llama 2\u3001Llama 3 \u548c Vicuna v1.5\uff0c\u4ee5\u53ca\u9589\u6e90\u6a21\u578b\uff0c\u4f8b\u5982 ChatGPT-3.5-turbo \u548c GPT-4o\uff0c\u5728 CWI\u3001LCP \u548c MWE \u8a2d\u5b9a\u4e2d\u3002\u6211\u5011\u8a55\u4f30\u4e86\u96f6\u6b21\u3001\u5c11\u6b21\u548c\u5fae\u8abf\u8a2d\u5b9a\uff0c\u4e26\u986f\u793a LLM \u5728\u67d0\u4e9b\u689d\u4ef6\u4e0b\u6703\u9047\u5230\u56f0\u96e3\uff0c\u6216\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\u7372\u5f97\u76f8\u7576\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u4e9b\u95dc\u65bc\u5143\u5b78\u7fd2\u7d50\u5408\u63d0\u793a\u5b78\u7fd2\u7684\u89c0\u9ede\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5f97\u51fa\u7d50\u8ad6\uff0cLLM \u7684\u7576\u524d\u72c0\u614b\u7121\u6cd5\u6216\u50c5\u80fd\u52c9\u5f37\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u800c\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u5c0f\u5f97\u591a\u3002", "author": "R\u0103zvan-Alexandru Sm\u0103du et.al.", "authors": "R\u0103zvan-Alexandru Sm\u0103du, David-Gabriel Ion, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel", "id": "2411.01706v1", "paper_url": "http://arxiv.org/abs/2411.01706v1", "repo": "null"}}