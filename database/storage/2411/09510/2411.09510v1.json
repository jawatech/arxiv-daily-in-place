{"2411.09510": {"publish_time": "2024-11-14", "title": "Communication Compression for Tensor Parallel LLM Inference", "paper_summary": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u63a8\u9032\u4e86\u4eba\u5de5\u667a\u6167\u7684\u7586\u754c\uff0c\u4f46\u7531\u6578\u767e\u5104\u500b\u53c3\u6578\u548c\u904b\u7b97\u7d44\u6210\u3002\u70ba\u4e86\u66f4\u5feb\u7684\u63a8\u8ad6\u5ef6\u9072\uff0cLLM \u900f\u904e\u5404\u7a2e\u6a21\u578b\u5e73\u884c\u7b56\u7565\u90e8\u7f72\u5728\u591a\u500b\u786c\u9ad4\u52a0\u901f\u5668\u4e0a\u3002\u6211\u5011\u7684\u8ad6\u6587\u63a2\u8a0e\u4e86\u5176\u4e2d\u4e00\u7a2e\u7b56\u7565 - \u5f35\u91cf\u5e73\u884c - \u7684\u8a73\u7d30\u8cc7\u8a0a\uff0c\u4e26\u63d0\u51fa\u900f\u904e\u58d3\u7e2e\u52a0\u901f\u5668\u9593\u7684\u901a\u8a0a\u4f86\u964d\u4f4e\u5ef6\u9072\u3002\u6211\u5011\u5229\u7528\u7d30\u7dfb\u7684\u91cf\u5316\u6280\u8853\u5c07\u6240\u9078\u7684\u6fc0\u6d3b\u58d3\u7e2e 3.5 - 4.5 \u500d\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u5c07\u9996\u6b21\u6a19\u8a18\u6642\u9593 (TTFT) \u6e1b\u5c11\u591a\u9054 2 \u500d\uff0c\u4e14\u6a21\u578b\u6548\u80fd\u5e7e\u4e4e\u6c92\u6709\u4e0b\u964d\u3002", "author": "Jan Hansen-Palmus et.al.", "authors": "Jan Hansen-Palmus, Michael Truong-Le, Oliver Hausd\u00f6rfer, Alok Verma", "id": "2411.09510v1", "paper_url": "http://arxiv.org/abs/2411.09510v1", "repo": "null"}}