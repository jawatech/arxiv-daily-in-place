
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-31**|**Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**|Misha P. T Kaandorp et.al.|[2501.19338v1](http://arxiv.org/abs/2501.19338v1)|null|
|**2025-01-31**|**Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**|Halil Ibrahim Aysel et.al.|[2501.19271v1](http://arxiv.org/abs/2501.19271v1)|null|
|**2025-01-31**|**Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**|Aurora Rofena et.al.|[2501.19176v1](http://arxiv.org/abs/2501.19176v1)|null|
|**2025-01-31**|**Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**|Xiangyu Sun et.al.|[2501.19086v1](http://arxiv.org/abs/2501.19086v1)|null|
|**2025-01-30**|**Survey and Improvement Strategies for Gene Prioritization with Large Language Models**|Matthew Neeley et.al.|[2501.18794v1](http://arxiv.org/abs/2501.18794v1)|null|
|**2025-01-30**|**Synthetic Data Generation for Augmenting Small Samples**|Dan Liu et.al.|[2501.18741v1](http://arxiv.org/abs/2501.18741v1)|null|
|**2025-01-30**|**A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**|Yifan Wang et.al.|[2501.18367v1](http://arxiv.org/abs/2501.18367v1)|null|
|**2025-01-30**|**MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**|Yuxin Zuo et.al.|[2501.18362v1](http://arxiv.org/abs/2501.18362v1)|null|
|**2025-01-30**|**CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**|Yicheng Wu et.al.|[2501.18328v1](http://arxiv.org/abs/2501.18328v1)|null|
|**2025-01-30**|**A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**|Shayli Farshchiha et.al.|[2501.18294v1](http://arxiv.org/abs/2501.18294v1)|null|
|**2025-01-30**|**The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**|Anup Saha et.al.|[2501.18270v1](http://arxiv.org/abs/2501.18270v1)|null|
|**2025-01-30**|**Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**|Malte TÃ¶lle et.al.|[2501.18237v1](http://arxiv.org/abs/2501.18237v1)|null|
|**2025-01-30**|**Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**|Min Hun Lee et.al.|[2501.18108v1](http://arxiv.org/abs/2501.18108v1)|null|
|**2025-01-30**|**Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**|Pratik S. Sachdeva et.al.|[2501.18081v1](http://arxiv.org/abs/2501.18081v1)|null|
|**2025-01-30**|**Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**|Pir Bakhsh Khokhar et.al.|[2501.18071v1](http://arxiv.org/abs/2501.18071v1)|null|
|**2025-01-29**|**Current Pathology Foundation Models are unrobust to Medical Center Differences**|Edwin D. de Jong et.al.|[2501.18055v1](http://arxiv.org/abs/2501.18055v1)|null|
|**2025-01-29**|**Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**|Zijie Liu et.al.|[2501.17860v1](http://arxiv.org/abs/2501.17860v1)|null|
|**2025-01-29**|**GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**|Ziang Liu et.al.|[2501.17855v1](http://arxiv.org/abs/2501.17855v1)|null|
|**2025-01-29**|**Tonguescape: Exploring Language Models Understanding of Vowel Articulation**|Haruki Sakajo et.al.|[2501.17643v1](http://arxiv.org/abs/2501.17643v1)|[link](https://github.com/sj-h4/tonguescape-builder)|
|**2025-01-29**|**Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**|Manish Sanwal et.al.|[2501.18645v1](http://arxiv.org/abs/2501.18645v1)|null|
|**2025-01-29**|**An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**|Wenqi Li et.al.|[2501.17555v1](http://arxiv.org/abs/2501.17555v1)|null|
|**2025-01-29**|**LLM Assistance for Pediatric Depression**|Mariia Ignashina et.al.|[2501.17510v1](http://arxiv.org/abs/2501.17510v1)|null|
|**2025-01-28**|**Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**|Chongyu Qu et.al.|[2501.17343v1](http://arxiv.org/abs/2501.17343v1)|null|
|**2025-01-28**|**Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**|Mingyu Derek Ma et.al.|[2501.17338v1](http://arxiv.org/abs/2501.17338v1)|null|
|**2025-01-28**|**Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**|Mingyu Derek Ma et.al.|[2501.17326v1](http://arxiv.org/abs/2501.17326v1)|null|
|**2025-01-28**|**Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**|Peilong Wang et.al.|[2501.17286v1](http://arxiv.org/abs/2501.17286v1)|null|
|**2025-01-28**|**ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**|Mohammadreza Saraei et.al.|[2501.17260v1](http://arxiv.org/abs/2501.17260v1)|[link](https://github.com/mrsaraei/vit-2spn)|
|**2025-01-28**|**A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**|Suresh Babu Nettur et.al.|[2501.17160v1](http://arxiv.org/abs/2501.17160v1)|null|
|**2025-01-28**|**Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**|Reza Ghorbani et.al.|[2501.17152v1](http://arxiv.org/abs/2501.17152v1)|null|
|**2025-01-28**|**Irony Detection, Reasoning and Understanding in Zero-shot Learning**|Peiling Yi et.al.|[2501.16884v1](http://arxiv.org/abs/2501.16884v1)|null|
|**2025-01-28**|**Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**|Keqi Han et.al.|[2501.17207v1](http://arxiv.org/abs/2501.17207v1)|[link](https://github.com/learningkeqi/rethinkingbca)|
|**2025-01-28**|**Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**|Fengpei Yuan et.al.|[2501.17206v1](http://arxiv.org/abs/2501.17206v1)|null|
|**2025-01-28**|**Efficient Knowledge Distillation of SAM for Medical Image Segmentation**|Kunal Dasharath Patil et.al.|[2501.16740v1](http://arxiv.org/abs/2501.16740v1)|null|
|**2025-01-28**|**VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**|Philip Chung et.al.|[2501.16672v1](http://arxiv.org/abs/2501.16672v1)|[link](https://github.com/philipchung/verifact)|
|**2025-01-28**|**Vision-based autonomous structural damage detection using data-driven methods**|Seyyed Taghi Ataei et.al.|[2501.16662v2](http://arxiv.org/abs/2501.16662v2)|null|
|**2025-01-28**|**Molecular-driven Foundation Model for Oncologic Pathology**|Anurag Vaidya et.al.|[2501.16652v1](http://arxiv.org/abs/2501.16652v1)|null|
|**2025-01-27**|**Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**|Jing Zhang et.al.|[2501.16282v1](http://arxiv.org/abs/2501.16282v1)|null|
|**2025-01-27**|**Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**|Huayu Li et.al.|[2501.16215v1](http://arxiv.org/abs/2501.16215v1)|[link](https://github.com/HuayuLiArizona/Conformalized-Multiple-Instance-Learning-For-MedTS)|
|**2025-01-27**|**Atla Selene Mini: A General Purpose Evaluation Model**|Andrei Alexandru et.al.|[2501.17195v1](http://arxiv.org/abs/2501.17195v1)|[link](https://huggingface.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B)|
|**2025-01-27**|**An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases**|Shaheer Ahmad Khan et.al.|[2501.15969v1](http://arxiv.org/abs/2501.15969v1)|null|
|**2025-01-27**|**Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI**|Taymaz Akan et.al.|[2501.15733v1](http://arxiv.org/abs/2501.15733v1)|null|
|**2025-01-27**|**A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks**|Dong Li et.al.|[2501.15724v1](http://arxiv.org/abs/2501.15724v1)|null|
|**2025-01-26**|**Beyond Benchmarks: On The False Promise of AI Regulation**|Gabriel Stanovsky et.al.|[2501.15693v1](http://arxiv.org/abs/2501.15693v1)|null|
|**2025-01-26**|**Comparative clinical evaluation of "memory-efficient" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest**|Mahshid shiri et.al.|[2501.15572v1](http://arxiv.org/abs/2501.15572v1)|null|
|**2025-01-26**|**AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications**|Muhammad Aftab et.al.|[2501.15489v1](http://arxiv.org/abs/2501.15489v1)|null|
|**2025-01-26**|**Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models**|Solha Kang et.al.|[2501.15452v1](http://arxiv.org/abs/2501.15452v1)|null|
|**2025-01-25**|**An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis**|Arya Rahgozar et.al.|[2501.17181v1](http://arxiv.org/abs/2501.17181v1)|null|
|**2025-01-25**|**Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations**|Harshita Chopra et.al.|[2501.15056v1](http://arxiv.org/abs/2501.15056v1)|null|
|**2025-01-24**|**Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach**|Md. Kamrul Hasan et.al.|[2501.14929v1](http://arxiv.org/abs/2501.14929v1)|null|
|**2025-01-24**|**Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs**|Hang Luo et.al.|[2501.14892v1](http://arxiv.org/abs/2501.14892v1)|null|
|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719v1](http://arxiv.org/abs/2501.14719v1)|null|
|**2025-01-24**|**GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration**|Ziwen Li et.al.|[2501.16382v1](http://arxiv.org/abs/2501.16382v1)|[link](https://github.com/aaronli43/grappi)|
|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693v1](http://arxiv.org/abs/2501.14693v1)|null|
|**2025-01-24**|**Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**|Dmitry Ryabtsev et.al.|[2501.14689v1](http://arxiv.org/abs/2501.14689v1)|null|
|**2025-01-24**|**Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**|Fuping Wu et.al.|[2501.14685v1](http://arxiv.org/abs/2501.14685v1)|null|
|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654v1](http://arxiv.org/abs/2501.14654v1)|[link](https://github.com/stanfordmlgroup/medagentbench)|
|**2025-01-24**|**Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis**|Xu Chen et.al.|[2501.18614v1](http://arxiv.org/abs/2501.18614v1)|null|
|**2025-01-24**|**Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**|Walid Yassine et.al.|[2501.14483v1](http://arxiv.org/abs/2501.14483v1)|null|
|**2025-01-24**|**Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**|Taehan Kim et.al.|[2501.14469v1](http://arxiv.org/abs/2501.14469v1)|null|
|**2025-01-24**|**ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**|Yoni Schirris et.al.|[2501.14379v1](http://arxiv.org/abs/2501.14379v1)|[link](https://github.com/nki-ai/ectil)|
|**2025-01-24**|**Optimal Signal Decomposition-based Multi-Stage Learning for Battery Health Estimation**|Vijay Babu Pamshetti et.al.|[2501.16377v1](http://arxiv.org/abs/2501.16377v1)|null|
|**2025-01-24**|**Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation**|Cong-Duy Nguyen et.al.|[2501.14166v1](http://arxiv.org/abs/2501.14166v1)|null|
|**2025-01-24**|**Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration**|Mojtaba Safari et.al.|[2501.14158v1](http://arxiv.org/abs/2501.14158v1)|[link](https://github.com/mosaf/awesome-dl-based-cs-mri)|
|**2025-01-23**|**MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning**|Joshua Davis et.al.|[2501.14105v1](http://arxiv.org/abs/2501.14105v1)|[link](https://github.com/lindvalllab/medslice)|
|**2025-01-23**|**Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models**|Jakob Krogh Petersen et.al.|[2501.14051v1](http://arxiv.org/abs/2501.14051v1)|[link](https://github.com/jakekrogh/3d-clip-for-brain-mri)|
|**2025-01-23**|**Leveraging Multiphase CT for Quality Enhancement of Portal Venous CT: Utility for Pancreas Segmentation**|Xinya Wang et.al.|[2501.14013v1](http://arxiv.org/abs/2501.14013v1)|null|
|**2025-01-23**|**Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data**|Frederik Pahde et.al.|[2501.13818v1](http://arxiv.org/abs/2501.13818v1)|[link](https://github.com/frederikpahde/medical-ai-safety)|
|**2025-01-23**|**Question Answering on Patient Medical Records with Private Fine-Tuned LLMs**|Sara Kothari et.al.|[2501.13687v1](http://arxiv.org/abs/2501.13687v1)|null|
|**2025-01-23**|**How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization**|Shezheng Song et.al.|[2501.13669v1](http://arxiv.org/abs/2501.13669v1)|null|
|**2025-01-23**|**Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management**|Yuxuan Liu et.al.|[2501.13587v2](http://arxiv.org/abs/2501.13587v2)|null|
|**2025-01-23**|**LLMs Can Plan Only If We Tell Them**|Bilgehan Sel et.al.|[2501.13545v1](http://arxiv.org/abs/2501.13545v1)|null|
|**2025-01-23**|**Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs**|Bhumika Gupta et.al.|[2501.13984v1](http://arxiv.org/abs/2501.13984v1)|null|
|**2025-01-23**|**A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability**|Bishwash Paneru et.al.|[2501.13369v1](http://arxiv.org/abs/2501.13369v1)|null|
|**2025-01-23**|**Unveiling Discrete Clues: Superior Healthcare Predictions for Rare Diseases**|Chuang Zhao et.al.|[2501.16373v1](http://arxiv.org/abs/2501.16373v1)|[link](https://github.com/data-designer/udchealth)|
|**2025-01-22**|**QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks**|Naman Jain et.al.|[2501.13165v1](http://arxiv.org/abs/2501.13165v1)|null|
|**2025-01-22**|**AirRadar: Inferring Nationwide Air Quality in China with Deep Neural Networks**|Qiongyan Wang et.al.|[2501.13141v1](http://arxiv.org/abs/2501.13141v1)|null|
|**2025-01-22**|**Estimating the Conformal Prediction Threshold from Noisy Labels**|Coby Penso et.al.|[2501.12749v1](http://arxiv.org/abs/2501.12749v1)|[link](https://github.com/cobypenso/noise-aware-conformal-prediction)|
|**2025-01-22**|**Applications and Challenges of AI and Microscopy in Life Science Research: A Review**|Himanshu Buckchash et.al.|[2501.13135v1](http://arxiv.org/abs/2501.13135v1)|null|
|**2025-01-22**|**FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis**|Haoxuan Che et.al.|[2501.13967v2](http://arxiv.org/abs/2501.13967v2)|null|
|**2025-01-22**|**CAND: Cross-Domain Ambiguity Inference for Early Detecting Nuanced Illness Deterioration**|Lo Pang-Yun Ting et.al.|[2501.16365v1](http://arxiv.org/abs/2501.16365v1)|null|
|**2025-01-21**|**Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related to Post COVID-19 Condition**|Juan Andres Medina Florez et.al.|[2501.12538v2](http://arxiv.org/abs/2501.12538v2)|null|
|**2025-01-21**|**Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature Extractor**|Jiaqi Guo et.al.|[2501.12524v1](http://arxiv.org/abs/2501.12524v1)|[link](https://github.com/guojiaqi-1020/medivlad)|
|**2025-01-21**|**FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression**|Phuoc Duong Huy Chu et.al.|[2501.12336v1](http://arxiv.org/abs/2501.12336v1)|null|
|**2025-01-21**|**CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**|Cristiano PatrÃ­cio et.al.|[2501.12266v1](http://arxiv.org/abs/2501.12266v1)|null|
|**2025-01-21**|**Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes**|Stefan Lenz et.al.|[2501.12106v1](http://arxiv.org/abs/2501.12106v1)|[link](https://github.com/stefan-m-lenz/urollmeval)|
|**2025-01-21**|**Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET**|Fatih Aksu et.al.|[2501.12425v1](http://arxiv.org/abs/2501.12425v1)|null|
|**2025-01-21**|**Adaptive Class Learning to Screen Diabetic Disorders in Fundus Images of Eye**|Shramana Dey et.al.|[2501.12048v1](http://arxiv.org/abs/2501.12048v1)|null|
|**2025-01-21**|**Tackling Small Sample Survival Analysis via Transfer Learning: A Study of Colorectal Cancer Prognosis**|Yonghao Zhao et.al.|[2501.12421v1](http://arxiv.org/abs/2501.12421v1)|[link](https://github.com/yonghaozhao722/tsf)|
|**2025-01-21**|**Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)**|Jadon Geathers et.al.|[2501.13957v1](http://arxiv.org/abs/2501.13957v1)|null|
|**2025-01-21**|**Data-driven Detection and Evaluation of Damages in Concrete Structures: Using Deep Learning and Computer Vision**|Saeid Ataei et.al.|[2501.11836v1](http://arxiv.org/abs/2501.11836v1)|null|
|**2025-01-20**|**GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease**|Wenjie Kang et.al.|[2501.11715v1](http://arxiv.org/abs/2501.11715v1)|null|
|**2025-01-20**|**Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)**|Brian E. Perron et.al.|[2501.11705v1](http://arxiv.org/abs/2501.11705v1)|null|
|**2025-01-20**|**Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data**|Majid Farhadloo et.al.|[2501.11695v1](http://arxiv.org/abs/2501.11695v1)|null|
|**2025-01-20**|**Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness**|Ambreesh Parthasarathy et.al.|[2501.13120v1](http://arxiv.org/abs/2501.13120v1)|null|
|**2025-01-20**|**Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications**|Yuxing Lu et.al.|[2501.11632v2](http://arxiv.org/abs/2501.11632v2)|null|
|**2025-01-20**|**Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing**|Chaoqing Tang et.al.|[2501.11592v2](http://arxiv.org/abs/2501.11592v2)|[link](https://github.com/billttzqgbt/cscoefficientslearning)|
|**2025-01-20**|**Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography**|Jakub Nalepa et.al.|[2501.11428v1](http://arxiv.org/abs/2501.11428v1)|null|
|**2025-01-20**|**Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations**|Alicia Vidler et.al.|[2501.16356v1](http://arxiv.org/abs/2501.16356v1)|null|
|**2025-01-20**|**RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?**|Haotian Xu et.al.|[2501.11284v1](http://arxiv.org/abs/2501.11284v1)|null|
|**2025-01-20**|**Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features**|Osama Ahmad et.al.|[2501.11270v1](http://arxiv.org/abs/2501.11270v1)|null|

#### Abstracts
##### **Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates**
2501.19338v1 by Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente IstvÃ¡n LÃ¡nczi, Andras Jakab

Developing new methods for the automated analysis of clinical fetal and
neonatal MRI data is limited by the scarcity of annotated pathological datasets
and privacy concerns that often restrict data sharing, hindering the
effectiveness of deep learning models. We address this in two ways. First, we
introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to
generate high-quality synthetic pathological fetal and neonatal MRIs from
semantic label images. Second, we enhance training data by modifying healthy
label images through morphological alterations to simulate conditions such as
ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.
By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs
from these modified pathological label images. Radiologists rated the synthetic
MRIs as significantly (p < 0.05) superior in quality and diagnostic value
compared to real MRIs, demonstrating features such as blood vessels and choroid
plexus, and improved alignment with label annotations. Synthetic pathological
data enhanced state-of-the-art nnUNet segmentation performance, particularly
for severe ventriculomegaly cases, with the greatest improvements achieved in
ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores
the potential of generative AI as transformative tool for data augmentation,
offering improved segmentation performance in pathological cases. This
development represents a significant step towards improving analysis and
segmentation accuracy in prenatal imaging, and also offers new ways for data
anonymization through the generation of pathologic image data.

æè¦ï¼<paragraph>éç¼ç¨æ¼èªååæè¨åºèååæ°çå MRI è³æçæ°æ¹æ³åå°æ¨è¨»ççè³æéç¨å°åé±ç§åé¡çéå¶ï¼éäºåé¡éå¸¸æéå¶è³æå±äº«ï¼å¾èé»ç¤æ·±åº¦å­¸ç¿æ¨¡åçæææ§ãæåä»¥å©ç¨®æ¹å¼è§£æ±ºéååé¡ãé¦åï¼æåå¼å¥äº Fetal&Neonatal-DDPMï¼éæ¯ä¸åæ°ç©çæ´æ£æ¨¡åæ¶æ§ï¼æ¨å¨å¾èªç¾©æ¨ç±¤å½±åçæé«åè³ªçåæççèååæ°çå MRIãå¶æ¬¡ï¼æåééå½¢ææ¹è®ä¾ä¿®æ¹å¥åº·çæ¨ç±¤å½±åï¼ä»¥æ¨¡æ¬è¦å®¤æ´å¤§ãå°è¦åæ©è¦å°è¦ç¼è²ä¸å¨ä»¥åå°é ­ç¸å½¢ç­ææ³ï¼å¾èå¢å¼·è¨ç·´è³æãééå©ç¨ Fetal&Neonatal-DDPMï¼æåå¾éäºä¿®æ¹å¾çççæ¨ç±¤å½±åä¸­åæäºé¼çççç MRIãæ¾å°ç§é«å¸«è©ä¼°åæ MRI çåè³ªåè¨ºæ·å¹å¼é¡¯èåªæ¼çå¯¦ MRIï¼p < 0.05ï¼ï¼å±ç¤ºäºè¡ç®¡åèçµ¡å¢ç­ç¹å¾µï¼ä¸¦æ¹åäºèæ¨ç±¤è¨»è§£çä¸è´æ§ãåæççè³æå¢å¼·äºæåé²ç nnUNet åå²æè½ï¼ç¹å¥æ¯å°æ¼å´éçè¦å®¤æ´å¤§çä¾ï¼å¶ä¸­è¦å®¤åå²ï¼Dice åæ¸ï¼0.9253 å° 0.7317ï¼çæ¹åæå¤§ãéé ç ç©¶å¼·èª¿äºçæå¼ AI ä½çºè³ææ´åè½åå·¥å·çæ½åï¼å¨çççä¾ä¸­æä¾äºæ¹åçåå²æè½ãéé ç¼å±ä»£è¡¨äºæ¹åç¢åå½±ååæååå²æºç¢ºæ§çéè¦ä¸æ­¥ï¼ä¹çºééçæççå½±åè³æä¾é²è¡è³æå¿ååæä¾äºæ°æ¹æ³ã</paragraph>

##### **Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks**
2501.19271v1 by Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett

Concept-based explanation methods, such as concept bottleneck models (CBMs),
aim to improve the interpretability of machine learning models by linking their
decisions to human-understandable concepts, under the critical assumption that
such concepts can be accurately attributed to the network's feature space.
However, this foundational assumption has not been rigorously validated, mainly
because the field lacks standardised metrics and benchmarks to assess the
existence and spatial alignment of such concepts. To address this, we propose
three metrics: the concept global importance metric, the concept existence
metric, and the concept location metric, including a technique for visualising
concept activations, i.e., concept activation mapping. We benchmark post-hoc
CBMs to illustrate their capabilities and challenges. Through qualitative and
quantitative experiments, we demonstrate that, in many cases, even the most
important concepts determined by post-hoc CBMs are not present in input images;
moreover, when they are present, their saliency maps fail to align with the
expected regions by either activating across an entire object or misidentifying
relevant concept-specific regions. We analyse the root causes of these
limitations, such as the natural correlation of concepts. Our findings
underscore the need for more careful application of concept-based explanation
techniques especially in settings where spatial interpretability is critical.

æè¦ï¼åºæ¼æ¦å¿µçè§£éæ¹æ³ï¼ä¾å¦æ¦å¿µç¶é ¸æ¨¡å (CBM)ï¼æ¨å¨ééå°æ©å¨å­¸ç¿æ¨¡åçæ±ºç­èäººé¡å¯çè§£çæ¦å¿µé£çµï¼ä¾æåæ©å¨å­¸ç¿æ¨¡åçå¯è§£éæ§ï¼å¶ééµåè¨­çºæ­¤é¡æ¦å¿µå¯ä»¥æºç¢ºå°æ­¸å æ¼ç¶²è·¯çç¹å¾µç©ºéãç¶èï¼æ­¤é åºç¤åè¨­å°æªç¶éå´æ ¼é©è­ï¼ä¸»è¦æ¯å çºè©²é åç¼ºä¹æ¨æºåææ¨ååºæºä¾è©ä¼°æ­¤é¡æ¦å¿µçå­å¨åç©ºéå°é½ãçºäºè§£æ±ºéååé¡ï¼æåæåºä¸é ææ¨ï¼æ¦å¿µæ´é«éè¦æ§ææ¨ãæ¦å¿µå­å¨ææ¨åæ¦å¿µä½ç½®ææ¨ï¼åæ¬ä¸ç¨®ç¨æ¼è¦è¦ºåæ¦å¿µæ´»åï¼å³æ¦å¿µæ´»åå°æçæè¡ãæåå°äºå¾ CBM é²è¡åºæºæ¸¬è©¦ï¼ä»¥èªªæå®åçè½ååææ°ãééå®æ§åå®éå¯¦é©ï¼æåè­æï¼å¨è¨±å¤ææ³ä¸ï¼å³ä½¿æ¯ç±äºå¾ CBM ç¢ºå®çæéè¦æ¦å¿µä¹ä¸å­å¨æ¼è¼¸å¥å½±åä¸­ï¼æ­¤å¤ï¼ç¶å®åå­å¨æï¼å®åçé¡¯èæ§åç¡æ³èé æçååå°é½ï¼åå å¯è½æ¯å®åå¨æ´åç©ä»¶ä¸­æ´»åï¼æé¯èª¤è¾¨è­åºç¸éçæ¦å¿µç¹å®ååãæååæäºéäºéå¶çæ ¹æ¬åå ï¼ä¾å¦æ¦å¿µçèªç¶ç¸éæ§ãæåçç ç©¶çµæå¼·èª¿éè¦æ´å°å¿å°æç¨åºæ¼æ¦å¿µçè§£éæè¡ï¼ç¹å¥æ¯å¨ç©ºéå¯è§£éæ§è³ééè¦çè¨­å®ä¸­ã

##### **Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence**
2501.19176v1 by Aurora Rofena, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi

Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field.

æè¦ï¼å¨è¦éæ¸ä½ä¹³æ¿æå½± (FFDM) æ¯å¸¸è¦ä¹³çç¯©æª¢çä¸»è¦å½±åæ¨¡å¼ï¼ç¶èï¼å°æ¼ä¹³æ¿çµç¹ç·»å¯æçºç¶­åè«çè®çæ£èï¼å¶æææ§åå°éå¶ãå°æ¯å¢å¼·åè­ä¹³æ¿æå½± (CESM) æ¯ä¸ç¨®äºç´å½±åæè¡ï¼å¯æåè«ç¤åµæ¸¬çæºç¢ºåº¦ãåç®¡å¦æ­¤ï¼ç±æ¼è¼é«çè¼»å°æé²ãå°æ¯åçä½¿ç¨åæéçå¯åæ§ï¼éå¶äºå¶æç¨ãå æ­¤ï¼CESM éå¸¸åä¿çå¨ç¹å®ææ³ä¸ä½¿ç¨ï¼åç®¡ CESM çè¨ºæ·æè½è¼ä½³ï¼ä½è¨±å¤æ£èä»åªè½ä¾è³´ FFDMãéç¶åçæª¢æ¥ä»ç¶æ¯æç¢ºè¨ºæ·çé»éæ¨æºï¼ä½éæ¯ä¸ç¨®ä¾µå¥æ§ç¨åºï¼å¯è½æè®æ£èæå°ä¸é©ãæåå¼å¥ä¸ç¨®å¤æ¨¡å¼ãå¤è¦åçæ·±åº¦å­¸ç¿æ¹æ³é²è¡èæ¬åçæª¢æ¥ï¼å° FFDM å CESM æ¨¡å¼æ´åå¨é ­å°¾ååå§å¤å´æè¦åä¸­ï¼ä»¥å°çç¶åé¡çºæ¡æ§æè¯æ§ãçºäºè§£æ±º CESM è³æç¼ºå¤±çææ°ï¼æåå©ç¨çæå¼äººå·¥æºæ§å¾ FFDM ææä¸­æ¨ç® CESM å½±åãå¯¦é©çµæè­æï¼æ´å CESM æ¨¡å¼å°æ¼æåèæ¬åçæª¢æ¥çæè½è³ééè¦ãç¶çå¯¦ CESM è³æç¼ºå¤±æï¼åæ CESM å½±åè¢«è­ææ¯ææçï¼å¶æè½åªæ¼å®ç¨ä½¿ç¨ FFDMï¼ç¹å¥æ¯å¨çµå FFDM å CESM æ¨¡å¼çå¤æ¨¡å¼éç½®ä¸­ãææåºçæ¹æ³ææ½åæ¹åè¨ºæ·å·¥ä½æµç¨ï¼çºè¨åºé«å¸«æä¾å¢å¼·çæºæ§å·¥å·ï¼ä»¥æé«è¨ºæ·æºç¢ºåº¦åæ£èç§è­·ãæ­¤å¤ï¼ä½çºå°ç ç©¶ç¤¾ç¾¤çè²¢ç»ï¼æåå¬éç¼å¸å¨å¯¦é©ä¸­ä½¿ç¨çè³æéï¼ä»¥ä¿é²æ­¤é åçé²ä¸æ­¥é²å±ã

##### **Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification**
2501.19086v1 by Xiangyu Sun, Xiaoguang Zou, Yuanquan Wu, Guotai Wang, Shaoting Zhang

X-ray imaging is pivotal in medical diagnostics, offering non-invasive
insights into a range of health conditions. Recently, vision-language models,
such as the Contrastive Language-Image Pretraining (CLIP) model, have
demonstrated potential in improving diagnostic accuracy by leveraging
large-scale image-text datasets. However, since CLIP was not initially designed
for medical images, several CLIP-like models trained specifically on medical
images have been developed. Despite their enhanced performance, issues of
fairness - particularly regarding demographic attributes - remain largely
unaddressed. In this study, we perform a comprehensive fairness analysis of
CLIP-like models applied to X-ray image classification. We assess their
performance and fairness across diverse patient demographics and disease
categories using zero-shot inference and various fine-tuning techniques,
including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation
(LoRA), and full fine-tuning. Our results indicate that while fine-tuning
improves model accuracy, fairness concerns persist, highlighting the need for
further fairness interventions in these foundational models.

æè¦ï¼X åå½±åå¨é«çè¨ºæ·ä¸­è³ééè¦ï¼è½æä¾åç¨®å¥åº·çæ³çéä¾µå¥æ§è¦è§£ãæè¿ï¼è¦è¦ºèªè¨æ¨¡åï¼ä¾å¦å°æ¯èªè¨å½±åé è¨ç·´ (CLIP) æ¨¡åï¼å·²è­æææ½åééå©ç¨å¤§è¦æ¨¡å½±åæå­è³æéä¾æ¹åè¨ºæ·æºç¢ºæ§ãç¶èï¼ç±æ¼ CLIP æåä¸¦éè¨­è¨ç¨æ¼é«çå½±åï¼å æ­¤å·²ç¶éç¼äºæ¸åç¹å¥éå°é«çå½±åè¨ç·´çé¡ä¼¼ CLIP æ¨¡åãåç®¡å®åçæè½æææåï¼ä½å¬å¹³æ§çåé¡ï¼ç¹å¥æ¯éæ¼äººå£çµ±è¨å±¬æ§ï¼ä»å¤§å¤æªç²è§£æ±ºãå¨æ¬ç ç©¶ä¸­ï¼æåå°æç¨æ¼ X åå½±ååé¡çé¡ä¼¼ CLIP æ¨¡åå·è¡å¨é¢çå¬å¹³æ§åæãæåä½¿ç¨é¶æ¬¡å­¸ç¿æ¨è«ååç¨®å¾®èª¿æè¡ï¼åæ¬ç·æ§æ¢æ¥ãå¤å±¤æç¥å¨ (MLP)ãä½ç§©é©æ (LoRA) åå®æ´å¾®èª¿ï¼ä¾è©ä¼°å®åå¨ä¸åæ£èäººå£çµ±è¨åç¾çé¡å¥ä¸­çæè½åå¬å¹³æ§ãæåççµæè¡¨æï¼éç¶å¾®èª¿ææ¹åæ¨¡åæºç¢ºæ§ï¼ä½å¬å¹³æ§åé¡ä»ç¶å­å¨ï¼å¼·èª¿éè¦å¨éäºåºç¤æ¨¡åä¸­é²ä¸æ­¥æ¡åå¬å¹³æ§å¹²é æªæ½ã

##### **Survey and Improvement Strategies for Gene Prioritization with Large Language Models**
2501.18794v1 by Matthew Neeley, Guantong Qi, Guanchu Wang, Ruixiang Tang, Dongxue Mao, Chaozhong Liu, Sasidhar Pasupuleti, Bo Yuan, Fan Xia, Pengfei Liu, Zhandong Liu, Xia Hu

Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.

æè¦ï¼ç½è¦ç¾çç±æ¼æ£èæ¸ææéåéºå³å¤æ¨£æ§ï¼è¨ºæ·èµ·ä¾å·æææ°æ§ãåç®¡è®ç°åªåç´æåºæè¡é²æ­¥ï¼ä½è¨±å¤çä¾ä»æªå¾å°è¨ºæ·ãåç®¡å¤§åèªè¨æ¨¡å (LLM) å¨é«å­¸èè©¦ä¸­è¡¨ç¾è¯å¥½ï¼ä½å®åå¨è¨ºæ·ç½è¦éºå³ç¾çæ¹é¢çæææ§å°æªå¾å°è©ä¼°ãçºäºè­å¥è´çåºå ï¼æåå°åç¨® LLM é²è¡äºåºå åªåç´æåºåºæºæ¸¬è©¦ãä½¿ç¨å¤æºè½é«åäººé¡è¡¨åæ¬ä½ (HPO) åé¡ï¼æåæ ¹æè¡¨ååå¯è§£æ±ºæ§å°æ£èé²è¡äºåé¡ãé¨èåºå çµå¤§å°çå¢å ï¼LLM æ§è½ä¸éï¼å æ­¤æåä½¿ç¨åèæ²»ä¹ç­ç¥å°ä»»ååè§£çºæ´å°çå­éãå¨åºç·ä¸­ï¼GPT-4 åªæ¼å¶ä» LLMï¼å¨æ­£ç¢ºæåºè´çåºå æ¹é¢éå°è¿ 30% çæºç¢ºåº¦ãå¤æºè½é«å HPO æ¹æ³æå©æ¼ååè§£æ±ºæä¿¡å¿ççä¾åå·æææ°æ§ççä¾ï¼å¼·èª¿å·²ç¥åºå -è¡¨åéè¯åè¡¨åç¹ç°æ§çéè¦æ§ãæåç¼ç¾å·æç¹å®è¡¨åææç¢ºéè¯ççä¾å¾å°æ´æºç¢ºçè§£æ±ºãç¶èï¼æåè§å¯å°å°ç ç©¶ååçåºå åè¼¸å¥é åºæææ§çåå·®ï¼éé»ç¤äºåºå åªåç´æåºãæåçåèæ²»ä¹ç­ç¥ééåæéäºåå·®ä¾æé«æºç¢ºæ§ãééå©ç¨ HPO åé¡ãæ°ç©çå¤æºè½é«æè¡åæåç LLM ç­ç¥ï¼æåèæåçåºç·è©ä¼°ç¸æ¯æé«äºè´çåºå è­å¥æºç¢ºæ§ãéç¨®æ¹æ³ç°¡åäºç½è¦ç¾ççè¨ºæ·ï¼ä¿é²äºå°æªè§£æ±ºçä¾çéæ°åæï¼ä¸¦å éäºåºå ç¼ç¾ï¼æ¯æäºé¶åè¨ºæ·åæ²»ççéç¼ã

##### **Synthetic Data Generation for Augmenting Small Samples**
2501.18741v1 by Dan Liu, Samer El Kababji, Nicholas Mitsakakis, Lisa Pilgram, Thomas Walters, Mark Clemons, Greg Pond, Alaa El-Hussuna, Khaled El Emam

Small datasets are common in health research. However, the generalization
performance of machine learning models is suboptimal when the training datasets
are small. To address this, data augmentation is one solution. Augmentation
increases sample size and is seen as a form of regularization that increases
the diversity of small datasets, leading them to perform better on unseen data.
We found that augmentation improves prognostic performance for datasets that:
have fewer observations, with smaller baseline AUC, have higher cardinality
categorical variables, and have more balanced outcome variables. No specific
generative model consistently outperformed the others. We developed a decision
support model that can be used to inform analysts if augmentation would be
useful. For seven small application datasets, augmenting the existing data
results in an increase in AUC between 4.31% (AUC from 0.71 to 0.75) and 43.23%
(AUC from 0.51 to 0.73), with an average 15.55% relative improvement,
demonstrating the nontrivial impact of augmentation on small datasets
(p=0.0078). Augmentation AUC was higher than resampling only AUC (p=0.016). The
diversity of augmented datasets was higher than the diversity of resampled
datasets (p=0.046).

æè¦ï¼å¨å¥åº·ç ç©¶ä¸­ï¼å°åæ°æ®éå¾å¸¸è§ãç¶èï¼å½è®­ç»æ°æ®éè¾å°æ¶ï¼æºå¨å­¦ä¹ æ¨¡åçæ³åæ§è½å¹¶ä¸çæ³ãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æ°æ®å¢å¼ºæ¯ä¸ç§è§£å³æ¹æ¡ãå¢å¼ºå¢å äºæ ·æ¬éï¼å¹¶è¢«è§ä¸ºä¸ç§æ­£ååå½¢å¼ï¼å®å¢å äºå°åæ°æ®éçå¤æ ·æ§ï¼ä»èä½¿å¶å¨æªè§æ°æ®ä¸è¡¨ç°å¾æ´å¥½ãæä»¬åç°ï¼å¢å¼ºæé«äºä»¥ä¸æ°æ®éçé¢æµæ§è½ï¼å·æè¾å°çè§æµå¼ãè¾å°çåºçº¿ AUCãè¾é«çåºæ°åç±»åéä»¥åæ´å¹³è¡¡çç»æåéãæ²¡æç¹å®ççææ¨¡åå§ç»ä¼äºå¶ä»æ¨¡åãæä»¬å¼åäºä¸ä¸ªå³ç­æ¯ææ¨¡åï¼å¯ç¨äºåç¥åæå¸å¢å¼ºæ¯å¦æç¨ãå¯¹äºä¸ä¸ªå°ååºç¨ç¨åºæ°æ®éï¼å¢å¼ºç°ææ°æ®å¯¼è´ AUC å¢å  4.31%ï¼AUC ä» 0.71 å¢å å° 0.75ï¼å 43.23%ï¼AUC ä» 0.51 å¢å å° 0.73ï¼ï¼å¹³åç¸å¯¹æ¹è¿ 15.55%ï¼è¿è¡¨æäºå¢å¼ºå¯¹å°åæ°æ®éçéå¹³å¡å½±åï¼p=0.0078ï¼ãå¢å¼º AUC é«äºä»éæ°éæ ·ç AUCï¼p=0.016ï¼ãå¢å¼ºæ°æ®éçå¤æ ·æ§é«äºéæ°éæ ·æ°æ®éçå¤æ ·æ§ï¼p=0.046ï¼ã

##### **A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series**
2501.18367v1 by Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li

In medical time series disease diagnosis, two key challenges are
identified.First, the high annotation cost of medical data leads to overfitting
in models trained on label-limited, single-center datasets. To address this, we
propose incorporating external data from related tasks and leveraging AE-GAN to
extract prior knowledge,providing valuable references for downstream tasks.
Second, many existing studies employ contrastive learning to derive more
generalized medical sequence representations for diagnostic tasks, usually
relying on manually designed diverse positive and negative sample
pairs.However, these approaches are complex, lack generalizability, and fail to
adaptively capture disease-specific features across different conditions.To
overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework),
a framework that integrates a multi-head attention mechanism and adaptively
learns representations from different views through inter-view and intra-view
contrastive learning strategies.Additionally, the pre-trained AE-GAN is used to
reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process.Experiments on three
target datasets demonstrate that our method consistently outperforms seven
other baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease.

æè¦ï¼å¨å»çæ¶é´åºåç¾çè¯æ­ä¸­ï¼ç¡®å®äºä¸¤ä¸ªå³é®ææãé¦åï¼å»çæ°æ®çæ æ³¨ææ¬é«ï¼å¯¼è´å¨æ ç­¾åéçåä¸­å¿æ°æ®éä¸è®­ç»çæ¨¡ååºç°è¿æåãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æä»¬å»ºè®®åå¹¶æ¥èªç¸å³ä»»å¡çå¤é¨æ°æ®ï¼å¹¶å©ç¨ AE-GAN æååéªç¥è¯ï¼ä¸ºä¸æ¸¸ä»»å¡æä¾æä»·å¼çåèãå¶æ¬¡ï¼è®¸å¤ç°æçç ç©¶éç¨å¯¹æ¯å­¦ä¹ æ¥æ¨å¯¼åºæ´éç¨çå»çåºåè¡¨ç¤ºï¼ç¨äºè¯æ­ä»»å¡ï¼éå¸¸ä¾èµäºæå¨è®¾è®¡çåç§æ­£è´æ ·æ¬å¯¹ãç¶èï¼è¿äºæ¹æ³å¤æï¼ç¼ºä¹éç¨æ§ï¼å¹¶ä¸æ æ³èªéåºå°æè·ä¸åæ¡ä»¶ä¸çç¹å®ç¾çç¹å¾ãä¸ºäºåæè¿ä¸ªé®é¢ï¼æä»¬å¼å¥äº LMCFï¼å¯å­¦ä¹ çå¤è§å¾å¯¹æ¯æ¡æ¶ï¼ï¼è¿æ¯ä¸ä¸ªéæäºå¤å¤´æ³¨ææºå¶çæ¡æ¶ï¼å¹¶éè¿è§å¾é´åè§å¾åå¯¹æ¯å­¦ä¹ ç­ç¥èªéåºå°å­¦ä¹ æ¥èªä¸åè§å¾çè¡¨ç¤ºãæ­¤å¤ï¼é¢è®­ç»ç AE-GAN ç¨äºéå»ºç®æ æ°æ®ä¸­çå·®å¼ä½ä¸ºç¾çæ¦çï¼ç¶åå°å¶éæå°å¯¹æ¯å­¦ä¹ è¿ç¨ä¸­ãå¨ä¸ä¸ªç®æ æ°æ®éä¸çå®éªè¡¨æï¼æä»¬çæ¹æ³å§ç»ä¼äºå¶ä»ä¸ä¸ªåºçº¿ï¼çªåºäºå¶å¯¹å»çä¿å¥åºç¨ï¼å¦å¿èæ¢å¡ãé¿å°è¨æµ·é»çåå¸éæ£®ççè¯æ­ï¼çéå¤§å½±åã

##### **MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding**
2501.18362v1 by Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou

We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.

æè¦ï¼æåæ¨åºäº MedXpertQAï¼éæ¯ä¸åæ¥µå·ææ°æ§ä¸å¨é¢çåºæºï¼ç¨æ¼è©ä¼°å°å®¶ç´çé«å­¸ç¥è­ååé²çæ¨çè½åãMedXpertQA åå« 4,460 ååé¡ï¼æ¶µè 17 åå°ç§å 11 åèº«é«ç³»çµ±ãå®åå«å©åå­éï¼ææ¬ç¨æ¼ææ¬è©ä¼°ï¼MM ç¨æ¼å¤æ¨¡å¼è©ä¼°ãå¼å¾æ³¨æçæ¯ï¼MM å¼å¥äºå°å®¶ç´èè©¦é¡ç®ï¼å¶ä¸­åå«å¤æ¨£åçå½±ååè±å¯çè¨åºè³è¨ï¼åæ¬æ£èè¨éåæª¢æ¥çµæï¼éè®å®æå¥æ¼å³çµ±çé«å­¸å¤æ¨¡å¼åºæºï¼å¾èæ¯å¾å½±åæ¨é¡ä¸­ç¢ççç°¡å®åç­å°ãMedXpertQA æ¡ç¨å´æ ¼çéæ¿¾åæ´åï¼ä»¥è§£æ±º MedQA ç­ç¾æåºæºçé£åº¦ä¸è¶³åé¡ï¼ä¸¦ç´å¥å°ç§å§å¡æåé¡ä»¥æé«è¨åºç¸éæ§åå¨é¢æ§ãæåå·è¡è³æåæä»¥éä½è³æå¤æ´©é¢¨éªï¼ä¸¦é²è¡å¤è¼ªå°å®¶å¯©æ¥ä»¥ç¢ºä¿æºç¢ºæ§åå¯é æ§ãæåå¨ MedXpertQA ä¸è©ä¼°äº 16 åé åçæ¨¡åãæ­¤å¤ï¼é«å­¸èç¾å¯¦ä¸ççæ±ºç­å¶å®æå¯åçè¯ç¹«ï¼æä¾äºè±å¯ä¸å·ä»£è¡¨æ§çç°å¢ï¼ç¨æ¼è©ä¼°è¶è¶æ¸å­¸åç¨å¼ç¢¼çæ¨çè½åãçºæ­¤ï¼æåéç¼äºä¸åä»¥æ¨ççºå°åçå­éï¼ä»¥å©æ¼è©ä¼°é¡ o1 çæ¨¡åã

##### **CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes**
2501.18328v1 by Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai

MRI imputation aims to synthesize the missing modality from one or more
available ones, which is highly desirable since it reduces scanning costs and
delivers comprehensive MRI information to enhance clinical diagnosis. In this
paper, we propose a unified model, CodeBrain, designed to adapt to various
brain MRI imputation scenarios. The core design lies in casting various
inter-modality transformations as a full-modality code prediction task. To this
end, CodeBrain is trained in two stages: Reconstruction and Code Prediction.
First, in the Reconstruction stage, we reconstruct each MRI modality, which is
mapped into a shared latent space followed by a scalar quantization. Since such
quantization is lossy and the code is low dimensional, another MRI modality
belonging to the same subject is randomly selected to generate common features
to supplement the code and boost the target reconstruction. In the second
stage, we train another encoder by a customized grading loss to predict the
full-modality codes from randomly masked MRI samples, supervised by the
corresponding quantized codes generated from the first stage. In this way, the
inter-modality transformation is achieved by mapping the instance-specific
codes in a finite scalar space. We evaluated the proposed CodeBrain model on
two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments
demonstrate that our CodeBrain model achieves superior imputation performance
compared to four existing methods, establishing a new state of the art for
unified brain MRI imputation. Codes will be released.

æè¦ï¼MRI è£å®æ¨å¨å¾ä¸åæå¤åå¯ç¨æ¹å¼ä¸­åæéºå¤±çæ¨¡æï¼éæ¯éå¸¸çæ³çï¼å çºå®éä½äºææææ¬ï¼ä¸¦æä¾äºå¨é¢ç MRI è³è¨ä»¥å¢å¼·è¨åºè¨ºæ·ãå¨æ¬æä¸­ï¼æåæåºäºä¸åçµ±ä¸æ¨¡å CodeBrainï¼æ¨å¨é©æåç¨®è¦é¨ MRI è£å®å ´æ¯ãæ ¸å¿è¨­è¨å¨æ¼å°åç¨®æ¨¡æéè½æè½æçºå¨æ¨¡æç¢¼é æ¸¬ä»»åãçºæ­¤ï¼CodeBrain åå©åéæ®µé²è¡è¨ç·´ï¼éå»ºåç¢¼é æ¸¬ãé¦åï¼å¨éå»ºéæ®µï¼æåéå»ºæ¯å MRI æ¨¡æï¼å®è¢«æ å°å°ä¸åå±äº«æ½å¨ç©ºéï¼ç¶å¾é²è¡æ¨ééåãç±æ¼éç¨®éåæ¯ææçï¼ä¸¦ä¸ç¢¼çç¶­åº¦å¾ä½ï¼å æ­¤é¨æ©é¸æå±¬æ¼åä¸ååè©¦èçå¦ä¸å MRI æ¨¡æä¾ç¢çå±åç¹å¾µä»¥è£åç¢¼ä¸¦æåç®æ¨éå»ºãå¨ç¬¬äºéæ®µï¼æåééèªè¨åç´æå¤±è¨ç·´å¦ä¸åç·¨ç¢¼å¨ï¼å¾é¨æ©é®ç½©ç MRI æ¨£æ¬é æ¸¬å¨æ¨¡æç¢¼ï¼ä¸¦ç±ç¬¬ä¸éæ®µç¢ççå°æéåç¢¼é²è¡ç£ç£ãéæ¨£ï¼æ¨¡æéè½ææ¯ééå°ç¹å®æ¼ä¾é çç¢¼æ å°å°ä¸åæéçæ¨éç©ºéä¾å¯¦ç¾çãæåå¨å©åå¬éçè¦é¨ MRI è³æéï¼å³ IXI å BraTS 2023ï¼ä¸è©ä¼°äºææåºç CodeBrain æ¨¡åãå¤§éçå¯¦é©è­æï¼èåç¨®ç¾ææ¹æ³ç¸æ¯ï¼æåç CodeBrain æ¨¡åå¯¦ç¾äºåªç°çè£å®æè½ï¼çºçµ±ä¸çè¦é¨ MRI è£å®å»ºç«äºæ°çæè¡æ°´æºãç¢¼å°æéåºã

##### **A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification**
2501.18294v1 by Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki

Lung cancer is a major issue in worldwide public health, requiring early
diagnosis using stable techniques. This work begins a thorough investigation of
the use of machine learning (ML) methods for precise classification of lung
cancer stages. A cautious analysis is performed to overcome overfitting issues
in model performance, taking into account minimum child weight and learning
rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM,
Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF),
CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted.
Furthermore, the correlation between features and targets is examined using the
deep neural network (DNN) model and thus their capability in detecting complex
patternsis established. It is argued that several ML models can be capable of
classifying lung cancer stages with great accuracy. In spite of the complexity
of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic
Regression excel with superior performance. The models perform better than the
others in lung cancer prediction on the complete set of comparative metrics
like accuracy, precision, recall, and F-1 score

æè¦ï¼èºçæ¯å¨çå¬å±è¡ççä¸å¤§åé¡ï¼éè¦ä½¿ç¨ç©©å®çæè¡é²è¡æ©æè¨ºæ·ãéé å·¥ä½éå§å¾¹åºèª¿æ¥ä½¿ç¨æ©å¨å­¸ç¿ (ML) æ¹æ³ç²¾ç¢ºåé¡èºçåæçä½¿ç¨ææ³ãå·è¡è¬¹æçåæä»¥åææ¨¡åæè½ä¸­çéåº¦æ¬ååé¡ï¼ä¸¦èæ®æå°å­æ¬éåå­¸ç¿çãä¸çµæ©å¨å­¸ç¿ (ML) æ¨¡åï¼åæ¬ XGBoost (XGB)ãLGBMãAdaboostãéè¼¯è¿´æ­¸ (LR)ãæ±ºç­æ¨¹ (DT)ãé¨æ©æ£®æ (RF)ãCatBoost å k æè¿é° (k-NN)ï¼ä»¥ææ¢ççæ¹å¼å·è¡ä¸¦é²è¡å°æ¯ãæ­¤å¤ï¼ä½¿ç¨æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) æ¨¡åæª¢æ¥ç¹å¾µåç®æ¨ä¹éçéè¯æ§ï¼å¾èå»ºç«å®åå¨æª¢æ¸¬è¤éæ¨¡å¼ä¸­çè½åãæäººèªçºï¼å¤å ML æ¨¡åè½å¤ ä»¥å¾é«çæºç¢ºåº¦å°èºçåæé²è¡åé¡ãåç®¡ DNN æ¶æ§å¾è¤éï¼ä½å³çµ± ML æ¨¡åï¼å¦ XGBoostãLGBM åéè¼¯è¿´æ­¸ï¼è¡¨ç¾åºè²ï¼æè½åªç°ãéäºæ¨¡åå¨èºçé æ¸¬ä¸­è¡¨ç¾åªæ¼å¶ä»æ¨¡åï¼å¨æºç¢ºåº¦ãç²¾ç¢ºåº¦ãå¬åçå F-1 åæ¸ç­å®æ´çæ¯è¼ææ¨ä¸­è¡¨ç¾åºè²ã

##### **The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection**
2501.18270v1 by Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, SÃ©raphin Gaborit, Brian D'Alessandro, James Hudson, Gyula SzabÃ³, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia

Artificial intelligence has significantly advanced skin cancer diagnosis by
enabling rapid and accurate detection of malignant lesions. In this domain,
most publicly available image datasets consist of single, isolated skin lesions
positioned at the center of the image. While these lesion-centric datasets have
been fundamental for developing diagnostic algorithms, they lack the context of
the surrounding skin, which is critical for improving lesion detection. The
iToBoS dataset was created to address this challenge. It includes 16,954 images
of skin regions from 100 participants, captured using 3D total body
photography. Each image roughly corresponds to a $7 \times 9$ cm section of
skin with all suspicious lesions annotated using bounding boxes. Additionally,
the dataset provides metadata such as anatomical location, age group, and sun
damage score for each image. This dataset aims to facilitate training and
benchmarking of algorithms, with the goal of enabling early detection of skin
cancer and deployment of this technology in non-clinical environments.

æè¦ï¼äººå·¥æºæ§ééå¿«éä¸æºç¢ºåµæ¸¬æ¡æ§çç¶ï¼å¤§å¹æåç®èççè¨ºæ·ãå¨éåé åä¸­ï¼å¤§å¤æ¸å¬éçå½±åè³æéé½åå«å®ä¸ãå­¤ç«çç®èçç¶ï¼ç½®æ¼å½±åçä¸­å¤®ãåç®¡éäºä»¥çç¶çºä¸­å¿çè³æéå°æ¼éç¼è¨ºæ·æ¼ç®æ³è³ééè¦ï¼ä½å®åå»ç¼ºä¹å¨åç®èçèæ¯ï¼éå°æ¼æ¹åçç¶åµæ¸¬è³ééè¦ãiToBoS è³æéçå»ºç«å°±æ¯çºäºæå°éåææ°ãå®åå« 100 ä½åèèç 16,954 å¼µç®èååå½±åï¼ä½¿ç¨ 3D å¨èº«æå½±æè¡æ·åãæ¯å¼µå½±åå¤§è´å°ææ¼ $7 \times 9$ å¬åçç®èååï¼ææå¯ççç¶é½ä½¿ç¨éçæ¡æ¨è¨»ãæ­¤å¤ï¼è©²è³æééæä¾æ¯å¼µå½±åçåè³æï¼ä¾å¦è§£åä½ç½®ãå¹´é½¡çµåæ¥æ¬æå·è©åãæ­¤è³æéæ¨å¨ä¿é²æ¼ç®æ³çè¨ç·´ååºæºæ¸¬è©¦ï¼ç®æ¨æ¯å¯¦ç¾ç®èççæ©æåµæ¸¬ï¼ä¸¦å°æ­¤æè¡é¨ç½²å¨éè¨åºç°å¢ä¸­ã

##### **Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers**
2501.18237v1 by Malte TÃ¶lle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt

A patient undergoes multiple examinations in each hospital stay, where each
provides different facets of the health status. These assessments include
temporal data with varying sampling rates, discrete single-point measurements,
therapeutic interventions such as medication administration, and images. While
physicians are able to process and integrate diverse modalities intuitively,
neural networks need specific modeling for each modality complicating the
training procedure. We demonstrate that this complexity can be significantly
reduced by visualizing all information as images along with unstructured text
and subsequently training a conventional vision-text transformer. Our approach,
Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not
only simplifies data preprocessing and modeling but also outperforms current
state-of-the-art methods in predicting in-hospital mortality and phenotyping,
as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities
include patient's clinical measurements, medications, X-ray images, and
electrocardiography scans. We hope our work inspires advancements in
multi-modal medical AI by reducing the training complexity to (visual) prompt
engineering, thus lowering entry barriers and enabling no-code solutions for
training. The source code will be made publicly available.

æè¦ï¼å¨æ¯æ¬¡ä½é¢æéï¼æ£èææ¥åå¤é æª¢æ¥ï¼æ¯ä¸é æª¢æ¥é½è½æä¾å¥åº·çæçä¸åé¢åãéäºè©ä¼°åæ¬å·æä¸ååæ¨£ççæéè³æãé¢æ£å®é»æ¸¬éå¼ãæ²»çä»å¥ï¼å¦è¥ç©ç®¡çï¼åå½±åãéç¶é«çè½å¤ ç´è§å°èçåæ´åä¸åçæ¨¡å¼ï¼ä½ç¥ç¶ç¶²è·¯éè¦éå°æ¯ç¨®æ¨¡å¼é²è¡ç¹å®çå»ºæ¨¡ï¼éä½¿å¾è¨ç·´ç¨åºè®å¾è¤éãæåè­æï¼ééå°ææè³è¨è¦è¦ºåçºå½±åï¼ä¸¦çµåéçµæ§åæå­ï¼é¨å¾è¨ç·´ä¸åå³çµ±çè¦è¦ºæå­è½æå¨ï¼å¯ä»¥å¤§å¹éä½éç¨®è¤éæ§ãæåçåæ³ï¼å³ç¨æ¼ä¸è¦åæ¡æ¨£å¤æ¨¡å¼æ¸¬éçè¦è¦ºè½æå¨ (ViTiMM)ï¼ä¸åç°¡åäºè³æé èçåå»ºæ¨¡ï¼èä¸å¨é æ¸¬é¢å§æ­»äº¡çåè¡¨åæ¹é¢ä¹åªæ¼ç®åçææ°æ¹æ³ï¼éæ¯æ ¹æ MIMIC-IV è³æéä¸­ç 6,175 åæ£èè©ä¼°çãéäºæ¨¡å¼åæ¬æ£èçè¨åºæ¸¬éå¼ãè¥ç©ãX åå½±ååå¿é»åææãæåå¸ææåçå·¥ä½è½éééä½è¨ç·´è¤éåº¦å°ï¼è¦è¦ºï¼æç¤ºå·¥ç¨ï¼å¾èéä½é²å¥éæª»ï¼ä¸¦çºè¨ç·´åç¨ç¡ç¨å¼ç¢¼è§£æ±ºæ¹æ¡ï¼é²èæ¿åµå¤æ¨¡å¼é«ç AI çé²æ­¥ãåå§ç¨å¼ç¢¼å°å¬éæä¾ã

##### **Investigating an Intelligent System to Monitor \& Explain Abnormal Activity Patterns of Older Adults**
2501.18108v1 by Min Hun Lee, Daniel P. Siewiorek, Alexandre Bernardino

Despite the growing potential of older adult care technologies, the adoption
of these technologies remains challenging. In this work, we conducted a
focus-group session with family caregivers to scope designs of the older adult
care technology. We then developed a high-fidelity prototype and conducted its
qualitative study with professional caregivers and older adults to understand
their perspectives on the system functionalities. This system monitors abnormal
activity patterns of older adults using wireless motion sensors and machine
learning models and supports interactive dialogue responses to explain abnormal
activity patterns of older adults to caregivers and allow older adults
proactively sharing their status with caregivers for an adequate intervention.
Both older adults and professional caregivers appreciated that our system can
provide a faster, personalized service while proactively controlling what
information is to be shared through interactive dialogue responses. We further
discuss other considerations to realize older adult technology in practice.

æè¦ï¼åç®¡èå¹´äººç§è­·æè¡çæ½åæ¥çå¢é·ï¼ä½æ¡ç¨éäºæè¡ä»å·æææ°æ§ãå¨éé ç ç©¶ä¸­ï¼æåèå®¶åº­ç§è­·èé²è¡ç¦é»å°çµæè­°ï¼ä»¥çå®èå¹´äººç§è­·æè¡çè¨­è¨ç¯åãæ¥èï¼æåéç¼äºä¸åé«ä¿çååï¼ä¸¦èå°æ¥­ç§è­·èåèå¹´äººé²è¡è³ªæ§ç ç©¶ï¼ä»¥äºè§£ä»åå°ç³»çµ±åè½çè§é»ãæ­¤ç³»çµ±ä½¿ç¨ç¡ç·åä½ææ¸¬å¨åæ©å¨å­¸ç¿æ¨¡åç£æ§èå¹´äººçç°å¸¸æ´»åæ¨¡å¼ï¼ä¸¦æ¯æ´äºåå¼å°è©±åæï¼åç§è­·èè§£éèå¹´äººçç°å¸¸æ´»åæ¨¡å¼ï¼ä¸¦è®èå¹´äººä¸»åèç§è­·èåäº«ä»åççæï¼ä»¥é²è¡é©ç¶çä»å¥ãèå¹´äººåå°æ¥­ç§è­·èé½è®è³æåçç³»çµ±è½æä¾æ´å¿«éãåäººåçæåï¼åæééäºåå¼å°è©±åæä¸»åæ§å¶è¦åäº«åªäºè³è¨ãæåé²ä¸æ­¥è¨è«å¶ä»èéå ç´ ï¼ä»¥å¨å¯¦åä¸­å¯¦ç¾èå¹´äººæè¡ã

##### **Normative Evaluation of Large Language Models with Everyday Moral Dilemmas**
2501.18081v1 by Pratik S. Sachdeva, Tom van Nuenen

The rapid adoption of large language models (LLMs) has spurred extensive
research into their encoded moral norms and decision-making processes. Much of
this research relies on prompting LLMs with survey-style questions to assess
how well models are aligned with certain demographic groups, moral beliefs, or
political ideologies. While informative, the adherence of these approaches to
relatively superficial constructs tends to oversimplify the complexity and
nuance underlying everyday moral dilemmas. We argue that auditing LLMs along
more detailed axes of human interaction is of paramount importance to better
assess the degree to which they may impact human beliefs and actions. To this
end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am
I the Asshole" (AITA) community on Reddit, where users seek moral judgments on
everyday conflicts from other community members. We prompted seven LLMs to
assign blame and provide explanations for over 10,000 AITA moral dilemmas. We
then compared the LLMs' judgments and explanations to those of Redditors and to
each other, aiming to uncover patterns in their moral reasoning. Our results
demonstrate that large language models exhibit distinct patterns of moral
judgment, varying substantially from human evaluations on the AITA subreddit.
LLMs demonstrate moderate to high self-consistency but low inter-model
agreement. Further analysis of model explanations reveals distinct patterns in
how models invoke various moral principles. These findings highlight the
complexity of implementing consistent moral reasoning in artificial systems and
the need for careful evaluation of how different models approach ethical
judgment. As LLMs continue to be used in roles requiring ethical
decision-making such as therapists and companions, careful evaluation is
crucial to mitigate potential biases and limitations.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çå¿«éæ¡ç¨å·²ä¿ä½¿äººåæ·±å¥ç ç©¶å¶ç·¨ç¢¼çéå¾·è¦ç¯åæ±ºç­éç¨ãè¨±å¤éé¡ç ç©¶ä¾è³´æ¼ä»¥èª¿æ¥å¼åé¡æç¤º LLMï¼ä»¥è©ä¼°æ¨¡åèç¹å®äººå£ç¾¤é«ãéå¾·ä¿¡å¿µææ¿æ²»æè­å½¢æçå¥åç¨åº¦ãåç®¡ææä¾è³è¨ï¼ä½éäºæ¹æ³å°ç¸å°èæ·ºççµæ§çå æå¾åæ¼éåº¦ç°¡åæ¥å¸¸éå¾·å°å¢èå¾çè¤éæ§åç´°å¾®å·®å¥ãæåèªçºï¼æ²¿èæ´è©³ç´°çäººé¡äºåè»¸ç·å¯©æ¥ LLM å°æ¼æ´å¥½å°è©ä¼°å®åå¯è½å½±é¿äººé¡ä¿¡å¿µåè¡çºçç¨åº¦è³ééè¦ãçºæ­¤ï¼æåæ ¹æ Reddit ä¸ãææ¯æ··èåã(AITA) ç¤¾ç¾¤è©ä¼° LLM å¨è¤éçæ¥å¸¸éå¾·å°å¢ä¸­ï¼ä½¿ç¨èå¨å¶ä¸­å°æ±å¶ä»ç¤¾ç¾¤æå¡å°æ¥å¸¸è¡çªçéå¾·å¤æ·ãæåæç¤ºä¸å LLM å°è¶é 10,000 å AITA éå¾·å°å¢åéè²¬ä»»ä¸¦æä¾è§£éãç¶å¾ï¼æåå° LLM çå¤æ·åè§£éè Reddit ä½¿ç¨èçå¤æ·åè§£éä»¥åå½¼æ­¤é²è¡æ¯è¼ï¼æ¨å¨æ­ç¤ºå¶éå¾·æ¨çä¸­çæ¨¡å¼ãæåççµæè¡¨æï¼å¤§åèªè¨æ¨¡åå±ç¾åºä¸åçéå¾·å¤æ·æ¨¡å¼ï¼è AITA å­çå¡ä¸çäººé¡è©ä¼°æå¾å¤§å·®ç°ãLLM è¡¨ç¾åºä¸­åº¦å°é«åº¦çèªæä¸è´æ§ï¼ä½æ¨¡åéåè­°ä½ãé²ä¸æ­¥åææ¨¡åè§£éæ­ç¤ºäºæ¨¡åå¦ä½æ´å¼åç¨®éå¾·ååçä¸åæ¨¡å¼ãéäºç¼ç¾çªé¡¯äºå¨äººå·¥ç³»çµ±ä¸­å¯¦æ½ä¸è´çéå¾·æ¨ççè¤éæ§ï¼ä»¥åä»ç´°è©ä¼°ä¸åæ¨¡åå¦ä½é²è¡éå¾·å¤æ·çå¿è¦æ§ãé¨è LLM æçºç¨æ¼éè¦éå¾·æ±ºç­çè§è²ï¼ä¾å¦æ²»çå¸«åä¼´ä¾¶ï¼ä»ç´°è©ä¼°å°æ¼æ¸è¼æ½å¨åè¦åéå¶è³ééè¦ã

##### **Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence**
2501.18071v1 by Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino

Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.

æè¦ï¼ç³å°¿ç (DM) æ¯ä¸é éè¦çå¨çå¥åº·è­°é¡ï¼å¿é ç¡æ©è¨ºæ·ä¸¦å¦¥åç®¡çãæ¬ç ç©¶æåºä¸åç³å°¿çé æ¸¬æ¶æ§ï¼ä½¿ç¨æ©å¨å­¸ç¿ (ML) æ¨¡åï¼ä¸¦æ­éå¯è§£éäººå·¥æºæ§ (XAI) å·¥å·ï¼ä¾æ¢è¨ ML æ¨¡åé æ¸¬çæºç¢ºåº¦åå¯è§£éæ§ãè³æåèçåºæ¼åæå°æ¸éæ¡æ¨£æè¡ (SMOTE) åç¹å¾µç¸®æ¾ï¼ç¨æ¼ç³å°¿çäºåå¥åº·ææ¨è³æéï¼ä»¥èçé¡å¥ä¸å¹³è¡¡åè¨åºç¹å¾µçå¯è®æ§ãæ´åæ¨¡åæä¾äºé«æºç¢ºåº¦ï¼æ¸¬è©¦æºç¢ºåº¦çº 92.50%ï¼ROC-AUC çº 0.975ãæ ¹ææ¨¡åè§£éï¼BMIãå¹´é½¡ãä¸è¬å¥åº·çæ³ãæ¶å¥åèº«é«æ´»åæ¯æå·å½±é¿åçé æ¸¬å å­ãæ¬ç ç©¶çµæè¡¨æï¼ML çµå XAI æ¯ä¸ç¨®æåéçæ¹å¼ï¼å¯ä»¥éç¼åºæºç¢ºä¸å¨éç®ä¸éæçå·¥å·ï¼ç¨æ¼é«çä¿å¥ç³»çµ±ã

##### **Current Pathology Foundation Models are unrobust to Medical Center Differences**
2501.18055v1 by Edwin D. de Jong, Eric Marcus, Jonas Teuwen

Pathology Foundation Models (FMs) hold great promise for healthcare. Before
they can be used in clinical practice, it is essential to ensure they are
robust to variations between medical centers. We measure whether pathology FMs
focus on biological features like tissue and cancer type, or on the well known
confounding medical center signatures introduced by staining procedure and
other differences. We introduce the Robustness Index. This novel robustness
metric reflects to what degree biological features dominate confounding
features. Ten current publicly available pathology FMs are evaluated. We find
that all current pathology foundation models evaluated represent the medical
center to a strong degree. Significant differences in the robustness index are
observed. Only one model so far has a robustness index greater than one,
meaning biological features dominate confounding features, but only slightly. A
quantitative approach to measure the influence of medical center differences on
FM-based prediction performance is described. We analyze the impact of
unrobustness on classification performance of downstream models, and find that
cancer-type classification errors are not random, but specifically attributable
to same-center confounders: images of other classes from the same medical
center. We visualize FM embedding spaces, and find these are more strongly
organized by medical centers than by biological factors. As a consequence, the
medical center of origin is predicted more accurately than the tissue source
and cancer type. The robustness index introduced here is provided with the aim
of advancing progress towards clinical adoption of robust and reliable
pathology FMs.

æè¦ï¼ççåºç¤æ¨¡å (FM) å°é«çä¿å¥èè¨æ¥µå·åæ¯ãå¨è¨åºå¯¦åä¸­ä½¿ç¨ä¹åï¼å¿é ç¢ºä¿å®åè½é©æé«çä¸­å¿ä¹éçå·®ç°ãæåè¡¡éçç FM æ¯å¦èéæ¼çµç¹åççé¡åç­çç©ç¹å¾µï¼æèéæ¼æè²ç¨åºåå¶ä»å·®ç°æé æçç¾æå¨ç¥çæ··æ·é«çä¸­å¿ç¹å¾µãæåå¼å¥äºç©©å¥æ§ææ¸ãéåæ°ç©çç©©å¥æ§ææ¨åæ äºçç©ç¹å¾µå¨å¤å¤§ç¨åº¦ä¸ä¸»å°æ··æ·ç¹å¾µãæåè©ä¼°äºååç¶åå¬éå¯ç¨ççç FMãæåç¼ç¾ï¼ææç¶åè©ä¼°çççåºç¤æ¨¡åé½å¼·çå°ä»£è¡¨äºé«çä¸­å¿ãè§å¯å°ç©©å¥æ§ææ¸æé¡¯èå·®ç°ãå°ç®åçºæ­¢ï¼åªæä¸åæ¨¡åçç©©å¥æ§ææ¸å¤§æ¼ 1ï¼è¡¨ç¤ºçç©ç¹å¾µä¸»å°æ··æ·ç¹å¾µï¼ä½åç¥å¾®ä¸»å°ãæè¿°äºè¡¡éé«çä¸­å¿å·®ç°å°åºæ¼ FM çé æ¸¬æè½å½±é¿çéåæ¹æ³ãæååæäºä¸ç©©å¥æ§å°ä¸æ¸¸æ¨¡ååé¡æè½çå½±é¿ï¼ç¼ç¾ççé¡ååé¡é¯èª¤ä¸¦éé¨æ©ï¼èæ¯ç¹å¥æ­¸å æ¼åä¸­å¿æ··æ·å å­ï¼ä¾èªåä¸é«çä¸­å¿çå¶ä»é¡å¥çå½±åãæåå° FM åµå¥ç©ºéè¦è¦ºåï¼ç¼ç¾éäºç©ºéæ¯çç©å ç´ æ´å¼·çå°ç±é«çä¸­å¿çµç¹èµ·ä¾ãå æ­¤ï¼æ¯çµç¹ä¾æºåççé¡åæ´æºç¢ºå°é æ¸¬äºé«çä¸­å¿çä¾æºãéè£¡ä»ç´¹çç©©å¥æ§ææ¸æ¨å¨æ¨åé²å±ï¼æèè¨åºæ¡ç¨ç©©å¥ä¸å¯é ççç FM éé²ã

##### **Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations**
2501.17860v1 by Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen

Current medical AI systems often fail to replicate real-world clinical
reasoning, as they are predominantly trained and evaluated on static text and
question-answer tasks. These tuning methods and benchmarks overlook critical
aspects like evidence-based reasoning and handling distracting information. To
bridge this gap, we introduce a novel benchmark that simulates real-world
diagnostic scenarios, integrating noise and difficulty levels aligned with
USMLE standards. Moreover, we explore dialogue-based fine-tuning, which
transforms static datasets into conversational formats to better capture
iterative reasoning processes. Experiments show that dialogue-tuned models
outperform traditional methods, with improvements of $9.64\%$ in multi-round
reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our
findings highlight dialogue tuning as a promising approach for advancing
clinically aligned and robust medical AI systems.

æè¦ï¼ç®åçé«ç AI ç³»çµ±å¸¸ç¡æ³è¤è£½çå¯¦ä¸ççè¨åºæ¨çï¼å çºå®åä¸»è¦å¨éææå­ååç­ä»»åä¸åè¨åè©ä¼°ãéäºèª¿æ´æ¹æ³ååºæºå¿½ç¥äºåºæ¼è­æçæ¨çåèçåæ£è³è¨ç­ééµé¢åãçºäºå½è£éåå·®è·ï¼æåæåºä¸åæ¨¡æ¬çå¯¦ä¸çè¨ºæ·æå¢çå¨æ°åºæºï¼æ´åè USMLE æ¨æºä¸è´çéè¨åé£åº¦ç­ç´ãæ­¤å¤ï¼æåæ¢ç´¢ä»¥å°è©±çºåºç¤çå¾®èª¿ï¼å°éæè³æéè½æçºå°è©±æ ¼å¼ï¼ä»¥æ´å¥½å°ææåè¦çæ¨çéç¨ãå¯¦é©é¡¯ç¤ºï¼å°è©±å¾®èª¿æ¨¡ååªæ¼å³çµ±æ¹æ³ï¼å¨å¤è¼ªæ¨çæå¢ä¸­æåäº 9.64%ï¼å¨æéè¨çç°å¢ä¸­æåäº 6.18% çæºç¢ºåº¦ãæåçç¼ç¾å¼·èª¿å°è©±å¾®èª¿æ¯ä¸ç¨®æææ¨é²èè¨åºç¸ç¬¦ä¸å¼·å¥çé«ç AI ç³»çµ±çæ¹æ³ã

##### **GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings**
2501.17855v1 by Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee

Robot caregiving should be personalized to meet the diverse needs of care
recipients -- assisting with tasks as needed, while taking user agency in
action into account. In physical tasks such as handover, bathing, dressing, and
rehabilitation, a key aspect of this diversity is the functional range of
motion (fROM), which can vary significantly between individuals. In this work,
we learn to predict personalized fROM as a way to generalize robot
decision-making in a wide range of caregiving tasks. We propose a novel
data-driven method for predicting personalized fROM using functional assessment
scores from occupational therapy. We develop a neural model that learns to
embed functional assessment scores into a latent representation of the user's
physical function. The model is trained using motion capture data collected
from users with emulated mobility limitations. After training, the model
predicts personalized fROM for new users without motion capture. Through
simulated experiments and a real-robot user study, we show that the
personalized fROM predictions from our model enable the robot to provide
personalized and effective assistance while improving the user's agency in
action. See our website for more visualizations:
https://emprise.cs.cornell.edu/grace/.

æè¦ï¼æ©å¨äººç§è­·ææ ¹æç§è­·å°è±¡çä¸åéæ±é²è¡å®¢è£½åï¼å¨éè¦æåå©å·è¡ä»»åï¼åæèéä½¿ç¨èçèªä¸»è¡åãå¨ç§»äº¤ãæ²æµ´ãç©¿è¡£åå¾©å¥ç­èº«é«ä»»åä¸­ï¼éç¨®å¤æ¨£æ§çééµé¢åæ¯åè½æ§åä½ç¯å (fROM)ï¼èéå¨ä¸ååé«ä¹éå¯è½å·®ç°å¾å¤§ãå¨éé å·¥ä½ä¸­ï¼æåå­¸ç¿é æ¸¬å®¢è£½å fROMï¼ä½çºå¨å»£æ³ç§è­·ä»»åä¸­æ¦åæ©å¨äººæ±ºç­å¶å®çä¸ç¨®æ¹å¼ãæåæåºäºä¸ç¨®ä½¿ç¨è·è½æ²»çåè½è©ä¼°åæ¸ä¾é æ¸¬å®¢è£½å fROM çæ°ç©è³æé©åæ¹æ³ãæåéç¼äºä¸åç¥ç¶æ¨¡åï¼å­¸ç¿å°åè½è©ä¼°åæ¸åµå¥å°ä½¿ç¨èçèº«é«åè½æ½å¨è¡¨å¾µä¸­ãè©²æ¨¡åä½¿ç¨å¾å·ææ¨¡æ¬è¡åéå¶çä½¿ç¨èæ¶éçåä½æ·åè³æé²è¡è¨ç·´ãè¨ç·´å¾ï¼è©²æ¨¡åæçºæ²æåä½æ·åçæ°ä½¿ç¨èé æ¸¬å®¢è£½å fROMãééæ¨¡æ¬å¯¦é©åçå¯¦æ©å¨äººä½¿ç¨èç ç©¶ï¼æåå±ç¤ºäºæåæ¨¡åçå®¢è£½å fROM é æ¸¬ä½¿æ©å¨äººè½å¤ æä¾å®¢è£½åä¸ææçåå©ï¼åææé«ä½¿ç¨èçèªä¸»è¡åãè«åé±æåçç¶²ç«ä»¥åå¾æ´å¤è¦è¦ºåè³æï¼https://emprise.cs.cornell.edu/grace/ã

##### **Tonguescape: Exploring Language Models Understanding of Vowel Articulation**
2501.17643v1 by Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe

Vowels are primarily characterized by tongue position. Humans have discovered
these features of vowel articulation through their own experience and explicit
objective observation such as using MRI. With this knowledge and our
experience, we can explain and understand the relationship between tongue
positions and vowels, and this knowledge is helpful for language learners to
learn pronunciation. Since language models (LMs) are trained on a large amount
of data that includes linguistic and medical fields, our preliminary studies
indicate that an LM is able to explain the pronunciation mechanisms of vowels.
However, it is unclear whether multi-modal LMs, such as vision LMs, align
textual information with visual information. One question arises: do LMs
associate real tongue positions with vowel articulation? In this study, we
created video and image datasets from the existing real-time MRI dataset and
investigated whether LMs can understand vowel articulation based on tongue
positions using vision-based information. Our findings suggest that LMs exhibit
potential for understanding vowels and tongue positions when reference examples
are provided while they have difficulties without them. Our code for dataset
building is available on GitHub.

æè¦ï¼åé³ä¸»è¦ç±èé ­ä½ç½®æ±ºå®ãäººé¡ééèªå·±çç¶é©åæç¢ºçå®¢è§è§å¯ï¼ä¾å¦ä½¿ç¨ MRIï¼ç¼ç¾äºåé³ç¼é³çéäºç¹å¾µãæäºéäºç¥è­åç¶é©ï¼æåå¯ä»¥è§£éåçè§£èé ­ä½ç½®ååé³ä¹éçéä¿ï¼èéäºç¥è­å°èªè¨å­¸ç¿èå­¸ç¿ç¼é³å¾æå¹«å©ãç±æ¼èªè¨æ¨¡å (LM) æ¯å¨åå«èªè¨å­¸åé«å­¸é åçå¤§éè³æä¸è¨ç·´çï¼æåçåæ­¥ç ç©¶è¡¨æï¼LM è½å¤ è§£éåé³çç¼é³æ©å¶ãç¶èï¼å°ä¸æ¸æ¥å¤æ¨¡æ LMï¼ä¾å¦è¦è¦º LMï¼æ¯å¦å°æå­è³è¨èè¦è¦ºè³è¨å°é½ãä¸ååé¡ç¢çäºï¼LM æ¯å¦å°çå¯¦çèé ­ä½ç½®èåé³ç¼é³è¯ç¹«èµ·ä¾ï¼å¨éé ç ç©¶ä¸­ï¼æåå¾ç¾æçå³æ MRI è³æéä¸­å»ºç«äºå½±çåå½±åè³æéï¼ä¸¦æ¢è¨ LM æ¯å¦è½æ ¹æèé ­ä½ç½®ä½¿ç¨åºæ¼è¦è¦ºçè³è¨ä¾çè§£åé³ç¼é³ãæåçç ç©¶çµæè¡¨æï¼ç¶æä¾åèç¯ä¾æï¼LM å·æçè§£åé³åèé ­ä½ç½®çæ½åï¼èæ²æåèç¯ä¾æåæå°é£ãæåç¨æ¼å»ºç«è³æéçç¨å¼ç¢¼å¯å¨ GitHub ä¸åå¾ã

##### **Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models**
2501.18645v1 by Manish Sanwal

Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to
provide step-by-step rationales, improving performance on complex tasks.
Despite its benefits, vanilla CoT often fails to fully verify intermediate
inferences and can produce misleading explanations. In this work, we propose
Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that
systematically segments the reasoning process into multiple layers, each
subjected to external checks and optional user feedback. We expand on the key
concepts, present three scenarios -- medical triage, financial risk assessment,
and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT
in terms of transparency, correctness, and user engagement. By integrating
references from recent arXiv papers on interactive explainability, multi-agent
frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves
the way for more reliable and grounded explanations in high-stakes domains.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å©ç¨ææ³é (CoT) æç¤ºæä¾éæ­¥ççç±ï¼å¾èæé«è¤éä»»åçå·è¡æè½ãåç®¡æå¶å¥½èï¼ä½é¦è CoT ç¶å¸¸ç¡æ³å®å¨é©è­ä¸­éæ¨è«ï¼ä¸¦ä¸å¯è½ç¢çèª¤å°æ§çè§£éãå¨éé å·¥ä½ä¸­ï¼æåæåºåå±¤ææ³é (Layered-CoT) æç¤ºï¼éæ¯ä¸åæ°ç©çæ¡æ¶ï¼å®ç³»çµ±æ§å°å°æ¨çéç¨åæ®µæå¤åå±¤æ¬¡ï¼æ¯åå±¤æ¬¡é½ç¶éå¤é¨æª¢æ¥åå¯é¸çä½¿ç¨èåé¥ãæåæ´å±äºééµæ¦å¿µï¼æåºäºä¸ç¨®å ´æ¯ââé«çåæµãè²¡åé¢¨éªè©ä¼°åææ·å·¥ç¨ââä¸¦å±ç¤ºäºåå±¤ CoT å¨éæåº¦ãæ­£ç¢ºæ§åä½¿ç¨èåèåº¦æ¹é¢å¦ä½è¶è¶é¦è CoTãééæ´åä¾èªè¿æ arXiv è«æéæ¼äºåå¯è§£éæ§ãå¤ä¸»é«æ¡æ¶ååºæ¼ä¸»é«çåä½çåèï¼æåèªªæäºåå±¤ CoT å¦ä½çºé«é¢¨éªé åæä¾æ´å¯é ä¸ææ ¹æçè§£ééªè·¯ã

##### **An Exceptional Dataset For Rare Pancreatic Tumor Segmentation**
2501.17555v1 by Wenqi Li, Yingli Chen, Keyang Zhou, Xiaoxiao Hu, Zilu Zheng, Yue Yan, Xinpeng Zhang, Wei Tang, Zhenxing Qian

Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms
that account for less than 5% of all pancreatic malignancies, with an incidence
of only 1-1.5 cases per 100,000. Early detection of pNETs is critical for
improving patient survival, but the rarity of pNETs makes segmenting them from
CT a very challenging problem. So far, there has not been a dataset
specifically for pNETs available to researchers. To address this issue, we
propose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography
(CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors,
containing data from 469 patients. This is the first dataset solely dedicated
to pNETs, distinguishing it from previous collections. Additionally, we provide
the baseline detection networks with a new slice-wise weight loss function
designed for the UNet-based model, improving the overall pNET segmentation
performance. We hope that our dataset can enhance the understanding and
diagnosis of pNET Tumors within the medical community, facilitate the
development of more accurate diagnostic tools, and ultimately improve patient
outcomes and advance the field of oncology.

æè¦ï¼è°èç¥ç¶å§åæ³è«ç¤ (pNETs) æ¯éå¸¸ç½è¦çå§åæ³è«ç¤ï¼åä½ææè°èæ¡æ§è«ç¤çä¸å° 5%ï¼æ¯ 100,000 äººä¸­åç¼ç 1-1.5 åçä¾ãæ©æç¼ç¾ pNETs å°æ¹åæ£èå­æ´»çè³ééè¦ï¼ä½ pNETs çç½è¦æ§ä½¿å¾å¾ CT ä¸­åå²å®åæçºä¸åéå¸¸å·æææ°æ§çåé¡ãå°ç®åçºæ­¢ï¼éæ²æå°ééå° pNETs çæ¸æéå¯ä¾ç ç©¶äººå¡ä½¿ç¨ãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸å pNETs æ¸æéï¼ä¸åå°æ³¨æ¼è°èç¥ç¶å§åæ³è«ç¤çæ¨è¨»è¯å¥½çå°æ¯å¢å¼·é»è¦æ·å±¤ææ (CECT) æ¸æéï¼åå«ä¾èª 469 åæ£èçæ¸æãéæ¯ç¬¬ä¸åå°ééå° pNETs çæ¸æéï¼éä½¿å¶æå¥æ¼ä¹åçæ¶éãæ­¤å¤ï¼æåçºåºç·æª¢æ¸¬ç¶²è·¯æä¾äºä¸åæ°çåºæ¼ UNet æ¨¡åè¨­è¨çåçå æ¬æå¤±å½æ¸ï¼æ¹åäºæ´é« pNET åå²æ§è½ãæåå¸ææåçæ¸æéè½å¤ å¢å¼·é«å­¸çå° pNET è«ç¤ççè§£åè¨ºæ·ï¼ä¿é²æ´æºç¢ºçè¨ºæ·å·¥å·çéç¼ï¼æçµæ¹åæ£èçé å¾ä¸¦æ¨é²è«ç¤å­¸é åã

##### **LLM Assistance for Pediatric Depression**
2501.17510v1 by Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive

Traditional depression screening methods, such as the PHQ-9, are particularly
challenging for children in pediatric primary care due to practical
limitations. AI has the potential to help, but the scarcity of annotated
datasets in mental health, combined with the computational costs of training,
highlights the need for efficient, zero-shot approaches. In this work, we
investigate the feasibility of state-of-the-art LLMs for depressive symptom
extraction in pediatric settings (ages 6-24). This approach aims to complement
traditional screening and minimize diagnostic errors.
  Our findings show that all LLMs are 60% more efficient than word match, with
Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the
extraction of more rare symptoms like "sleep problems" (F1: 0.92) and
"self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and
recall (0.60), performing well in categories like "Feeling depressed" (0.69)
and "Weight change" (0.78). Llama 3, with the highest recall (0.90),
overgeneralizes symptoms, making it less suitable for this type of analysis.
Challenges include the complexity of clinical notes and overgeneralization from
PHQ-9 scores. The main challenges faced by LLMs include navigating the complex
structure of clinical notes with content from different times in the patient
trajectory, as well as misinterpreting elevated PHQ-9 scores.
  We finally demonstrate the utility of symptom annotations provided by Flan as
features in an ML algorithm, which differentiates depression cases from
controls with high precision of 0.78, showing a major performance boost
compared to a baseline that does not use these features.

æè¦ï¼<paragraph>å³çµ±çæé¬±çç¯©æª¢æ¹æ³ï¼ä¾å¦ PHQ-9ï¼ç±æ¼å¯¦ééå¶ï¼å°æ¼å°åç§åç´ç§è­·ä¸­çåç«¥ä¾èªªç¹å¥å·æææ°æ§ãAI æå¯è½æä¾å¹«å©ï¼ä½å¿çå¥åº·ä¸­è¨»è§£è³æéçç¨å°ï¼å ä¸è¨ç·´çéç®ææ¬ï¼çªé¡¯äºå°ææççé¶æ¬¡å­¸ç¿æ¹æ³çéæ±ãå¨éé å·¥ä½ä¸­ï¼æåæ¢è¨äºæåé²ç LLM å¨å°åç§ç°å¢ï¼6-24 æ­²ï¼ä¸­æåæé¬±çççå¯è¡æ§ãéç¨®æ¹æ³æ¨å¨è£åå³çµ±ç¯©æª¢ä¸¦å°è¨ºæ·é¯èª¤éè³æä½ãæåçç ç©¶çµæé¡¯ç¤ºï¼ææ LLM çæçé½æ¯å­è©æ¯å°é«åº 60%ï¼è Flan å¨ç²¾ç¢ºåº¦æ¹é¢é åï¼å¹³å F1ï¼0.65ï¼ç²¾ç¢ºåº¦ï¼0.78ï¼ï¼å¨æåè¼ç½è¦çççæ¹é¢è¡¨ç¾åºè²ï¼ä¾å¦ãç¡ç åé¡ãï¼F1ï¼0.92ï¼åãèªæå­æ¡ãï¼F1ï¼0.8ï¼ãPhi å¨ç²¾ç¢ºåº¦ï¼0.44ï¼åå¬åçï¼0.60ï¼ä¹éåå¾å¹³è¡¡ï¼å¨ãæå°æ²®åªãï¼0.69ï¼åãé«éæ¹è®ãï¼0.78ï¼ç­é¡å¥ä¸­è¡¨ç¾è¯å¥½ãæææé«å¬åçï¼0.90ï¼ç Llama 3 æéåº¦æ¦æ¬ççï¼ä½¿å¶ä¸å¤ªé©åæ­¤é¡åæãææ°åæ¬è¨åºç­è¨çè¤éæ§å PHQ-9 åæ¸çéåº¦æ¦æ¬ãLLM é¢è¨çä¸»è¦ææ°åæ¬å¨æ£èæ­·ç¨ä¸­ä¸åæéçå§å®¹ä¸­å°èªè¨åºç­è¨çè¤éçµæ§ï¼ä»¥åèª¤è§£ PHQ-9 åæ¸åé«ãæåæå¾å±ç¤ºäº Flan æä¾çççè¨»è§£ä½çºæ©å¨å­¸ç¿æ¼ç®æ³ä¸­ç¹å¾µçæç¨ï¼å®ä»¥ 0.78 çé«ç²¾ç¢ºåº¦å°æé¬±ççä¾èå°ç§çµååéä¾ï¼èä¸ä½¿ç¨éäºç¹å¾µçåºæºç¸æ¯ï¼é¡¯ç¤ºåºä¸»è¦çæè½æåã</paragraph>

##### **Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines**
2501.17343v1 by Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo

Quantizing deep neural networks ,reducing the precision (bit-width) of their
computations, can remarkably decrease memory usage and accelerate processing,
making these models more suitable for large-scale medical imaging applications
with limited computational resources. However, many existing methods studied
"fake quantization", which simulates lower precision operations during
inference, but does not actually reduce model size or improve real-world
inference speed. Moreover, the potential of deploying real 3D low-bit
quantization on modern GPUs is still unexplored. In this study, we introduce a
real post-training quantization (PTQ) framework that successfully implements
true 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation
models, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,
ST-UNet,and VISTA3D. Our approach involves two main steps. First, we use
TensorRT to perform fake quantization for both weights and activations with
unlabeled calibration dataset. Second, we convert this fake quantization into
real quantization via TensorRT engine on real GPUs, resulting in real-world
reductions in model size and inference latency. Extensive experiments
demonstrate that our framework effectively performs 8-bit quantization on GPUs
without sacrificing model performance. This advancement enables the deployment
of efficient deep learning models in medical imaging applications where
computational resources are constrained. The code and models have been
released, including U-Net, TransUNet pretrained on the BTCV dataset for
abdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset
for whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and
VISTA3D pretrained on TotalSegmentator V2 for full body (104-label)
segmentation. https://github.com/hrlblab/PTQ.

æè¦ï¼<paragraph>éåæ·±åº¦ç¥ç»ç½ç»ï¼éä½å¶è®¡ç®çç²¾åº¦ï¼ä½å®½ï¼ï¼å¯ä»¥æ¾èåå°åå­ä½¿ç¨éå¹¶å éå¤çï¼ä½¿è¿äºæ¨¡åæ´éåäºå·ææéè®¡ç®èµæºçå¤§è§æ¨¡å»å­¦å½±ååºç¨ãç¶èï¼è®¸å¤ç°ææ¹æ³ç ç©¶äºâä¼ªéåâï¼å®å¨æ¨çæé´æ¨¡æè¾ä½ç²¾åº¦çæä½ï¼ä½å®éä¸å¹¶æ²¡æåå°æ¨¡åå¤§å°ææé«å®éæ¨çéåº¦ãæ­¤å¤ï¼å¨ç°ä»£ GPU ä¸é¨ç½²çæ­£ç 3D ä½ä½éåçæ½åä»æªå¾å°æ¢ç´¢ãå¨è¿é¡¹ç ç©¶ä¸­ï¼æä»¬å¼å¥äºä¸ä¸ªçæ­£çè®­ç»åéå (PTQ) æ¡æ¶ï¼è¯¥æ¡æ¶æåå°å¨æåè¿ç (SOTA) 3D å»å­¦åå²æ¨¡åï¼å³ U-NetãSegResNetãSwinUNETRãnnU-NetãUNesTãTransUNetãST-UNet å VISTA3Dï¼ä¸å®ç°äºçæ­£ç 8 ä½éåãæä»¬çæ¹æ³æ¶åä¸¤ä¸ªä¸»è¦æ­¥éª¤ãé¦åï¼æä»¬ä½¿ç¨ TensorRT å¯¹æéåæ¿æ´»è¿è¡ä¼ªéåï¼å¹¶ä½¿ç¨æªæ è®°çæ ¡åæ°æ®éãå¶æ¬¡ï¼æä»¬å°è¿ç§ä¼ªéåéè¿çå® GPU ä¸ç TensorRT å¼æè½¬æ¢ä¸ºçæ­£çéåï¼ä»èå¨æ¨¡åå¤§å°åæ¨çå»¶è¿æ¹é¢å®ç°äºå®éçåå°ãå¤§éçå®éªè¡¨æï¼æä»¬çæ¡æ¶å¨ GPU ä¸ææå°æ§è¡ 8 ä½éåï¼èä¸ä¼çºç²æ¨¡åæ§è½ãè¿ä¸è¿æ­¥ä½¿å¾å¨è®¡ç®èµæºåéçå»å­¦å½±ååºç¨ä¸­é¨ç½²é«æçæ·±åº¦å­¦ä¹ æ¨¡åæä¸ºå¯è½ãä»£ç åæ¨¡åå·²ç»åå¸ï¼åæ¬ U-NetãTransUNETï¼å¨ BTCV æ°æ®éä¸é¢è®­ç»ç¨äºè¹é¨ï¼13 æ ç­¾ï¼åå²ï¼UNesT å¨ Whole Brain æ°æ®éä¸é¢è®­ç»ç¨äºå¨èï¼133 æ ç­¾ï¼åå²ï¼ä»¥å nnU-NetãSegResNetãSwinUNETR å VISTA3D å¨ TotalSegmentator V2 ä¸é¢è®­ç»ç¨äºå¨èº«ï¼104 æ ç­¾ï¼åå²ãhttps://github.com/hrlblab/PTQã</paragraph>

##### **Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection**
2501.17338v1 by Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang

Generative Language Models rely on autoregressive decoding to produce the
output sequence token by token. Many tasks such as preference optimization,
require the model to produce task-level output consisting of multiple tokens
directly by selecting candidates from a pool as predictions. Determining a
task-level prediction from candidates using the ordinary token-level decoding
mechanism is constrained by time-consuming decoding and interrupted gradients
by discrete token selection. Existing works have been using decoding-free
candidate selection methods to obtain candidate probability from initial output
logits over vocabulary. Though these estimation methods are widely used, they
are not systematically evaluated, especially on end tasks. We introduce an
evaluation of a comprehensive collection of decoding-free candidate selection
approaches on a comprehensive set of tasks, including five multiple-choice QA
tasks with a small candidate pool and four clinical decision tasks with a
massive amount of candidates, some with 10k+ options. We evaluate the
estimation methods paired with a wide spectrum of foundation LMs covering
different architectures, sizes and training paradigms. The results and insights
from our analysis inform the future model design.

æè¦ï¼çæèªè¨æ¨¡åä¾é èªè¿´æ­¸è§£ç¢¼ä¾éåç¬¦èç¢çè¼¸åºåºåãè¨±å¤ä»»åï¼å¦åå¥½æä½³åï¼è¦æ±æ¨¡åç´æ¥å¾åé¸æ± ä¸­é¸æé æ¸¬ï¼ç¢çç±å¤åç¬¦èçµæçä»»åç´å¥è¼¸åºãä½¿ç¨ä¸è¬çç¬¦èç´å¥è§£ç¢¼æ©å¶å¾åé¸èä¸­ç¢ºå®ä»»åç´å¥é æ¸¬åå°èæçè§£ç¢¼åé¢æ£ç¬¦èé¸æä¸­æ·çæ¢¯åº¦çç´æãç¾æå·¥ä½ä¸ç´ä½¿ç¨ç¡è§£ç¢¼åé¸èé¸ææ¹æ³å¾åå§è¼¸åºéè¼¯å¼ä¸­ç²å¾åé¸èæ©çãåç®¡éäºä¼°è¨æ¹æ³è¢«å»£æ³ä½¿ç¨ï¼ä½å®åä¸¦æªç¶éç³»çµ±è©ä¼°ï¼ç¹å¥æ¯å¨æçµä»»åä¸ãæåéå°å¨é¢çä»»åéï¼åæ¬äºåå·æå°ååé¸èæ± çå¤é¸é¡åç­ä»»ååååå·æå¤§éåé¸èçè¨åºæ±ºç­ä»»åï¼å¶ä¸­ä¸äºæ 10k+ é¸é ï¼ï¼å°å¨é¢çç¡è§£ç¢¼åé¸èé¸ææ¹æ³é²è¡è©ä¼°ãæåè©ä¼°èå»£æ³åºç¤èªè¨æ¨¡åéå°çä¼°è¨æ¹æ³ï¼éäºæ¨¡åæ¶µèä¸åçæ¶æ§ãå¤§å°åè¨ç·´ç¯ä¾ãæååæççµæåè¦è§£çºæªä¾çæ¨¡åè¨­è¨æä¾äºè³è¨ã

##### **Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction**
2501.17326v1 by Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang

Clinical diagnosis prediction models, when provided with a patient's medical
history, aim to detect potential diseases early, facilitating timely
intervention and improving prognostic outcomes. However, the inherent scarcity
of patient data and large disease candidate space often pose challenges in
developing satisfactory models for this intricate task. The exploration of
leveraging Large Language Models (LLMs) for encapsulating clinical decision
processes has been limited. We introduce MERA, a clinical diagnosis prediction
model that bridges pertaining natural language knowledge with medical practice.
We apply hierarchical contrastive learning on a disease candidate ranking list
to alleviate the large decision space issue. With concept memorization through
fine-tuning, we bridge the natural language clinical knowledge with medical
codes. Experimental results on MIMIC-III and IV datasets show that MERA
achieves the state-of-the-art diagnosis prediction performance and dramatically
elevates the diagnosis prediction capabilities of generative LMs.

æè¦ï¼è¨åºè¨ºæ·é æ¸¬æ¨¡åå¨æä¾æ£èçæ­·çåæï¼æ¨å¨åæ©ç¼ç¾æ½å¨ç¾çï¼ä¿é²åæå¹²é ä¸¦æ¹åé å¾çµæãç¶èï¼æ£èæ¸æçåºæç¨ç¼ºæ§åå¤§éçç¾çåé¸ç©ºééå¸¸å°éç¼ä»¤äººæ»¿æçæ¨¡åä»¥æå°éé è¤éçä»»åæ§æææ°ãå©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾å°è£è¨åºæ±ºç­æµç¨çæ¢ç´¢åå°éå¶ãæåå¼å¥äº MERAï¼éæ¯ä¸åè¨åºè¨ºæ·é æ¸¬æ¨¡åï¼å®å°ç¸éçèªç¶èªè¨ç¥è­èé«çå¯¦è¸è¯ç¹«èµ·ä¾ãæåå¨ç¾çåé¸æåæ¸å®ä¸æç¨åå±¤å°æ¯å­¸ç¿ï¼ä»¥ç·©è§£å¤§åæ±ºç­ç©ºéåé¡ãééå¾®èª¿æ¦å¿µè¨æ¶ï¼æåå°èªç¶èªè¨è¨åºç¥è­èé«çä»£ç¢¼è¯ç¹«èµ·ä¾ãå¨ MIMIC-III å IV æ¸æéä¸çå¯¦é©çµæè¡¨æï¼MERA éå°äºæåé²çè¨ºæ·é æ¸¬æ§è½ï¼ä¸¦é¡¯èæåäºçæå¼ LM çè¨ºæ·é æ¸¬è½åã

##### **Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology**
2501.17286v1 by Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu

Background: The radiation oncology clinical practice involves many steps
relying on the dynamic interplay of abundant text data. Large language models
have displayed remarkable capabilities in processing complex text information.
But their direct applications in specific fields like radiation oncology remain
underexplored.
  Purpose: This study aims to investigate whether fine-tuning LLMs with domain
knowledge can improve the performance on Task (1) treatment regimen generation,
Task (2) treatment modality selection (photon, proton, electron, or
brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.
  Methods: Data for 15,724 patient cases were extracted. Cases where patients
had a single diagnostic record, and a clearly identifiable primary treatment
plan were selected for preprocessing and manual annotation to have 7,903 cases
of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.
Each case was used to construct a pair consisting of patient diagnostics
details and an answer (treatment regimen, treatment modality, or ICD-10 code
respectively) for the supervised fine-tuning of these three tasks. Open source
LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the
Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for
the fine-tuned models and original models. Clinical evaluation was performed on
Task (1) by radiation oncologists, while precision, recall, and F-1 score were
evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used
to statistically analyze the results.
  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with
p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the
fine-tuned LLMs-generated treatment regimens were clinically acceptable.
Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.

æè¦ï¼<paragraph>èæ¯ï¼æ¾å°è¿ç¤ä¸´åºå®è·µæ¶åè®¸å¤æ­¥éª¤ï¼è¿äºæ­¥éª¤ä¾èµäºä¸°å¯ææ¬æ°æ®çå¨æäº¤äºãå¤§åè¯­è¨æ¨¡åå¨å¤çå¤æçææ¬ä¿¡æ¯æ¹é¢è¡¨ç°åºäºåè¶çè½åãä½å®ä»¬å¨æ¾å°è¿ç¤ç­ç¹å®é¢åçç´æ¥åºç¨ä»æªå¾å°ååæ¢ç´¢ã
ç®çï¼æ¬ç ç©¶æ¨å¨è°æ¥éè¿é¢åç¥è¯å¾®è° LLM æ¯å¦å¯ä»¥æé«ä»»å¡ (1) æ²»çæ¹æ¡çæãä»»å¡ (2) æ²»çæ¹å¼éæ©ï¼åå­ãè´¨å­ãçµå­æè¿è·ç¦»æ¾å°æ²»çï¼åä»»å¡ (3) æ¾å°è¿ç¤ä¸­ ICD-10 ä»£ç é¢æµçæ§è½ã
æ¹æ³ï¼æåäº 15,724 ä¾æ£èçä¾çæ°æ®ãéæ©äºæ£èæåä¸è¯æ­è®°å½ä¸ææç¡®å¯è¯å«çä¸»è¦æ²»çè®¡åççä¾ï¼è¿è¡é¢å¤çåæå¨æ³¨éï¼å¾å° 7,903 ä¾æ£èè¯æ­ãæ²»çè®¡åãæ²»çæ¹å¼å ICD-10 ä»£ç ãæ¯ä¸ªçä¾é½ç¨äºæå»ºä¸å¯¹ï¼åæ¬æ£èè¯æ­è¯¦æåç­æ¡ï¼åå«æ¯æ²»çæ¹æ¡ãæ²»çæ¹å¼æ ICD-10 ä»£ç ï¼ï¼ç¨äºè¿ä¸ä¸ªä»»å¡ççç£å¾®è°ãå¼æº LLaMA2-7B å Mistral-7B æ¨¡åè¢«ç¨äºä½¿ç¨ä½ç§©é¼è¿æ¹æ³è¿è¡å¾®è°ãæ¥åäºå¾®è°æ¨¡åååå§æ¨¡åçåç¡®æ§å ROUGE-1 åæ°ãä»»å¡ (1) ç±æ¾å°è¿ç¤ç§å»å¸è¿è¡ä¸´åºè¯ä¼°ï¼èä»»å¡ (2) å (3) åè¯ä¼°äºç²¾ç¡®åº¦ãå¬åçå F-1 åæ°ãåä¾§ Wilcoxon ç¬¦å·ç§©æ£éªç¨äºå¯¹ç»æè¿è¡ç»è®¡åæã
ç»æï¼å¾®è°åç LLM å¨ææä»»å¡ä¸­é½ä¼äºåå§ LLMï¼p å¼ <= 0.001ãä¸´åºè¯ä¼°è¡¨æï¼è¶è¿ 60% çå¾®è° LLM çæçæ²»çæ¹æ¡å¨ä¸´åºä¸æ¯å¯æ¥åçãç²¾ç¡®åº¦ãå¬åçå F1 åæ°æ¾ç¤ºå¾®è°åç LLM æ§è½å¾å°æ¹åã</paragraph>

##### **ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification**
2501.17260v1 by Mohammadreza Saraei, Igor Kozak, Eung-Joo Lee

Optical Coherence Tomography (OCT) is a non-invasive imaging modality
essential for diagnosing various eye diseases. Despite its clinical
significance, developing OCT-based diagnostic tools faces challenges, such as
limited public datasets, sparse annotations, and privacy concerns. Although
deep learning has made progress in automating OCT analysis, these challenges
remain unresolved. To address these limitations, we introduce the Vision
Transformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a
novel framework designed to enhance feature extraction and improve diagnostic
accuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,
Self-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining
phase leverages the OCTMNIST dataset (97,477 unlabeled images across four
disease classes) with data augmentation to create dual-augmented views. A
Vision Transformer (ViT-Base) backbone extracts features, while a negative
cosine similarity loss aligns feature representations. Pretraining is conducted
over 50 epochs with a learning rate of 0.0001 and momentum of 0.999.
Fine-tuning is performed on a stratified 5.129% subset of OCTMNIST using
10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of
0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming
existing SSP-based methods.

æè¦ï¼åå­¸ç¸å¹²æ·å±¤ææï¼OCTï¼æ¯ä¸ç¨®éä¾µå¥å¼å½±åæ¨¡å¼ï¼å°æ¼è¨ºæ·åç¨®ç¼ç¾è³ééè¦ãåç®¡å¶è¨åºæç¾©éå¤§ï¼ä½éç¼åºæ¼ OCT çè¨ºæ·å·¥å·é¢è¨ææ°ï¼ä¾å¦å¬å±æ¸æéæéãè¨»è§£ç¨çåé±ç§åé¡ãåç®¡æ·±åº¦å­¸ç¿å¨èªåå OCT åææ¹é¢åå¾äºé²å±ï¼ä½éäºææ°ä»ç¶æ²æè§£æ±ºãçºäºæå°éäºéå¶ï¼æåå¼å¥äºåºæ¼ Vision Transformer çéæµèªç£ç£é è¨ç·´ç¶²è·¯ï¼ViT-2SPNï¼ï¼éæ¯ä¸åæ°ç©çæ¡æ¶ï¼æ¨å¨å¢å¼·ç¹å¾µæåä¸¦æé«è¨ºæ·æºç¢ºæ§ãViT-2SPN æ¡ç¨ä¸éæ®µå·¥ä½æµç¨ï¼ç£ç£é è¨ç·´ãèªç£ç£é è¨ç·´ï¼SSPï¼åç£ç£å¾®èª¿ãé è¨ç·´éæ®µå©ç¨ OCTMNIST æ¸æéï¼è·¨è¶ååç¾çé¡å¥ç 97,477 å¼µæªæ¨è¨å½±åï¼åæ¸ææ´åä¾å»ºç«ééæ´åçæª¢è¦ãè¦è¦ºè½æå¨ï¼ViT-Baseï¼ä¸»å¹¹æåç¹å¾µï¼èè² é¤å¼¦ç¸ä¼¼åº¦æå¤±åæ ¡æºç¹å¾µè¡¨ç¤ºãé è¨ç·´å¨ 50 åä¸ä»£ä¸­é²è¡ï¼å­¸ç¿ççº 0.0001ï¼åè½çº 0.999ãå¾®èª¿å¨ OCTMNIST çåå±¤ 5.129% å­éä¸å·è¡ï¼ä½¿ç¨ 10 åäº¤åé©è­ãViT-2SPN éå°äº 0.93 çå¹³å AUCã0.77 çæºç¢ºçã0.81 çç²¾ç¢ºåº¦ã0.75 çå¬åçå 0.76 ç F1 åæ¸ï¼åªæ¼ç¾æçåºæ¼ SSP çæ¹æ³ã

##### **A Hybrid Deep Learning CNN Model for Enhanced COVID-19 Detection from Computed Tomography (CT) Scan Images**
2501.17160v1 by Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, Akhil Dusi, Lalithya Posham

Early detection of COVID-19 is crucial for effective treatment and
controlling its spread. This study proposes a novel hybrid deep learning model
for detecting COVID-19 from CT scan images, designed to assist overburdened
medical professionals. Our proposed model leverages the strengths of VGG16,
DenseNet121, and MobileNetV2 to extract features, followed by Principal
Component Analysis (PCA) for dimensionality reduction, after which the features
are stacked and classified using a Support Vector Classifier (SVC). We
conducted comparative analysis between the proposed hybrid model and individual
pre-trained CNN models, using a dataset of 2,108 training images and 373 test
images comprising both COVID-positive and non-COVID images. Our proposed hybrid
model achieved an accuracy of 98.93%, outperforming the individual models in
terms of precision, recall, F1 scores, and ROC curve performance.

æè¦ï¼æ©æåµæ¸¬ COVID-19 å°æææ²»çåæ§å¶å¶å³æ­è³ééè¦ãæ¬ç ç©¶æåºä¸åæ°ç©çæ·±åº¦å­¸ç¿æ··åæ¨¡åï¼ç¨æ¼å¾é»è¦æ·å±¤ææå½±åä¸­åµæ¸¬ COVID-19ï¼æ¨å¨åå©è² æééçé«çå°æ¥­äººå¡ãæåæåºçæ¨¡åå©ç¨ VGG16ãDenseNet121 å MobileNetV2 çåªé»ä¾èåç¹å¾µï¼æ¥èé²è¡ä¸»æååæ (PCA) ä»¥é²è¡éç¶­ï¼ç¶å¾å°ç¹å¾µå çä¸¦ä½¿ç¨æ¯æåéåé¡å¨ (SVC) é²è¡åé¡ãæåå°æåºçæ··åæ¨¡åååå¥é è¨ç·´ç CNN æ¨¡åé²è¡æ¯è¼åæï¼ä½¿ç¨åå« 2,108 å¼µè¨ç·´å½±åå 373 å¼µæ¸¬è©¦å½±åçè³æéï¼å¶ä¸­åå« COVID-19 é½æ§å½±ååé COVID-19 å½±åãæåæåºçæ··åæ¨¡åéå°äº 98.93% çæºç¢ºåº¦ï¼å¨ç²¾æºåº¦ãå¬åçãF1 åæ¸å ROC æ²ç·æè½æ¹é¢åªæ¼åå¥æ¨¡åã

##### **Three-Dimensional Diffusion-Weighted Multi-Slab MRI With Slice Profile Compensation Using Deep Energy Model**
2501.17152v1 by Reza Ghorbani, Jyothi Rikhab Chand, Chu-Yu Lee, Mathews Jacob, Merry Mani

Three-dimensional (3D) multi-slab acquisition is a technique frequently
employed in high-resolution diffusion-weighted MRI in order to achieve the best
signal-to-noise ratio (SNR) efficiency. However, this technique is limited by
slab boundary artifacts that cause intensity fluctuations and aliasing between
slabs which reduces the accuracy of anatomical imaging. Addressing this issue
is crucial for advancing diffusion MRI quality and making high-resolution
imaging more feasible for clinical and research applications. In this work, we
propose a regularized slab profile encoding (PEN) method within a Plug-and-Play
ADMM framework, incorporating multi-scale energy (MuSE) regularization to
effectively improve the slab combined reconstruction. Experimental results
demonstrate that the proposed method significantly improves image quality
compared to non-regularized and TV-regularized PEN approaches. The regularized
PEN framework provides a more robust and efficient solution for high-resolution
3D diffusion MRI, potentially enabling clearer, more reliable anatomical
imaging across various applications.

æè¦ï¼ä¸ç¶­ (3D) å¤å±¤æ¿æ·åæ¯ä¸ç¨®æè¡ï¼ç¶å¸¸ä½¿ç¨æ¼é«è§£æåº¦æ´æ£å æ¬ MRIï¼ä»¥éå°æä½³çè¨èéè¨æ¯ (SNR) æçãç¶èï¼æ­¤æè¡åå°å±¤æ¿éçå½å½±çéå¶ï¼æé æå¼·åº¦æ³¢ååå±¤æ¿ä¹éçæ··çï¼éä½è§£åå½±åçæºç¢ºåº¦ãè§£æ±ºéååé¡å°æ¼æåæ´æ£ MRI åè³ªè³ééè¦ï¼ä¸¦ä½¿é«è§£æåº¦å½±åæ´é©ç¨æ¼è¨åºåç ç©¶æç¨ãå¨éé å·¥ä½ä¸­ï¼æåå¨ Plug-and-Play ADMM æ¶æ§å§æåºæ­£è¦åçå±¤æ¿è¼ªå»ç·¨ç¢¼ (PEN) æ¹æ³ï¼ä¸¦çµåå¤å°ºåº¦è½é (MuSE) æ­£è¦åï¼ä»¥æææ¹åå±¤æ¿çµåéå»ºãå¯¦é©çµæè­æï¼èéæ­£è¦åå TV æ­£è¦å PEN æ¹æ³ç¸æ¯ï¼ææåºçæ¹æ³é¡¯èæåäºå½±ååè³ªãæ­£è¦åç PEN æ¶æ§çºé«è§£æåº¦ 3D æ´æ£ MRI æä¾æ´å¼·åºä¸ææççè§£æ±ºæ¹æ¡ï¼æ½å¨å¯å¯¦ç¾æ´æ¸æ°ãæ´å¯é çè§£åå½±åï¼é©ç¨æ¼åç¨®æç¨ã

##### **Irony Detection, Reasoning and Understanding in Zero-shot Learning**
2501.16884v1 by Peiling Yi, Yuhan Xia

Irony is a powerful figurative language (FL) on social media that can
potentially mislead various NLP tasks, such as recommendation systems,
misinformation checks, and sentiment analysis. Understanding the implicit
meaning of this kind of subtle language is essential to mitigate irony's
negative impact on NLP tasks. However, building models to understand irony
presents a unique set of challenges, because irony is a complex form of
language that often relies on context, tone, and subtle cues to convey meaning
that is opposite or different from the literal interpretation. Large language
models, such as ChatGPT, are increasingly able to capture implicit and
contextual information. In this study, we investigate the generalization,
reasoning and understanding ability of ChatGPT on irony detection across six
different genre irony detection datasets. Our findings suggest that ChatGPT
appears to show an enhanced language understanding and reasoning ability. But
it needs to be very careful in prompt engineering design. Thus, we propose a
prompt engineering design framework IDADP to achieve higher irony detection
accuracy, improved understanding of irony, and more effective explanations
compared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain
via experiments that the practice generated under the framework is likely to be
the promised solution to resolve the generalization issues of LLMs.

æè¦ï¼åè«·æ¯ä¸ç¨®å¼·å¤§çç¤¾äº¤åªé«æ¯å»èªè¨ (FL)ï¼å¯è½æèª¤å°åç¨® NLP ä»»åï¼ä¾å¦æ¨è¦ç³»çµ±ãé¯èª¤è¨æ¯æª¢æ¥åæç·åæãçè§£éç¨®å¾®å¦èªè¨çé±å«å«ç¾©å°æ¼æ¸è¼åè«·å° NLP ä»»åçè² é¢å½±é¿è³ééè¦ãç¶èï¼å»ºç«æ¨¡åä¾çè§£åè«·æå¸¶ä¾ä¸ç³»åç¨ç¹çææ°ï¼å çºåè«·æ¯ä¸ç¨®è¤éçèªè¨å½¢å¼ï¼éå¸¸ä¾è³´æ¼ä¸ä¸æãèªæ°£åå¾®å¦çç·ç´¢ä¾å³éèå­é¢è§£éç¸åæä¸åçå«ç¾©ãå¤§åèªè¨æ¨¡åï¼ä¾å¦ ChatGPTï¼è¶ä¾è¶è½å¤ ææé±å«åä¸ä¸æä¿¡æ¯ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨äº ChatGPT å¨å­åä¸åé¡ååè«·æª¢æ¸¬æ¸æéä¸çåè«·æª¢æ¸¬çæ¦æ¬ãæ¨çåçè§£è½åãæåçç ç©¶çµæè¡¨æï¼ChatGPT ä¼¼ä¹è¡¨ç¾åºå¢å¼·çèªè¨çè§£åæ¨çè½åãä½å®éè¦å¨æç¤ºå·¥ç¨è¨­è¨ä¸­éå¸¸å°å¿ãå æ­¤ï¼æåæåºäºä¸åæç¤ºå·¥ç¨è¨­è¨æ¡æ¶ IDADPï¼ä»¥å¯¦ç¾æ´é«çåè«·æª¢æ¸¬æºç¢ºåº¦ãæ¹é²çåè«·çè§£ä»¥åèå¶ä»æåé²ç ChatGPT é¶æ¬¡å­¸ç¿æ¹æ³ç¸æ¯æ´ææçè§£éãä¸¦ééå¯¦é©ç¢ºå®å¨è©²æ¡æ¶ä¸ç¢ççå¯¦è¸å¾å¯è½æ¯è§£æ±º LLM æ¦æ¬åé¡çæ¿è«¾è§£æ±ºæ¹æ¡ã

##### **Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help?**
2501.17207v1 by Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang

Functional brain connectome is crucial for deciphering the neural mechanisms
underlying cognitive functions and neurological disorders. Graph deep learning
models have recently gained tremendous popularity in this field. However, their
actual effectiveness in modeling the brain connectome remains unclear. In this
study, we re-examine graph deep learning models based on four large-scale
neuroimaging studies encompassing diverse cognitive and clinical outcomes.
Surprisingly, we find that the message aggregation mechanism, a hallmark of
graph deep learning models, does not help with predictive performance as
typically assumed, but rather consistently degrades it. To address this issue,
we propose a hybrid model combining a linear model with a graph attention
network through dual pathways, achieving robust predictions and enhanced
interpretability by revealing both localized and global neural connectivity
patterns. Our findings urge caution in adopting complex deep learning models
for functional brain connectome analysis, emphasizing the need for rigorous
experimental designs to establish tangible performance gains and perhaps more
importantly, to pursue improvements in model interpretability.

æè¦ï¼åè½æ§è¦é£æ¥é«å°æ¼ç ´è­¯èªç¥åè½åç¥ç¶ç¾çèå¾çæ©å¶è³ééè¦ãåå½¢æ·±åº¦å­¸ç¿æ¨¡åæè¿å¨éåé åç²å¾æ¥µå¤§çæ­¡è¿ãç¶èï¼å®åå¨å»ºæ¨¡è¦é£æ¥é«çå¯¦éæè½ä»ä¸æç¢ºãå¨éé ç ç©¶ä¸­ï¼æåæ ¹æåé æ¶µèä¸åèªç¥åè¨åºçµæçå¤§è¦æ¨¡ç¥ç¶å½±åç ç©¶ï¼éæ°æª¢è¦åå½¢æ·±åº¦å­¸ç¿æ¨¡åãä»¤äººé©è¨çæ¯ï¼æåç¼ç¾è¨æ¯èåæ©å¶ï¼åå½¢æ·±åº¦å­¸ç¿æ¨¡åçæ¨èªï¼ä¸¦ä¸åéå¸¸åè¨­çé£æ¨£æå©æ¼é æ¸¬æè½ï¼åèæçºéä½æè½ãçºäºè§£æ±ºéååé¡ï¼æåæåºä¸åæ··åæ¨¡åï¼éééè·¯å¾çµåç·æ§æ¨¡åèåå½¢æ³¨æåç¶²è·¯ï¼éæç©©å¥çé æ¸¬åå¢å¼·çå¯è§£éæ§ï¼æ¹æ³æ¯æ­é²å±é¨åæ´é«çç¥ç¶é£æ¥æ¨¡å¼ãæåçç¼ç¾æ¦ä¿å¨æ¡ç¨è¤éçæ·±åº¦å­¸ç¿æ¨¡åé²è¡åè½æ§è¦é£æ¥é«åææä¿æè¬¹æï¼å¼·èª¿éè¦å´è¬¹çå¯¦é©è¨­è¨ï¼ä»¥å»ºç«å·é«çæè½å¢çï¼æè¨±æ´éè¦çæ¯ï¼è¿½æ±æ¨¡åå¯è§£éæ§çæ¹é²ã

##### **Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care**
2501.17206v1 by Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao

This study explores a novel approach to advancing dementia care by
integrating socially assistive robotics, reinforcement learning (RL), large
language models (LLMs), and clinical domain expertise within a simulated
environment. This integration addresses the critical challenge of limited
experimental data in socially assistive robotics for dementia care, providing a
dynamic simulation environment that realistically models interactions between
persons living with dementia (PLWDs) and robotic caregivers. The proposed
framework introduces a probabilistic model to represent the cognitive and
emotional states of PLWDs, combined with an LLM-based behavior simulation to
emulate their responses. We further develop and train an adaptive RL system
enabling humanoid robots, such as Pepper, to deliver context-aware and
personalized interactions and assistance based on PLWDs' cognitive and
emotional states. The framework also generalizes to computer-based agents,
highlighting its versatility. Results demonstrate that the RL system, enhanced
by LLMs, effectively interprets and responds to the complex needs of PLWDs,
providing tailored caregiving strategies. This research contributes to
human-computer and human-robot interaction by offering a customizable AI-driven
caregiving platform, advancing understanding of dementia-related challenges,
and fostering collaborative innovation in assistive technologies. The proposed
approach has the potential to enhance the independence and quality of life for
PLWDs while alleviating caregiver burden, underscoring the transformative role
of interaction-focused AI systems in dementia care.

æè¦ï¼æ¬ç ç©¶æ¢ç´¢ä¸ç¨®åµæ°çæ¹æ³ï¼ééæ´åç¤¾æè¼å©æ©å¨äººãå¼·åå­¸ç¿ (RL)ãå¤§åèªè¨æ¨¡å (LLM) åè¨åºé åå°æ¥­ç¥è­æ¼æ¨¡æ¬ç°å¢ä¸­ï¼ä»¥æ¨é²å¤±æºçç§è­·ãéç¨®æ´åè§£æ±ºäºå¤±æºçç§è­·ä¸­ç¤¾æè¼å©æ©å¨äººå¯¦é©æ¸ææéçéå¤§ææ°ï¼æä¾äºä¸ååæçæ¨¡æ¬ç°å¢ï¼çå¯¦å°æ¨¡æ¬å¤±æºçæ£è (PLWD) åæ©å¨äººç§è­·èä¹éçäºåãææåºçæ¶æ§å¼å¥äºæ©çæ¨¡åä¾è¡¨ç¤º PLWD çèªç¥åæç·çæï¼ä¸¦çµåäºåºæ¼ LLM çè¡çºæ¨¡æ¬ä¾æ¨¡æ¬ä»åçåæãæåé²ä¸æ­¥éç¼ä¸¦è¨ç·´äºä¸åé©ææ§ RL ç³»çµ±ï¼ä½¿ Pepper ç­äººå½¢æ©å¨äººè½å¤ æ ¹æ PLWD çèªç¥åæç·çææä¾æå¢æç¥ååäººåçäºåååå©ãè©²æ¶æ§ä¹æ¦æ¬å°é»è¦ä»£çï¼çªé¡¯äºå®çå¤åè½æ§ãçµæè¡¨æï¼ç± LLM å¢å¼·ç RL ç³»çµ±ææå°è§£éååæ PLWD çè¤ééæ±ï¼æä¾éèº«æé çç§è­·ç­ç¥ãéé ç ç©¶ééæä¾ä¸åå¯èªè¨ç AI é©åç§è­·å¹³å°ï¼ä¿é²å°å¤±æºçç¸éææ°çäºè§£ï¼ä¸¦ä¿é²è¼å©æè¡çåä½åµæ°ï¼çºäººæ©äºååäººæ©äºåååºè²¢ç»ãææåºçæ¹æ³æå¯è½æé« PLWD çç¨ç«æ§åçæ´»åè³ªï¼åææ¸è¼ç§è­·èçè² æï¼å¼·èª¿äºäºåå°å AI ç³»çµ±å¨å¤±æºçç§è­·ä¸­çè½åä½ç¨ã

##### **Efficient Knowledge Distillation of SAM for Medical Image Segmentation**
2501.16740v1 by Kunal Dasharath Patil, Gowthamaan Palani, Ganapathy Krishnamurthi

The Segment Anything Model (SAM) has set a new standard in interactive image
segmentation, offering robust performance across various tasks. However, its
significant computational requirements limit its deployment in real-time or
resource-constrained environments. To address these challenges, we propose a
novel knowledge distillation approach, KD SAM, which incorporates both encoder
and decoder optimization through a combination of Mean Squared Error (MSE) and
Perceptual Loss. This dual-loss framework captures structural and semantic
features, enabling the student model to maintain high segmentation accuracy
while reducing computational complexity. Based on the model evaluation on
datasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast
Ultrasound, we demonstrate that KD SAM achieves comparable or superior
performance to the baseline models, with significantly fewer parameters. KD SAM
effectively balances segmentation accuracy and computational efficiency, making
it well-suited for real-time medical image segmentation applications in
resource-constrained environments.

æè¦ï¼åæ®µä»»ä½æ¨¡å (SAM) å·²å¨äºåå¼å½±ååå²ä¸­æ¨¹ç«æ°æ¨æºï¼å¨åé ä»»åä¸­çè½æä¾ç©©å¥çæè½ãç¶èï¼å¶é¾å¤§çéç®éæ±éå¶äºå®å¨å³ææè³æºåéç°å¢ä¸­çé¨ç½²ãçºäºæå°éäºææ°ï¼æåæåºäºä¸ç¨®æ°ç©çç¥è­è¸é¤¾æ¹æ³ï¼KD SAMï¼å®ééçµååæ¹èª¤å·® (MSE) åæç¥æå¤±ï¼å°ç·¨ç¢¼å¨åè§£ç¢¼å¨æä½³åç´å¥å¶ä¸­ãæ­¤ééæå¤±æ¶æ§æ·åçµæ§åèªç¾©ç¹å¾µï¼è®å­¸çæ¨¡åè½å¤ å¨éä½éç®è¤éåº¦çåæï¼ç¶­æé«åå²æºç¢ºåº¦ãæ ¹æå¨ Kvasir-SEGãISIC 2017ãèåé ­é¨è¶é³æ³¢åä¹³æ¿è¶é³æ³¢ç­è³æéä¸çæ¨¡åè©ä¼°ï¼æåè­æ KD SAM éå°äºèåºæºæ¨¡åç¸ç¶ææ´åªç°çæè½ï¼ä¸åæ¸æé¡¯æ´å°ãKD SAM ææå°å¹³è¡¡äºåå²æºç¢ºåº¦åéç®æçï¼ä½¿å¶éå¸¸é©åå¨è³æºåéç°å¢ä¸­çå³æé«å­¸å½±ååå²æç¨ã

##### **VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records**
2501.16672v1 by Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour

Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨è¨åºé«å­¸ä¸­çæææ¬çäºå¯¦æºç¢ºæ§ï¼ç¼ºä¹ç¢ºä¿çæ¹æ³ãVeriFact æ¯ä¸ç¨®äººå·¥æºæ§ç³»çµ±ï¼çµåäºæª¢ç´¢å¢å¼·çæå LLM-as-a-Judgeï¼ç¨æ¼é©è­ LLM çæçææ¬æ¯å¦åºæ¼çäººçé»å­å¥åº·è¨é (EHR) ç²å¾çäººççæ­·äºå¯¦æ¯æãçºäºè©ä¼°éåç³»çµ±ï¼æåå¼å¥äº VeriFact-BHCï¼éæ¯ä¸åæ°çè³æéï¼å°åºé¢æè¦ä¸­çç°¡è¦ä½é¢çç¨åè§£æä¸çµç°¡å®çé³è¿°ï¼ä¸¦ç±è¨åºé«çè¨»è§£æ¯ä¸åé³è¿°æ¯å¦ç²å¾çäººç EHR çæ­·æè¦æ¯æãåç®¡è¨åºé«çä¹éçæé«ä¸è´æ§çº 88.5%ï¼ä½èå»åªåè£æ±ºçå¹³åäººé¡è¨åºé«çåºæ¬äºå¯¦ç¸æ¯ï¼VeriFact çä¸è´æ§é«é 92.7%ï¼éè¡¨æ VeriFact è¶è¶äºå¹³åè¨åºé«çæ ¹æçäººççæ­·æª¢æ¥ææ¬äºå¯¦çè½åãVeriFact å¯è½æééç§»é¤ç®åçè©ä¼°ç¶é ¸ï¼å éåºæ¼ LLM ç EHR æç¨ç¨å¼çéç¼ã

##### **Vision-based autonomous structural damage detection using data-driven methods**
2501.16662v2 by Seyyed Taghi Ataei, Parviz Mohammad Zadeh, Saeid Ataei

This study addresses the urgent need for efficient and accurate damage
detection in wind turbine structures, a crucial component of renewable energy
infrastructure. Traditional inspection methods, such as manual assessments and
non-destructive testing (NDT), are often costly, time-consuming, and prone to
human error. To tackle these challenges, this research investigates advanced
deep learning algorithms for vision-based structural health monitoring (SHM). A
dataset of wind turbine surface images, featuring various damage types and
pollution, was prepared and augmented for enhanced model training. Three
algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to
detect and classify surface damage. The models were trained and evaluated on a
dataset split into training, testing, and evaluation subsets (80%-10%-10%).
Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50
and high processing speed, making it suitable for real-time inspections. By
optimizing hyperparameters like learning rate and batch size, the models'
accuracy and efficiency improved further. YOLOv7 demonstrated significant
advancements in detection precision and execution speed, especially for
real-time applications. However, challenges such as dataset limitations and
environmental variability were noted, suggesting future work on segmentation
methods and larger datasets. This research underscores the potential of
vision-based deep learning techniques to transform SHM practices by reducing
costs, enhancing safety, and improving reliability, thus contributing to the
sustainable maintenance of critical infrastructure and supporting the longevity
of wind energy systems.

æè¦ï¼æ¬ç ç©¶è§£æ±ºäºé¢¨åæ¸¦è¼ªæ©çµæ§ä¸­è¿«åéè¦çææä¸æºç¢ºçæå·æª¢æ¸¬ï¼éæ¯å¯åçè½æºåºç¤è¨­æ½çééµçµæé¨åãå³çµ±çæª¢æ¥æ¹æ³ï¼ä¾å¦æåè©ä¼°åéç ´å£æ§æª¢æ¸¬ (NDT)ï¼éå¸¸ææ¬é«æãèæä¸å®¹æåºé¯ãçºäºæå°éäºææ°ï¼æ¬ç ç©¶èª¿æ¥äºç¨æ¼åºæ¼è¦è¦ºççµæ§å¥åº·ç£æ¸¬ (SHM) çåé²æ·±åº¦å­¸ç¿æ¼ç®æ³ãæºåäºä¸çµé¢¨åæ¸¦è¼ªæ©è¡¨é¢å½±åçè³æéï¼å¶ä¸­åå«åç¨®æå£é¡ååæ±¡æï¼ä¸¦æ´åäºå¢å¼·æ¨¡åè¨ç·´ãæ¡ç¨äºä¸ç¨®æ¼ç®æ³ââYOLOv7ãå¶è¼éç´è®é«å Faster R-CNNââä¾æª¢æ¸¬ååé¡è¡¨é¢æå£ãéäºæ¨¡åå¨åå²æè¨ç·´ãæ¸¬è©¦åè©ä¼°å­éï¼80%-10%-10%ï¼çè³æéä¸é²è¡è¨ç·´åè©ä¼°ãçµæè¡¨æï¼YOLOv7 åªæ¼å¶ä»æ¼ç®æ³ï¼å¯¦ç¾äº 82.4% ç mAP@50 åè¼é«çèçéåº¦ï¼ä½¿å¶é©ç¨æ¼å³ææª¢æ¥ãééæä½³åå­¸ç¿çåæ¹æ¬¡å¤§å°ç­è¶åæ¸ï¼æ¨¡åçæºç¢ºæ§åæçé²ä¸æ­¥æé«ãYOLOv7 å¨æª¢æ¸¬ç²¾åº¦åå·è¡éåº¦æ¹é¢è¡¨ç¾åºé¡¯èçé²æ­¥ï¼ç¹å¥æ¯å°æ¼å³ææç¨ç¨å¼ãç¶èï¼æ³¨æå°è³æééå¶åç°å¢è®ç°æ§ç­ææ°ï¼éè¡¨ææªä¾å¨åå²æ¹æ³åæ´å¤§çè³æéæ¹é¢çå·¥ä½ãæ¬ç ç©¶å¼·èª¿äºåºæ¼è¦è¦ºçæ·±åº¦å­¸ç¿æè¡å¨è½æ SHM å¯¦åæ¹é¢çæ½åï¼æ¹æ³æ¯éä½ææ¬ãå¢å¼·å®å¨æ§ä¸¦æé«å¯é æ§ï¼å¾èæå©æ¼ç¶­è­·ééµåºç¤è¨­æ½çå¯æçºæ§ä¸¦æ¯æé¢¨è½ç³»çµ±çé·å£½å½ã

##### **Molecular-driven Foundation Model for Oncologic Pathology**
2501.16652v1 by Anurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner, Ming Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina ElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, Faisal Mahmood

Foundation models are reshaping computational pathology by enabling transfer
learning, where models pre-trained on vast datasets can be adapted for
downstream diagnostic, prognostic, and therapeutic response tasks. Despite
these advances, foundation models are still limited in their ability to encode
the entire gigapixel whole-slide images without additional training and often
lack complementary multimodal data. Here, we introduce Threads, a slide-level
foundation model capable of generating universal representations of whole-slide
images of any size. Threads was pre-trained using a multimodal learning
approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained
tissue sections, paired with corresponding genomic and transcriptomic profiles
- the largest such paired dataset to be used for foundation model development
to date. This unique training paradigm enables Threads to capture the tissue's
underlying molecular composition, yielding powerful representations applicable
to a wide array of downstream tasks. In extensive benchmarking across 54
oncology tasks, including clinical subtyping, grading, mutation prediction,
immunohistochemistry status determination, treatment response prediction, and
survival prediction, Threads outperformed all baselines while demonstrating
remarkable generalizability and label efficiency. It is particularly well
suited for predicting rare events, further emphasizing its clinical utility. We
intend to make the model publicly available for the broader community.

æè¦ï¼åºç¤æ¨¡åééåç¨è½ç§»å­¸ç¿ä¾éå¡è¨ç®ççå­¸ï¼å¶ä¸­é åå¨é¾å¤§è³æéä¸è¨ç·´çæ¨¡åå¯é©ææ¼ä¸æ¸¸è¨ºæ·ãé å¾åæ²»çåæä»»åãåç®¡æéäºé²å±ï¼åºç¤æ¨¡åå¨ç·¨ç¢¼æ´ååååç´ å¨å¹»ççå½±åçè½åä¸ä»æéï¼ä¸ç¶å¸¸ç¼ºä¹è£åå¤æ¨¡å¼è³æãå¨æ­¤ï¼æåä»ç´¹ Threadsï¼éæ¯ä¸åå¹»ççå±¤ç´åºç¤æ¨¡åï¼è½å¤ ç¢çä»»ä½å¤§å°çå¨å¹»ççå½±åçéç¨è¡¨ç¤ºãThreads ä½¿ç¨å¤æ¨¡å¼å­¸ç¿æ¹æ³é åè¨ç·´ï¼ä¸¦éå° 47,171 åèæ¨ç²¾åæç´ (H&E) æè²ççµç¹åççå¤åç¾¤çµé²è¡è¨ç·´ï¼ä¸¦æ­éå°æçåºå é«åè½éçµç¹å¾µæªï¼éæ¯è¿ä»çºæ­¢ç¨æ¼åºç¤æ¨¡åéç¼çæå¤§æ­¤é¡éå°è³æéãéç¨®ç¨ç¹çè¨ç·´ç¯ä¾ä½¿ Threads è½å¤ æ·åçµç¹çåºç¤åå­çµæï¼ç¢çå¼·å¤§çè¡¨ç¤ºï¼é©ç¨æ¼å»£æ³çä¸æ¸¸ä»»åãå¨æ¶µè 54 åè«ç¤å­¸ä»»åçå»£æ³åºæºæ¸¬è©¦ä¸­ï¼åæ¬è¨åºååãåç´ãçªè®é æ¸¬ãåç«çµç¹åå­¸çæå¤å®ãæ²»çåæé æ¸¬åå­æ´»é æ¸¬ï¼Threads åªæ¼ææåºæºï¼åæå±ç¾åºé¡¯èçæ¦æ¬æ§åæ¨ç±¤æçãå®ç¹å¥é©åé æ¸¬ç½è¦äºä»¶ï¼é²ä¸æ­¥å¼·èª¿å¶è¨åºæç¨ãæåæç®è®è©²æ¨¡åå¬éï¼ä¾æ´å»£æ³çç¤¾ç¾¤ä½¿ç¨ã

##### **Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models**
2501.16282v1 by Jing Zhang, Xiaowei Yu, Yanjun Lyu, Lu Zhang, Tong Chen, Chao Cao, Yan Zhuang, Minheng Chen, Tianming Liu, Dajiang Zhu

Understanding brain disorders is crucial for accurate clinical diagnosis and
treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a
promising approach to interpreting medical images with the support of text
descriptions. However, previous research has primarily focused on 2D medical
images, leaving richer spatial information of 3D images under-explored, and
single-modality-based methods are limited by overlooking the critical clinical
information contained in other modalities. To address this issue, this paper
proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck
layer to learn new knowledge and instill it into the original pre-trained
knowledge. The major idea is to incorporate a lightweight bottleneck layer to
train fewer parameters while capturing essential information and utilize a
Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal
data within a unified representation space. Extensive experiments demonstrated
the effectiveness of our approach in integrating multimodal data to
significantly improve the diagnosis accuracy without high computational costs,
highlighting the potential to enhance real-world diagnostic workflows.

æè¦ï¼äºè§£è¦é¨ç¾çå°æ¼æºç¢ºçè¨åºè¨ºæ·åæ²»çè³ééè¦ãå¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) çææ°é²å±æä¾äºä¸åæåéçæ¹æ³ï¼å¯ä»¥å¨ææ¬æè¿°çæ¯æ´ä¸è©®éé«å­¸å½±åãç¶èï¼ååçç ç©¶ä¸»è¦éä¸­å¨ 2D é«å­¸å½±åï¼å¿½ç¥äº 3D å½±åæ´è±å¯çç©ºéè³è¨ï¼èå®ä¸æ¨¡ææ¹æ³åå°å¿½è¦å¶ä»æ¨¡æä¸­ééµè¨åºè³è¨çéå¶ãçºäºè§£æ±ºéååé¡ï¼æ¬ææåºäº Brain-Adapterï¼éæ¯ä¸ç¨®æ°çæ¹æ³ï¼å®çµåäºä¸åé¡å¤çç¶é ¸å±¤ä¾å­¸ç¿æ°ç¥è­ä¸¦å°å¶çè¼¸å°åå§é è¨ç·´çç¥è­ä¸­ãä¸»è¦çæ³æ³æ¯çµåä¸åè¼éç´ç¶é ¸å±¤ï¼å¨æ·åå¿è¦è³è¨çåæè¨ç·´è¼å°çåæ¸ï¼ä¸¦å©ç¨å°æ¯èªè¨å½±åé è¨ç·´ (CLIP) ç­ç¥å¨çµ±ä¸çè¡¨ç¤ºç©ºéä¸­å°é½å¤æ¨¡æè³æãå»£æ³çå¯¦é©è­æäºæåçæ¹æ³å¨æ´åå¤æ¨¡æè³æä»¥é¡¯èæé«è¨ºæ·æºç¢ºæ§æ¹é¢çæææ§ï¼èä¸æé æé«éç®ææ¬ï¼çªé¡¯äºå¢å¼·çå¯¦ä¸çè¨ºæ·å·¥ä½æµç¨çæ½åã

##### **Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models**
2501.16215v1 by Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D. S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li

Large language models (LLMs) exhibit remarkable capabilities in visual
inspection of medical time-series data, achieving proficiency comparable to
human clinicians. However, their broad scope limits domain-specific precision,
and proprietary weights hinder fine-tuning for specialized datasets. In
contrast, small specialized models (SSMs) excel in targeted tasks but lack the
contextual reasoning required for complex clinical decision-making. To address
these challenges, we propose ConMIL (Conformalized Multiple Instance Learning),
a decision-support SSM that integrates seamlessly with LLMs. By using Multiple
Instance Learning (MIL) to identify clinically significant signal segments and
conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'
interpretative capabilities for medical time-series analysis. Experimental
results demonstrate that ConMIL significantly improves the performance of
state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,
\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for
confident samples in arrhythmia detection and sleep staging, compared to
standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the
potential of ConMIL to bridge task-specific precision and broader contextual
reasoning, enabling more reliable and interpretable AI-driven clinical decision
support.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çæéåºåè³æçè¦è¦ºæª¢æ¥ä¸­å±ç¾åºéå¡çè½åï¼éå°äºèäººé¡è¨åºé«çç¸ç¶ççç·´åº¦ãç¶èï¼å®åçå»£æ³ç¯åéå¶äºç¹å®é åçç²¾ç¢ºåº¦ï¼èå°ææ¬éé»ç¤äºéå°ç¹å®è³æéçå¾®èª¿ãç¸æ¯ä¹ä¸ï¼å°åå°ç¨æ¨¡å (SSM) å¨ç®æ¨ä»»åä¸­è¡¨ç¾åºè²ï¼ä½ç¼ºä¹è¤éè¨åºæ±ºç­å¶å®æéçèæ¯æ¨çãçºäºæå°éäºææ°ï¼æåæåºäº ConMILï¼å±å½¢å¤å¯¦ä¾å­¸ç¿ï¼ï¼éæ¯ä¸åè LLM ç¡ç¸«æ´åçæ±ºç­æ¯æ´ SSMãééä½¿ç¨å¤å¯¦ä¾å­¸ç¿ (MIL) ä¾è­å¥è¨åºé¡¯èè¨èåæ®µï¼ä¸¦å°æ ¡æºçéåå¼è¼¸åºé²è¡å±å½¢é æ¸¬ï¼ConMIL å¢å¼·äº LLM å°é«çæéåºååæçè§£éè½åãå¯¦é©çµæè¡¨æï¼ConMIL æé¡¯æ¹åäºæåé² LLM çæè½ï¼ä¾å¦ ChatGPT4.0 å Qwen2-VL-7Bãå·é«ä¾èªªï¼\ConMIL{}- æ¯æ´ç Qwen2-VL-7B å¨å¿å¾ä¸æ´åµæ¸¬åç¡ç åæä¸­ï¼å°æ¼æä¿¡å¿çæ¨£æ¬éå°äº 94.92% å 96.82% çç²¾ç¢ºåº¦ï¼èç¨ç« LLM çæºç¢ºåº¦åçº 46.13% å 13.16%ãéäºç¼ç¾çªé¡¯äº ConMIL å¨æ©æ¥ç¹å®ä»»åçç²¾ç¢ºåº¦åæ´å»£æ³çèæ¯æ¨çæ¹é¢çæ½åï¼å¾èå¯¦ç¾æ´å¯é ä¸å¯è§£éç AI é©åè¨åºæ±ºç­æ¯æ´ã

##### **Atla Selene Mini: A General Purpose Evaluation Model**
2501.17195v1 by Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, Mathias Leys, Maurice Burger, Max Bartolo, Roman Engeler, Sashank Pisupati, Toby Drane, Young Sun Park

We introduce Atla Selene Mini, a state-of-the-art small language
model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that
outperforms the best SLMJs and GPT-4o-mini on overall performance across 11
out-of-distribution benchmarks, spanning absolute scoring, classification, and
pairwise preference tasks. It is the highest-scoring 8B generative model on
RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To
achieve this, we develop a principled data curation strategy that augments
public datasets with synthetically generated critiques and ensures high quality
through filtering and dataset ablations. We train our model on a combined
direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and
produce a highly promptable evaluator that excels in real-world scenarios.
Selene Mini shows dramatically improved zero-shot agreement with human expert
evaluations on financial and medical industry datasets. It is also robust to
variations in prompt format. Preliminary results indicate that Selene Mini is
the top-ranking evaluator in a live, community-driven Judge Arena. We release
the model weights on HuggingFace
(https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage
widespread community adoption.

æè¦ï¼æåä»ç´¹ Atla Selene Miniï¼éæ¯ä¸åæåé²çå°åèªè¨æ¨¡åè©å¯© (SLMJ)ãSelene Mini æ¯ä¸åéç¨è©ä¼°å¨ï¼å¨ 11 ååä½å¤åºæºæ¸¬è©¦ï¼åå«çµå°è©åãåé¡åæå°åå¥½ä»»åï¼çæ´é«æè½ä¸åªæ¼æä½³ç SLMJ å GPT-4o-miniãå®æ¯ RewardBench ä¸å¾åæé«ç 8B çææ¨¡åï¼è¶è¶äº GPT-4o åå°æ¥­è©å¯©ç­å¼·å¤§çåºæºãçºäºéææ­¤ç®æ¨ï¼æåéç¼äºä¸ç¨®æååçè³æç­å±ç­ç¥ï¼å©ç¨åæç¢ççè©è«æ´åå¬éè³æéï¼ä¸¦ééç¯©é¸åè³æéæ¶èç¢ºä¿é«åè³ªãæåå¨çµåç´æ¥åå¥½æä½³å (DPO) åç£ç£å¾®èª¿ (SFT) æå¤±çè³æéä¸è¨ç·´æåçæ¨¡åï¼ä¸¦ç¢çä¸åé«åº¦å¯æç¤ºçè©ä¼°å¨ï¼å¨å¯¦éå ´æ¯ä¸­è¡¨ç¾åºè²ãSelene Mini å¨è²¡ååé«çç¢æ¥­è³æéä¸é¡¯ç¤ºåºé¡¯èæ¹åçé¶æ¬¡å­¸ç¿èäººé¡å°å®¶è©ä¼°çä¸è´æ§ãå®ä¹å°æç¤ºæ ¼å¼çè®åå·æç©©å¥æ§ãåæ­¥çµæé¡¯ç¤º Selene Mini æ¯ç¤¾ç¾¤é©åç Judge Arena ä¸­æåç¬¬ä¸çè©ä¼°å¨ãæåå¨ HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) å Ollama ä¸éåºæ¨¡åæ¬éï¼ä»¥é¼åµå»£æ³çç¤¾ç¾¤æ¡ç¨ã

##### **An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases**
2501.15969v1 by Shaheer Ahmad Khan, Muhammad Usamah Shahid, Ahmad Abdullah, Ibrahim Hashmat, Muddassar Farooq

This study addresses a critical gap in the healthcare system by developing a
clinically meaningful, practical, and explainable disease surveillance system
for multiple chronic diseases, utilizing routine EHR data from multiple U.S.
practices integrated with CureMD's EMR/EHR system. Unlike traditional
systems--using AI models that rely on features from patients' labs--our
approach focuses on routinely available data, such as medical history, vitals,
diagnoses, and medications, to preemptively assess the risks of chronic
diseases in the next year. We trained three distinct models for each chronic
disease: prediction models that forecast the risk of a disease 3, 6, and 12
months before a potential diagnosis. We developed Random Forest models, which
were internally validated using F1 scores and AUROC as performance metrics and
further evaluated by a panel of expert physicians for clinical relevance based
on inferences grounded in medical knowledge. Additionally, we discuss our
implementation of integrating these models into a practical EMR system. Beyond
using Shapley attributes and surrogate models for explainability, we also
introduce a new rule-engineering framework to enhance the intrinsic
explainability of Random Forests.

æè¦ï¼æ¬ç ç©¶éééç¼ä¸åè¨åºææç¾©ãå¯¦ç¨ä¸å¯è§£éçå¤éæ¢æ§ç¾çç¾çç£æ¸¬ç³»çµ±ï¼ä¾è§£æ±ºé«çä¿å¥ç³»çµ±ä¸­çéå¤§ç¼ºå£ï¼å©ç¨æ´å CureMD ç EMR/EHR ç³»çµ±ï¼ä¾èªå¤åç¾åå¯¦åçä¾è¡ EHR è³æãèå³çµ±ç³»çµ±ä¸åçæ¯ï¼æåçåæ³èéå¨ä¾è¡å¯å¾çè³æï¼ä¾å¦çæ­·ãçå½å¾µè±¡ãè¨ºæ·åè¥ç©ï¼ä»¥é åè©ä¼°æªä¾ä¸å¹´æ¢æ§ç¾ççé¢¨éªï¼èéä»°è³´çæ£å¯¦é©å®¤ç¹å¾µç AI æ¨¡åãæåéå°æ¯ç¨®æ¢æ§ç¾çè¨ç·´äºä¸åä¸åçæ¨¡åï¼é æ¸¬æ¨¡åï¼ç¨ä»¥é æ¸¬å¨æ½å¨è¨ºæ·å 3ã6 å 12 åæçç¾çé¢¨éªãæåéç¼äºé¨æ©æ£®ææ¨¡åï¼ä¸¦ä½¿ç¨ F1 åæ¸å AUROC ä½çºæè½ææ¨ï¼é²è¡å§é¨é©è­ï¼ä¸¦é²ä¸æ­¥ç±å°å®¶é«å¸«å°çµæ ¹ææ¤åºæ¼é«å­¸ç¥è­çæ¨è«ï¼è©ä¼°å¶è¨åºç¸éæ§ãæ­¤å¤ï¼æåè¨è«äºå°éäºæ¨¡åæ´åå°å¯¦ç¨ EMR ç³»çµ±ä¸­çå¯¦ä½æ¹å¼ãé¤äºä½¿ç¨ Shapley å±¬æ§åä»£çæ¨¡åä¾è§£éå¤ï¼æåéå¼é²äºä¸åæ°çè¦åå·¥ç¨æ¶æ§ï¼ä»¥å¢å¼·é¨æ©æ£®æçå§å¨å¯è§£éæ§ã

##### **Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI**
2501.15733v1 by Taymaz Akan, Sait Alp, Md. Shenuarin Bhuiyan, Elizabeth A. Disbrow, Steven A. Conrad, John A. Vanchiere, Christopher G. Kevil, Mohammad A. N. Bhuiyan

Alzheimer's disease (AD) is a neurodegenerative disorder affecting millions
worldwide, necessitating early and accurate diagnosis for optimal patient
management. In recent years, advancements in deep learning have shown
remarkable potential in medical image analysis. Methods In this study, we
present "ViTranZheimer," an AD diagnosis approach which leverages video vision
transformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as
videos, we exploit the temporal dependencies between slices to capture
intricate structural relationships. The video vision transformer's
self-attention mechanisms enable the model to learn long-range dependencies and
identify subtle patterns that may indicate AD progression. Our proposed deep
learning framework seeks to enhance the accuracy and sensitivity of AD
diagnosis, empowering clinicians with a tool for early detection and
intervention. We validate the performance of the video vision transformer using
the ADNI dataset and conduct comparative analyses with other relevant models.
Results The proposed ViTranZheimer model is compared with two hybrid models,
CNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional
neural network (CNN) and a bidirectional long-short-term memory network
(BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT)
with BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and
ViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer
demonstrated the highest accuracy at 98.6%, outperforming other models in this
evaluation metric, indicating its superior performance in this specific
evaluation metric. Conclusion This research advances the understanding of
applying deep learning techniques in neuroimaging and Alzheimer's disease
research, paving the way for earlier and less invasive clinical diagnosis.

æè¦ï¼é¿è²æµ·é»ç (AD) æ¯ä¸ç¨®ç¥ç¶éåæ§ç¾çï¼å½±é¿èå¨çæ¸ç¾è¬äººï¼å æ­¤éè¦æ©ææºç¢ºè¨ºæ·ä»¥é²è¡æä½³æ£èç®¡çãè¿å¹´ä¾ï¼æ·±åº¦å­¸ç¿çé²æ­¥å¨é«å­¸å½±ååæä¸­å±ç¾äºéå¡çæ½åãæ¹æ³å¨éåç ç©¶ä¸­ï¼æåæåºãViTranZheimerãï¼ä¸ç¨® AD è¨ºæ·æ¹æ³ï¼å®å©ç¨å½±çè¦è¦ºè½æå¨ä¾åæ 3D å¤§è¦ MRI è³æãééå° 3D MRI é«ç©è¦çºå½±çï¼æåå©ç¨åçä¹éçæéä¾è³´æ§ä¾ææè¤éççµæ§éä¿ãå½±çè¦è¦ºè½æå¨çèªæ³¨æåæ©å¶ä½¿æ¨¡åè½å¤ å­¸ç¿é·ç¨ä¾è³´æ§ï¼ä¸¦è­å¥å¯è½è¡¨ç¤º AD é²å±çç´°å¾®æ¨¡å¼ãæåæåºçæ·±åº¦å­¸ç¿æ¶æ§æ¨å¨æé« AD è¨ºæ·çæºç¢ºæ§åæææ§ï¼çºè¨åºé«çæä¾æ©ææª¢æ¸¬åå¹²é çå·¥å·ãæåä½¿ç¨ ADNI è³æéé©è­å½±çè¦è¦ºè½æå¨çæè½ï¼ä¸¦èå¶ä»ç¸éæ¨¡åé²è¡æ¯è¼åæãçµæææåºç ViTranZheimer æ¨¡åèå©åæ··åæ¨¡å CNN-BiLSTM å ViT-BiLSTM é²è¡æ¯è¼ãCNN-BiLSTM æ¯å·ç©ç¥ç¶ç¶²è·¯ (CNN) åéåé·ç­æè¨æ¶ç¶²è·¯ (BiLSTM) ççµåï¼è ViT-BiLSTM æ¯è¦è¦ºè½æå¨ (ViT) è BiLSTM ççµåãå¨ ViTranZheimerãCNN-BiLSTM å ViT-BiLSTM æ¨¡åä¸­éå°çæºç¢ºåº¦åå¥çº 98.6%ã96.479% å 97.465%ãViTranZheimer ä»¥ 98.6% çæºç¢ºåº¦è¡¨ç¾æä½³ï¼å¨éåè©ä¼°ææ¨ä¸­åªæ¼å¶ä»æ¨¡åï¼é¡¯ç¤ºåºå¶å¨éåç¹å®è©ä¼°ææ¨ä¸­çåè¶æè½ãçµè«éé ç ç©¶ä¿é²äºå¨ç¥ç¶å½±ååé¿è²æµ·é»çç ç©¶ä¸­æç¨æ·±åº¦å­¸ç¿æè¡ççè§£ï¼çºæ´æ©ãæ´ä½ä¾µå¥æ§çè¨åºè¨ºæ·éªè·¯ã

##### **A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks**
2501.15724v1 by Dong Li, Guihong Wan, Xintao Wu, Xinyu Wu, Ajit J. Nirmal, Christine G. Lian, Peter K. Sorger, Yevgeniy R. Semenov, Chen Zhao

Computational pathology foundation models (CPathFMs) have emerged as a
powerful approach for analyzing histopathological data, leveraging
self-supervised learning to extract robust feature representations from
unlabeled whole-slide images. These models, categorized into uni-modal and
multi-modal frameworks, have demonstrated promise in automating complex
pathology tasks such as segmentation, classification, and biomarker discovery.
However, the development of CPathFMs presents significant challenges, such as
limited data accessibility, high variability across datasets, the necessity for
domain-specific adaptation, and the lack of standardized evaluation benchmarks.
This survey provides a comprehensive review of CPathFMs in computational
pathology, focusing on datasets, adaptation strategies, and evaluation tasks.
We analyze key techniques, such as contrastive learning and multi-modal
integration, and highlight existing gaps in current research. Finally, we
explore future directions from four perspectives for advancing CPathFMs. This
survey serves as a valuable resource for researchers, clinicians, and AI
practitioners, guiding the advancement of CPathFMs toward robust and clinically
applicable AI-driven pathology solutions.

æè¦ï¼è¨ç®ççåºç¤æ¨¡å (CPathFM) å·²æçºåæçµç¹ççå­¸æ¸æçä¸ç¨®å¼·å¤§æ¹æ³ï¼å©ç¨èªæç£ç£å­¸ç¿å¾æªæ¨è¨çå¨å¹»ççå½±åä¸­æåç©©å¥çç¹è²è¡¨å¾µãéäºæ¨¡ååçºå®æ¨¡æåå¤æ¨¡ææ¡æ¶ï¼å·²è­æææèªååè¤éçççä»»åï¼ä¾å¦åå²ãåé¡åçç©æ¨è¨ç¼ç¾ãç¶èï¼CPathFM çéç¼æåºäºéå¤§ææ°ï¼ä¾å¦æ¸æå¯åæ§æéãæ¸æéä¹éè®ç°æ§é«ãéè¦ç¹å®é åçé©ææ§ï¼ä»¥åç¼ºä¹æ¨æºåçè©ä¼°åºæºãéé èª¿æ¥å°è¨ç®ççå­¸ä¸­ç CPathFM é²è¡äºå¨é¢çåé¡§ï¼éé»éæ³¨æ¸æéãé©æç­ç¥åè©ä¼°ä»»åãæååæäºå°æ¯å­¸ç¿åå¤æ¨¡ææ´åç­ééµæè¡ï¼ä¸¦å¼·èª¿äºç¶åç ç©¶ä¸­å­å¨çå·®è·ãæå¾ï¼æåå¾ååè§åº¦æ¢è¨äºæ¨é² CPathFM çæªä¾æ¹åãéé èª¿æ¥ä½çºç ç©¶äººå¡ãè¨åºé«çåäººå·¥æºè½å¾æ¥­äººå¡çå¯¶è²´è³æºï¼æå° CPathFM æèç©©å¥ä¸è¨åºä¸é©ç¨ç AI é©åççè§£æ±ºæ¹æ¡éé²ã

##### **Beyond Benchmarks: On The False Promise of AI Regulation**
2501.15693v1 by Gabriel Stanovsky, Renana Keydar, Gadi Perl, Eliya Habba

The rapid advancement of artificial intelligence (AI) systems in critical
domains like healthcare, justice, and social services has sparked numerous
regulatory initiatives aimed at ensuring their safe deployment. Current
regulatory frameworks, exemplified by recent US and EU efforts, primarily focus
on procedural guidelines while presuming that scientific benchmarking can
effectively validate AI safety, similar to how crash tests verify vehicle
safety or clinical trials validate drug efficacy. However, this approach
fundamentally misunderstands the unique technical challenges posed by modern AI
systems. Through systematic analysis of successful technology regulation case
studies, we demonstrate that effective scientific regulation requires a causal
theory linking observable test outcomes to future performance - for instance,
how a vehicle's crash resistance at one speed predicts its safety at lower
speeds. We show that deep learning models, which learn complex statistical
patterns from training data without explicit causal mechanisms, preclude such
guarantees. This limitation renders traditional regulatory approaches
inadequate for ensuring AI safety. Moving forward, we call for regulators to
reckon with this limitation, and propose a preliminary two-tiered regulatory
framework that acknowledges these constraints: mandating human oversight for
high-risk applications while developing appropriate risk communication
strategies for lower-risk uses. Our findings highlight the urgent need to
reconsider fundamental assumptions in AI regulation and suggest a concrete path
forward for policymakers and researchers.

æè¦ï¼äººå·¥æºæ§ (AI) ç³»çµ±å¨é«çä¿å¥ãå¸æ³åç¤¾ææåç­ééµé åçå¿«éé²å±ï¼å¼ç¼äºè¨±å¤æ³è¦å¡è­°ï¼æ¨å¨ç¢ºä¿å¶å®å¨é¨ç½²ãä»¥æè¿ç¾ååæ­ççåªåçºä¾ï¼ç¶åçæ³è¦æ¡æ¶ä¸»è¦éæ³¨ç¨åºæåï¼åæåè¨­ç§å­¸åºæºæ¸¬è©¦å¯ä»¥ææé©è­ AI å®å¨æ§ï¼é¡ä¼¼æ¼ç¢°ææ¸¬è©¦é©è­è»è¼å®å¨æ§æè¨åºè©¦é©é©è­è¥ç©åæãç¶èï¼éç¨®æ¹æ³æ ¹æ¬èª¤è§£äºç¾ä»£ AI ç³»çµ±å¸¶ä¾çç¨ç¹æè¡ææ°ãééå°æåçæè¡æ³è¦æ¡ä¾ç ç©¶é²è¡ç³»çµ±åæï¼æåè­æææçç§å­¸æ³è¦éè¦ä¸åå æçè«ï¼å°å¯è§å¯çæ¸¬è©¦çµæèæªä¾çæè½è¯ç¹«èµ·ä¾ââä¾å¦ï¼è»è¼å¨æä¸éåº¦ä¸çæææ§å¦ä½é æ¸¬å¶å¨è¼ä½éåº¦ä¸çå®å¨æ§ãæåè¡¨ææ·±åº¦å­¸ç¿æ¨¡åå¾è¨ç·´è³æä¸­å­¸ç¿è¤éççµ±è¨æ¨¡å¼ï¼èæ²ææç¢ºçå ææ©å¶ï¼æé¤äºéæ¨£çä¿è­ãéç¨®éå¶ä½¿å¾å³çµ±çæ³è¦æ¹æ³ä¸è¶³ä»¥ç¢ºä¿ AI å®å¨æ§ãå±ææªä¾ï¼æåå¼ç±²ç£ç®¡æ©æ§æ­£è¦éä¸éå¶ï¼ä¸¦æåºä¸ååæ­¥çå©å±¤æ³è¦æ¡æ¶ï¼æ¿èªéäºç´æï¼å°é«é¢¨éªæç¨å¼·å¶äººå·¥ç£ç£ï¼åæçºä½é¢¨éªç¨éå¶å®é©ç¶çé¢¨éªæºéç­ç¥ãæåçç¼ç¾å¼·èª¿äºéæ°èæ® AI æ³è¦ä¸­åºæ¬åè¨­çè¿«åéè¦ï¼ä¸¦çºæ¿ç­å¶å®èåç ç©¶äººå¡æåºäºå·é«çé²å±éè·¯ã

##### **Comparative clinical evaluation of "memory-efficient" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest**
2501.15572v1 by Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, Lorenzo Preda

Introduction: Generative Adversarial Networks (GANs) are increasingly used to
generate synthetic medical images, addressing the critical shortage of
annotated data for training Artificial Intelligence (AI) systems. This study
introduces a novel memory-efficient GAN architecture, incorporating Conditional
Random Fields (CRFs) to generate high-resolution 3D medical images and
evaluates its performance against the state-of-the-art hierarchical (HA)-GAN
model.
  Materials and Methods: The CRF-GAN was trained using the open-source lung CT
LUNA16 dataset. The architecture was compared to HA-GAN through a quantitative
evaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy
(MMD) metrics, and a qualitative evaluation, through a two-alternative forced
choice (2AFC) test completed by a pool of 12 resident radiologists, in order to
assess the realism of the generated images.
  Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD
(0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test
showed a significant preference for images generated by CRF-Gan over those
generated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN
demonstrated 9.34% lower memory usage at 256 resolution and achieved up to
14.6% faster training speeds, offering substantial computational savings.
  Discussion: CRF-GAN model successfully generates high-resolution 3D medical
images with non-inferior quality to conventional models, while being more
memory-efficient and faster. Computational power and time saved can be used to
improve the spatial resolution and anatomical accuracy of generated images,
which is still a critical factor limiting their direct clinical applicability.

æè¦ï¼<paragraph>å¼è¨ï¼çæå°æç¶²è·¯ (GAN) æä¾æå¸¸è¢«ç¨æ¼çæåæé«å­¸å½±åï¼ä»¥è§£æ±ºäººå·¥æºæ§ (AI) ç³»çµ±è¨ç·´ä¸­æ¨è¨»è³æå´éç­ç¼ºçåé¡ãæ¬ç ç©¶æåºäºä¸ç¨®æ°ç©çè¨æ¶é«é«æ GAN æ¶æ§ï¼çµåæ¢ä»¶é¨æ©å ´ (CRF) ä¾çæé«è§£æåº¦ 3D é«å­¸å½±åï¼ä¸¦è©ä¼°å¶ç¸å°æ¼æåé²çåå±¤ (HA)-GAN æ¨¡åçæè½ã
ææåæ¹æ³ï¼CRF-GAN ä½¿ç¨éæºèºé¨é»è¦æ·å±¤ææ LUNA16 è³æéé²è¡è¨ç·´ãééä½¿ç¨ FrÃ©chet Inception Distance (FID) å Maximum Mean Discrepancy (MMD) ææ¨é²è¡éåè©ä¼°ï¼ä»¥åç± 12 ä½ä½é¢æ¾å°ç§é«å¸«çµæçåéå®æçå©æä¸å¼·å¶é¸æ (2AFC) æ¸¬è©¦é²è¡å®æ§è©ä¼°ï¼å°è©²æ¶æ§è HA-GAN é²è¡æ¯è¼ï¼ä»¥è©ä¼°çæå½±åççå¯¦æ§ã
çµæï¼CRF-GAN ä»¥è¼ä½ç FID (0.047 å° 0.061) å MMD (0.084 å° 0.086) åæ¸åªæ¼ HA-GANï¼è¡¨ç¤ºå½±åä¿çåº¦è¼ä½³ã2AFC æ¸¬è©¦é¡¯ç¤ºï¼åè©¦èé¡¯èåå¥½ç± CRF-GAN çæçå½±åï¼èéç± HA-GAN çæçå½±åï¼p å¼çº 1.93e-05ãæ­¤å¤ï¼CRF-GAN å¨è§£æåº¦çº 256 æçè¨æ¶é«ä½¿ç¨ééä½äº 9.34%ï¼ä¸¦å°è¨ç·´éåº¦æåäº 14.6%ï¼æä¾äºå¤§éçéç®ç¯çã
è¨è«ï¼CRF-GAN æ¨¡åæåå°çæäºé«è§£æåº¦ 3D é«å­¸å½±åï¼å¶åè³ªä¸éæ¼å³çµ±æ¨¡åï¼åææ´çè¨æ¶é«ä¸éåº¦æ´å¿«ãç¯ççéç®è½ååæéå¯ç¨æ¼æåçæå½±åçç©ºéè§£æåº¦åè§£åç²¾ç¢ºåº¦ï¼éä»ç¶æ¯éå¶å¶ç´æ¥è¨åºæç¨æ§çééµå ç´ ã</paragraph>

##### **AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications**
2501.15489v1 by Muhammad Aftab, Faisal Mehmood, Chengjuan Zhang, Alishba Nadeem, Zigang Dong, Yanan Jiang, Kangdongs Liu

Artificial intelligence (AI) has potential to revolutionize the field of
oncology by enhancing the precision of cancer diagnosis, optimizing treatment
strategies, and personalizing therapies for a variety of cancers. This review
examines the limitations of conventional diagnostic techniques and explores the
transformative role of AI in diagnosing and treating cancers such as lung,
breast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate,
and skin cancers. The primary objective of this paper is to highlight the
significant advancements that AI algorithms have brought to oncology within the
medical industry. By enabling early cancer detection, improving diagnostic
accuracy, and facilitating targeted treatment delivery, AI contributes to
substantial improvements in patient outcomes. The integration of AI in medical
imaging, genomic analysis, and pathology enhances diagnostic precision and
introduces a novel, less invasive approach to cancer screening. This not only
boosts the effectiveness of medical facilities but also reduces operational
costs. The study delves into the application of AI in radiomics for detailed
cancer characterization, predictive analytics for identifying associated risks,
and the development of algorithm-driven robots for immediate diagnosis.
Furthermore, it investigates the impact of AI on addressing healthcare
challenges, particularly in underserved and remote regions. The overarching
goal of this platform is to support the development of expert recommendations
and to provide universal, efficient diagnostic procedures. By reviewing
existing research and clinical studies, this paper underscores the pivotal role
of AI in improving the overall cancer care system. It emphasizes how AI-enabled
systems can enhance clinical decision-making and expand treatment options,
thereby underscoring the importance of AI in advancing precision oncology

æè¦ï¼äººå·¥æºè½ï¼AIï¼ææ½åééæåççè¨ºæ·çæºç¢ºæ§ãåªåæ²»çç­ç¥ï¼ä»¥åçºåç¨®ççæä¾åäººåçæ³ï¼ä¾å¾¹åºæ¹è®è«ç¤å­¸é åãæ¬ç¯è©è«æ¢è¨å³çµ±è¨ºæ·æè¡çéå¶ï¼ä¸¦æ¢è¨ AI å¨è¨ºæ·åæ²»çèºçãä¹³çãå¤§è¸ç´è¸çãèçãèçãé£éçãå­å®®é ¸çãç²çèºçãæè­·èºçåç®èçç­ççä¸­è½è®æ§çè§è²ãæ¬æçä¸»è¦ç®çæ¯å¼·èª¿ AI æ¼ç®æ³å¨é«çç¢æ¥­è«ç¤å­¸é åå¸¶ä¾çéå¤§é²å±ãééå¯¦ç¾æ©æççåµæ¸¬ãæåè¨ºæ·æºç¢ºæ§ï¼ä»¥åä¿é²æ¨é¶æ²»ççæä¾ï¼AI æå©æ¼å¤§å¹æ¹åçæ£çæ²»ççµæãAI æ´åå¨é«å­¸å½±åãåºå é«åæåççå­¸ä¸­ï¼æåäºè¨ºæ·çæºç¢ºæ§ï¼ä¸¦å¼é²äºä¸ç¨®åµæ°ãä¾µå¥æ§è¼ä½çæ¹æ³ä¾é²è¡ççç¯©æª¢ãéä¸åæåäºé«çæ©æ§çæçï¼ä¹éä½äºçéææ¬ãæ¬ç ç©¶æ¢è¨äº AI å¨æ¾å°ç·çµå­¸ä¸­çæç¨ï¼ä»¥é²è¡è©³ç´°çççç¹å¾µåæãé æ¸¬åæä»¥æ¾åºç¸éé¢¨éªï¼ä»¥åéç¼æ¼ç®æ³é©åçæ©å¨äººé²è¡ç«å³è¨ºæ·ãæ­¤å¤ï¼å®éæ¢è¨äº AI å¨è§£æ±ºé«çä¿å¥ææ°ä¸­çå½±é¿ï¼ç¹å¥æ¯å¨æåä¸è¶³ååé å°åãæ­¤å¹³å°çç¸½é«ç®æ¨æ¯æ¯æ´å°å®¶å»ºè­°çç¼å±ï¼ä¸¦æä¾æ®éãææçè¨ºæ·ç¨åºãééæª¢è¦ç¾æçç ç©¶åè¨åºè©¦é©ï¼æ¬æå¼·èª¿äº AI å¨æ¹åæ´é«ççç§è­·ç³»çµ±ä¸­çééµä½ç¨ãå®å¼·èª¿äº AI é©åçç³»çµ±å¦ä½è½æåè¨åºæ±ºç­å¶å®ï¼ä¸¦æ´å±æ²»çé¸é ï¼å¾èå¼·èª¿äº AI å¨æ¨é²ç²¾æºè«ç¤å­¸ä¸­çéè¦æ§ã

##### **Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models**
2501.15452v1 by Solha Kang, Joris Vankerschaver, Utku Ozbulak

With the advancements in self-supervised learning (SSL), transformer-based
computer vision models have recently demonstrated superior results compared to
convolutional neural networks (CNNs) and are poised to dominate the field of
artificial intelligence (AI)-based medical imaging in the upcoming years.
Nevertheless, similar to CNNs, unveiling the decision-making process of
transformer-based models remains a challenge. In this work, we take a step
towards demystifying the decision-making process of transformer-based medical
imaging models and propose Token Insight, a novel method that identifies the
critical tokens that contribute to the prediction made by the model. Our method
relies on the principled approach of token discarding native to
transformer-based models, requires no additional module, and can be applied to
any transformer model. Using the proposed approach, we quantify the importance
of each token based on its contribution to the prediction and enable a more
nuanced understanding of the model's decisions. Our experimental results which
are showcased on the problem of colonic polyp identification using both
supervised and self-supervised pretrained vision transformers indicate that
Token Insight contributes to a more transparent and interpretable
transformer-based medical imaging model, fostering trust and facilitating
broader adoption in clinical settings.

æè¦ï¼é¨èèªç£ç£å­¸ç¿ (SSL) çé²å±ï¼åºæ¼ Transformer çé»è¦è¦è¦ºæ¨¡åæè¿å·²å±ç¤ºåºåªæ¼å·ç©ç¥ç¶ç¶²è·¯ (CNN) çåè¶ææï¼ä¸¦æºåå¨æªä¾å¹¾å¹´ä¸»å°åºæ¼äººå·¥æºæ§ (AI) çé«å­¸å½±åé åãåç®¡å¦æ­¤ï¼è CNN é¡ä¼¼ï¼æ­ç¤ºåºæ¼ Transformer çæ¨¡åçæ±ºç­éç¨ä»ç¶æ¯ä¸é ææ°ãå¨éé å·¥ä½ä¸­ï¼æåæèæ­éåºæ¼ Transformer çé«å­¸å½±åæ¨¡åçæ±ºç­éç¨éåºäºä¸æ­¥ï¼ä¸¦æåºäº Token Insightï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼å¯ä»¥è­å¥å°æ¨¡åæåçé æ¸¬æè²¢ç»çééµ Tokenãæåçæ¨¡åä¾è³´æ¼åºæ¼ Transformer çæ¨¡åçåç Token æ¨æ£åçåæ¹æ³ï¼ä¸éè¦é¡å¤çæ¨¡çµï¼ä¸¦ä¸å¯ä»¥æç¨æ¼ä»»ä½ Transformer æ¨¡åãä½¿ç¨ææåºçæ¹æ³ï¼æåæ ¹ææ¯å Token å°é æ¸¬çè²¢ç»éåå¶éè¦æ§ï¼ä¸¦è½æ´ç´°ç·»å°äºè§£æ¨¡åçæ±ºç­ãæåçå¯¦é©çµæå±ç¤ºå¨ä½¿ç¨æç£ç£åèªç£ç£é è¨ç·´è¦è¦º Transformer ççµè¸æ¯èè­å¥åé¡ä¸ï¼è¡¨æ Token Insight æå©æ¼å»ºç«æ´éæä¸å¯è§£éçåºæ¼ Transformer çé«å­¸å½±åæ¨¡åï¼å¹é¤ä¿¡ä»»ä¸¦ä¿é²å¨è¨åºç°å¢ä¸­çæ´å»£æ³æ¡ç¨ã

##### **An AI-Driven Live Systematic Reviews in the Brain-Heart Interconnectome: Minimizing Research Waste and Advancing Evidence Synthesis**
2501.17181v1 by Arya Rahgozar, Pouria Mortezaagha, Jodi Edwards, Douglas Manuel, Jessie McGowen, Merrick Zwarenstein, Dean Fergusson, Andrea Tricco, Kelly Cobey, Margaret Sampson, Malcolm King, Dawn Richards, Alexandra Bodnaruc, David Moher

The Brain-Heart Interconnectome (BHI) combines neurology and cardiology but
is hindered by inefficiencies in evidence synthesis, poor adherence to quality
standards, and research waste. To address these challenges, we developed an
AI-driven system to enhance systematic reviews in the BHI domain. The system
integrates automated detection of Population, Intervention, Comparator,
Outcome, and Study design (PICOS), semantic search using vector embeddings,
graph-based querying, and topic modeling to identify redundancies and
underexplored areas. Core components include a Bi-LSTM model achieving 87%
accuracy for PICOS compliance, a study design classifier with 95.7% accuracy,
and Retrieval-Augmented Generation (RAG) with GPT-3.5, which outperformed GPT-4
for graph-based and topic-driven queries. The system provides real-time
updates, reducing research waste through a living database and offering an
interactive interface with dashboards and conversational AI. While initially
developed for BHI, the system's adaptable architecture enables its application
across various biomedical fields, supporting rigorous evidence synthesis,
efficient resource allocation, and informed clinical decision-making.

æè¦ï¼è¦å¿äº¤äºçµå­¸ (BHI) çµåç¥ç¶å­¸åå¿èå­¸ï¼ä½åå°è­æåææçä½ãå°åè³ªæ¨æºçéµå®åº¦ä¸ä½³åç ç©¶æµªè²»çé»ç¤ãçºäºæå°éäºææ°ï¼æåéç¼äºä¸å AI é©åç³»çµ±ï¼ä»¥å¢å¼· BHI é åä¸­çç³»çµ±æ§åé¡§ãæ­¤ç³»çµ±æ´åäºäººå£ãå¹²é ãæ¯è¼ãçµæåç ç©¶è¨­è¨ (PICOS) çèªåæª¢æ¸¬ï¼ä½¿ç¨åéåµå¥çèªç¾©æå°ãåºæ¼åå½¢æ¥è©¢åä¸»é¡å»ºæ¨¡ï¼ä»¥è­å¥åé¤åæªååæ¢ç´¢çé åãæ ¸å¿çµæåæ¬ä¸åéå LSTM æ¨¡åï¼PICOS åè¦æ§éå° 87% çæºç¢ºåº¦ï¼ç ç©¶è¨­è¨åé¡å¨æºç¢ºåº¦çº 95.7%ï¼ä»¥åä½¿ç¨ GPT-3.5 çæª¢ç´¢å¢å¼·çæ (RAG)ï¼å¶å¨åºæ¼åå½¢åä¸»é¡é©åçæ¥è©¢æ¹é¢åªæ¼ GPT-4ãè©²ç³»çµ±æä¾å³ææ´æ°ï¼ééåæè³æåº«æ¸å°ç ç©¶æµªè²»ï¼ä¸¦æä¾å·æåè¡¨æ¿åå°è©±å¼ AI çäºåä»é¢ãåç®¡æåæ¯çº BHI éç¼ï¼ä½è©²ç³»çµ±çå¯é©ææ¶æ§ä½¿å¶è½å¤ æç¨æ¼åç¨®çç©é«å­¸é åï¼æ¯æ´å´è¬¹çè­æåæãææçè³æºéç½®åææºçè¨åºæ±ºç­å¶å®ã

##### **Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations**
2501.15056v1 by Harshita Chopra, Chirag Shah

The ability to identify and acquire missing information is a critical
component of effective decision making and problem solving. With the rise of
conversational artificial intelligence (AI) systems, strategically formulating
information-seeking questions becomes crucial and demands efficient methods to
guide the search process. We introduce a novel approach to adaptive
question-asking through a combination of Large Language Models (LLM) for
generating questions that maximize information gain, Monte Carlo Tree Search
(MCTS) for constructing and leveraging a decision tree across multiple samples,
and a hierarchical feedback mechanism to learn from past interactions. We
present two key innovations: (1) an adaptive MCTS algorithm that balances
exploration and exploitation for efficient search over potential questions; and
(2) a clustering-based feedback algorithm that leverages prior experience to
guide future interactions. Each incoming sample is assigned to a cluster based
on its semantic similarity with previously observed samples. Our UCT (Upper
Confidence bound for Trees) formulation selects optimal questions by combining
expected rewards, a function of information gain, with a cluster-specific bonus
that decays with depth, to emphasize the importance of early-stage questions
that have proven effective for narrowing the solution space in similar samples.
Experiments across three domains, including medical diagnosis and
troubleshooting, demonstrate that our method leads to an average of 12%
improvement in success rates and a 10x reduction in the average number of LLM
calls made per conversation for the search process, in comparison to the state
of the art.

æè¦ï¼å·åè¾¨è­èåå¾éºæ¼è³è¨çè½åæ¯æææ±ºç­ååé¡è§£æ±ºçééµçµæé¨åãé¨èå°è©±å¼äººå·¥æºæ§ (AI) ç³»çµ±çå´èµ·ï¼ç­ç¥æ§å°æ¬å®å°æ±è³è¨çåé¡è®å¾è³ééè¦ï¼ä¸¦éè¦ææççæ¹æ³ä¾å¼å°æå°æµç¨ãæåééçµåå¤§åèªè¨æ¨¡å (LLM) ä¾ç¢çæå¤§åè³è¨ç²åçåé¡ãèå°å¡ç¾æ¨¹çæå° (MCTS) ä¾å»ºæ§ä¸¦å©ç¨å¤åæ¨£æ¬çæ±ºç­æ¨¹ï¼ä»¥ååå±¤åé¥æ©å¶ä¾å¾éå»çäºåä¸­å­¸ç¿ï¼æåºäºä¸ç¨®èªé©ææåçæ°æ¹æ³ãæåæåºäºå©é ééµåµæ°ï¼(1) ä¸ç¨®èªé©æ MCTS æ¼ç®æ³ï¼ç¨æ¼å¹³è¡¡æ¢ç´¢åå©ç¨ï¼ä»¥æææå°æ½å¨åé¡ï¼ä»¥å (2) ä¸ç¨®åºæ¼èé¡çåé¥æ¼ç®æ³ï¼ç¨æ¼å©ç¨ååçç¶é©ä¾å¼å°æªä¾çäºåãæ¯åè¼¸å¥æ¨£æ¬ææ ¹æå¶èååè§å¯å°çæ¨£æ¬çèªç¾©ç¸ä¼¼æ§åéå°ä¸åç¾¤éãæåç UCT (æ¨¹ççµæ§çä¸ç½®ä¿¡ç) å¬å¼ééçµåé æçåé¥ï¼è³è¨ç²åå½æ¸ï¼åé¨èæ·±åº¦è¡°æ¸çç¹å®ç¾¤éå æï¼ä¾é¸ææä½³åé¡ï¼ä»¥å¼·èª¿æ©æéæ®µåé¡å°æ¼ç¸®å°é¡ä¼¼æ¨£æ¬ä¸­çè§£ç©ºéçéè¦æ§ãå¨åæ¬é«çè¨ºæ·åæéæé¤å¨å§çä¸åé åä¸­çå¯¦é©è­æï¼èç¾ææè¡ç¸æ¯ï¼æåçæè¡å¯å°æåçå¹³åæå 12%ï¼ä¸¦å°æå°æµç¨ä¸­æ¯æ¬¡å°è©±æé²è¡ç LLM å¼å«å¹³åæ¬¡æ¸æ¸å° 10 åã

##### **Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach**
2501.14929v1 by Md. Kamrul Hasan, Guang Yang, Choon Hwai Yap

Cardiac anatomy segmentation is essential for clinical assessment of cardiac
function and disease diagnosis to inform treatment and intervention. In
performing segmentation, deep learning (DL) algorithms improved accuracy
significantly compared to traditional image processing approaches. More
recently, studies showed that enhancing DL segmentation with motion information
can further improve it. A range of methods for injecting motion information has
been proposed, but many of them increase the dimensionality of input images
(which is computationally expensive) or have not used an optimal method to
insert motion information, such as non-DL registration, non-attention-based
networks or single-headed attention. Here, we present a novel,
computation-efficient alternative where a novel, scalable temporal attention
module (TAM) extracts temporal feature interactions multiple times and where
TAM has a multi-headed, KQV projection cross-attention architecture. The module
can be seamlessly integrated into a wide range of existing CNN- or
Transformer-based networks, providing novel flexibility for inclusion in future
implementations. Extensive evaluations on different cardiac datasets, 2D
echocardiography (CAMUS), and 3D echocardiography (MITEA) demonstrate the
model's effectiveness when integrated into well-established backbone networks
like UNet, FCN8s, UNetR, SwinUNetR, and the recent I2UNet. We further find that
the optimized TAM-enhanced FCN8s network performs well compared to contemporary
alternatives. Our results confirm TAM's robustness, scalability, and
generalizability across diverse datasets and backbones.

æè¦ï¼<paragraph>å¿èè§£ååå²å°æ¼è©ä¼°å¿èåè½åç¾çè¨ºæ·ä»¥æä¾æ²»çåä»å¥è³è¨è³ééè¦ãå¨å·è¡åå²æï¼æ·±åº¦å­¸ç¿ (DL) æ¼ç®æ³èå³çµ±å½±åèçæ¹æ³ç¸æ¯ï¼æºç¢ºåº¦é¡¯èæåãæè¿çç ç©¶é¡¯ç¤ºï¼ééåä½è³è¨å¼·å DL åå²å¯ä»¥é²ä¸æ­¥æååå²ææãå·²ç¶æåºè¨±å¤ç¨æ¼æ³¨å¥åä½è³è¨çæ¹æ³ï¼ä½å¶ä¸­è¨±å¤æ¹æ³æå¢å è¼¸å¥å½±åçç¶­åº¦ï¼å¨éç®ä¸å¾èè²»ææ¬ï¼ï¼ææªä½¿ç¨æä½³æ¹æ³ä¾æå¥åä½è³è¨ï¼ä¾å¦é DL éæºãéåºæ¼æ³¨æåçç¶²è·¯æå®é ­æ³¨æåãå¨æ­¤ï¼æåæåºä¸åæ°ç©ä¸éç®æçé«çæ¿ä»£æ¹æ¡ï¼å¶ä¸­ä¸åæ°ç©ä¸å¯æ´åçæåºæ³¨æåæ¨¡çµ (TAM) å¤æ¬¡æåæåºç¹å¾µäºåï¼ä¸ TAM å·æå¤é ­ãKQV æå½±äº¤åæ³¨æåçæ¶æ§ãæ­¤æ¨¡çµå¯ä»¥ç¡ç¸«æ´åå°åç¨®ç¾æçåºæ¼ CNN æ Transformer çç¶²è·¯ä¸­ï¼çºæªä¾å¯¦ä½çç´å¥æä¾æ°ç©çéæ´»æ§ãå¨ä¸åçå¿èè³æéã2D è¶é³æ³¢å¿åå (CAMUS) å 3D è¶é³æ³¢å¿åå (MITEA) ä¸é²è¡çå»£æ³è©ä¼°è­æäºæ­¤æ¨¡åå¨æ´åå° UNetãFCN8sãUNetRãSwinUNetR åæè¿ç I2UNet ç­å®åçä¸»å¹¹ç¶²è·¯ä¸­çæè½ãæåé²ä¸æ­¥ç¼ç¾ï¼ç¶éæä½³åç TAM å¢å¼· FCN8s ç¶²è·¯èç¶ä»£æ¿ä»£æ¹æ¡ç¸æ¯è¡¨ç¾è¯å¥½ãæåççµæè­å¯¦äº TAM å¨ä¸åè³æéåä¸»å¹¹ä¸­çç©©å¥æ§ãå¯æ´åæ§åæ³åæ§ã</paragraph>

##### **Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs**
2501.14892v1 by Hang Luo, Jian Zhang, Chujun Li

In knowledge-intensive tasks, especially in high-stakes domains like medicine
and law, it is critical not only to retrieve relevant information but also to
provide causal reasoning and explainability. Large language models (LLMs) have
achieved remarkable performance in natural language understanding and
generation tasks. However, they often suffer from limitations such as
difficulty in incorporating new knowledge, generating hallucinations, and
explaining their reasoning process. To address these challenges, integrating
knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has
emerged as an effective solution. Traditional Graph RAG methods often rely on
simple graph traversal or semantic similarity, which do not capture causal
relationships or align well with the model's internal reasoning steps. This
paper proposes a novel pipeline that filters large knowledge graphs to
emphasize cause-effect edges, aligns the retrieval process with the model's
chain-of-thought (CoT), and enhances reasoning through multi-stage path
improvements. Experiments on medical question-answering tasks show consistent
gains, with up to a 10\% absolute improvement across multiple large language
models (LLMs). This approach demonstrates the value of combining causal
reasoning with stepwise retrieval, leading to more interpretable and logically
grounded solutions for complex queries.

æè¦ï¼å¨ç¥è­å¯éåä»»åä¸­ï¼ç¹å¥æ¯å¨é«å­¸åæ³å¾ç­é«é¢¨éªé åï¼ä¸åæª¢ç´¢ç¸éè³è¨è³ééè¦ï¼éå¿é æä¾å ææ¨çåå¯è§£éæ§ãå¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨çè§£åçæä»»åä¸­åå¾äºé¡¯èçè¡¨ç¾ãç¶èï¼å®åéå¸¸æéå°ä¸äºéå¶ï¼ä¾å¦é£ä»¥ç´å¥æ°ç¥è­ãç¢çå¹»è¦ºï¼ä»¥åè§£éå¶æ¨çéç¨ãçºäºæå°éäºææ°ï¼å°ç¥è­åèåå½¢æª¢ç´¢å¢å¼·çæ (Graph RAG) æ´åå¨ä¸èµ·å·²æçºä¸ç¨®ææçè§£æ±ºæ¹æ¡ãå³çµ±ç Graph RAG æ¹æ³éå¸¸ä¾è³´æ¼ç°¡å®çåå½¢éæ­·æèªç¾©ç¸ä¼¼æ§ï¼éç¡æ³ææå æéä¿æèæ¨¡åçå§é¨æ¨çæ­¥é©å¾å¥½å°å°é½ãæ¬ææåºäºä¸åæ°ç©çç®¡éï¼è©²ç®¡ééæ¿¾å¤§åç¥è­åä»¥å¼·èª¿å æéç·£ï¼å°æª¢ç´¢éç¨èæ¨¡åçææ³é (CoT) å°é½ï¼ä¸¦ééå¤éæ®µè·¯å¾æ¹é²ä¾å¢å¼·æ¨çãå¨é«çåé¡è§£ç­ä»»åä¸çå¯¦é©é¡¯ç¤ºåºä¸è´çæ¶çï¼å¨å¤åå¤§åèªè¨æ¨¡å (LLM) ä¸­çµå°æ¹é²å¹åº¦é«é 10%ãéç¨®æ¹æ³å±ç¤ºäºå°å ææ¨çèéæ­¥æª¢ç´¢ç¸çµåçå¹å¼ï¼å¾èçºè¤éæ¥è©¢æä¾æ´å·å¯è§£éæ§åéè¼¯ä¾æçè§£æ±ºæ¹æ¡ã

##### **Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**
2501.14719v1 by Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso

Equitable access to reliable health information is vital for public health,
but the quality of online health resources varies by language, raising concerns
about inconsistencies in Large Language Models (LLMs) for healthcare. In this
study, we examine the consistency of responses provided by LLMs to
health-related questions across English, German, Turkish, and Chinese. We
largely expand the HealthFC dataset by categorizing health-related questions by
disease type and broadening its multilingual scope with Turkish and Chinese
translations. We reveal significant inconsistencies in responses that could
spread healthcare misinformation. Our main contributions are 1) a multilingual
health-related inquiry dataset with meta-information on disease categories, and
2) a novel prompt-based evaluation workflow that enables sub-dimensional
comparisons between two languages through parsing. Our findings highlight key
challenges in deploying LLM-based tools in multilingual contexts and emphasize
the need for improved cross-lingual alignment to ensure accurate and equitable
healthcare information.

æè¦ï¼å¯é çå¥åº·è³è¨çå¬å¹³åå¾å°å¬å±è¡çè³ééè¦ï¼
ä½ç¶²è·¯å¥åº·è³æºçåè³ªå èªè¨èç°ï¼éå¼ç¼äºå°å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥æ¹é¢çä¸ä¸è´æ§çææãå¨éé ç ç©¶ä¸­ï¼æåæ¢è¨äº LLM å°è±èªãå¾·èªãåè³å¶èªåä¸­æçå¥åº·ç¸éåé¡ææä¾åæçä¸è´æ§ãæåééä¾ç¾çé¡ååé¡å¥åº·ç¸éåé¡ï¼ä¸¦ééåè³å¶èªåä¸­æç¿»è­¯æ´å±å¶å¤èªè¨ç¯åï¼å¤§å¹æ´å±äº HealthFC è³æéãæåæ­é²äºåæä¸­å­å¨é¡¯èçä¸ä¸è´æ§ï¼éå¯è½ææ£å¸é«çä¿å¥é¯èª¤è³è¨ãæåçè²¢ç»ä¸»è¦æ 1) ä¸ååå«ç¾çé¡å¥åè³è¨çå¤èªè¨å¥åº·ç¸éæ¥è©¢è³æéï¼ä»¥å 2) ä¸åæ°ç©çæç¤ºå¼è©ä¼°å·¥ä½æµç¨ï¼å®è½ééè§£æå¨å©ç¨®èªè¨ä¹éé²è¡æ¬¡ç¶­åº¦æ¯è¼ãæåçç ç©¶çµæçªé¡¯äºå¨å¤èªè¨ç°å¢ä¸­é¨ç½²åºæ¼ LLM çå·¥å·çä¸»è¦ææ°ï¼ä¸¦å¼·èª¿éè¦æ¹åè·¨èªè¨å°é½ä»¥ç¢ºä¿æºç¢ºä¸å¬å¹³çé«çä¿å¥è³è¨ã

##### **GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration**
2501.16382v1 by Ziwen Li, Xiang 'Anthony' Chen, Youngseung Jeon

Drug discovery (DD) has tremendously contributed to maintaining and improving
public health. Hypothesizing that inhibiting protein misfolding can slow
disease progression, researchers focus on target identification (Target ID) to
find protein structures for drug binding. While Large Language Models (LLMs)
and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug
discovery, integrating models into cohesive workflows remains challenging. We
conducted a user study with drug discovery researchers to identify the
applicability of LLMs and RAGs in Target ID. We identified two main findings:
1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on
an initial protein and protein candidates that have a therapeutic impact; 2)
the model must provide the PPI and relevant explanations for better
understanding. Based on these observations, we identified three limitations in
previous approaches for Target ID: 1) semantic ambiguity, 2) lack of
explainability, and 3) short retrieval units. To address these issues, we
propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve
agent pipeline RAG framework to support large-scale PPI signaling pathway
exploration in understanding therapeutic impacts by decomposing the analysis of
entire PPI pathways into sub-tasks focused on the analysis of PPI edges.

æè¦ï¼è¯ç©åç° (DD) æå¤§å°ä¿è¿äºå¬å±å«ççç»´æ¤åæ¹åãç ç©¶äººååè®¾æå¶èç½è´¨éè¯¯æå å¯ä»¥åç¼ç¾çè¿å±ï¼å æ­¤ä¸æ³¨äºé¶ç¹è¯å« (Target ID) ä»¥æ¾å°ç¨äºè¯ç©ç»åçèç½è´¨ç»æãè½ç¶å¤§åè¯­è¨æ¨¡å (LLM) åæ£ç´¢å¢å¼ºçæ (RAG) æ¡æ¶å éäºè¯ç©åç°ï¼ä½å°æ¨¡åæ´åå°åèå·¥ä½æµä¸­ä»ç¶å·ææææ§ãæä»¬ä¸è¯ç©åç°ç ç©¶äººåè¿è¡äºä¸é¡¹ç¨æ·ç ç©¶ï¼ä»¥ç¡®å® LLM å RAG å¨ Target ID ä¸­çéç¨æ§ãæä»¬ç¡®å®äºä¸¤ä¸ªä¸»è¦åç°ï¼1) LLM åºè¯¥åºäºåå§èç½è´¨åå·ææ²»çä½ç¨çèç½è´¨åéç©æä¾å¤ä¸ªèç½è´¨-èç½è´¨ç¸äºä½ç¨ (PPI)ï¼2) è¯¥æ¨¡åå¿é¡»æä¾ PPI åç¸å³è§£éä»¥æ´å¥½å°çè§£ãåºäºè¿äºè§å¯ï¼æä»¬åç°äºåå Target ID æ¹æ³ä¸­çä¸ä¸ªå±éæ§ï¼1) è¯­ä¹æ­§ä¹ï¼2) ç¼ºä¹å¯è§£éæ§ï¼3) æ£ç´¢ååç­ãä¸ºäºè§£å³è¿äºé®é¢ï¼æä»¬æåºäº GraPPIï¼è¿æ¯ä¸ç§åºäºå¤§è§æ¨¡ç¥è¯å¾ (KG) çæ£ç´¢-åè§£-æ±è§£ä»£çç®¡é RAG æ¡æ¶ï¼ä»¥æ¯æå¤§è§æ¨¡ PPI ä¿¡å·éè·¯æ¢ç´¢ï¼éè¿å°æ´ä¸ª PPI éè·¯çåæåè§£ä¸ºä¸æ³¨äº PPI è¾¹ç¼åæçå­ä»»å¡æ¥çè§£æ²»çå½±åã

##### **Rethinking Table Instruction Tuning**
2501.14693v1 by Naihao Deng, Rada Mihalcea

Recent advances in table understanding have focused on instruction-tuning
large language models (LLMs) for table-related tasks. However, existing
research has overlooked the impact of hyperparameter choices and lacks a
comprehensive evaluation of the out-of-domain table understanding ability and
the general capabilities of these table LLMs. In this paper, we evaluate these
abilities in existing table LLMs, and reveal significant declines in both
out-of-domain table understanding and general capabilities compared to their
base models. Through systematic analysis, we show that hyperparameters, such as
learning rate, can significantly influence both table-specific and general
capabilities. Contrary to the existing table instruction-tuning works, we
demonstrate that smaller learning rates and fewer training instances can
enhance table understanding while preserving general capabilities. Based on our
findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B
Instruct, which achieves performance on par with, or surpassing GPT-3.5 and
GPT-4 on table tasks, while maintaining strong out-of-domain generalization and
general capabilities. Our findings highlight the potential for reduced data
annotation costs and more efficient model development through careful
hyperparameter selection.

æè¦ï¼æè¿è¡¨çè§£çé²å±éä¸­å¨æä»¤èª¿æ ¡å¤§åèªè¨æ¨¡å (LLM) ä»¥å·è¡èè¡¨æ ¼ç¸éçä»»åãç¶èï¼ç¾æçç ç©¶å¿½ç¥äºè¶åæ¸é¸æçå½±é¿ï¼ä¸¦ä¸ç¼ºä¹å°é åå¤è¡¨æ ¼çè§£è½ååéäºè¡¨æ ¼ LLM çä¸è¬è½åçå¨é¢è©ä¼°ãå¨æ¬æä¸­ï¼æåè©ä¼°äºç¾æè¡¨æ ¼ LLM ä¸­çéäºè½åï¼ä¸¦æ­ç¤ºäºèå¶åºç¤æ¨¡åç¸æ¯ï¼é åå¤è¡¨æ ¼çè§£åä¸è¬è½åé½æé¡¯èä¸éãééç³»çµ±åæï¼æåè¡¨æè¶åæ¸ï¼ä¾å¦å­¸ç¿çï¼å¯ä»¥é¡¯èå½±é¿ç¹å®è¡¨æ ¼åä¸è¬è½åãèç¾æè¡¨æ ¼æä»¤èª¿æ ¡å·¥ä½ç¸åï¼æåè­æè¼å°çå­¸ç¿çåè¼å°çè¨ç·´å¯¦ä¾å¯ä»¥å¨ä¿çä¸è¬è½åçåæå¢å¼·è¡¨æ ¼çè§£ãæ ¹ææåçç¼ç¾ï¼æåå¼å¥äº TAMAï¼éæ¯ä¸åå¾ LLaMA 3.1 8B Instruct èª¿æ ¡çè¡¨æ ¼ LLMï¼å®å¨è¡¨æ ¼ä»»åä¸å¯¦ç¾äºè GPT-3.5 å GPT-4 ç¸ç¶æè¶è¶çæè½ï¼åæä¿æå¼·å¤§çé åå¤æ¦ååä¸è¬è½åãæåçç¼ç¾å¼·èª¿äºééä»ç´°é¸æè¶åæ¸ï¼éä½è³ææ¨è¨»ææ¬åæ´ææççæ¨¡åéç¼çå¯è½æ§ã

##### **Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**
2501.14689v1 by Dmitry Ryabtsev, Boris Vasilyev, Sergey Shershakov

This paper introduces an innovative software system for fundus image analysis
that deliberately diverges from the conventional screening approach, opting not
to predict specific diagnoses. Instead, our methodology mimics the diagnostic
process by thoroughly analyzing both normal and pathological features of fundus
structures, leaving the ultimate decision-making authority in the hands of
healthcare professionals. Our initiative addresses the need for objective
clinical analysis and seeks to automate and enhance the clinical workflow of
fundus image examination. The system, from its overarching architecture to the
modular analysis design powered by artificial intelligence (AI) models, aligns
seamlessly with ophthalmological practices. Our unique approach utilizes a
combination of state-of-the-art deep learning methods and traditional computer
vision algorithms to provide a comprehensive and nuanced analysis of fundus
structures. We present a distinctive methodology for designing medical
applications, using our system as an illustrative example. Comprehensive
verification and validation results demonstrate the efficacy of our approach in
revolutionizing fundus image analysis, with potential applications across
various medical domains.

æè¦ï¼æ¬è«æä»ç´¹äºä¸ç¨®åµæ°çè»é«ç³»çµ±ï¼ç¨æ¼ç¼åºå½±ååæï¼å®å»æåé¢å³çµ±çç¯©æª¢æ¹æ³ï¼é¸æä¸é æ¸¬å·é«çè¨ºæ·ãç¸åå°ï¼æåçåææ¹æ³æ¨¡æ¬è¨ºæ·éç¨ï¼å¾¹åºåæç¼åºçµæ§çæ­£å¸¸åççç¹å¾µï¼å°æçµçæ±ºç­æ¬äº¤å°é«çä¿å¥å°æ¥­äººå¡æä¸­ãæåçè¨ç«æ¨å¨æ»¿è¶³å®¢è§è¨åºåæçéæ±ï¼ä¸¦å°æ±èªåååå¼·åç¼åºå½±åæª¢æ¥çè¨åºå·¥ä½æµç¨ãè©²ç³»çµ±å¾å¶æ´é«æ¶æ§å°ç±äººå·¥æºæ§ (AI) æ¨¡åé©åçæ¨¡çµååæè¨­è¨ï¼é½èç¼ç§å¯¦åç¡ç¸«å°é½ãæåç¨ç¹çæ¹æ³çµåäºæåé²çæ·±åº¦å­¸ç¿æ¹æ³åå³çµ±çé»è¦è¦è¦ºæ¼ç®æ³ï¼æä¾ç¼åºçµæ§çå¨é¢ä¸ç´°ç·»çåæãæåæåºäºä¸ç¨®ç¨ç¹çè¨­è¨é«çæç¨æ¹æ³ï¼ä¸¦ä»¥æåçç³»çµ±ä½çºèªªæç¯ä¾ãå¨é¢çé©è­åé©è­çµæè­æäºæåçæ¹æ³å¨é©æ°ç¼åºå½±ååææ¹é¢çæåï¼ä¸¦å·æå¨åç¨®é«çé åçæ½å¨æç¨ã

##### **Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**
2501.14685v1 by Fuping Wu, Bartlomiej W. Papiez

Foundation models are widely employed in medical image analysis, due to their
high adaptability and generalizability for downstream tasks. With the
increasing number of foundation models being released, model selection has
become an important issue. In this work, we study the capabilities of
foundation models in medical image classification tasks by conducting a
benchmark study on the MedMNIST dataset. Specifically, we adopt various
foundation models ranging from convolutional to Transformer-based models and
implement both end-to-end training and linear probing for all classification
tasks. The results demonstrate the significant potential of these pre-trained
models when transferred for medical image classification. We further conduct
experiments with different image sizes and various sizes of training data. By
analyzing all the results, we provide preliminary, yet useful insights and
conclusions on this topic.

æè¦ï¼åºç¤æ¨¡åå»£æ³ç¨æ¼é«å­¸å½±ååæï¼å çºå®åå°ä¸æ¸¸ä»»åå·æé«åº¦çé©ææ§åæ¦æ¬æ§ãé¨èç¼å¸çåºç¤æ¨¡åæ¸éè¶ä¾è¶å¤ï¼æ¨¡åé¸æå·²æçºä¸åéè¦åé¡ãå¨éé å·¥ä½ä¸­ï¼æåééå° MedMNIST è³æéé²è¡åºæºç ç©¶ä¾ç ç©¶åºç¤æ¨¡åå¨é«å­¸å½±ååé¡ä»»åä¸­çè½åãå·é«ä¾èªªï¼æåæ¡ç¨äºå¾å·ç©å°åºæ¼ Transformer çæ¨¡åç­åç¨®åºç¤æ¨¡åï¼ä¸¦å°ææåé¡ä»»åå¯¦æ½ç«¯å°ç«¯è¨ç·´åç·æ§æ¢æ¸¬ãçµæè­æäºéäºé è¨ç·´æ¨¡åå¨è½ç§»å°é«å­¸å½±ååé¡æå·æé¡¯èçæ½åãæåé²ä¸æ­¥é²è¡äºä¸åå½±åå¤§å°ååç¨®è¨ç·´è³æå¤§å°çå¯¦é©ãééåæææçµæï¼æåå°æ­¤ä¸»é¡æä¾äºåæ­¥ä½æç¨çè¦è§£åçµè«ã

##### **MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**
2501.14654v1 by Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, Andrew Y. Ng, Jonathan H. Chen

Recent large language models (LLMs) have demonstrated significant
advancements, particularly in their ability to serve as agents thereby
surpassing their traditional role as chatbots. These agents can leverage their
planning and tool utilization capabilities to address tasks specified at a high
level. However, a standardized dataset to benchmark the agent capabilities of
LLMs in medical applications is currently lacking, making the evaluation of
LLMs on complex tasks in interactive healthcare environments challenging. To
address this gap, we introduce MedAgentBench, a broad evaluation suite designed
to assess the agent capabilities of large language models within medical
records contexts. MedAgentBench encompasses 100 patient-specific
clinically-derived tasks from 10 categories written by human physicians,
realistic profiles of 100 patients with over 700,000 data elements, a
FHIR-compliant interactive environment, and an accompanying codebase. The
environment uses the standard APIs and communication infrastructure used in
modern EMR systems, so it can be easily migrated into live EMR systems.
MedAgentBench presents an unsaturated agent-oriented benchmark that current
state-of-the-art LLMs exhibit some ability to succeed at. The best model
(GPT-4o) achieves a success rate of 72%. However, there is still substantial
space for improvement to give the community a next direction to optimize.
Furthermore, there is significant variation in performance across task
categories. MedAgentBench establishes this and is publicly available at
https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable
framework for model developers to track progress and drive continuous
improvements in the agent capabilities of large language models within the
medical domain.

æè¦ï¼<paragraph>æè¿çå¤§åè¯­è¨æ¨¡å (LLM) å·²å±ç¤ºåºæ¾èçè¿æ­¥ï¼ç¹å«æ¯å¨å¶ä½ä¸ºä»£ççè½åæ¹é¢ï¼ä»èè¶è¶äºå¶ä½ä¸ºèå¤©æºå¨äººçä¼ ç»è§è²ãè¿äºä»£çå¯ä»¥å©ç¨å¶è§ååå·¥å·å©ç¨è½åæ¥è§£å³å¨é«å±æå®çä»»å¡ãç¶èï¼ç®åç¼ºä¹ç¨äºå¯¹å»çåºç¨ä¸­ LLM çä»£çè½åè¿è¡åºåæµè¯çæ ååæ°æ®éï¼è¿ä½¿å¾å¨äº¤äºå¼å»çä¿å¥ç¯å¢ä¸­å¯¹ LLM å¨å¤æä»»å¡ä¸çè¯ä¼°å·ææææ§ãä¸ºäºè§£å³è¿ä¸å·®è·ï¼æä»¬å¼å¥äº MedAgentBenchï¼è¿æ¯ä¸ä¸ªå¹¿æ³çè¯ä¼°å¥ä»¶ï¼æ¨å¨è¯ä¼°å¤§åè¯­è¨æ¨¡åå¨å»çè®°å½èæ¯ä¸çä»£çè½åãMedAgentBench åå« 100 ä¸ªç±äººç±»å»çç¼åçæ¥èª 10 ä¸ªç±»å«çç¹å®äºæ£èçä¸´åºä»»å¡ã100 ä¸ªæ£èççå®ä¸ªäººèµæï¼åå«è¶è¿ 700,000 ä¸ªæ°æ®åç´ ï¼ãä¸ä¸ªç¬¦å FHIR çäº¤äºå¼ç¯å¢ä»¥åä¸ä¸ªéå¥çä»£ç åºãè¯¥ç¯å¢ä½¿ç¨ç°ä»£ EMR ç³»ç»ä¸­ä½¿ç¨çæ å API åéä¿¡åºç¡è®¾æ½ï¼å æ­¤å¯ä»¥è½»æ¾å°è¿ç§»å°å®æ¶ EMR ç³»ç»ä¸­ãMedAgentBench åç°äºä¸ä¸ªæªé¥±åçä»¥ä»£çä¸ºå¯¼åçåºåï¼å½åæåè¿ç LLM è¡¨ç°åºä¸å®ç¨åº¦çæåè½åãæå¥½çæ¨¡å (GPT-4o) çæåçè¾¾å° 72%ãç¶èï¼ä»ç¶æå¾å¤§çæ¹è¿ç©ºé´ï¼å¯ä»¥ä¸ºç¤¾åºæä¾ä¼åæ¹åãæ­¤å¤ï¼ä¸åä»»å¡ç±»å«ä¹é´çæ§è½å·®å¼å¾å¤§ãMedAgentBench å»ºç«äºè¿ä¸ç¹ï¼å¹¶å¨ https://github.com/stanfordmlgroup/MedAgentBench å¬å¼æä¾ï¼ä¸ºæ¨¡åå¼åèæä¾äºä¸ä¸ªæä»·å¼çæ¡æ¶ï¼ç¨äºè·è¸ªè¿åº¦å¹¶æ¨å¨å¤§åè¯­è¨æ¨¡åå¨å»çé¢åçä»£çè½åçæç»­æ¹è¿ã</paragraph>

##### **Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis**
2501.18614v1 by Xu Chen, Yuan Huang, Benn Jessney, Jason Sangha, Sophie Gu, Carola-Bibiane SchÃ¶nlieb, Martin Bennett, Michael Roberts

Artificial intelligence (AI) methodologies hold great promise for the rapid
and accurate diagnosis of coronary artery disease (CAD) from intravascular
optical coherent tomography (IVOCT) images. Numerous papers have been published
describing AI-based models for different diagnostic tasks, yet it remains
unclear which models have potential clinical utility and have been properly
validated. This systematic review considered published literature between
January 2015 and February 2023 describing AI-based diagnosis of CAD using
IVOCT. Our search identified 5,576 studies, with 513 included after initial
screening and 35 studies included in the final systematic review after quality
screening. Our findings indicate that most of the identified models are not
currently suitable for clinical use, primarily due to methodological flaws and
underlying biases. To address these issues, we provide recommendations to
improve model quality and research practices to enhance the development of
clinically useful AI products.

æè¦ï¼äººå·¥æºæ§ (AI) æ¹æ³è«å°æ¼å¾è¡ç®¡å§åå­¸ç¸å¹²æ·å±¤ææ (IVOCT) å½±åå¿«éä¸æºç¢ºè¨ºæ·å çåèç¾ç (CAD) èè¨ï¼å·æå¾å¤§çåæ¯ãè¨±å¤è«æå·²ç¼è¡¨ï¼èªªæäºç¨æ¼ä¸åè¨ºæ·ä»»åç AI åºç¤æ¨¡åï¼ä½ä»ä¸æ¸æ¥åªäºæ¨¡åå·ææ½å¨çè¨åºç¨éï¼ä¸å·²ç²å¾é©ç¶é©è­ãéé ç³»çµ±æ§åé¡§èéäº 2015 å¹´ 1 æè³ 2023 å¹´ 2 æéç¼è¡¨çæç»ï¼èªªæäºä½¿ç¨ IVOCT ç CAD AI åºç¤è¨ºæ·ãæåçæå°è¾¨è­åº 5,576 é ç ç©¶ï¼å¨åæ­¥ç¯©é¸å¾ç´å¥ 513 é ï¼å¨åè³ªç¯©é¸å¾ç´å¥ 35 é ç ç©¶é²è¡æçµçç³»çµ±æ§åé¡§ãæåçç¼ç¾é¡¯ç¤ºï¼å¤§å¤æ¸å·²è¾¨è­åºçæ¨¡åç®åä¸é©åç¨æ¼è¨åºï¼ä¸»è¦æ¯å çºæ¹æ³è«ä¸çç¼ºé·åæ½å¨åå·®ãçºäºè§£æ±ºéäºåé¡ï¼æåæä¾å»ºè­°ä¾æ¹åæ¨¡ååè³ªåç ç©¶å¯¦åï¼ä»¥å¢å¼·è¨åºæç¨ AI ç¢åçéç¼ã

##### **Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**
2501.14483v1 by Walid Yassine, Martin Charachon, CÃ©line Hudelot, Roberto Ardon

Assessing cancer progression in liver CT scans is a clinical challenge,
requiring a comparison of scans at different times for the same patient.
Practitioners must identify existing tumors, compare them with prior exams,
identify new tumors, and evaluate overall disease evolution. This process is
particularly complex in liver examinations due to misalignment between exams
caused by several factors. Indeed, longitudinal liver examinations can undergo
different non-pathological and pathological changes due to non-rigid
deformations, the appearance or disappearance of pathologies, and other
variations. In such cases, existing registration approaches, mainly based on
intrinsic features may distort tumor regions, biasing the tumor progress
evaluation step and the corresponding diagnosis. This work proposes a
registration method based only on geometrical and anatomical information from
liver segmentation, aimed at aligning longitudinal liver images for aided
diagnosis. The proposed method is trained and tested on longitudinal liver CT
scans, with 317 patients for training and 53 for testing. Our experimental
results support our claims by showing that our method is better than other
registration techniques by providing a smoother deformation while preserving
the tumor burden (total volume of tissues considered as tumor) within the
volume. Qualitative results emphasize the importance of smooth deformations in
preserving tumor appearance.

æè¦ï¼è©ä¼°èèé»è¦æ·å±¤ææä¸­çççé²ç¨æ¯ä¸é è¨åºä¸çææ°ï¼
éè¦æ¯è¼åä¸çæ£å¨ä¸åæéé»çææçµæã
å¾æ¥­äººå¡å¿é è¾¨è­ç¾æçè«ç¤ï¼å°å¶èååçæª¢æ¥çµæé²è¡æ¯è¼ï¼
è¾¨è­æ°çè«ç¤ï¼ä¸¦è©ä¼°æ´é«ç¾ççæ¼è®ãç±æ¼ç¨®ç¨®å ç´ é ææª¢æ¥çµæä¹éçé¯ä½ï¼éåéç¨å¨èèæª¢æ¥ä¸­ç¹å¥è¤éãäºå¯¦ä¸ï¼ç¸±åçèèæª¢æ¥å¯è½æå çºéåæ§è®å½¢ãçççåºç¾ææ¶å¤±ï¼ä»¥åå¶ä»è®åèç¢çä¸åçéççæ§åççæ§çè®åãå¨éç¨®ææ³ä¸ï¼ç¾æçéæºæ¹æ³ï¼ä¸»è¦åºæ¼å§å¨ç¹å¾µï¼å¯è½ææ­æ²è«ç¤ååï¼é æè«ç¤é²ç¨è©ä¼°æ­¥é©åç¸æè¨ºæ·çåå·®ãæ¬ç ç©¶æåºäºä¸ç¨®ååºæ¼èèåå²çå¹¾ä½åè§£åè³è¨çéæºæ¹æ³ï¼æ¨å¨å°ç¸±åèèå½±åé²è¡éæºï¼ä»¥åå©è¨ºæ·ãææåºçæ¹æ³å¨ç¸±åèèé»è¦æ·å±¤ææä¸é²è¡è¨ç·´åæ¸¬è©¦ï¼è¨ç·´è³ææ 317 ä½çæ£ï¼æ¸¬è©¦è³ææ 53 ä½ãæåçå¯¦é©çµææ¯ææåçèªªæ³ï¼è­ææåçéæºæ¹æ³æ¯å¶ä»éæºæè¡æ´å¥½ï¼å çºå®å¨ä¿çè«ç¤è² æï¼è¢«è¦çºè«ç¤ççµç¹ç¸½é«ç©ï¼çåæï¼æä¾äºæ´å¹³æ»çè®å½¢ãå®æ§çµæå¼·èª¿äºå¹³æ»è®å½¢å¨ä¿çè«ç¤å¤è§æ¹é¢çéè¦æ§ã

##### **Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**
2501.14469v1 by Taehan Kim, Wonduk Seo

Global climate change has reduced crop resilience and pesticide efficacy,
making reliance on synthetic pesticides inevitable, even though their
widespread use poses significant health and environmental risks. While these
pesticides remain a key tool in pest management, previous machine-learning
applications in pesticide and agriculture have focused on classification or
regression, leaving the fundamental challenge of generating new molecular
structures or designing novel candidates unaddressed. In this paper, we propose
Pesti-Gen, a novel generative model based on variational auto-encoders,
designed to create pesticide candidates with optimized properties for the first
time. Specifically, Pesti-Gen leverages a two-stage learning process: an
initial pre-training phase that captures a generalized chemical structure
representation, followed by a fine-tuning stage that incorporates
toxicity-specific information. The model simultaneously optimizes over multiple
toxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to
generate environmentally friendly pesticide candidates. Notably, Pesti-Gen
achieves approximately 68\% structural validity in generating new molecular
structures, demonstrating the model's effectiveness in producing optimized and
feasible pesticide candidates, thereby providing a new way for safer and more
sustainable pest management solutions.

æè¦ï¼å¨çæ°£åè®é·éä½äºä½ç©çå¾©ååèæ®ºè²åçæåï¼
ä½¿å¾ä»°è³´åææ®ºè²åæçºç¡å¯é¿åçè¶¨å¢ï¼åç®¡å®åçå»£æ³ä½¿ç¨æå¸¶ä¾éå¤§çå¥åº·åç°å¢é¢¨éªãåç®¡éäºæ®ºè²åä»ç¶æ¯è²å®³ç®¡çä¸­çééµå·¥å·ï¼éå»å¨æ®ºè²ååè¾²æ¥­æ¹é¢çæ©å¨å­¸ç¿æç¨é½èéæ¼åé¡æè¿´æ­¸ï¼èæªè§£æ±ºç¢çæ°çåå­çµæ§æè¨­è¨æ°åé¸è¥åçåºæ¬ææ°ãå¨æ¬æä¸­ï¼æåæåº Pesti-Genï¼ä¸ç¨®åºæ¼è®ç°èªåç·¨ç¢¼å¨çåµæ°çææ¨¡åï¼æ¨å¨é¦æ¬¡å»ºç«å·ææä½³åç¹æ§çæ®ºè²ååé¸è¥åãå·é«ä¾èªªï¼Pesti-Gen æ¡ç¨å©éæ®µå­¸ç¿æµç¨ï¼ä¸åæ·åå»£ç¾©åå­¸çµæ§è¡¨ç¤ºçåå§é è¨ç·´éæ®µï¼æ¥èæ¯ä¸åç´å¥æ¯æ§ç¹å®è³è¨çå¾®èª¿éæ®µãæ­¤æ¨¡ååæéå°å¤ç¨®æ¯æ§ææ¨é²è¡æä½³åï¼ä¾å¦ (1) ç²çæ¯æ§å (2) æ°´çæ¯æ§ï¼ä»¥ç¢çå°ç°å¢ååçæ®ºè²ååé¸è¥åãå¼å¾æ³¨æçæ¯ï¼Pesti-Gen å¨ç¢çæ°çåå­çµæ§æ¹é¢éå°äºç´ 68% ççµæ§æåº¦ï¼è­æäºæ­¤æ¨¡åå¨ç¢çæä½³åä¸å¯è¡çæ®ºè²ååé¸è¥åæ¹é¢çæè½ï¼é²èçºæ´å®å¨ä¸æ´æ°¸çºçè²å®³ç®¡çè§£æ±ºæ¹æ¡æä¾äºä¸ç¨®æ°æ¹æ³ã

##### **ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**
2501.14379v1 by Yoni Schirris, Rosie Voorthuis, Mark Opdam, Marte Liefaard, Gabe S Sonke, Gwen Dackus, Vincent de Jong, Yuwei Wang, Annelot Van Rossum, Tessa G Steenbruggen, Lars C Steggink, Liesbeth G. E. de Vries, Marc van de Vijver, Roberto Salgado, Efstratios Gavves, Paul J van Diest, Sabine C Linn, Jonas Teuwen, Renee Menezes, Marleen Kok, Hugo Horlings

The level of tumour-infiltrating lymphocytes (TILs) is a prognostic factor
for patients with (triple-negative) breast cancer (BC). Computational TIL
assessment (CTA) has the potential to assist pathologists in this
labour-intensive task, but current CTA models rely heavily on many detailed
annotations. We propose and validate a fundamentally simpler deep learning
based CTA that can be trained in only ten minutes on hundredfold fewer
pathologist annotations. We collected whole slide images (WSIs) with TILs
scores and clinical data of 2,340 patients with BC from six cohorts including
three randomised clinical trials. Morphological features were extracted from
whole slide images (WSIs) using a pathology foundation model. Our
label-efficient Computational stromal TIL assessment model (ECTIL) directly
regresses the TILs score from these features. ECTIL trained on only a few
hundred samples (ECTIL-TCGA) showed concordance with the pathologist over five
heterogeneous external cohorts (r=0.54-0.74, AUROC=0.80-0.94). Training on all
slides of five cohorts (ECTIL-combined) improved results on a held-out test set
(r=0.69, AUROC=0.85). Multivariable Cox regression analyses indicated that
every 10% increase of ECTIL scores was associated with improved overall
survival independent of clinicopathological variables (HR 0.86, p<0.01),
similar to the pathologist score (HR 0.87, p<0.001). We demonstrate that ECTIL
is highly concordant with an expert pathologist and obtains a similar hazard
ratio. ECTIL has a fundamentally simpler design than existing methods and can
be trained on orders of magnitude fewer annotations. Such a CTA may be used to
pre-screen patients for, e.g., immunotherapy clinical trial inclusion, or as a
tool to assist clinicians in the diagnostic work-up of patients with BC. Our
model is available under an open source licence
(https://github.com/nki-ai/ectil).

æè¦ï¼è¿ç¤æµ¸æ¶¦æ·å·´ç»è (TIL) çæ°´å¹³æ¯ (ä¸é´æ§) ä¹³èºç (BC) æ£èçé¢åå ç´ ãè®¡ç® TIL è¯ä¼° (CTA) æå¯è½åå©ççå­¦å®¶å®æè¿é¡¹å³å¨å¯éåä»»å¡ï¼ä½ç®åç CTA æ¨¡åä¸¥éä¾èµäºè®¸å¤è¯¦ç»çæ³¨éãæä»¬æåºå¹¶éªè¯äºä¸ä¸ªåºäºæ·±åº¦å­¦ä¹ ç CTAï¼å®å¯ä»¥å¨å ç¾åæ´å°çççå­¦å®¶æ³¨éä¸ä»å¨ååéåè¿è¡è®­ç»ãæä»¬ä»å­ä¸ªéåä¸­æ¶éäº 2,340 å BC æ£èç TILs è¯ååä¸´åºæ°æ®çå¨ç»çå¾å (WSI)ï¼å¶ä¸­åæ¬ä¸é¡¹éæºä¸´åºè¯éªãä½¿ç¨ççåºç¡æ¨¡åä»å¨ç»çå¾å (WSI) ä¸­æåå½¢æå­¦ç¹å¾ãæä»¬çæ ç­¾é«æè®¡ç®åºè´¨ TIL è¯ä¼°æ¨¡å (ECTIL) ç´æ¥ä»è¿äºç¹å¾ä¸­åå½ TILs è¯åãä»å¨å ç¾ä¸ªæ ·æ¬ä¸è¿è¡è®­ç»ç ECTILï¼ECTIL-TCGAï¼æ¾ç¤ºåºä¸ççå­¦å®¶å¨äºä¸ªå¼è´¨å¤é¨éåä¸­çä¸è´æ§ï¼r=0.54-0.74ï¼AUROC=0.80-0.94ï¼ãå¨äºä¸ªéåçææç»çä¸è¿è¡è®­ç»ï¼ECTIL-combinedï¼æ¹åäºä¿çæµè¯éä¸çç»æï¼r=0.69ï¼AUROC=0.85ï¼ãå¤åé Cox åå½åæè¡¨æï¼ECTIL è¯åæ¯å¢å  10%ï¼ä¸ä¸´åºççå­¦åéæ å³çæ»ä½çå­çå°±ä¼æé«ï¼HR 0.86ï¼p<0.01ï¼ï¼ç±»ä¼¼äºççå­¦å®¶è¯åï¼HR 0.87ï¼p<0.001ï¼ãæä»¬è¯æ ECTIL ä¸ä¸å®¶ççå­¦å®¶é«åº¦ä¸è´ï¼å¹¶è·å¾äºç±»ä¼¼çé£é©æ¯ãECTIL çè®¾è®¡æ¯ç°ææ¹æ³ä»æ ¹æ¬ä¸æ´ç®åï¼å¹¶ä¸å¯ä»¥å¨æ°éçº§æ´å°çæ³¨éä¸è¿è¡è®­ç»ãè¿ç§ CTA å¯ç¨äºå¯¹æ£èè¿è¡é¢ç­éï¼ä¾å¦åç«æ²»çä¸´åºè¯éªçº³å¥ï¼æä½ä¸ºä¸ç§å·¥å·æ¥å¸®å©ä¸´åºå»çå¯¹ BC æ£èè¿è¡è¯æ­æ£æ¥ãæä»¬çæ¨¡åå¯å¨å¼æ¾æºä»£ç è®¸å¯ä¸è·å¾ (https://github.com/nki-ai/ectil)ã

##### **Optimal Signal Decomposition-based Multi-Stage Learning for Battery Health Estimation**
2501.16377v1 by Vijay Babu Pamshetti, Wei Zhang, King Jet Tseng, Bor Kiat Ng, Qingyu Yan

Battery health estimation is fundamental to ensure battery safety and reduce
cost. However, achieving accurate estimation has been challenging due to the
batteries' complex nonlinear aging patterns and capacity regeneration
phenomena. In this paper, we propose OSL, an optimal signal decomposition-based
multi-stage machine learning for battery health estimation. OSL treats battery
signals optimally. It uses optimized variational mode decomposition to extract
decomposed signals capturing different frequency bands of the original battery
signals. It also incorporates a multi-stage learning process to analyze both
spatial and temporal battery features effectively. An experimental study is
conducted with a public battery aging dataset. OSL demonstrates exceptional
performance with a mean error of just 0.26%. It significantly outperforms
comparison algorithms, both those without and those with suboptimal signal
decomposition and analysis. OSL considers practical battery challenges and can
be integrated into real-world battery management systems, offering a good
impact on battery monitoring and optimization.

æè¦ï¼é»æ± å¥åº·è©ä¼°å°æ¼ç¢ºä¿é»æ± å®å¨åéä½ææ¬è³ééè¦ãç¶èï¼ç±æ¼é»æ± è¤éçéç·æ§èåæ¨¡å¼åå®¹éåçç¾è±¡ï¼è¦å¯¦ç¾æºç¢ºçè©ä¼°ä¸ç´æ¯ä¸é ææ°ãå¨æ¬æä¸­ï¼æåæåº OSLï¼ä¸ç¨®åºæ¼æä½³ä¿¡èåè§£çå¤éæ®µæ©å¨å­¸ç¿ï¼ç¨æ¼é»æ± å¥åº·è©ä¼°ãOSL æä½³åèçé»æ± ä¿¡èãå®ä½¿ç¨æä½³åçè®åæ¨¡å¼åè§£ä¾æååè§£ä¿¡èï¼ææåå§é»æ± ä¿¡èçä¸åé »çå¸¶ãå®éçµåäºä¸åå¤éæ®µå­¸ç¿éç¨ï¼ä»¥ææåæé»æ± çç©ºéåæéç¹å¾µãä½¿ç¨å¬éé»æ± èåæ¸æéé²è¡äºä¸é å¯¦é©ç ç©¶ãOSL è¡¨ç¾åºåè¶çæ§è½ï¼å¹³åèª¤å·®åçº 0.26%ãå®æé¡¯åªæ¼æ¯è¼æ¼ç®æ³ï¼åæ¬æ²ææ¬¡åªä¿¡èåè§£ååæçæ¼ç®æ³åå·ææ¬¡åªä¿¡èåè§£ååæçæ¼ç®æ³ãOSL èæ®äºå¯¦éé»æ± ææ°ï¼å¯ä»¥æ´åå°å¯¦éé»æ± ç®¡çç³»çµ±ä¸­ï¼å°é»æ± ç£æ§åæä½³åç¢çè¯å¥½çå½±é¿ã

##### **Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation**
2501.14166v1 by Cong-Duy Nguyen, Xiaobao Wu, Thong Nguyen, Shuai Zhao, Khoi Le, Viet-Anh Nguyen, Feng Yichao, Anh Tuan Luu

Previous research on multimodal entity linking (MEL) has primarily employed
contrastive learning as the primary objective. However, using the rest of the
batch as negative samples without careful consideration, these studies risk
leveraging easy features and potentially overlook essential details that make
entities unique. In this work, we propose JD-CCL (Jaccard Distance-based
Conditional Contrastive Learning), a novel approach designed to enhance the
ability to match multimodal entity linking models. JD-CCL leverages
meta-information to select negative samples with similar attributes, making the
linking task more challenging and robust. Additionally, to address the
limitations caused by the variations within the visual modality among mentions
and entities, we introduce a novel method, CVaCPT (Contextual Visual-aid
Controllable Patch Transform). It enhances visual representations by
incorporating multi-view synthetic images and contextual textual
representations to scale and shift patch representations. Experimental results
on benchmark MEL datasets demonstrate the strong effectiveness of our approach.

æè¦ï¼ååéå°å¤æ¨¡æå¯¦é«é£çµ (MEL) çç ç©¶ä¸»è¦æ¡ç¨å°æ¯å­¸ç¿ä½çºä¸»è¦ç®æ¨ãç¶èï¼éäºç ç©¶å¨æªç¶ä»ç´°èéçææ³ä¸å°æ¹æ¬¡å¶é¤é¨åç¨ä½è² æ¨£æ¬ï¼å æ­¤æé¢¨éªæå©ç¨å®¹æè¾¨è­çç¹å¾µï¼ä¸¦å¯è½å¿½ç¥ä½¿å¯¦é«ç¨ä¸ç¡äºçéè¦ç´°ç¯ãå¨æ¬æä¸­ï¼æåæåº JD-CCLï¼Jaccard è·é¢åºç¤æ¢ä»¶å°æ¯å­¸ç¿ï¼ï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼æ¨å¨å¢å¼·å¤æ¨¡æå¯¦é«é£çµæ¨¡åçå¹éè½åãJD-CCL å©ç¨åè³è¨ä¾é¸æå·æé¡ä¼¼å±¬æ§çè² æ¨£æ¬ï¼ä½¿é£çµä»»åæ´å·ææ°æ§åç©©å¥æ§ãæ­¤å¤ï¼çºäºè§£æ±ºå¨æååå¯¦é«ä¹éçè¦è¦ºæ¨¡å¼ä¸­è®ç°æé æçéå¶ï¼æåå¼å¥äºä¸ç¨®æ°æ¹æ³ï¼ç¨±çº CVaCPTï¼èçµ¡è¦è¦ºè¼å©å¯æ§åå¡è½æï¼ãå®ééçµåå¤è¦è§åæå½±ååèçµ¡æå­è¡¨å¾µä¾å¢å¼·è¦è¦ºè¡¨å¾µï¼ä»¥ç¸®æ¾åè½ç§»åå¡è¡¨å¾µãå¨åºæº MEL è³æéä¸çå¯¦é©çµæè­æäºæåæ¹æ³çå¼·å¤§æè½ã

##### **Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration**
2501.14158v1 by Mojtaba Safari, Zach Eidex, Chih-Wei Chang, Richard L. J. Qiu, Xiaofeng Yang

Magnetic resonance imaging (MRI) is a non-invasive imaging modality and
provides comprehensive anatomical and functional insights into the human body.
However, its long acquisition times can lead to patient discomfort, motion
artifacts, and limiting real-time applications. To address these challenges,
strategies such as parallel imaging have been applied, which utilize multiple
receiver coils to speed up the data acquisition process. Additionally,
compressed sensing (CS) is a method that facilitates image reconstruction from
sparse data, significantly reducing image acquisition time by minimizing the
amount of data collection needed. Recently, deep learning (DL) has emerged as a
powerful tool for improving MRI reconstruction. It has been integrated with
parallel imaging and CS principles to achieve faster and more accurate MRI
reconstructions. This review comprehensively examines DL-based techniques for
MRI reconstruction. We categorize and discuss various DL-based methods,
including end-to-end approaches, unrolled optimization, and federated learning,
highlighting their potential benefits. Our systematic review highlights
significant contributions and underscores the potential of DL in MRI
reconstruction. Additionally, we summarize key results and trends in DL-based
MRI reconstruction, including quantitative metrics, the dataset, acceleration
factors, and the progress of and research interest in DL techniques over time.
Finally, we discuss potential future directions and the importance of DL-based
MRI reconstruction in advancing medical imaging. To facilitate further research
in this area, we provide a GitHub repository that includes up-to-date DL-based
MRI reconstruction publications and public
datasets-https://github.com/mosaf/Awesome-DL-based-CS-MRI.

æè¦ï¼ç£å±æ¯æå (MRI) æ¯ä¸ç¨®éä¾µå¥æ§çå½±åæ¨¡å¼ï¼å¯æä¾äººé«å¨é¢çè§£åååè½è¦è§£ãç¶èï¼å¶æ¼«é·çæ·åæéå¯è½æå°è´æ£èä¸é©ãåä½å½å½±ï¼ä¸¦éå¶å¯¦ææç¨ãçºäºæå°éäºææ°ï¼å·²æç¨å¹³è¡å½±åç­ç­ç¥ï¼å©ç¨å¤åæ¥æ¶å¨ç·åä¾å éè³ææ·åéç¨ãæ­¤å¤ï¼å£ç¸®ææ¸¬ (CS) æ¯ä¸ç¨®ä¿é²å¾ç¨çè³æä¸­éå»ºå½±åçæ¹æ³ï¼ééå°æéçè³ææ¶ééæ¸è³æå°ï¼å¤§å¹ç¸®ç­å½±åæ·åæéãæè¿ï¼æ·±åº¦å­¸ç¿ (DL) å·²æçºæ¹é² MRI éå»ºçå¼·å¤§å·¥å·ãå®å·²èå¹³è¡å½±åå CS åçæ´åï¼ä»¥å¯¦ç¾æ´å¿«ãæ´æºç¢ºç MRI éå»ºãæ¬ç¯è©è«å¨é¢æ¢è¨äºåºæ¼ DL ç MRI éå»ºæè¡ãæåå°åç¨®åºæ¼ DL çæ¹æ³é²è¡åé¡åè¨è«ï¼åæ¬ç«¯å°ç«¯æ¹æ³ãå±éæä½³ååè¯åå­¸ç¿ï¼ä¸¦å¼·èª¿å¶æ½å¨åªé»ãæåçç³»çµ±æ§è©è«çªåºäºéè¦çè²¢ç»ï¼ä¸¦å¼·èª¿äº DL å¨ MRI éå»ºä¸­çæ½åãæ­¤å¤ï¼æåç¸½çµäºåºæ¼ DL ç MRI éå»ºä¸­çééµçµæåè¶¨å¢ï¼åæ¬éåææ¨ãè³æéãå éå å­ï¼ä»¥å DL æè¡é¨æéçé²å±åç ç©¶èè¶£ãæå¾ï¼æåè¨è«äºæ½å¨çæªä¾æ¹åï¼ä»¥ååºæ¼ DL ç MRI éå»ºå¨æ¨é²é«å­¸å½±åä¸­çéè¦æ§ãçºäºä¿é²éæ¹é¢çé²ä¸æ­¥ç ç©¶ï¼æåæä¾äºä¸å GitHub å²å­åº«ï¼å¶ä¸­åæ¬ææ°çåºæ¼ DL ç MRI éå»ºåºçç©åå¬éè³æé - https://github.com/mosaf/Awesome-DL-based-CS-MRIã

##### **MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning**
2501.14105v1 by Joshua Davis, Thomas Sounack, Kate Sciacca, Jessie M Brain, Brigitte N Durieux, Nicole D Agaronnik, Charlotta Lindvall

Extracting sections from clinical notes is crucial for downstream analysis
but is challenging due to variability in formatting and labor-intensive nature
of manual sectioning. While proprietary large language models (LLMs) have shown
promise, privacy concerns limit their accessibility. This study develops a
pipeline for automated note sectioning using open-source LLMs, focusing on
three sections: History of Present Illness, Interval History, and Assessment
and Plan. We fine-tuned three open-source LLMs to extract sections using a
curated dataset of 487 progress notes, comparing results relative to
proprietary models (GPT-4o, GPT-4o mini). Internal and external validity were
assessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B
outperformed GPT-4o (F1=0.92). On the external validity test set, performance
remained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary
models in clinical note sectioning, offering advantages in cost, performance,
and accessibility.

æè¦ï¼å¾è¨åºè¨éä¸­èååå¡å°æ¼ä¸æ¸¸åæè³ééè¦ï¼ä½ç±æ¼æ ¼å¼è®ç°åæåååçååå¯éæ§è³ªï¼éæ¯ä¸é ææ°ãå°æå¤§åèªè¨æ¨¡å (LLM) å·²å±ç¾æ½åï¼ä½é±ç§åé¡éå¶äºå¶å¯åæ§ãæ¬ç ç©¶éç¼äºä¸åä½¿ç¨éæ¾åå§ç¢¼ LLM çèªååè¨éååç®¡ç·ï¼å°æ³¨æ¼ä¸ååå¡ï¼ç¾çå²ãééçå²ä»¥åè©ä¼°åè¨ç«ãæåå¾®èª¿äºä¸åéæ¾åå§ç¢¼ LLM ä»¥ä½¿ç¨ 487 åé²åº¦è¨éçç²¾é¸è³æéèååå¡ï¼ä¸¦å°çµæèå°ææ¨¡å (GPT-4oãGPT-4o mini) é²è¡æ¯è¼ãå§é¨åå¤é¨æåº¦ééæºç¢ºåº¦ãå¬åçå F1 åæ¸é²è¡è©ä¼°ãå¾®èª¿å¾ç Llama 3.1 8B åªæ¼ GPT-4o (F1=0.92)ãå¨å¤é¨æåº¦æ¸¬è©¦éä¸­ï¼æè½ä»ç¶å¾é« (F1= 0.85)ãå¾®èª¿å¾çéæ¾åå§ç¢¼ LLM è½å¨è¨åºè¨éååä¸­è¶è¶å°ææ¨¡åï¼å¨ææ¬ãæè½åå¯åæ§æ¹é¢æä¾åªå¢ã

##### **Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models**
2501.14051v1 by Jakob Krogh Petersen, Valdemar Licht, Mads Nielsen, AsbjÃ¸rn Munk

Multi-modal models require aligned, shared embedding spaces. However, common
CLIP-based approaches need large amounts of samples and do not natively support
3D or tabular data, both of which are crucial in the medical domain. To address
these issues, we revisit CLIP-style alignment by training a domain-specific 3D
foundation model as an image encoder and demonstrate that modality alignment is
feasible with only 62 MRI scans. Our approach is enabled by a simple embedding
accumulation strategy required for training in 3D, which scales the amount of
negative pairs across batches in order to stabilize training. We perform a
thorough evaluation of various design choices, including the choice of backbone
and loss functions, and evaluate the proposed methodology on zero-shot
classification and image-retrieval tasks. While zero-shot image-retrieval
remains challenging, zero-shot classification results demonstrate that the
proposed approach can meaningfully align the representations of 3D MRI with
tabular data.

æè¦ï¼å¤æ¨¡ææ¨¡åéè¦å°é½çå±ç¨åµå¥ç©ºéãç¶èï¼å¸¸è¦çåºæ¼ CLIP çæ¹æ³éè¦å¤§éçæ¨£æ¬ï¼ä¸¦ä¸åçä¸æ¯æ´ 3D æè¡¨æ ¼è³æï¼èéå©èå¨é«çé åä¸­é½è³ééè¦ãçºäºè§£æ±ºéäºåé¡ï¼æåééè¨ç·´ä¸åé åç¹å®ç 3D åºç¤æ¨¡åä½çºå½±åç·¨ç¢¼å¨ï¼éæ°æª¢è¦ CLIP é¢¨æ ¼çå°é½ï¼ä¸¦è­æåªè¦ 62 å MRI ææå³å¯éææ¨¡æå°é½ãæåçåæ³å¾çæ¼ä¸åç°¡å®çåµå¥ç´¯ç©ç­ç¥ï¼éæ¯ 3D è¨ç·´æå¿éçï¼å®æèª¿æ´æ¹æ¬¡ä¸­çè² å°æ¸éä»¥ç©©å®è¨ç·´ãæåå°åç¨®è¨­è¨é¸æé²è¡äºå¾¹åºçè©ä¼°ï¼åæ¬ä¸»å¹¹åæå¤±å½æ¸çé¸æï¼ä¸¦å¨é¶æ¨£æ¬åé¡åå½±åæª¢ç´¢ä»»åä¸è©ä¼°ææåºçæ¹æ³ãåç®¡é¶æ¨£æ¬å½±åæª¢ç´¢ä»ç¶å·æææ°æ§ï¼ä½é¶æ¨£æ¬åé¡çµæè­æï¼ææåºçæ¹æ³å¯ä»¥ææç¾©å°å° 3D MRI çè¡¨ç¤ºèè¡¨æ ¼è³æå°é½ã

##### **Leveraging Multiphase CT for Quality Enhancement of Portal Venous CT: Utility for Pancreas Segmentation**
2501.14013v1 by Xinya Wang, Tejas Sudharshan Mathai, Boah Kim, Ronald M. Summers

Multiphase CT studies are routinely obtained in clinical practice for
diagnosis and management of various diseases, such as cancer. However, the CT
studies can be acquired with low radiation doses, different scanners, and are
frequently affected by motion and metal artifacts. Prior approaches have
targeted the quality improvement of one specific CT phase (e.g., non-contrast
CT). In this work, we hypothesized that leveraging multiple CT phases for the
quality enhancement of one phase may prove advantageous for downstream tasks,
such as segmentation. A 3D progressive fusion and non-local (PFNL) network was
developed. It was trained with three degraded (low-quality) phases
(non-contrast, arterial, and portal venous) to enhance the quality of the
portal venous phase. Then, the effect of scan quality enhancement was evaluated
using a proxy task of pancreas segmentation, which is useful for tracking
pancreatic cancer. The proposed approach improved the pancreas segmentation by
3% over the corresponding low-quality CT scan. To the best of our knowledge, we
are the first to harness multiphase CT for scan quality enhancement and
improved pancreas segmentation.

æè¦ï¼å¤ç¸é»è¦æ·å±¤ææç ç©¶å¨è¨åºå¯¦åä¸­å¸¸è¦åå¾ï¼ç¨æ¼è¨ºæ·åç®¡çåç¨®ç¾çï¼ä¾å¦ççãç¶èï¼é»è¦æ·å±¤ææç ç©¶å¯ä»¥ç¨ä½è¼»å°åéãä¸åçææååå¾ï¼ä¸ç¶å¸¸åå°éååéå±¬è£½åå½±é¿ãååçåæ³å·²éå°ç¹å®é»è¦æ·å±¤ææç¸ä½ï¼ä¾å¦éå°æ¯é»è¦æ·å±¤ææï¼çåè³ªæ¹åãå¨éé å·¥ä½ä¸­ï¼æååè¨­å©ç¨å¤åé»è¦æ·å±¤ææç¸ä½ä¾æ¹åä¸åç¸ä½çåè³ªï¼å¯è½æå°ä¸æ¸¸ä»»åï¼ä¾å¦åå²ï¼æå©ãéç¼äºä¸å 3D æ¼¸é²èååéå±é¨ (PFNL) ç¶²è·¯ãå®ä½¿ç¨ä¸åéåçï¼ä½åè³ªï¼ç¸ä½ï¼éå°æ¯ãåèåééèï¼é²è¡è¨ç·´ï¼ä»¥å¢å¼·ééèç¸ä½çåè³ªãç¶å¾ï¼ä½¿ç¨è°èåå²çä»£çä»»åè©ä¼°ææåè³ªæ¹åçææï¼éå°æ¼è¿½è¹¤è°èçå¾æç¨ãææåºçæ¹æ³å°è°èåå²æ¹åäº 3%ï¼é«æ¼å°æçä½åè³ªé»è¦æ·å±¤ææãææåæç¥ï¼æåæ¯ç¬¬ä¸åå©ç¨å¤ç¸é»è¦æ·å±¤ææé²è¡ææåè³ªæ¹ååæ¹åè°èåå²çäººã

##### **Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data**
2501.13818v1 by Frederik Pahde, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

Deep neural networks are increasingly employed in high-stakes medical
applications, despite their tendency for shortcut learning in the presence of
spurious correlations, which can have potentially fatal consequences in
practice. Detecting and mitigating shortcut behavior is a challenging task that
often requires significant labeling efforts from domain experts. To alleviate
this problem, we introduce a semi-automated framework for the identification of
spurious behavior from both data and model perspective by leveraging insights
from eXplainable Artificial Intelligence (XAI). This allows the retrieval of
spurious data points and the detection of model circuits that encode the
associated prediction rules. Moreover, we demonstrate how these shortcut
encodings can be used for XAI-based sample- and pixel-level data annotation,
providing valuable information for bias mitigation methods to unlearn the
undesired shortcut behavior. We show the applicability of our framework using
four medical datasets across two modalities, featuring controlled and
real-world spurious correlations caused by data artifacts. We successfully
identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision
Transformer models, ultimately increasing their robustness and applicability
for real-world medical tasks.

æè¦ï¼æ·±åº¦ç¥ç»ç½ç»è¶æ¥è¶å¤å°ç¨äºé«é£é©å»çåºç¨ä¸­ï¼å°½ç®¡å®ä»¬å¨å­å¨èåç¸å³æ§çæåµä¸å¾åäºæ·å¾å­¦ä¹ ï¼è¿å¨å®è·µä¸­å¯è½äº§çè´å½çåæãæ£æµåç¼è§£æ·å¾è¡ä¸ºæ¯ä¸é¡¹è°å·¨çä»»å¡ï¼éå¸¸éè¦é¢åä¸å®¶çå¤§éæ è®°å·¥ä½ãä¸ºäºç¼è§£è¿ä¸ªé®é¢ï¼æä»¬å¼å¥äºä¸ä¸ªåèªå¨æ¡æ¶ï¼ç¨äºä»æ°æ®åæ¨¡åçè§åº¦è¯å«èåè¡ä¸ºï¼æ¹æ³æ¯å©ç¨å¯è§£éäººå·¥æºè½ (XAI) çè§è§£ãè¿åè®¸æ£ç´¢èåæ°æ®ç¹å¹¶æ£æµå¯¹å³èé¢æµè§åè¿è¡ç¼ç çæ¨¡åçµè·¯ãæ­¤å¤ï¼æä»¬æ¼ç¤ºäºå¦ä½ä½¿ç¨è¿äºæ·å¾ç¼ç è¿è¡åºäº XAI çæ ·æ¬ååç´ çº§æ°æ®æ³¨éï¼ä¸ºåå·®ç¼è§£æ¹æ³æä¾æä»·å¼çä¿¡æ¯ï¼ä»¥æ¶é¤ä¸éè¦çæ·å¾è¡ä¸ºãæä»¬ä½¿ç¨è·¨è¶ä¸¤ç§æ¹å¼çåä¸ªå»å­¦æ°æ®éå±ç¤ºäºæä»¬æ¡æ¶çéç¨æ§ï¼è¿äºæ°æ®éå·æç±æ°æ®ä¼ªåå¼èµ·çåæ§åçå®ä¸çèåç¸å³æ§ãæä»¬æåå°è¯å«å¹¶åè½»äº VGG16ãResNet50 åå½ä»£ Vision Transformer æ¨¡åä¸­çè¿äºåå·®ï¼æç»æé«äºå®ä»¬çé²æ£æ§åå¨çå®ä¸çå»çä»»å¡ä¸­çéç¨æ§ã

##### **Question Answering on Patient Medical Records with Private Fine-Tuned LLMs**
2501.13687v1 by Sara Kothari, Ayush Gupta

Healthcare systems continuously generate vast amounts of electronic health
records (EHRs), commonly stored in the Fast Healthcare Interoperability
Resources (FHIR) standard. Despite the wealth of information in these records,
their complexity and volume make it difficult for users to retrieve and
interpret crucial health insights. Recent advances in Large Language Models
(LLMs) offer a solution, enabling semantic question answering (QA) over medical
data, allowing users to interact with their health records more effectively.
However, ensuring privacy and compliance requires edge and private deployments
of LLMs.
  This paper proposes a novel approach to semantic QA over EHRs by first
identifying the most relevant FHIR resources for a user query (Task1) and
subsequently answering the query based on these resources (Task2). We explore
the performance of privately hosted, fine-tuned LLMs, evaluating them against
benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that
fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by
0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we
examine advanced aspects of LLM usage, including sequential fine-tuning, model
self-evaluation (narcissistic evaluation), and the impact of training data size
on performance. The models and datasets are available here:
https://huggingface.co/genloop

æè¦ï¼é«çä¿å¥ç³»çµ±æçºç¢çå¤§éçé»å­å¥åº·ç´é (EHR)ï¼éå¸¸å²å­å¨å¿«éé«çäºéæ§è³æº (FHIR) æ¨æºä¸­ãåç®¡éäºç´éä¸­åå«è±å¯çè³è¨ï¼ä½å¶è¤éæ§åé¾å¤§æ¸éè®ä½¿ç¨èé£ä»¥æ·ååè©®ééè¦çå¥åº·è¦è§£ãå¤§åèªè¨æ¨¡å (LLM) çææ°é²å±æä¾äºè§£æ±ºæ¹æ¡ï¼è½å°é«çè³æé²è¡èªç¾©åç­ (QA)ï¼è®ä½¿ç¨èè½æ´ææå°èå¶å¥åº·ç´éäºåãç¶èï¼ç¢ºä¿é±ç§åç¸å®¹æ§éè¦ LLM çéç·£åç§äººé¨ç½²ãæ¬ææåºäºèªç¾©åç­çæ°æ¹æ³ï¼åæ¾åºèä½¿ç¨èæ¥è©¢æç¸éç FHIR è³æº (ä»»å 1)ï¼ç¶å¾æ ¹æéäºè³æºåç­æ¥è©¢ (ä»»å 2)ãæåæ¢è¨äºç§äººä¸»æ©ãå¾®èª¿ LLM çæè½ï¼ä¸¦æ ¹æ GPT-4 å GPT-4o ç­åºæºæ¨¡åè©ä¼°å®åãæåççµæé¡¯ç¤ºï¼å¾®èª¿ LLM çå¤§å°éç¶å° 250 åï¼ä½å¨ä»»å 1 ç F1 åæ¸ä¸åªæ¼ GPT-4 ç³»åæ¨¡å 0.55%ï¼å¨ä»»å 2 ç Meteor ä»»åä¸­åªæ¼ 42%ãæ­¤å¤ï¼æåæ¢è¨äº LLM ä½¿ç¨çé«éé¢åï¼åæ¬å¾ªåºå¾®èª¿ãæ¨¡åèªæè©ä¼°ï¼èªæå¼è©ä¼°ï¼åè¨ç·´è³æå¤§å°å°æè½çå½±é¿ãæ¨¡ååè³æéå¨æ­¤èæä¾ï¼https://huggingface.co/genloop

##### **How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization**
2501.13669v1 by Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu

Large Language Models (LLMs) exhibit strong general-purpose language
capabilities. However, fine-tuning these models on domain-specific tasks often
leads to catastrophic forgetting, where the model overwrites or loses essential
knowledge acquired during pretraining. This phenomenon significantly limits the
broader applicability of LLMs. To address this challenge, we propose a novel
approach to compute the element-wise importance of model parameters crucial for
preserving general knowledge during fine-tuning. Our method utilizes a
dual-objective optimization strategy: (1) regularization loss to retain the
parameter crucial for general knowledge; (2) cross-entropy loss to adapt to
domain-specific tasks. Additionally, we introduce layer-wise coefficients to
account for the varying contributions of different layers, dynamically
balancing the dual-objective optimization. Extensive experiments on scientific,
medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our
approach mitigates catastrophic forgetting while enhancing model adaptability.
Compared to previous methods, our solution is approximately 20 times faster and
requires only 10%-15% of the storage, highlighting the practical efficiency.
The code will be released.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å±ç¾å¼·å¤§çéç¨èªè¨è½åãç¶èï¼éå°ç¹å®é åä»»åå¾®èª¿éäºæ¨¡åæï¼å¸¸å¸¸æå°è´ç½é£æ§éºå¿ï¼æ¨¡åæè¦å¯«æéºå¤±é è¨ç·´æéç¿å¾çåºæ¬ç¥è­ãéç¨®ç¾è±¡å¤§å¹éå¶äº LLM çå»£æ³é©ç¨æ§ãçºäºæå°éé ææ°ï¼æåæåºäºä¸ç¨®æ°ç©æ¹æ³ï¼ç¨æ¼è¨ç®æ¨¡ååæ¸çåç´ ç´éè¦æ§ï¼éäºåæ¸å°æ¼å¨å¾®èª¿æéä¿çä¸è¬ç¥è­è³ééè¦ãæåçåæ³æ¡ç¨éç®æ¨åªåç­ç¥ï¼(1) æ­£ååæå¤±ï¼ç¨æ¼ä¿çå°ä¸è¬ç¥è­è³ééè¦çåæ¸ï¼(2) äº¤åçµæå¤±ï¼ç¨æ¼é©æç¹å®é åçä»»åãæ­¤å¤ï¼æåå¼å¥äºå±¤ç´ä¿æ¸ï¼ç¨æ¼èéä¸åå±¤çè®ç°è²¢ç»ï¼ä¸¦åæå¹³è¡¡éç®æ¨åªåãä½¿ç¨ GPT-J å LLaMA-3 å¨ç§å­¸ãé«çåç©çä»»åä¸é²è¡çå»£æ³å¯¦é©è­æï¼æåçåæ³æ¸è¼äºç½é£æ§éºå¿ï¼åæå¢å¼·äºæ¨¡åé©ææ§ãèä¹åçåæ³ç¸æ¯ï¼æåçè§£æ±ºæ¹æ¡éåº¦å¿«äºç´ 20 åï¼èä¸åªéè¦ 10%-15% çå²å­ç©ºéï¼çªé¡¯äºå¶å¯¦ç¨çæçãç¨å¼ç¢¼å°æéåºã

##### **Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management**
2501.13587v2 by Yuxuan Liu, Jinpei Han, Padmanabhan Ramnarayan, A. Aldo Faisal

Clinical machine learning deployment across institutions faces significant
challenges when patient populations and clinical practices differ
substantially. We present a systematic framework for cross-institutional
knowledge transfer in clinical time series, demonstrated through pediatric
ventilation management between a general pediatric intensive care unit (PICU)
and a cardiac-focused unit. Using contrastive predictive coding (CPC) for
representation learning, we investigate how different data regimes and
fine-tuning strategies affect knowledge transfer across institutional
boundaries. Our results show that while direct model transfer performs poorly,
CPC with appropriate fine-tuning enables effective knowledge sharing between
institutions, with benefits particularly evident in limited data scenarios.
Analysis of transfer patterns reveals an important asymmetry: temporal
progression patterns transfer more readily than point-of-care decisions,
suggesting practical pathways for cross-institutional deployment. Through a
systematic evaluation of fine-tuning approaches and transfer patterns, our work
provides insights for developing more generalizable clinical decision support
systems while enabling smaller specialized units to leverage knowledge from
larger centers.

æè¦ï¼è¨åºæ©å¨å­¸ç¿é¨ç½²å¨æ©æ§éé¢è¨éå¤§ææ°ï¼ç¶æ£èç¾¤é«åè¨åºå¯¦åæé¡¯èå·®ç°æãæåæåºäºè¨åºæéåºåè·¨æ©æ§ç¥è­è½ç§»çç³»çµ±æ§æ¡æ¶ï¼ééä¸è¬åç§å è­·çæ¿ (PICU) åä»¥å¿èçºä¸»ççæ¿ä¹éçåç§éæ°£ç®¡çé²è¡ç¤ºç¯ãééä½¿ç¨å°æ¯é æ¸¬ç·¨ç¢¼ (CPC) é²è¡è¡¨å¾µå­¸ç¿ï¼æåæ¢è¨ä¸åçè³æå¶åº¦åå¾®èª¿ç­ç¥å¦ä½å½±é¿æ©æ§éçç¥è­è½ç§»ãæåççµæé¡¯ç¤ºï¼éç¶ç´æ¥æ¨¡åè½ç§»è¡¨ç¾ä¸ä½³ï¼ä½é©ç¶å¾®èª¿ç CPC è½å¤ å¨æ©æ§éé²è¡ææçç¥è­åäº«ï¼å¨è³ææéçææ³ä¸ï¼å¶åªé»ç¹å¥æé¡¯ãè½ç§»æ¨¡å¼çåææ­ç¤ºäºä¸åéè¦çä¸å°ç¨±æ§ï¼æéé²ç¨æ¨¡å¼æ¯ç§è­·é»æ±ºç­æ´å®¹æè½ç§»ï¼éè¡¨ç¤ºè·¨æ©æ§é¨ç½²çå¯¦ééå¾ãééå°å¾®èª¿æ¹æ³åè½ç§»æ¨¡å¼é²è¡ç³»çµ±æ§è©ä¼°ï¼æåçç ç©¶æä¾äºè¦è§£ï¼ä»¥ä¾¿éç¼æ´å·æ¦æ¬æ§çè¨åºæ±ºç­æ¯æ´ç³»çµ±ï¼åæè®è¼å°çå°ç§å®ä½è½å¤ å©ç¨è¼å¤§åä¸­å¿çç¥è­ã

##### **LLMs Can Plan Only If We Tell Them**
2501.13545v1 by Bilgehan Sel, Ruoxi Jia, Ming Jin

Large language models (LLMs) have demonstrated significant capabilities in
natural language processing and reasoning, yet their effectiveness in
autonomous planning has been under debate. While existing studies have utilized
LLMs with external feedback mechanisms or in controlled environments for
planning, these approaches often involve substantial computational and
development resources due to the requirement for careful design and iterative
backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to
match human performance on standard planning benchmarks, such as the
Blocksworld, without additional support. This paper investigates whether LLMs
can independently generate long-horizon plans that rival human baselines. Our
novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help
achieve state-of-the-art results in planning benchmarks out-competing prior
methods and human baselines all autonomously.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨èçåæ¨çæ¹é¢å±ç¤ºäºé¡¯èçè½åï¼ä½å®åå¨èªä¸»è¦åä¸­çæææ§ä¸ç´å­å¨ç­è­°ãåç®¡ç¾æç ç©¶å·²å° LLM èå¤é¨åé¥æ©å¶çµåä½¿ç¨ï¼æå¨åæ§ç°å¢ä¸­é²è¡è¦åï¼ä½ç±æ¼éè¦ä»ç´°è¨­è¨ååè¦æç¤ºï¼éäºæ¹æ³éå¸¸æ¶åå¤§éçè¨ç®åéç¼è³æºãæ­¤å¤ï¼å³ä½¿æ¯æåé²ç LLMï¼ä¾å¦ GPT-4ï¼å¨æ²æé¡å¤æ¯æ´çææ³ä¸ï¼ä¹å¾é£å¨æ¨æºè¦ååºæºï¼ä¾å¦ Blocksworldï¼ä¸éå°äººé¡çè¡¨ç¾ãæ¬ææ¢è¨ LLM æ¯å¦è½ç¨ç«çæèäººé¡åºæºç¸åª²ç¾çé·é è¨ç«ãæåå°ææ³æ¼ç®æ³ (AoT) çåµæ°å¼·åï¼æåç¨±ä¹çº AoT+ï¼æå©æ¼å¨è¦ååºæºä¸­åå¾æåé²çææï¼å¨å®å¨èªä¸»çææ³ä¸åéååçåç¨®æ¹æ³åäººé¡åºæºã

##### **Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs**
2501.13984v1 by Bhumika Gupta, Pralaypati Ta, Keerthi Ram, Mohanasankar Sivaprakasam

The updated recommendations on diagnostic procedures and treatment pathways
for a medical condition are documented as graphical flows in Clinical Practice
Guidelines (CPGs). For effective use of the CPGs in helping medical
professionals in the treatment decision process, it is necessary to fully
capture the guideline knowledge, particularly the contexts and their
relationships in the graph. While several existing works have utilized these
guidelines to create rule bases for Clinical Decision Support Systems, limited
work has been done toward directly capturing the full medical knowledge
contained in CPGs. This work proposes an approach to create a contextually
enriched, faithful digital representation of National Comprehensive Cancer
Network (NCCN) Cancer CPGs in the form of graphs using automated extraction and
node & relationship classification. We also implement semantic enrichment of
the model by using Large Language Models (LLMs) for node classification,
achieving an accuracy of 80.86% and 88.47% with zero-shot learning and few-shot
learning, respectively. Additionally, we introduce a methodology for answering
natural language questions with constraints to guideline text by leveraging
LLMs to extract the relevant subgraph from the guideline knowledge base. By
generating natural language answers based on subgraph paths and semantic
information, we mitigate the risk of incorrect answers and hallucination
associated with LLMs, ensuring factual accuracy in medical domain Question
Answering.

æè¦ï¼å·²æ´æ°çé«ççæ³è¨ºæ·ç¨åºåæ²»çéå¾å»ºè­°ï¼ä»¥è¨åºå¯¦åæå (CPG) ä¸­çåå½¢æµç¨è¨éãçºäºææä½¿ç¨ CPG åå©é«çå°æ¥­äººå¡é²è¡æ²»çæ±ºç­ï¼å¿é å®æ´æ·åæåç¥è­ï¼ç¹å¥æ¯åè¡¨ä¸­çèçµ¡åå¶éä¿ãéç¶ç¾æè¨±å¤ç ç©¶å·²å©ç¨éäºæåçºè¨åºæ±ºç­æ¯æ´ç³»çµ±å»ºç«è¦ååºç¤ï¼ä½ç´æ¥æ·å CPG ä¸­åå«çå®æ´é«çç¥è­çå·¥ä½å»æéãéé ç ç©¶æåºäºä¸ç¨®æ¹æ³ï¼ä»¥èªååæ·ååç¯é»èéä¿åé¡çæ¹å¼ï¼å»ºç«èçµ¡è±å¯ãå¿ å¯¦çåå®¶ç¶åççç¶²è·¯ (NCCN) çç CPG åå½¢æ¸ä½è¡¨ç¤ºãæåä¹ééä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡ç¯é»åé¡ï¼å¯¦ä½æ¨¡åçèªæè±å¯åï¼åå¥å¨é¶æ¬¡å­¸ç¿åå°æ¬¡å­¸ç¿ä¸­éå° 80.86% å 88.47% çæºç¢ºåº¦ãæ­¤å¤ï¼æåå¼é²äºä¸ç¨®æ¹æ³ï¼éééç¨ LLM å¾æåç¥è­åº«ä¸­æ·åç¸éå­åï¼ä¾åç­å·ææåæå­éå¶çèªç¶èªè¨åé¡ãééæ ¹æå­åè·¯å¾åèªæè³è¨ç¢çèªç¶èªè¨ç­æ¡ï¼æåéä½äºè LLM ç¸éçé¯èª¤ç­æ¡åå¹»è¦ºé¢¨éªï¼ç¢ºä¿äºé«çé ååé¡è§£ç­ä¸­çäºå¯¦æºç¢ºæ§ã

##### **A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability**
2501.13369v1 by Bishwash Paneru, Biplov Paneru, Tanka Mukhiya, Khem Narayan Poudyal

In Nepal, air pollution is a serious public health concern, especially in
cities like Kathmandu where particulate matter (PM2.5 and PM10) has a major
influence on respiratory health and air quality. The Air Quality Index (AQI) is
predicted in this work using a Random Forest Regressor, and the model's
predictions are interpreted using SHAP (SHapley Additive exPlanations)
analysis. With the lowest Testing RMSE (0.23) and flawless R2 scores (1.00),
CatBoost performs better than other models, demonstrating its greater accuracy
and generalization which is cross validated using a nested cross validation
approach. NowCast Concentration and Raw Concentration are the most important
elements influencing AQI values, according to SHAP research, which shows that
the machine learning results are highly accurate. Their significance as major
contributors to air pollution is highlighted by the fact that high values of
these characteristics significantly raise the AQI. This study investigates the
Hydrogen-Alpha (HA) biodegradable filter as a novel way to reduce the related
health hazards. With removal efficiency of more than 98% for PM2.5 and 99.24%
for PM10, the HA filter offers exceptional defense against dangerous airborne
particles. These devices, which are biodegradable face masks and cigarette
filters, address the environmental issues associated with traditional filters'
non-biodegradable trash while also lowering exposure to air contaminants.

æè¦ï¼å¨å°¼æ³ç¾ï¼ç©ºæ°£æ±¡ææ¯ä¸åå´éçå¬å±è¡çåé¡ï¼ç¹å¥æ¯å¨å å¾·æ»¿é½ç­åå¸ï¼é£è£¡çæ¸æµ®å¾®ç²ï¼PM2.5 å PM10ï¼å°å¼å¸ç³»çµ±å¥åº·åç©ºæ°£åè³ªæéå¤§å½±é¿ãéé å·¥ä½ä½¿ç¨é¨æ©æ£®æåæ­¸å¨é æ¸¬ç©ºæ°£åè³ªææ¸ (AQI)ï¼ä¸¦ä½¿ç¨ SHAPï¼SHapley å æ³è§£éï¼åæä¾è§£éæ¨¡åçé æ¸¬ãCatBoost çæ¸¬è©¦ RMSE æä½ï¼0.23ï¼ï¼R2 åæ¸å®ç¾ï¼1.00ï¼ï¼è¡¨ç¾åªæ¼å¶ä»æ¨¡åï¼è­æå¶å·ææ´é«çæºç¢ºæ§åæ³åæ§ï¼ä¸¦ä½¿ç¨åµå¥äº¤åé©è­æ¹æ³é²è¡äº¤åé©è­ãæ ¹æ SHAP ç ç©¶ï¼ç¾å¨æ¿åº¦ååå§æ¿åº¦æ¯å½±é¿ AQI å¼æéè¦çåç´ ï¼éè¡¨ææ©å¨å­¸ç¿çµæéå¸¸æºç¢ºãå®åä½çºç©ºæ°£æ±¡æçä¸»è¦è²¢ç»èçéè¦æ§å¨æ¼ï¼éäºç¹å¾µçé«å¼æé¡¯èæé« AQIãæ¬ç ç©¶æ¢è¨äºæ°«-Î±ï¼HAï¼å¯çç©éè§£éæ¿¾å¨ä½çºæ¸å°ç¸éå¥åº·å±å®³çä¸ç¨®æ°æ¹æ³ãHA éæ¿¾å¨å° PM2.5 çå»é¤æçè¶é 98%ï¼å° PM10 çå»é¤æçè¶é 99.24%ï¼å¯æä¾é²ç¯å±éªç©ºæ°£æ¸æµ®å¾®ç²çåºè²é²è­·ãéäºå¯çç©éè§£å£ç½©åé¦è¸éæ¿¾å¨çè£ç½®è§£æ±ºäºå³çµ±éæ¿¾å¨ä¸å¯çç©éè§£åå¾ç¸éçç°å¢åé¡ï¼åæä¹éä½äºæ¥è§¸ç©ºæ°£æ±¡æç©çé¢¨éªã

##### **Unveiling Discrete Clues: Superior Healthcare Predictions for Rare Diseases**
2501.16373v1 by Chuang Zhao, Hui Tang, Jiheng Zhang, Xiaomeng Li

Accurate healthcare prediction is essential for improving patient outcomes.
Existing work primarily leverages advanced frameworks like attention or graph
networks to capture the intricate collaborative (CO) signals in electronic
health records. However, prediction for rare diseases remains challenging due
to limited co-occurrence and inadequately tailored approaches. To address this
issue, this paper proposes UDC, a novel method that unveils discrete clues to
bridge consistent textual knowledge and CO signals within a unified semantic
space, thereby enriching the representation semantics of rare diseases.
Specifically, we focus on addressing two key sub-problems: (1) acquiring
distinguishable discrete encodings for precise disease representation and (2)
achieving semantic alignment between textual knowledge and the CO signals at
the code level. For the first sub-problem, we refine the standard vector
quantized process to include condition awareness. Additionally, we develop an
advanced contrastive approach in the decoding stage, leveraging synthetic and
mixed-domain targets as hard negatives to enrich the perceptibility of the
reconstructed representation for downstream tasks. For the second sub-problem,
we introduce a novel codebook update strategy using co-teacher distillation.
This approach facilitates bidirectional supervision between textual knowledge
and CO signals, thereby aligning semantically equivalent information in a
shared discrete latent space. Extensive experiments on three datasets
demonstrate our superiority.

æè¦ï¼æºç¢ºçé«çä¿å¥é æ¸¬å°æ¼æ¹åæ£èçæ²»ççµæè³ééè¦ã
ç¾æçå·¥ä½ä¸»è¦å©ç¨æ³¨æåæåå½¢ç¶²è·¯ç­åé²çæ¡æ¶ä¾ææé»å­å¥åº·è¨éä¸­è¤éçåä½ (CO) ä¿¡èãç¶èï¼ç±æ¼å±ç¾æéåæ¹æ³èª¿æ´ä¸è¶³ï¼ç½è¦ç¾ççé æ¸¬ä»ç¶å·æææ°æ§ãçºäºè§£æ±ºéååé¡ï¼æ¬ææåºäº UDCï¼éæ¯ä¸ç¨®æ°çæ¹æ³ï¼å®æ­ç¤ºäºé¢æ£ç·ç´¢ï¼ä»¥å¨çµ±ä¸çèªç¾©ç©ºéä¸­æ©æ¥ä¸è´çææ¬ç¥è­å CO ä¿¡èï¼å¾èè±å¯ç½è¦ç¾ççè¡¨ç¤ºèªç¾©ã
å·é«ä¾èªªï¼æåå°æ³¨æ¼è§£æ±ºå©åééµçå­åé¡ï¼(1) ç²åå¯ååçé¢æ£ç·¨ç¢¼ä»¥é²è¡ç²¾ç¢ºçç¾çè¡¨ç¤ºï¼ä»¥å (2) å¨ä»£ç¢¼å±¤ç´ä¸å¯¦ç¾ææ¬ç¥è­å CO ä¿¡èä¹éçèªç¾©å°é½ãå°æ¼ç¬¬ä¸åå­åé¡ï¼æåæ¹é²äºæ¨æºåééåæµç¨ä»¥åå«æ¢ä»¶æç¥ãæ­¤å¤ï¼æåå¨è§£ç¢¼éæ®µéç¼äºä¸ç¨®åé²çå°æ¯æ¹æ³ï¼å©ç¨åæåæ··ååç®æ¨ä½çºç¡¬è² ä¾ä¾è±å¯éå»ºè¡¨ç¤ºå°ä¸æ¸¸ä»»åçå¯æç¥æ§ãå°æ¼ç¬¬äºåå­åé¡ï¼æåä½¿ç¨å±åæå¸«è¸é¤¾å¼å¥äºä¸ç¨®æ°çä»£ç¢¼ç°¿æ´æ°ç­ç¥ãéç¨®æ¹æ³ä¿é²äºææ¬ç¥è­å CO ä¿¡èä¹éçéåç£ç£ï¼å¾èå¨å±äº«çé¢æ£æ½å¨ç©ºéä¸­å°é½èªç¾©ç­å¹è³è¨ãå¨ä¸åè³æéä¸çå¤§éå¯¦é©è­æäºæåçåªè¶æ§ã

##### **QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks**
2501.13165v1 by Naman Jain, Amir Kalev

We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine
learning module. The proposed module enables feature extraction in a
reduced-dimensional space, significantly decreasing the number of parallel
evaluations required in typical quantum convolutional neural network
architectures. Its design allows seamless integration into deep classical
neural networks, making it particularly suitable for hybrid quantum-classical
models. As an application of QuFeX, we propose Qu-Net -- a hybrid architecture
which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is
widely used for image segmentation tasks such as medical imaging and autonomous
driving. Our numerical analysis indicates that the Qu-Net can achieve superior
segmentation performance compared to a U-Net baseline. These results highlight
the potential of QuFeX to enhance deep neural networks by leveraging hybrid
computational paradigms, providing a path towards a robust framework for
real-world applications requiring precise feature extraction.

æè¦ï¼æåå¼å¥äºéå­ç¹å¾µèå (QuFeX)ï¼éæ¯ä¸ååµæ°çéå­æ©å¨å­¸ç¿æ¨¡çµãææåºçæ¨¡çµå¯ä»¥å¨éç¶­ç©ºéä¸­é²è¡ç¹å¾µèåï¼å¤§å¹æ¸å°å¸åéå­å·ç©ç¥ç¶ç¶²è·¯æ¶æ§ä¸­æéçä¸¦è¡è©ä¼°æ¸éãå¶è¨­è¨åè¨±ç¡ç¸«æ´åå°æ·±åº¦å¤å¸ç¥ç¶ç¶²è·¯ä¸­ï¼ä½¿å¶ç¹å¥é©åæ¼æ··åéå­å¤å¸æ¨¡åãä½çº QuFeX çæç¨ï¼æåæåºäº Qu-Netï¼éæ¯ä¸ç¨®æ··åæ¶æ§ï¼å®å¨ U-Net æ¶æ§çç¶é ¸èæ´åäº QuFeXãå¾èå»£æ³ç¨æ¼å½±ååå²ä»»åï¼ä¾å¦é«å­¸å½±ååèªåé§é§ãæåçæ¸å¼åæè¡¨æï¼è U-Net åºæºç¸æ¯ï¼Qu-Net å¯ä»¥å¯¦ç¾åªç°çåå²æè½ãéäºçµæçªé¡¯äº QuFeX ééå©ç¨æ··åéç®ç¯ä¾ä¾å¢å¼·æ·±åº¦ç¥ç¶ç¶²è·¯çæ½åï¼çºéè¦ç²¾ç¢ºç¹å¾µèåççå¯¦ä¸çæç¨ç¨å¼æä¾äºä¸åéåç©©å¥æ¶æ§çéå¾ã

##### **AirRadar: Inferring Nationwide Air Quality in China with Deep Neural Networks**
2501.13141v1 by Qiongyan Wang, Yutong Xia, Siru ZHong, Weichuang Li, Yuankai Wu, Shifen Cheng, Junbo Zhang, Yu Zheng, Yuxuan Liang

Monitoring real-time air quality is essential for safeguarding public health
and fostering social progress. However, the widespread deployment of air
quality monitoring stations is constrained by their significant costs. To
address this limitation, we introduce \emph{AirRadar}, a deep neural network
designed to accurately infer real-time air quality in locations lacking
monitoring stations by utilizing data from existing ones. By leveraging
learnable mask tokens, AirRadar reconstructs air quality features in
unmonitored regions. Specifically, it operates in two stages: first capturing
spatial correlations and then adjusting for distribution shifts. We validate
AirRadar's efficacy using a year-long dataset from 1,085 monitoring stations
across China, demonstrating its superiority over multiple baselines, even with
varying degrees of unobserved data. The source code can be accessed at
https://github.com/CityMind-Lab/AirRadar.

æè¦ï¼ç£æ§å³æç©ºæ°£åè³ªå°æ¼ä¿éå¬å±å¥åº·åä¿é²ç¤¾æé²æ­¥è³ééè¦ãç¶èï¼ç©ºæ°£åè³ªç£æ¸¬ç«çå»£æ³é¨ç½²åå°å¶é«æææ¬çéå¶ãçºäºè§£æ±ºéåéå¶ï¼æåå¼å¥äº \emph{AirRadar}ï¼éæ¯ä¸åæ·±åº¦ç¥ç¶ç¶²è·¯ï¼æ¨å¨å©ç¨ç¾æç£æ¸¬ç«çè³æï¼ç²¾æºæ¨è«æ²æç£æ¸¬ç«çå°åçå³æç©ºæ°£åè³ªãééå©ç¨å¯å­¸ç¿çé®ç½©ç¬¦èï¼AirRadar éå»ºæªç£æ§ååçç©ºæ°£åè³ªç¹å¾µãå·é«ä¾èªªï¼å®åå©åéæ®µéä½ï¼é¦åæ·åç©ºééè¯æ§ï¼ç¶å¾èª¿æ´åä½è½ç§»ãæåä½¿ç¨ä¾èªä¸­å 1,085 åç£æ¸¬ç«ä¸æ´å¹´çè³æéé©è­äº AirRadar çåæï¼è­æäºå®åªæ¼å¤ååºæºï¼å³ä½¿å¨ä¸åç¨åº¦çæªè§å¯è³æä¸­ä¹æ¯å¦æ­¤ãå¯ä»¥å¨ https://github.com/CityMind-Lab/AirRadar åå¾åå§ç¨å¼ç¢¼ã

##### **Estimating the Conformal Prediction Threshold from Noisy Labels**
2501.12749v1 by Coby Penso, Jacob Goldberger, Ethan Fetaya

Conformal Prediction (CP) is a method to control prediction uncertainty by
producing a small prediction set, ensuring a predetermined probability that the
true class lies within this set. This is commonly done by defining a score,
based on the model predictions, and setting a threshold on this score using a
validation set. In this study, we address the problem of CP calibration when we
only have access to a validation set with noisy labels. We show how we can
estimate the noise-free conformal threshold based on the noisy labeled data.
Our solution is flexible and can accommodate various modeling assumptions
regarding the label contamination process, without needing any information
about the underlying data distribution or the internal mechanisms of the
machine learning classifier. We develop a coverage guarantee for uniform noise
that is effective even in tasks with a large number of classes. We dub our
approach Noise-Aware Conformal Prediction (NACP) and show on several natural
and medical image classification datasets, including ImageNet, that it
significantly outperforms current noisy label methods and achieves results
comparable to those obtained with a clean validation set.

æè¦ï¼å±å½¢é¢æµ (CP) æ¯ä¸ç¨®ééç¢çä¸åå°åé æ¸¬éåä¾æ§å¶é æ¸¬ä¸ç¢ºå®æ§çæ¹æ³ï¼ç¢ºä¿çæ­£çé¡å¥è½å¨éåéåå§çé åç¢ºå®çæ©çãééå¸¸æ¯ééå®ç¾©ä¸ååºæ¼æ¨¡åé æ¸¬çåæ¸ä¾å®æï¼ä¸¦ä½¿ç¨é©è­éåå°éååæ¸è¨­å®ä¸åé¾å¼ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨äºç¶æååªè½å­åå·æéè¨æ¨ç±¤çé©è­éåæï¼CP æ ¡æ­£çåé¡ãæåå±ç¤ºäºå¦ä½æ ¹æéè¨æ¨ç±¤è³æä¼°è¨ç¡éè¨çå±å½¢é¾å¼ãæåçè§£æ±ºæ¹æ¡å·æå½æ§ï¼ä¸¦ä¸å¯ä»¥é©æéæ¼æ¨ç±¤æ±¡æéç¨çåç¨®å»ºæ¨¡åè¨­ï¼èä¸éè¦ä»»ä½éæ¼åºå±¤è³æåä½ææ©å¨å­¸ç¿åé¡å¨å§é¨æ©å¶çè³è¨ãæåéç¼äºä¸åå°æ¼åå»éè¨çè¦èä¿è­ï¼å³ä½¿å¨å·æå¤§éé¡å¥çä»»åä¸­ä¹å¾ææãæåå°æåçåæ³ç¨±çºéè¨æç¥å±å½¢é æ¸¬ (NACP)ï¼ä¸¦å¨å¹¾åèªç¶åé«å­¸å½±ååé¡è³æéï¼åæ¬ ImageNetï¼ä¸å±ç¤ºäºå®é¡¯èåªæ¼ç®åçéè¨æ¨ç±¤æ¹æ³ï¼ä¸¦ä¸éå°äºèä½¿ç¨ä¹¾æ·¨é©è­éåç²å¾ççµæç¸ç¶ççµæã

##### **Applications and Challenges of AI and Microscopy in Life Science Research: A Review**
2501.13135v1 by Himanshu Buckchash, Gyanendra Kumar Verma, Dilip K. Prasad

The complexity of human biology and its intricate systems holds immense
potential for advancing human health, disease treatment, and scientific
discovery. However, traditional manual methods for studying biological
interactions are often constrained by the sheer volume and complexity of
biological data. Artificial Intelligence (AI), with its proven ability to
analyze vast datasets, offers a transformative approach to addressing these
challenges. This paper explores the intersection of AI and microscopy in life
sciences, emphasizing their potential applications and associated challenges.
We provide a detailed review of how various biological systems can benefit from
AI, highlighting the types of data and labeling requirements unique to this
domain. Particular attention is given to microscopy data, exploring the
specific AI techniques required to process and interpret this information. By
addressing challenges such as data heterogeneity and annotation scarcity, we
outline potential solutions and emerging trends in the field. Written primarily
from an AI perspective, this paper aims to serve as a valuable resource for
researchers working at the intersection of AI, microscopy, and biology. It
summarizes current advancements, key insights, and open problems, fostering an
understanding that encourages interdisciplinary collaborations. By offering a
comprehensive yet concise synthesis of the field, this paper aspires to
catalyze innovation, promote cross-disciplinary engagement, and accelerate the
adoption of AI in life science research.

æè¦ï¼äººé¡çç©å­¸åå¶è¤éç³»çµ±çè¤éæ§èèèä¿é²äººé¡å¥åº·ãç¾çæ²»çåç§å­¸ç¼ç¾çå·¨å¤§æ½åãç¶èï¼å³çµ±çäººå·¥çç©äº¤äºç ç©¶æ¹æ³éå¸¸åå°çç©æ¸æé¾å¤§çæ¸éåè¤éæ§çéå¶ãäººå·¥æºæ§ (AI) å·²è¢«è­å¯¦å·æåæé¾å¤§æ¸æéçè½åï¼å®æä¾äºä¸ç¨®è®é©æ§çæ¹æ³ä¾æå°éäºææ°ãæ¬ææ¢è¨äº AI åé¡¯å¾®é¡å¨çå½ç§å­¸ä¸­çäº¤éï¼å¼·èª¿äºå®åçæ½å¨æç¨åç¸éææ°ãæåè©³ç´°åé¡§äºåç¨®çç©ç³»çµ±å¦ä½å¾ AI ä¸­åçï¼éé»ä»ç´¹äºæ­¤é åç¨æçæ¸æé¡ååæ¨è¨è¦æ±ãç¹å¥éæ³¨é¡¯å¾®é¡æ¸æï¼æ¢è¨èçåè§£éæ­¤ä¿¡æ¯çç¹å® AI æè¡ãééæå°æ¸æç°è³ªæ§åè¨»éç¨ç¼ºæ§ç­ææ°ï¼æåæ¦è¿°äºè©²é åçæ½å¨è§£æ±ºæ¹æ¡åæ°è¶¨å¢ãæ¬æä¸»è¦å¾ AI çè§åº¦æ°å¯«ï¼æ¨å¨çºå¨ AIãé¡¯å¾®é¡åçç©å­¸äº¤åé åå·¥ä½çç ç©¶äººå¡æä¾å¯¶è²´çè³æºãå®ç¸½çµäºç¶åçé²å±ãééµè¦è§£åæªè§£æ±ºçåé¡ï¼å¹é¤äºé¼åµè·¨å­¸ç§åä½ççè§£ãééæä¾è©²é åå¨é¢èç°¡æ½çç¶åï¼æ¬ææ¨å¨å¬ååµæ°ãä¿é²è·¨å­¸ç§åèï¼ä¸¦å é AI å¨çå½ç§å­¸ç ç©¶ä¸­çæ¡ç¨ã

##### **FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis**
2501.13967v2 by Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, Hao Chen

Federated domain generalization aims to train a global model from multiple
source domains and ensure its generalization ability to unseen target domains.
Due to the target domain being with unknown domain shifts, attempting to
approximate these gaps by source domains may be the key to improving model
generalization capability. Existing works mainly focus on sharing and
recombining local domain-specific attributes to increase data diversity and
simulate potential domain shifts. However, these methods may be insufficient
since only the local attribute recombination can be hard to touch the
out-of-distribution of global data. In this paper, we propose a
simple-yet-efficient framework named Federated Domain Adversarial Generation
(FedDAG). It aims to simulate the domain shift and improve the model
generalization by adversarially generating novel domains different from local
and global source domains. Specifically, it generates novel-style images by
maximizing the instance-level feature discrepancy between original and
generated images and trains a generalizable task model by minimizing their
feature discrepancy. Further, we observed that FedDAG could cause different
performance improvements for local models. It may be due to inherent data
isolation and heterogeneity among clients, exacerbating the imbalance in their
generalization contributions to the global model. Ignoring this imbalance can
lead the global model's generalization ability to be sub-optimal, further
limiting the novel domain generation procedure. Thus, to mitigate this
imbalance, FedDAG hierarchically aggregates local models at the within-client
and across-client levels by using the sharpness concept to evaluate client
model generalization contributions. Extensive experiments across four medical
benchmarks demonstrate FedDAG's ability to enhance generalization in federated
medical scenarios.

æè¦ï¼è¯é¦é åæ³åæ¨å¨å¾å¤åä¾æºé åè¨ç·´ä¸åå¨çæ¨¡åï¼ä¸¦ç¢ºä¿å¶å°æªè¦ç®æ¨é åçæ³åè½åãç±æ¼ç®æ¨é åå·ææªç¥çé åè½ç§»ï¼å æ­¤åè©¦ééä¾æºé åä¾è¿ä¼¼éäºå·®è·å¯è½æ¯æ¹é²æ¨¡åæ³åè½åçééµãç¾æå·¥ä½ä¸»è¦éä¸­æ¼å±äº«åéæ°çµåæ¬å°ç¹å®é åå±¬æ§ï¼ä»¥å¢å æ¸æå¤æ¨£æ§åæ¨¡æ¬æ½å¨çé åè½ç§»ãç¶èï¼éäºæ¹æ³å¯è½ä¸è¶³ï¼å çºåæ¬å°å±¬æ§éæ°çµåå¯è½é£ä»¥è§¸åå¨çæ¸æçåå¸å¤ãå¨æ¬æä¸­ï¼æåæåºäºä¸åç°¡å®èé«æçæ¡æ¶ï¼ç¨±çºè¯é¦é åå°æçæï¼FedDAGï¼ãå®æ¨å¨æ¨¡æ¬é åè½ç§»ï¼ä¸¦ééå°ææ§å°çæä¸åæ¼æ¬å°åå¨çä¾æºé åçæ°ç©é åä¾æé«æ¨¡åæ³åè½åãå·é«ä¾èªªï¼å®ééæå¤§ååå§åååçæååä¹éçå¯¦ä¾ç´å¥ç¹å¾µå·®ç°ä¾çææ°é¢¨æ ¼çååï¼ä¸¦ééæå°åå®åçç¹å¾µå·®ç°ä¾è¨ç·´ä¸åå¯æ³åçä»»åæ¨¡åãæ­¤å¤ï¼æåè§å¯å° FedDAG å¯è½å°æ¬å°æ¨¡åé æä¸åçæ§è½æ¹é²ãéå¯è½æ¯ç±æ¼å®¢æ¶ç«¯ä¹éåºæçæ¸æéé¢åç°è³ªæ§ï¼å åäºå®åå°å¨çæ¨¡åæ³åè²¢ç»çä¸å¹³è¡¡ãå¿½è¦éç¨®ä¸å¹³è¡¡å¯è½æå°è´å¨çæ¨¡åçæ³åè½åæ¬¡åªï¼é²ä¸æ­¥éå¶æ°åçæéç¨ãå æ­¤ï¼çºäºæ¸è¼éç¨®ä¸å¹³è¡¡ï¼FedDAG ä½¿ç¨æ¸æ°åº¦æ¦å¿µè©ä¼°å®¢æ¶ç«¯æ¨¡åæ³åè²¢ç»ï¼å¨å®¢æ¶ç«¯å§åè·¨å®¢æ¶ç«¯å±¤ç´åå±¤èåæ¬å°æ¨¡åãå¨ååé«å­¸åºæºä¸çå»£æ³å¯¦é©è­æäº FedDAG å¢å¼·è¯é¦é«çå ´æ¯ä¸­æ³åçè½åã

##### **CAND: Cross-Domain Ambiguity Inference for Early Detecting Nuanced Illness Deterioration**
2501.16365v1 by Lo Pang-Yun Ting, Zhen Tan, Hong-Pei Chen, Cheng-Te Li, Po-Lin Chen, Kun-Ta Chuang, Huan Liu

Early detection of patient deterioration is essential for timely treatment,
with vital signs like heart rates being key health indicators. Existing methods
tend to solely analyze vital sign waveforms, ignoring transition relationships
of waveforms within each vital sign and the correlation strengths among various
vital signs. Such studies often overlook nuanced illness deterioration, which
is the early sign of worsening health but is difficult to detect. In this
paper, we introduce CAND, a novel method that organizes the transition
relationships and the correlations within and among vital signs as
domain-specific and cross-domain knowledge. CAND jointly models these knowledge
in a unified representation space, considerably enhancing the early detection
of nuanced illness deterioration. In addition, CAND integrates a Bayesian
inference method that utilizes augmented knowledge from domain-specific and
cross-domain knowledge to address the ambiguities in correlation strengths.
With this architecture, the correlation strengths can be effectively inferred
to guide joint modeling and enhance representations of vital signs. This allows
a more holistic and accurate interpretation of patient health. Our experiments
on a real-world ICU dataset demonstrate that CAND significantly outperforms
existing methods in both effectiveness and earliness in detecting nuanced
illness deterioration. Moreover, we conduct a case study for the interpretable
detection process to showcase the practicality of CAND.

æè¦ï¼æ©æç¼ç¾çæ£æ¡åæå½¢å°æ¼åææ²»çè³ééè¦ï¼
èåå¿çç­çå½å¾µè±¡æ¯ééµçå¥åº·ææ¨ãç¾ææ¹æ³
å¾åæ¼åªåæçå½å¾µè±¡æ³¢å½¢ï¼å¿½ç¥åçå½å¾µè±¡å§æ³¢å½¢çè½æéä¿
ä»¥ååç¨®çå½å¾µè±¡ä¹éçéè¯å¼·åº¦ãæ­¤é¡ç ç©¶å¾å¾å¿½ç¥ç´°å¾®ççæ³æ¡åï¼
éæ¯å¥åº·æ¡åçæ©æå¾µåï¼ä½é£ä»¥åµæ¸¬ãå¨æ¬æä¸­ï¼
æåä»ç´¹ CANDï¼éæ¯ä¸ç¨®æ°æ¹æ³ï¼å¯æ´ççå½å¾µè±¡å§é¨åä¹éçè½æ
éä¿åéè¯ï¼ä½çºç¹å®é ååè·¨é åç¥è­ãCAND è¯åå»ºæ¨¡éäºç¥è­
å¨çµ±ä¸çè¡¨ç¤ºç©ºéä¸­ï¼å¤§å¹æåç´°å¾®çæ³æ¡åçæ©æåµæ¸¬ãæ­¤å¤ï¼CAND æ´åè²æ°
æ¨è«æ¹æ³ï¼å©ç¨ä¾èªç¹å®é ååè·¨é åç¥è­çæ´åç¥è­ä¾è§£æ±ºéè¯å¼·åº¦ä¸­çæ¨¡ç³æ§ã
æäºéåæ¶æ§ï¼éè¯å¼·åº¦å¯ä»¥æææ¨è«ï¼ä»¥å¼å°è¯åå»ºæ¨¡ä¸¦å¢å¼·çå½å¾µè±¡çè¡¨ç¤ºãéåè¨±
æ´å¨é¢ä¸æºç¢ºå°è©®éçæ£å¥åº·ãæåå¨çå¯¦ä¸ç ICU è³æéä¸çå¯¦é©
è­æ CAND å¨åµæ¸¬ç´°å¾®çæ³æ¡åæ¹é¢ï¼ç¡è«å¨æææ§éæ¯æ©æç¼ç¾ä¸é½æé¡¯åªæ¼
ç¾ææ¹æ³ãæ­¤å¤ï¼æåé²è¡ä¸åå¯è©®éåµæ¸¬éç¨çæ¡ä¾ç ç©¶ï¼ä»¥å±ç¤º CAND çå¯¦ç¨æ§ã

##### **Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related to Post COVID-19 Condition**
2501.12538v2 by Juan Andres Medina Florez, Shaina Raza, Rashida Lynn, Zahra Shakeri, Brendan T. Smith, Elham Dolatabadi

Understanding the prevalence, disparities, and symptom variations of Post
COVID-19 Condition (PCC) for vulnerable populations is crucial to improving
care and addressing intersecting inequities. This study aims to develop a
comprehensive framework for integrating social determinants of health (SDOH)
into PCC research by leveraging NLP techniques to analyze disparities and
variations in SDOH representation within PCC case reports. Following
construction of a PCC Case Report Corpus, comprising over 7,000 case reports
from the LitCOVID repository, a subset of 709 reports were annotated with 26
core SDOH-related entity types using pre-trained named entity recognition (NER)
models, human review, and data augmentation to improve quality, diversity and
representation of entity types. An NLP pipeline integrating NER, natural
language inference (NLI), trigram and frequency analyses was developed to
extract and analyze these entities. Both encoder-only transformer models and
RNN-based models were assessed for the NER objective.
  Fine-tuned encoder-only BERT models outperformed traditional RNN-based models
in generalizability to distinct sentence structures and greater class sparsity.
Exploratory analysis revealed variability in entity richness, with prevalent
entities like condition, age, and access to care, and underrepresentation of
sensitive categories like race and housing status. Trigram analysis highlighted
frequent co-occurrences among entities, including age, gender, and condition.
The NLI objective (entailment and contradiction analysis) showed attributes
like "Experienced violence or abuse" and "Has medical insurance" had high
entailment rates (82.4%-80.3%), while attributes such as "Is
female-identifying," "Is married," and "Has a terminal condition" exhibited
high contradiction rates (70.8%-98.5%).

æè¦ï¼äºè§£èå¼±äººç¾¤ç COVID-19 å¾éºç (PCC) çæµè¡çæ³ãå·®ç°åççè®åå°æ¼æ¹åç§è­·åè§£æ±ºäº¤ç¹çä¸å¹³ç­è³ééè¦ãæ¬ç ç©¶æ¨å¨ééå©ç¨èªç¶èªè¨èçæè¡åæ PCC çä¾å ±åä¸­ SDOH çä»£è¡¨æ§å·®ç°åè®åï¼çºå°ç¤¾æå¥åº·æ±ºå®å ç´  (SDOH) æ´åå° PCC ç ç©¶ä¸­å»ºç«ä¸åå¨é¢çæ¶æ§ãå¨å»ºæ§åå«ä¾èª LitCOVID å²å­åº«ç 7,000 å¤ä»½çä¾å ±åç PCC çä¾å ±åèªæåº«å¾ï¼ä½¿ç¨é åè¨ç·´çåç¨±å¯¦é«è­å¥ (NER) æ¨¡åãäººå·¥å¯©æ¥åè³ææ´åå° 709 ä»½å ±åç 26 åæ ¸å¿ SDOH ç¸éå¯¦é«é¡åé²è¡è¨»è§£ï¼ä»¥æé«å¯¦é«é¡åçåè³ªãå¤æ¨£æ§åä»£è¡¨æ§ãéç¼äºä¸åæ´å NERãèªç¶èªè¨æ¨ç (NLI)ãä¸åçµåé »çåæç NLP ç®¡ç·ä¾èåååæéäºå¯¦é«ãè©ä¼°äºåç·¨ç¢¼å¨è½æå¨æ¨¡åååºæ¼ RNN çæ¨¡åç NER ç®æ¨ãç¶éå¾®èª¿çåç·¨ç¢¼å¨ BERT æ¨¡åå¨å°ä¸åå¥å­çµæ§åæ´å¤§çé¡å¥ç¨çæ§çæ¦æ¬æ§æ¹é¢åªæ¼å³çµ±çåºæ¼ RNN çæ¨¡åãæ¢ç´¢æ§åææ­ç¤ºäºå¯¦é«è±å¯åº¦çè®ç°æ§ï¼å¶ä¸­çè¡çå¯¦é«åæ¬çæ³ãå¹´é½¡åç²å¾ç§è­·çæ©æï¼èç¨®æåä½æ¿çæ³ç­ææé¡å¥çä»£è¡¨æ§ä¸è¶³ãä¸åçµåæçªåºäºå¯¦é«ä¹éçé »ç¹å±ç¾ï¼åæ¬å¹´é½¡ãæ§å¥åçæ³ãNLI ç®æ¨ï¼èæ¶µåçç¾åæï¼é¡¯ç¤ºãç¶æ­·éæ´åæèå¾ãåãæé«çä¿éªãç­å±¬æ§å·æå¾é«çèæ¶µçï¼82.4%-80.3%ï¼ï¼èãèªåèªå·±æ¯å¥³æ§ãããå·²å©ãåãææ«æç¾çãç­å±¬æ§åè¡¨ç¾åºå¾é«ççç¾çï¼70.8%-98.5%ï¼ã

##### **Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature Extractor**
2501.12524v1 by Jiaqi Guo, Yunnan Wu, Evangelos Kaimakamis, Georgios Petmezas, Vasileios E. Papageorgiou, Nicos Maglaveras, Aggelos K. Katsaggelos

With the advent of the COVID-19 pandemic, ultrasound imaging has emerged as a
promising technique for COVID-19 detection, due to its non-invasive nature,
affordability, and portability. In response, researchers have focused on
developing AI-based scoring systems to provide real-time diagnostic support.
However, the limited size and lack of proper annotation in publicly available
ultrasound datasets pose significant challenges for training a robust AI model.
This paper proposes MeDiVLAD, a novel pipeline to address the above issue for
multi-level lung-ultrasound (LUS) severity scoring. In particular, we leverage
self-knowledge distillation to pretrain a vision transformer (ViT) without
label and aggregate frame-level features via dual-level VLAD aggregation. We
show that with minimal finetuning, MeDiVLAD outperforms conventional
fully-supervised methods in both frame- and video-level scoring, while offering
classification reasoning with exceptional quality. This superior performance
enables key applications such as the automatic identification of critical lung
pathology areas and provides a robust solution for broader medical video
classification tasks.

æè¦ï¼é¨è COVID-19 å¤§æµè¡çå°ä¾ï¼è¶é³æ³¢å½±åå·²æçºä¸ç¨®æåéç COVID-19 æª¢æ¸¬æè¡ï¼å çºå®å·æéä¾µå¥æ§ãå¹æ ¼å¯¦æ ä¸å¯æå¸¶ç­ç¹æ§ãæéæ¼æ­¤ï¼ç ç©¶äººå¡å°æ³¨æ¼éç¼åºæ¼ AI çè©åç³»çµ±ï¼ä»¥æä¾å³æçè¨ºæ·æ¯æ´ãç¶èï¼å¬éå¯ç¨çè¶é³æ³¢è³æéè¦æ¨¡æéä¸ç¼ºä¹é©ç¶çè¨»è§£ï¼éå°è¨ç·´ç©©å¥ç AI æ¨¡åæ§æéå¤§ææ°ãæ¬ææåº MeDiVLADï¼éæ¯ä¸ç¨®æ°ç©çç®¡éï¼ç¨æ¼è§£æ±ºä¸è¿°å¤å±¤ç´èºé¨è¶é³æ³¢ (LUS) å´éåº¦è©åçè­°é¡ãå·é«ä¾èªªï¼æåå©ç¨èªæç¥è­è¸é¤¾æè¡ï¼å¨æ²ææ¨ç±¤çææ³ä¸é è¨ç·´è¦è¦ºè½æå¨ (ViT)ï¼ä¸¦éééå±¤ç´ VLAD èåä¾å½ç¸½å¹ç´ç¹å¾µãæåè­æï¼ééæå°çå¾®èª¿ï¼MeDiVLAD å¨å¹ç´åå½±çç´è©åä¸­é½åªæ¼å³çµ±çå¨ç£ç£å¼æ¹æ³ï¼åææä¾åè³ªæ¥µä½³çåé¡æ¨çãéç¨®åªç°çæè½æ¯æ´äºééµæç¨ï¼ä¾å¦èªåè­å¥èºé¨çç¶ååï¼ä¸¦çºæ´å»£æ³çé«å­¸å½±çåé¡ä»»åæä¾ç©©å¥çè§£æ±ºæ¹æ¡ã

##### **FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression**
2501.12336v1 by Phuoc Duong Huy Chu

This paper presents results of our system for CoMeDi Shared Task, focusing on
Subtask 2: Disagreement Ranking. Our system leverages sentence embeddings
generated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deep
neural regression model incorporating batch normalization and dropout for
improved generalization. By predicting the mean of pairwise judgment
differences between annotators, our method explicitly targets disagreement
ranking, diverging from traditional "gold label" aggregation approaches. We
optimized our system with a customized architecture and training procedure,
achieving competitive performance in Spearman correlation against mean
disagreement labels. Our results highlight the importance of robust embeddings,
effective model architecture, and careful handling of judgment differences for
ranking disagreement in multilingual contexts. These findings provide insights
into the use of contextualized representations for ordinal judgment tasks and
open avenues for further refinement of disagreement prediction models.

æè¦ï¼æ¬æå±ç¤ºäºæåå¨ CoMeDi å±äº«ä»»åç³»çµ±ä¸­ççµæï¼éé»å¨
å­ä»»å 2ï¼åæ­§æåãæåçç³»çµ±å©ç¨ paraphrase-xlm-r-multilingual-v1 æ¨¡åç¢ççå¥å­åµå¥ï¼çµåæ·±åº¦
ç¥ç¶è¿´æ­¸æ¨¡åï¼ä¸¦å å¥æ¹æ¬¡æ­£è¦ååä¸­æ·ä»¥æ¹åæ¦åãééé æ¸¬è¨»è§£èä¹éæå°å¤æ·å·®ç°çå¹³åå¼ï¼æåç
æ¹æ³æç¢ºéå°åæ­§æåï¼åé¢å³çµ±çãé»éæ¨ç±¤ãèåæ¹æ³ãæåä½¿ç¨èªè¨æ¶æ§åè¨ç·´ç¨åºåªåç³»çµ±ï¼
å¨èå¹³ååæ­§æ¨ç±¤ç Spearman ç¸éæ§ä¸­ç²å¾ç«¶ç­åè¡¨ç¾ãæåççµæå¼·èª¿äºç©©å¥åµå¥ãæææ¨¡åæ¶æ§å
è¬¹æèçå¤æ·å·®ç°å°æ¼å¨å¤èªè¨ç°å¢ä¸­å°åæ­§é²è¡æåçéè¦æ§ãéäºç¼ç¾æä¾äºä½¿ç¨æå¢åè¡¨å¾µé²è¡åºæ¸å¤æ·ä»»åçè¦è§£ï¼ä¸¦çºé²ä¸æ­¥åªååæ­§é æ¸¬æ¨¡åéé¢äºéè·¯ã

##### **CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**
2501.12266v1 by Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves

The main challenges limiting the adoption of deep learning-based solutions in
medical workflows are the availability of annotated data and the lack of
interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the
latter by constraining the final disease prediction on a set of predefined and
human-interpretable concepts. However, the increased interpretability achieved
through these concept-based explanations implies a higher annotation burden.
Moreover, if a new concept needs to be added, the whole system needs to be
retrained. Inspired by the remarkable performance shown by Large
Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet
effective, methodology, CBVLM, which tackles both of the aforementioned
challenges. First, for each concept, we prompt the LVLM to answer if the
concept is present in the input image. Then, we ask the LVLM to classify the
image based on the previous concept predictions. Moreover, in both stages, we
incorporate a retrieval module responsible for selecting the best examples for
in-context learning. By grounding the final diagnosis on the predicted
concepts, we ensure explainability, and by leveraging the few-shot capabilities
of LVLMs, we drastically lower the annotation cost. We validate our approach
with extensive experiments across four medical datasets and twelve LVLMs (both
generic and medical) and show that CBVLM consistently outperforms CBMs and
task-specific supervised methods without requiring any training and using just
a few annotated examples. More information on our project page:
https://cristianopatricio.github.io/CBVLM/.

æè¦ï¼éå¶å¨é«çå·¥ä½æµç¨ä¸­æ¡ç¨åºæ¼æ·±åº¦å­¸ç¿çè§£æ±ºæ¹æ¡çä¸»è¦ææ°æ¯æ¨è¨è³æçå¯ç¨æ§ä»¥åæ­¤é¡ç³»çµ±çå¯è§£éæ§ä¸è¶³ãæ¦å¿µç¶é ¸æ¨¡å (CBM) éééå¶ä¸çµé å®ç¾©ä¸äººé¡å¯è§£éçæ¦å¿µå°æçµç¾çé æ¸¬ï¼ä¾è§£æ±ºå¾èãç¶èï¼éééäºåºæ¼æ¦å¿µçè§£éæå¯¦ç¾çå¯è§£éæ§æåï¼æå³èæ´é«çæ¨è¨è² æãæ­¤å¤ï¼å¦æéè¦æ°å¢ä¸åæ°æ¦å¿µï¼åéè¦éæ°è¨ç·´æ´åç³»çµ±ãåå°å¤§åè¦è¦ºèªè¨æ¨¡å (LVLMs) å¨å°æ¨£æ¬è¨­å®ä¸­å±ç¾çåè¶æè½åç¼ï¼æåæåºäºä¸åç°¡å®ä½ææç CBVLM æ¹æ³ï¼ä¾è§£æ±ºä¸è¿°å©åææ°ãé¦åï¼å°æ¼æ¯åæ¦å¿µï¼æåæç¤º LVLM åç­è¼¸å¥å½±åä¸­æ¯å¦åå«è©²æ¦å¿µãç¶å¾ï¼æåè¦æ± LVLM æ ¹æååçæ¦å¿µé æ¸¬å°å½±åé²è¡åé¡ãæ­¤å¤ï¼å¨å©åéæ®µä¸­ï¼æåé½ç´å¥ä¸åæª¢ç´¢æ¨¡çµï¼è² è²¬é¸åºæé©åæ¼æå¢å­¸ç¿çç¯ä¾ãééå°æçµè¨ºæ·å»ºç«å¨é æ¸¬æ¦å¿µä¹ä¸ï¼æåç¢ºä¿äºå¯è§£éæ§ï¼ä¸¦ééå©ç¨ LVLMs çå°æ¨£æ¬è½åï¼æåå¤§å¹éä½äºæ¨è¨ææ¬ãæåééååé«çè³æéååäºå LVLMï¼éç¨åé«çï¼çå»£æ³å¯¦é©é©è­äºæåçä½æ³ï¼ä¸¦é¡¯ç¤º CBVLM å¨ç¡éä»»ä½è¨ç·´ä¸åä½¿ç¨å°æ¸æ¨è¨ç¯ä¾çææ³ä¸ï¼å§çµåªæ¼ CBM åç¹å®æ¼ä»»åçç£ç£å¼æ¹æ³ãæ´å¤è³è¨è«è¦æåçå°æ¡é é¢ï¼https://cristianopatricio.github.io/CBVLM/ã

##### **Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes**
2501.12106v1 by Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer

Tumor documentation in Germany is largely done manually, requiring reading
patient records and entering data into structured databases. Large language
models (LLMs) could potentially enhance this process by improving efficiency
and reliability. This evaluation tests eleven different open source LLMs with
sizes ranging from 1-70 billion model parameters on three basic tasks of the
tumor documentation process: identifying tumor diagnoses, assigning ICD-10
codes, and extracting the date of first diagnosis. For evaluating the LLMs on
these tasks, a dataset of annotated text snippets based on anonymized doctors'
notes from urology was prepared. Different prompting strategies were used to
investigate the effect of the number of examples in few-shot prompting and to
explore the capabilities of the LLMs in general. The models Llama 3.1 8B,
Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.
Models with less extensive training data or having fewer than 7 billion
parameters showed notably lower performance, while larger models did not
display performance gains. Examples from a different medical domain than
urology could also improve the outcome in few-shot prompting, which
demonstrates the ability of LLMs to handle tasks needed for tumor
documentation. Open source LLMs show a strong potential for automating tumor
documentation. Models from 7-12 billion parameters could offer an optimal
balance between performance and resource efficiency. With tailored fine-tuning
and well-designed prompting, these models might become important tools for
clinical documentation in the future. The code for the evaluation is available
from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset
as a new valuable resource that addresses the shortage of authentic and easily
accessible benchmarks in German-language medical NLP.

æè¦ï¼å¾·åçè«ç¤æä»¶è¨éå¤§é¨åæ¯æåå®æï¼éè¦é±è®çæ­·ä¸¦å°è³æè¼¸å¥çµæ§åçè³æåº«ä¸­ãå¤§åèªè¨æ¨¡å (LLM) å¯è½ééæåæçåå¯é æ§ä¾å¢å¼·æ­¤ç¨åºãæ­¤è©éæ¸¬è©¦äº 11 åä¸åçéæº LLMï¼æ¨¡ååæ¸å¤§å°å¾ 10 åå° 700 åä¸ç­ï¼éå°è«ç¤æä»¶è¨éç¨åºçä¸é åºæ¬ä»»åï¼è­å¥è«ç¤è¨ºæ·ãæå® ICD-10 ä»£ç¢¼ï¼ä»¥åæ·åé¦æ¬¡è¨ºæ·æ¥æãçºäºéå°éäºä»»åè©ä¼° LLMï¼æºåäºä¸ååºæ¼æ³å°¿ç§é«çå¿åç­è¨çè¨»è§£æå­çæ®µè³æéãä½¿ç¨ä¸åçæç¤ºç­ç¥ä¾èª¿æ¥å°éæç¤ºä¸­ç¯ä¾æ¸éçå½±é¿ï¼ä¸¦æ¢ç´¢ LLM çä¸è¬è½åãLlama 3.1 8BãMistral 7B å Mistral NeMo 12 B ç­æ¨¡åå¨éäºä»»åä¸­è¡¨ç¾ç¸ç¶å¥½ãè¨ç·´è³æè¼å°æåæ¸å°æ¼ 70 åçæ¨¡åè¡¨ç¾æé¡¯è¼å·®ï¼èè¼å¤§çæ¨¡åä¸¦æªå±ç¾æè½æåãèæ³å°¿ç§ä¸åçé«çé åçç¯ä¾ä¹å¯ä»¥æ¹åå°éæç¤ºççµæï¼éè­æäº LLM èçè«ç¤æä»¶è¨éæéä»»åçè½åãéæº LLM å¨èªååè«ç¤æä»¶è¨éæ¹é¢é¡¯ç¤ºåºå¼·å¤§çæ½åãåæ¸ä»æ¼ 70 åå° 120 åçæ¨¡åå¯ä»¥å¨æè½åè³æºæçä¹éæä¾æä½³å¹³è¡¡ãéééèº«æé å¾®èª¿åç²¾å¿è¨­è¨çæç¤ºï¼éäºæ¨¡åæªä¾å¯è½ææçºè¨åºæä»¶è¨éçéè¦å·¥å·ãè©ä¼°ç¨å¼ç¢¼å¯å¾ https://github.com/stefan-m-lenz/UroLlmEval åå¾ãæåä¹éåºè³æéä½çºä¸åæ°çæå¹å¼è³æºï¼ç¨æ¼è§£æ±ºå¾·èªé«çèªç¶èªè¨èçä¸­çå¯¦ä¸ææ¼åå¾çåºæºç­ç¼ºåé¡ã

##### **Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET**
2501.12425v1 by Fatih Aksu, Fabrizia Gelardi, Arturo Chiti, Paolo Soda

Accurate classification of histological subtypes of non-small cell lung
cancer (NSCLC) is essential in the era of precision medicine, yet current
invasive techniques are not always feasible and may lead to clinical
complications. This study presents a multi-stage intermediate fusion approach
to classify NSCLC subtypes from CT and PET images. Our method integrates the
two modalities at different stages of feature extraction, using voxel-wise
fusion to exploit complementary information across varying abstraction levels
while preserving spatial correlations. We compare our method against unimodal
approaches using only CT or PET images to demonstrate the benefits of modality
fusion, and further benchmark it against early and late fusion techniques to
highlight the advantages of intermediate fusion during feature extraction.
Additionally, we compare our model with the only existing intermediate fusion
method for histological subtype classification using PET/CT images. Our results
demonstrate that the proposed method outperforms all alternatives across key
metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This
non-invasive approach has the potential to significantly improve diagnostic
accuracy, facilitate more informed treatment decisions, and advance
personalized care in lung cancer management.

æè¦ï¼å¨ç²¾æºé«ççæä»£ï¼æºç¢ºåé¡éå°ç´°èèºç (NSCLC) ççµç¹å­¸äºåè³ééè¦ï¼ä½ç®åçä¾µå¥æ§æè¡ä¸¦ä¸ç¸½æ¯å¯è¡ï¼ä¸å¯è½æå°è´è¨åºä½µç¼çãæ¬ç ç©¶æåºäºä¸ç¨®å¤éæ®µä¸­éèåæ¹æ³ï¼å¾é»è¦æ·å±¤ (CT) åæ­£å­æ·å±¤ææ (PET) å½±åä¸­åé¡ NSCLC äºåãæåçæè¡å¨ç¹å¾µèåçä¸åéæ®µæ´åéå©ç¨®æ¹å¼ï¼å©ç¨éé«ç´ èåä¾å©ç¨ä¸åæ½è±¡å±¤ç´çäºè£è³è¨ï¼åæä¿çç©ºéç¸éæ§ãæåå°æåçæè¡èåä½¿ç¨é»è¦æ·å±¤ææ­£å­æ·å±¤ææå½±åçå®ä¸æ¨¡å¼æ¹æ³é²è¡æ¯è¼ï¼ä»¥è­ææ¨¡å¼èåçåªé»ï¼ä¸¦é²ä¸æ­¥å°å¶èæ©æåææèåæè¡é²è¡æ¯è¼ï¼ä»¥å¼·èª¿ç¹å¾µèåæéä¸­éèåçåªé»ãæ­¤å¤ï¼æåå°æåçæ¨¡åèå¯ä¸ç¾æçä¸­éèåæ¹æ³é²è¡æ¯è¼ï¼è©²æ¹æ³ä½¿ç¨æ­£å­æ·å±¤ææ/é»è¦æ·å±¤ææå½±åé²è¡çµç¹å­¸äºååé¡ãæåççµæè¡¨æï¼ææåºçæ¹æ³å¨æææ¿ä»£æ¹æ¡ä¸­è¡¨ç¾åªç°ï¼æºç¢ºçå AUC åå¥ç­æ¼ 0.724 å 0.681ãéç¨®éä¾µå¥æ§æ¹æ³æå¯è½é¡¯èæé«è¨ºæ·æºç¢ºçï¼ä¿é²æ´ææºçæ²»çæ±ºç­ï¼ä¸¦æ¨é²èºçç®¡çä¸­çåäººåç§è­·ã

##### **Adaptive Class Learning to Screen Diabetic Disorders in Fundus Images of Eye**
2501.12048v1 by Shramana Dey, Pallabi Dutta, Riddhasree Bhattacharyya, Surochita Pal, Sushmita Mitra, Rajiv Raman

The prevalence of ocular illnesses is growing globally, presenting a
substantial public health challenge. Early detection and timely intervention
are crucial for averting visual impairment and enhancing patient prognosis.
This research introduces a new framework called Class Extension with Limited
Data (CELD) to train a classifier to categorize retinal fundus images. The
classifier is initially trained to identify relevant features concerning
Healthy and Diabetic Retinopathy (DR) classes and later fine-tuned to adapt to
the task of classifying the input images into three classes: Healthy, DR, and
Glaucoma. This strategy allows the model to gradually enhance its
classification capabilities, which is beneficial in situations where there are
only a limited number of labeled datasets available. Perturbation methods are
also used to identify the input image characteristics responsible for
influencing the models decision-making process. We achieve an overall accuracy
of 91% on publicly available datasets.

æè¦ï¼å¨çç¼ç¾æ£ççæçºä¸åï¼å°å¬å±è¡çé æéå¤§ææ°ãæ©æç¼ç¾ååæå¹²é å°æ¼é é²è¦åéç¤åæ¹åæ£èé å¾è³ééè¦ãæ¬ç ç©¶æåºäºä¸ååçºæéæ¸æé¡å¥æ´å± (CELD) çæ°æ¡æ¶ï¼ç¨æ¼è¨ç·´åé¡å¨å°è¦ç¶²èç¼åºååé²è¡åé¡ãè©²åé¡å¨æåæ¥åè¨ç·´ä»¥è­å¥èå¥åº·åç³å°¿çè¦ç¶²èçè® (DR) é¡å¥ç¸éçç¹å¾µï¼ç¶å¾é²è¡å¾®èª¿ä»¥é©æå°è¼¸å¥åååé¡çºä¸é¡çä»»åï¼å¥åº·ãDR åéåç¼ãæ­¤ç­ç¥åè¨±æ¨¡åéæ­¥å¢å¼·å¶åé¡è½åï¼éå¨æ¨è¨æ¸æéæ¸éæéçææ³ä¸æ¯æççãæ¾åæ¹æ³ä¹ç¨æ¼è­å¥è² è²¬å½±é¿æ¨¡åæ±ºç­éç¨çè¼¸å¥ååç¹å¾µãæåå¨å¬éæ¸æéä¸å¯¦ç¾äº 91% çæ´é«æºç¢ºåº¦ã

##### **Tackling Small Sample Survival Analysis via Transfer Learning: A Study of Colorectal Cancer Prognosis**
2501.12421v1 by Yonghao Zhao, Changtao Li, Chi Shu, Qingbin Wu, Hong Li, Chuan Xu, Tianrui Li, Ziqiang Wang, Zhipeng Luo, Yazhou He

Survival prognosis is crucial for medical informatics. Practitioners often
confront small-sized clinical data, especially cancer patient cases, which can
be insufficient to induce useful patterns for survival predictions. This study
deals with small sample survival analysis by leveraging transfer learning, a
useful machine learning technique that can enhance the target analysis with
related knowledge pre-learned from other data. We propose and develop various
transfer learning methods designed for common survival models. For parametric
models such as DeepSurv, Cox-CC (Cox-based neural networks), and DeepHit
(end-to-end deep learning model), we apply standard transfer learning
techniques like pretraining and fine-tuning. For non-parametric models such as
Random Survival Forest, we propose a new transfer survival forest (TSF) model
that transfers tree structures from source tasks and fine-tunes them with
target data. We evaluated the transfer learning methods on colorectal cancer
(CRC) prognosis. The source data are 27,379 SEER CRC stage I patients, and the
target data are 728 CRC stage I patients from the West China Hospital. When
enhanced by transfer learning, Cox-CC's $C^{td}$ value was boosted from 0.7868
to 0.8111, DeepHit's from 0.8085 to 0.8135, DeepSurv's from 0.7722 to 0.8043,
and RSF's from 0.7940 to 0.8297 (the highest performance). All models trained
with data as small as 50 demonstrated even more significant improvement.
Conclusions: Therefore, the current survival models used for cancer prognosis
can be enhanced and improved by properly designed transfer learning techniques.
The source code used in this study is available at
https://github.com/YonghaoZhao722/TSF.

æè¦ï¼<paragraph>å­æ´»é æ¸¬å°é«çè³è¨å­¸è³ééè¦ãå¯¦åå·¥ä½èç¶å¸¸é¢å°å°è¦æ¨¡çè¨åºè³æï¼ç¹å¥æ¯çççæ£åæ¡ï¼éäºè³æå¯è½ä¸è¶³ä»¥èªç¼æç¨çæ¨¡å¼ä¾é²è¡å­æ´»é æ¸¬ãæ­¤ç ç©¶ééå©ç¨è½ç§»å­¸ç¿ä¾èçå°æ¨£æ¬å­æ´»åæï¼éæ¯ä¸ç¨®æç¨çæ©å¨å­¸ç¿æè¡ï¼å¯ä»¥ééå¾å¶ä»è³æé åå­¸ç¿å°çç¸éç¥è­ä¾å¢å¼·ç®æ¨åæãæåæåºä¸¦éç¼åç¨®å°çºå¸¸è¦å­æ´»æ¨¡åè¨­è¨çè½ç§»å­¸ç¿æ¹æ³ãå°æ¼åæ¸åæ¨¡åï¼ä¾å¦ DeepSurvãCox-CCï¼åºæ¼ Cox çç¥ç¶ç¶²è·¯ï¼å DeepHitï¼ç«¯å°ç«¯æ·±åº¦å­¸ç¿æ¨¡åï¼ï¼æåæç¨æ¨æºè½ç§»å­¸ç¿æè¡ï¼ä¾å¦é è¨ç·´åå¾®èª¿ãå°æ¼éåæ¸åæ¨¡åï¼ä¾å¦é¨æ©å­æ´»æ£®æï¼æåæåºä¸åæ°çè½ç§»å­æ´»æ£®æï¼TSFï¼æ¨¡åï¼å®å¾ä¾æºä»»åå³è¼¸æ¨¹ççµæ§ï¼ä¸¦ä½¿ç¨ç®æ¨è³æå¾®èª¿å®åãæåå¨çµç´è¸çï¼CRCï¼é å¾ä¸è©ä¼°äºè½ç§»å­¸ç¿æ¹æ³ãä¾æºè³æçº 27,379 å SEER CRC ç¬¬ä¸ææ£èï¼ç®æ¨è³æçºä¾èªä¸­åè¥¿é¨é«é¢ç 728 å CRC ç¬¬ä¸ææ£èãå¨ééè½ç§»å­¸ç¿å¢å¼·å¾ï¼Cox-CC ç $C^{td}$ å¼å¾ 0.7868 æåå° 0.8111ï¼DeepHit çå¾ 0.8085 æåå° 0.8135ï¼DeepSurv çå¾ 0.7722 æåå° 0.8043ï¼RSF çå¾ 0.7940 æåå° 0.8297ï¼æé«æè½ï¼ãææä»¥å°è³ 50 çè³æè¨ç·´çæ¨¡åé½å±ç¤ºåºæ´é¡¯èçé²æ­¥ãçµè«ï¼å æ­¤ï¼ç®åç¨æ¼ççé å¾çå­æ´»æ¨¡åå¯ä»¥ééé©ç¶è¨­è¨çè½ç§»å­¸ç¿æè¡ä¾å¢å¼·åæ¹åãæ¬ç ç©¶ä¸­ä½¿ç¨çåå§ç¢¼å¯å¨ https://github.com/YonghaoZhao722/TSF åå¾ã</paragraph>

##### **Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)**
2501.13957v1 by Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene Kizilcec, Dennis Shung

Introduction. Objective Structured Clinical Examinations (OSCEs) are widely
used to assess medical students' communication skills, but scoring
interview-based assessments is time-consuming and potentially subject to human
bias. This study explored the potential of large language models (LLMs) to
automate OSCE evaluations using the Master Interview Rating Scale (MIRS).
  Methods. We compared the performance of four state-of-the-art LLMs (GPT-4o,
Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts
across all 28 items of the MIRS under the conditions of zero-shot,
chain-of-thought (CoT), few-shot, and multi-step prompting. The models were
benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores
available. Model performance was measured using three accuracy metrics (exact,
off-by-one, thresholded).
  Results. Averaging across all MIRS items and OSCE cases, LLMs performed with
low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy
(0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature
parameter ensured high intra-rater reliability ($\alpha = 0.98$ for GPT-4o).
CoT, few-shot, and multi-step techniques proved valuable when tailored to
specific assessment items. The performance was consistent across MIRS items
independent of encounter phases and communication domains.
  Conclusion. We demonstrated the feasibility of AI-assisted OSCE evaluation
and provided benchmarking of multiple LLMs across multiple prompt techniques.
Our work provides a baseline performance assessment for LLMs that lays a
foundation for future research in automated assessment of clinical
communication skills.

æè¦ï¼<paragraph>ç·è«ãå®¢è§çµæ§åè¨åºèè©¦ (OSCE) å»£æ³ç¨æ¼è©éé«å­¸ççæºéæå·§ï¼ä½è©ååºæ¼è¨ªè«çè©ééå¸¸èæï¼ä¸æ½å¨åå°äººé¡åè¦çå½±é¿ãæ¬ç ç©¶æ¢è¨å¤§åèªè¨æ¨¡å (LLM) ä½¿ç¨å¤§å¸«è¨ªè«è©åéè¡¨ (MIRS) èªåå OSCE è©éçå¯è½æ§ã
æ¹æ³ãæåæ¯è¼äºåç¨®æåé²ç LLMï¼GPT-4oãClaude 3.5ãLlama 3.1 å Gemini 1.5 Proï¼å¨è©é OSCE æç¸¾å®çè¡¨ç¾ï¼ç¯åæ¶µè MIRS çææ 28 åé ç®ï¼æ¢ä»¶çºé¶æ¬¡å­¸ç¿ãæèé (CoT)ãå°æ¬¡å­¸ç¿åå¤æ­¥é©æç¤ºãéäºæ¨¡åä»¥ 10 å OSCE æ¡ä¾çè³æéçºåºæºï¼å¶ä¸­æ 174 åå°å®¶å±è­åæ¸å¯ç¨ãæ¨¡åè¡¨ç¾ä½¿ç¨ä¸åæºç¢ºæ§ææ¨ï¼å®å¨ãåé¢ä¸ãé¾å¼ï¼é²è¡è¡¡éã
çµæãå¹³åææ MIRS é ç®å OSCE æ¡ä¾ï¼LLM çå®å¨æºç¢ºæ§ä½ï¼0.27 å° 0.44ï¼ï¼åé¢ä¸æºç¢ºæ§ä¸­ç­è³é«ï¼0.67 å° 0.87ï¼ï¼é¾å¼æºç¢ºæ§é«ï¼0.75 å° 0.88ï¼ãé¶æº«åº¦åæ¸ç¢ºä¿äºå¾é«çè©åèå§é¨ä¿¡åº¦ï¼GPT-4o ç Î± = 0.98ï¼ãç¶éå°ç¹å®è©éé ç®é²è¡èª¿æ´æï¼CoTãå°æ¬¡å­¸ç¿åå¤æ­¥é©æè¡è¢«è­ææ¯æå¹å¼çãè¡¨ç¾è MIRS é ç®ä¸è´ï¼èé­ééæ®µåæºéé åç¡éã
çµè«ãæåå±ç¤ºäº AI è¼å© OSCE è©éçå¯è¡æ§ï¼ä¸¦æä¾äºå¤ç¨®æç¤ºæè¡ç LLM åºæºæ¸¬è©¦ãæåçç ç©¶çº LLM æä¾äºåºæºè¡¨ç¾è©éï¼çºè¨åºæºéæå·§èªååè©éçæªä¾ç ç©¶å¥ å®äºåºç¤ã</paragraph>

##### **Data-driven Detection and Evaluation of Damages in Concrete Structures: Using Deep Learning and Computer Vision**
2501.11836v1 by Saeid Ataei, Saeed Adibnazari, Seyyed Taghi Ataei

Structural integrity is vital for maintaining the safety and longevity of
concrete infrastructures such as bridges, tunnels, and walls. Traditional
methods for detecting damages like cracks and spalls are labor-intensive,
time-consuming, and prone to human error. To address these challenges, this
study explores advanced data-driven techniques using deep learning for
automated damage detection and analysis. Two state-of-the-art instance
segmentation models, YOLO-v7 instance segmentation and Mask R-CNN, were
evaluated using a dataset comprising 400 images, augmented to 10,995 images
through geometric and color-based transformations to enhance robustness. The
models were trained and validated using a dataset split into 90% training set,
validation and test set 10%. Performance metrics such as precision, recall,
mean average precision (mAP@0.5), and frames per second (FPS) were used for
evaluation. YOLO-v7 achieved a superior mAP@0.5 of 96.1% and processed 40 FPS,
outperforming Mask R-CNN, which achieved a mAP@0.5 of 92.1% with a slower
processing speed of 18 FPS. The findings recommend YOLO-v7 instance
segmentation model for real-time, high-speed structural health monitoring,
while Mask R-CNN is better suited for detailed offline assessments. This study
demonstrates the potential of deep learning to revolutionize infrastructure
maintenance, offering a scalable and efficient solution for automated damage
detection.

æè¦ï¼çµæ§å®æ´æ§å°æ¼ç¶­è­·æ©æ¨ãé§éåçå£ç­æ··åååºç¤è¨­æ½çå®å¨æ§åä½¿ç¨å£½å½è³ééè¦ãå³çµ±çæå£æª¢æ¸¬æ¹æ³ï¼ä¾å¦è£ç¸«ååè½ï¼éè¦å¤§éäººå·¥ï¼èæä¸å®¹æåºç¾äººçºé¯èª¤ãçºäºæå°éäºææ°ï¼æ¬ç ç©¶æ¢è¨äºä½¿ç¨æ·±åº¦å­¸ç¿çåé²æ¸æé©åæè¡ï¼ç¨æ¼èªåæå£æª¢æ¸¬ååæãä½¿ç¨åå« 400 å¼µååçæ¸æéè©ä¼°äºå©åæåé²çå¯¦ä¾åå²æ¨¡åï¼YOLO-v7 å¯¦ä¾åå²å Mask R-CNNï¼ééå¹¾ä½ååºæ¼é¡è²çè½ææ´å±å° 10,995 å¼µååï¼ä»¥å¢å¼·é­¯æ£æ§ãä½¿ç¨åçº 90% è¨ç·´éãé©è­åæ¸¬è©¦é 10% çæ¸æéè¨ç·´åé©è­æ¨¡åãä½¿ç¨ç²¾ç¢ºåº¦ãå¬åçãå¹³åå¹³åç²¾ç¢ºåº¦ (mAP@0.5) åæ¯ç§å¹æ¸ (FPS) ç­æ§è½ææ¨é²è¡è©ä¼°ãYOLO-v7 éå°äº 96.1% çåªç° mAP@0.5ï¼ä¸¦èçäº 40 FPSï¼åªæ¼ Mask R-CNNï¼å¾èä»¥ 18 FPS çè¼æ¢èçéåº¦éå°äº 92.1% ç mAP@0.5ãç ç©¶çµææ¨è¦ä½¿ç¨ YOLO-v7 å¯¦ä¾åå²æ¨¡åé²è¡å¯¦æãé«éçµæ§å¥åº·ç£æ¸¬ï¼è Mask R-CNN æ´é©åè©³ç´°çé¢ç·è©ä¼°ãæ¬ç ç©¶å±ç¤ºäºæ·±åº¦å­¸ç¿å¨åºç¤è¨­æ½ç¶­è­·æ¹é¢å·æé©å½æ§çæ½åï¼çºèªåæå£æª¢æ¸¬æä¾äºä¸åå¯æ´å±ä¸é«æçè§£æ±ºæ¹æ¡ã

##### **GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease**
2501.11715v1 by Wenjie Kang, Lize Jiskoot, Peter De Deyn, Geert Biessels, Huiberdina Koek, Jurgen Claassen, Huub Middelkoop, Wiesje Flier, Willemijn J. Jansen, Stefan Klein, Esther Bron

Deep learning methods based on Convolutional Neural Networks (CNNs) have
shown great potential to improve early and accurate diagnosis of Alzheimer's
disease (AD) dementia based on imaging data. However, these methods have yet to
be widely adopted in clinical practice, possibly due to the limited
interpretability of deep learning models. The Explainable Boosting Machine
(EBM) is a glass-box model but cannot learn features directly from input
imaging data. In this study, we propose a novel interpretable model that
combines CNNs and EBMs for the diagnosis and prediction of AD. We develop an
innovative training strategy that alternatingly trains the CNN component as a
feature extractor and the EBM component as the output block to form an
end-to-end model. The model takes imaging data as input and provides both
predictions and interpretable feature importance measures. We validated the
proposed model on the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset and the Health-RI Parelsnoer Neurodegenerative Diseases Biobank (PND)
as an external testing set. The proposed model achieved an area-under-the-curve
(AUC) of 0.956 for AD and control classification, and 0.694 for the prediction
of conversion of mild cognitive impairment (MCI) to AD on the ADNI cohort. The
proposed model is a glass-box model that achieves a comparable performance with
other state-of-the-art black-box models. Our code is publicly available at:
https://anonymous.4open.science/r/GL-ICNN.

æè¦ï¼<paragraph>åºæ¼å·ç©ç¥ç¶ç¶²è·¯ (CNN) çæ·±åº¦å­¸ç¿æ¹æ³å·²é¡¯ç¤ºåºæ¥µå¤§çæ½åï¼å¯æ ¹æå½±åè³ææ¹åé¿è²æµ·é»ç (AD) å¤±æºççæ©ææºç¢ºè¨ºæ·ãç¶èï¼éäºæ¹æ³å°æªå»£æ³æç¨æ¼è¨åºå¯¦åä¸­ï¼éå¯è½æ¯ç±æ¼æ·±åº¦å­¸ç¿æ¨¡åçå¯è§£éæ§æéãå¯è§£éæåæ© (EBM) æ¯åç»ççæ¨¡åï¼ä½ç¡æ³ç´æ¥å¾è¼¸å¥å½±åè³æä¸­å­¸ç¿ç¹å¾µãå¨éé ç ç©¶ä¸­ï¼æåæåºä¸åçµå CNN å EBM çæ°å¯è§£éæ¨¡åï¼ç¨æ¼è¨ºæ·åé æ¸¬ ADãæåéç¼äºä¸ç¨®åµæ°çè¨ç·´ç­ç¥ï¼äº¤æ¿è¨ç·´ CNN çµä»¶ä½çºç¹å¾µèåå¨ï¼ä¸¦è¨ç·´ EBM çµä»¶ä½çºè¼¸åºåå¡ï¼ä»¥å½¢æç«¯å°ç«¯æ¨¡åãæ­¤æ¨¡åå°å½±åè³æä½çºè¼¸å¥ï¼ä¸¦æä¾é æ¸¬åå¯è§£éçç¹å¾µéè¦æ§æ¸¬éãæåå¨é¿è²æµ·é»çç¥ç¶å½±åå¡è­° (ADNI) è³æéå Health-RI Parelsnoer ç¥ç¶éåç¾ççç©è³æåº« (PND) ä¸é©è­äºææåºçæ¨¡åï¼ä½çºå¤é¨æ¸¬è©¦éãææåºçæ¨¡åå¨ AD åå°ç§åé¡ä¸­éå°äº 0.956 çæ²ç·ä¸é¢ç© (AUC)ï¼ä¸¦å¨ ADNI éåä¸­é æ¸¬è¼åº¦èªç¥éç¤ (MCI) è½åçº AD æéå°äº 0.694ãææåºçæ¨¡åæ¯ä¸åç»ççæ¨¡åï¼å¶æè½èå¶ä»æåé²çé»çæ¨¡åç¸ç¶ãæåçç¨å¼ç¢¼å¯å¨ä»¥ä¸ç¶²åå¬éåå¾ï¼https://anonymous.4open.science/r/GL-ICNNã</paragraph>

##### **Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)**
2501.11705v1 by Brian E. Perron, Lauri Goldkind, Zia Qi, Bryan G. Victor

This paper examines the responsible integration of artificial intelligence
(AI) in human services organizations (HSOs), proposing a nuanced framework for
evaluating AI applications across multiple dimensions of risk. The authors
argue that ethical concerns about AI deployment -- including professional
judgment displacement, environmental impact, model bias, and data laborer
exploitation -- vary significantly based on implementation context and specific
use cases. They challenge the binary view of AI adoption, demonstrating how
different applications present varying levels of risk that can often be
effectively managed through careful implementation strategies. The paper
highlights promising solutions, such as local large language models, that can
facilitate responsible AI integration while addressing common ethical concerns.
The authors propose a dimensional risk assessment approach that considers
factors like data sensitivity, professional oversight requirements, and
potential impact on client wellbeing. They conclude by outlining a path forward
that emphasizes empirical evaluation, starting with lower-risk applications and
building evidence-based understanding through careful experimentation. This
approach enables organizations to maintain high ethical standards while
thoughtfully exploring how AI might enhance their capacity to serve clients and
communities effectively.

æè¦ï¼æ¬ææ¢è¨äºäººå·¥æºæ§ (AI) å¨äººé¡æåçµç¹ (HSO) ä¸­è² è²¬ä»»çæ´åï¼æåºäºä¸åç´°ç·»çæ¡æ¶ï¼ç¨æ¼è©ä¼° AI æç¨å¨å¤åé¢¨éªç¶­åº¦ãä½èèªçºï¼å° AI é¨ç½²çéå¾·èéââåæ¬å°æ¥­å¤æ·çåä»£ãç°å¢å½±é¿ãæ¨¡ååå·®åè³æå·¥ä½èçååââææ ¹æå¯¦æ½èæ¯åå·é«ä½¿ç¨æ¡ä¾èæé¡¯èçä¸åãä»åææ°äº AI æ¡ç¨äºåè«çè§é»ï¼èªªæäºä¸åçæç¨å¦ä½åç¾ä¸åç¨åº¦çé¢¨éªï¼èéäºé¢¨éªéå¸¸å¯ä»¥ééä»ç´°çå¯¦æ½ç­ç¥ä¾ææç®¡çãæ¬æéé»ä»ç´¹äºæåæ¯çè§£æ±ºæ¹æ¡ï¼ä¾å¦æ¬å°å¤§åèªè¨æ¨¡åï¼å®å¯ä»¥å¨è§£æ±ºå¸¸è¦çéå¾·åé¡çåæï¼ä¿é²è² è²¬ä»»ç AI æ´åãä½èæåºäºä¸ç¨®ç¶­åº¦é¢¨éªè©ä¼°æ¹æ³ï¼è©²æ¹æ³èæ®äºè³æææåº¦ãå°æ¥­ç£ç£éæ±åå°å®¢æ¶ç¦ç¥çæ½å¨å½±é¿ç­å ç´ ãä»åæå¾æ¦è¿°äºä¸æ¢åé²çéè·¯ï¼å¼·èª¿å¯¦è­è©ä¼°ï¼å¾ä½é¢¨éªæç¨éå§ï¼ä¸¦ééä»ç´°çå¯¦é©å»ºç«åºæ¼è­æççè§£ãéç¨®æ¹æ³ä½¿çµç¹è½å¤ å¨æ·±æçæ®å°æ¢è¨ AI å¦ä½å¢å¼·å¶æææåå®¢æ¶åç¤¾ç¾¤çè½åçåæï¼ç¶­æé«éå¾·æ¨æºã

##### **Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data**
2501.11695v1 by Majid Farhadloo, Arun Sharma, Alexey Leontovich, Svetomir N. Markovic, Shashi Shekhar

Given multi-type point maps from different place-types (e.g., tumor regions),
our objective is to develop a classifier trained on the source place-type to
accurately distinguish between two classes of the target place-type based on
their point arrangements. This problem is societally important for many
applications, such as generating clinical hypotheses for designing new
immunotherapies for cancer treatment. The challenge lies in the spatial
variability, the inherent heterogeneity and variation observed in spatial
properties or arrangements across different locations (i.e., place-types).
Previous techniques focus on self-supervised tasks to learn domain-invariant
features and mitigate domain differences; however, they often neglect the
underlying spatial arrangements among data points, leading to significant
discrepancies across different place-types. We explore a novel multi-task
self-learning framework that targets spatial arrangements, such as spatial
mix-up masking and spatial contrastive predictive coding, for
spatially-delineated domain-adapted AI classification. Experimental results on
real-world datasets (e.g., oncology data) show that the proposed framework
provides higher prediction accuracy than baseline methods.

æè¦ï¼å¾ä¸åé¡åçé»åï¼ä¾å¦ï¼è«ç¤ååï¼ä¸­çµ¦å®å¤é¡åé»åï¼
æåçç®æ¨æ¯éç¼ä¸åå¨ä¾æºé¡åä¸è¨ç·´çåé¡å¨ï¼ä»¥
æ ¹æå¶é»æåæºç¢ºååç®æ¨é¡åä¸­çå©é¡ãéååé¡å°æ¼è¨±å¤
æç¨ä¾èªªå·æç¤¾æéè¦æ§ï¼ä¾å¦çºççæ²»çè¨­è¨æ°çåç«çæ³èçæè¨åºåè¨­ãææ°å¨æ¼ç©ºé
è®ç°æ§ãåºæçç°è³ªæ§åå¨ä¸åä½ç½®ï¼å³é¡åï¼ä¸­è§å¯å°çç©ºé
å±¬æ§ææåçè®åãååçæè¡å°æ³¨æ¼èªç£ç£ä»»åä»¥å­¸ç¿ä¸è®é å
ç¹å¾µä¸¦æ¸è¼é åå·®ç°ï¼ç¶èï¼å®åéå¸¸å¿½è¦æ¸æé»ä¹éç
åºå±¤ç©ºéæåï¼å°è´ä¸åé¡åä¹éå­å¨é¡¯èå·®ç°ãæåæ¢ç´¢äºä¸ç¨®æ°ç©çå¤ä»»å
èªå­¸ç¿æ¡æ¶ï¼ä»¥éå°ç©ºéæåï¼ä¾å¦ç©ºéæ··åæ©è½åç©ºéå°æ¯é æ¸¬ç·¨ç¢¼ï¼ç¨æ¼
ç©ºéååçé åé©æ AI åé¡ãå¨
çå¯¦ä¸çæ¸æéï¼ä¾å¦ï¼è«ç¤å­¸æ¸æï¼ä¸çå¯¦é©çµæè¡¨æï¼ææåºçæ¡æ¶
æä¾çé æ¸¬æºç¢ºåº¦é«æ¼åºç·æ¹æ³ã

##### **Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness**
2501.13120v1 by Ambreesh Parthasarathy, Chandrasekar Subramanian, Ganesh Senrayan, Shreyash Adappanavar, Aparna Taneja, Balaraman Ravindran, Milind Tambe

Restless Multi-Armed Bandits (RMABs) have been successfully applied to
resource allocation problems in a variety of settings, including public health.
With the rapid development of powerful large language models (LLMs), they are
increasingly used to design reward functions to better match human preferences.
Recent work has shown that LLMs can be used to tailor automated allocation
decisions to community needs using language prompts. However, this has been
studied primarily for English prompts and with a focus on task performance
only. This can be an issue since grassroots workers, especially in developing
countries like India, prefer to work in local languages, some of which are
low-resource. Further, given the nature of the problem, biases along population
groups unintended by the user are also undesirable. In this work, we study the
effects on both task performance and fairness when the DLM algorithm, a recent
work on using LLMs to design reward functions for RMABs, is prompted with
non-English language commands. Specifically, we run the model on a synthetic
environment for various prompts translated into multiple languages. The prompts
themselves vary in complexity. Our results show that the LLM-proposed reward
functions are significantly better when prompted in English compared to other
languages. We also find that the exact phrasing of the prompt impacts task
performance. Further, as prompt complexity increases, performance worsens for
all languages; however, it is more robust with English prompts than with
lower-resource languages. On the fairness side, we find that low-resource
languages and more complex prompts are both highly likely to create unfairness
along unintended dimensions.

æè¦ï¼<paragraph>ä¸å®åçå¤èè³­å¾ (RMAB) å·²æåæç¨æ¼åç¨®ç°å¢ä¸­çè³æºåéåé¡ï¼åæ¬å¬å±è¡çãé¨èå¼·å¤§å¤§åèªè¨æ¨¡å (LLM) çå¿«éç¼å±ï¼å®åæ­£è¶ä¾è¶å¤å°ç¨æ¼è¨­è¨çåµå½æ¸ï¼ä»¥æ´å¥½å°å¹éäººé¡åå¥½ãæè¿çç ç©¶è¡¨æï¼LLM å¯ç¨æ¼ä½¿ç¨èªè¨æç¤ºæ ¹æç¤¾åéæ±èª¿æ´èªååéæ±ºç­ãç¶èï¼éä¸»è¦éå°è±èªæç¤ºé²è¡äºç ç©¶ï¼ä¸¦ä¸åéæ³¨ä»»åç¸¾æãéå¯è½æ¯ä¸ååé¡ï¼å çºåºå±¤å·¥ä½èï¼ç¹å¥æ¯åå°åº¦éæ¨£çç¼å±ä¸­åå®¶çå·¥ä½èï¼æ´é¡æä½¿ç¨ç¶å°èªè¨ï¼å¶ä¸­ä¸äºèªè¨æ¯ä½è³æºçãæ­¤å¤ï¼éæ¼åé¡çæ§è³ªï¼ç¨æ¶ç¡æä¸­å°äººå£ç¾¤é«ç¢ççåè¦ä¹æ¯ä¸åæ­¡è¿çãå¨éé å·¥ä½ä¸­ï¼æåç ç©¶äºç¶ DLM æ¼ç®æ³ï¼æè¿ä½¿ç¨ LLM çº RMAB è¨­è¨çåµå½æ¸çå·¥ä½ï¼æ¶å°éè±èªèªè¨å½ä»¤æï¼å°ä»»åç¸¾æåå¬å¹³æ§çå½±é¿ãå·é«ä¾èªªï¼æåå¨åæç°å¢ä¸­éè¡æ¨¡åï¼å°ç¿»è­¯æå¤ç¨®èªè¨çåç¨®æç¤ºé²è¡éè¡ãæç¤ºæ¬èº«çè¤éæ§åä¸ç¸åãæåççµæè¡¨æï¼èå¶ä»èªè¨ç¸æ¯ï¼ç¨è±èªæç¤ºæï¼LLM æåºççåµå½æ¸é¡¯èæ´å¥½ãæåéç¼ç¾æç¤ºçç¢ºåæªè¾­æå½±é¿ä»»åç¸¾æãæ­¤å¤ï¼é¨èæç¤ºè¤éæ§çå¢å ï¼ææèªè¨çæ§è½é½æä¸éï¼ç¶èï¼å®æ¯ä½è³æºèªè¨æ´å¥å£¯ãå¨å¬å¹³æ§æ¹é¢ï¼æåç¼ç¾ä½è³æºèªè¨åæ´è¤éçæç¤ºé½æ¥µæå¯è½å¨æå¤çç¶­åº¦ä¸é æä¸å¬å¹³ã</paragraph>

##### **Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications**
2501.11632v2 by Yuxing Lu, Sin Yee Goi, Xukai Zhao, Jinzhuo Wang

Biomedical knowledge graphs (BKGs) have emerged as powerful tools for
organizing and leveraging the vast and complex data found across the biomedical
field. Yet, current reviews of BKGs often limit their scope to specific domains
or methods, overlooking the broader landscape and the rapid technological
progress reshaping it. In this survey, we address this gap by offering a
systematic review of BKGs from three core perspectives: domains, tasks, and
applications. We begin by examining how BKGs are constructed from diverse data
sources, including molecular interactions, pharmacological datasets, and
clinical records. Next, we discuss the essential tasks enabled by BKGs,
focusing on knowledge management, retrieval, reasoning, and interpretation.
Finally, we highlight real-world applications in precision medicine, drug
discovery, and scientific research, illustrating the translational impact of
BKGs across multiple sectors. By synthesizing these perspectives into a unified
framework, this survey not only clarifies the current state of BKG research but
also establishes a foundation for future exploration, enabling both innovative
methodological advances and practical implementations.

æè¦ï¼çç©å»å­¦ç¥è¯å¾è°±ï¼BKGï¼å·²æä¸ºç»ç»åå©ç¨çç©å»å­¦é¢åä¸­åç°çåºå¤§ä¸å¤ææ°æ®çå¼ºå¤§å·¥å·ãç¶èï¼å½åå¯¹ BKG çå®¡æ¥éå¸¸å°å¶èå´éå¶å¨ç¹å®é¢åææ¹æ³ï¼å¿½è§äºæ´å¹¿æ³çæ ¼å±åæ­£å¨éå¡å®çå¿«éææ¯è¿æ­¥ãå¨è¿é¡¹è°æ¥ä¸­ï¼æä»¬éè¿ä»ä¸ä¸ªæ ¸å¿è§åº¦ï¼é¢åãä»»å¡ååºç¨ï¼å¯¹ BKG è¿è¡ç³»ç»å®¡æ¥æ¥è§£å³è¿ä¸å·®è·ãæä»¬é¦åæ£æ¥å¦ä½ä»åæ¬åå­ç¸äºä½ç¨ãè¯çæ°æ®éåä¸´åºè®°å½å¨åçåç§æ°æ®æºæå»º BKGãæ¥ä¸æ¥ï¼æä»¬è®¨è®º BKG å¯ç¨çåºæ¬ä»»å¡ï¼éç¹å³æ³¨ç¥è¯ç®¡çãæ£ç´¢ãæ¨çåè§£éãæåï¼æä»¬éç¹ä»ç»äºç²¾åå»çãè¯ç©åç°åç§å­¦ç ç©¶ä¸­çå®éåºç¨ï¼è¯´æäº BKG å¨å¤ä¸ªé¢åçè½¬åå½±åãéè¿å°è¿äºè§ç¹ç»¼åå°ä¸ä¸ªç»ä¸çæ¡æ¶ä¸­ï¼æ¬è°æ¥ä¸ä»éæäº BKG ç ç©¶çç°ç¶ï¼è¿ä¸ºæªæ¥çæ¢ç´¢å¥ å®äºåºç¡ï¼æ¢ä¿è¿äºåæ°æ¹æ³çè¿æ­¥ï¼ä¹ä¿è¿äºå®éå®æ½ã

##### **Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing**
2501.11592v2 by Chaoqing Tang, Huanze Zhuang, Guiyun Tian, Zhenli Zeng, Yi Ding, Wenzhong Liu, Xiang Bai

Pre-trained large models attract widespread attention in recent years, but
they face challenges in applications that require high interpretability or have
limited resources, such as physical sensing, medical imaging, and
bioinformatics. Compressed Sensing (CS) is a well-proved theory that drives
many recent breakthroughs in these applications. However, as a typical
under-determined linear system, CS suffers from excessively long sparse
reconstruction times when using traditional iterative methods, particularly
with large-scale data. Current AI methods like deep unfolding fail to
substitute them because pre-trained models exhibit poor generality beyond their
training conditions and dataset distributions, or lack interpretability.
Instead of following the big model fervor, this paper proposes ultra-small
artificial neural models called coefficients learning (CL), enabling
training-free and rapid sparse reconstruction while perfectly inheriting the
generality and interpretability of traditional iterative methods, bringing new
feature of incorporating prior knowledges. In CL, a signal of length $n$ only
needs a minimal of $n$ trainable parameters. A case study model called CLOMP is
implemented for evaluation. Experiments are conducted on both synthetic and
real one-dimensional and two-dimensional signals, demonstrating significant
improvements in efficiency and accuracy. Compared to representative iterative
methods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data.
Test results on eight diverse image datasets indicate that CLOMP improves
structural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3,
0.5, respectively. We believe this method can truly usher CS reconstruction
into the AI era, benefiting countless under-determined linear systems that rely
on sparse solution.

æè¦ï¼<paragraph>é è¨ç·´å¤§åæ¨¡åè¿å¹´ä¾å»£åéæ³¨ï¼ä½å®åå¨éè¦é«å¯è§£éæ§æè³æºåéçæç¨ä¸­é¢è¨ææ°ï¼ä¾å¦ç©çææ¸¬ãé«å­¸å½±ååçç©è³è¨å­¸ãå£ç¸®ææ¸¬ (CS) æ¯ä¸åç¶éé©è­ççè«ï¼æ¨åäºéäºæç¨ä¸­çè¨±å¤è¿æçªç ´ãç¶èï¼ä½çºä¸åå¸åçæ¬ å®ç·æ§ç³»çµ±ï¼CS å¨ä½¿ç¨å³çµ±è¿­ä»£æ¹æ³ææå°è´éé·çç¨çéå»ºæéï¼ç¹å¥æ¯å¨å¤§è¦æ¨¡è³æçææ³ä¸ãåæ·±åº¦å±éç­ç¶å AI æ¹æ³ç¡æ³åä»£å®åï¼å çºé è¨ç·´æ¨¡åå¨è¨ç·´æ¢ä»¶åè³æéåä½ä¹å¤è¡¨ç¾åºè¼å·®çæ¦æ¬æ§ï¼æç¼ºä¹å¯è§£éæ§ãæ¬è«ææ²æè¿½é¨å¤§åæ¨¡åç±æ½®ï¼èæ¯æåºäºç¨±çºä¿æ¸å­¸ç¿ (CL) çè¶å°åäººå·¥ç¥ç¶ç¶²è·¯æ¨¡åï¼å¯¦ç¾ç¡è¨ç·´ä¸å¿«éçç¨çéå»ºï¼åæå®ç¾ç¹¼æ¿å³çµ±è¿­ä»£æ¹æ³çæ¦æ¬æ§åå¯è§£éæ§ï¼å¸¶ä¾çµååé©ç¥è­çæ°ç¹é»ãå¨ CL ä¸­ï¼é·åº¦çº $n$ çä¿¡èåªéè¦æå° $n$ åå¯è¨ç·´åæ¸ãå¯¦ä½äºä¸åç¨±çº CLOMP çæ¡ä¾ç ç©¶æ¨¡åé²è¡è©ä¼°ãå¨åæåçå¯¦çä¸ç¶­åäºç¶­ä¿¡èä¸é²è¡äºå¯¦é©ï¼è­æäºæçåæºç¢ºæ§çé¡¯èæåãèå·ä»£è¡¨æ§çè¿­ä»£æ¹æ³ç¸æ¯ï¼CLOMP å°å¤§åè³æçæçæåäº 100 å° 1000 åãå¨å«åä¸åçå½±åè³æéä¸çæ¸¬è©¦çµæè¡¨æï¼CLOMP åå¥å°æ¡æ¨£ççº 0.1ã0.3ã0.5 ççµæ§ç¸ä¼¼æ§ææ¨æåäº 292%ã98%ã45%ãæåç¸ä¿¡éç¨®æ¹æ³å¯ä»¥çæ­£å° CS éå»ºå¸¶å¥ AI æä»£ï¼ä½¿ä¾è³´ç¨çè§£çç¡æ¸æ¬ å®ç·æ§ç³»çµ±åçã</paragraph>

##### **Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography**
2501.11428v1 by Jakub Nalepa, Tomasz Bartczak, Mariusz Bujny, JarosÅaw GoÅliÅski, Katarzyna Jesionek, Wojciech Malara, Filip Malawski, Karol Miszalski-Jamka, Patrycja Rewa, Marcin Kostur

Despite coronary artery calcium scoring being considered a largely solved
problem within the realm of medical artificial intelligence, this paper argues
that significant improvements can still be made. By shifting the focus from
pathology detection to a deeper understanding of anatomy, the novel algorithm
proposed in the paper both achieves high accuracy in coronary artery calcium
scoring and offers enhanced interpretability of the results. This approach not
only aids in the precise quantification of calcifications in coronary arteries,
but also provides valuable insights into the underlying anatomical structures.
Through this anatomically-informed methodology, the paper shows how a nuanced
understanding of the heart's anatomy can lead to more accurate and
interpretable results in the field of cardiovascular health. We demonstrate the
superior accuracy of the proposed method by evaluating it on an open-source
multi-vendor dataset, where we obtain results at the inter-observer level,
surpassing the current state of the art. Finally, the qualitative analyses show
the practical value of the algorithm in such tasks as labeling coronary artery
calcifications, identifying aortic calcifications, and filtering out false
positive detections due to noise.

æè¦ï¼åç®¡å çåèé£åè©åå¨é«å­¸äººå·¥æºæ§é åè¢«èªçºæ¯ä¸åå·²è§£æ±ºçåé¡ï¼ä½æ¬æè«è­ä»æé¡¯èé²æ­¥çç©ºéãééå°ç¦é»å¾ççæª¢æ¸¬è½ç§»å°å°è§£åçµæ§çæ´æ·±å¥çè§£ï¼æ¬ææåºçæ°æ¼ç®æ³å¨å çåèé£åè©åä¸­ç²å¾é«æºç¢ºåº¦ï¼ä¸¦æä¾äºå¢å¼·ççµæå¯è§£éæ§ãéç¨®æ¹æ³ä¸åæå©æ¼ç²¾ç¢ºéåå çåèçé£åï¼éæä¾äºå°åºå±¤è§£åçµæ§çå¯¶è²´è¦è§£ãéééç¨®è§£åå­¸æ¹æ³ï¼æ¬æå±ç¤ºäºå°å¿èè§£åçµæ§çç´°ç·»çè§£å¦ä½è½å°è´å¿è¡ç®¡å¥åº·é åæ´æºç¢ºä¸å¯è§£éççµæãæåééå¨éæ¾åå§ç¢¼çå¤å» åè³æéä¸è©ä¼°ææåºçæ¹æ³ï¼è­æäºå¶åªè¶çæºç¢ºåº¦ï¼æåå¨è§å¯èéå±¤ç´ç²å¾ççµæè¶è¶äºç®åçæè¡æ°´æºãæå¾ï¼å®æ§åæé¡¯ç¤ºäºè©²æ¼ç®æ³å¨æ¨è¨å çåèé£åãè­å¥ä¸»åèé£åä»¥åéæ¿¾æå éè¨èç¢ççåé½æ§åµæ¸¬ç­ä»»åä¸­çå¯¦ç¨å¹å¼ã

##### **Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations**
2501.16356v1 by Alicia Vidler, Toby Walsh

Large Language Models (LLMs) are increasingly being used to simulate
human-like decision making in agent-based financial market models (ABMs). As
models become more powerful and accessible, researchers can now incorporate
individual LLM decisions into ABM environments. However, integration may
introduce inherent biases that need careful evaluation. In this paper we test
three state-of-the-art GPT models for bias using two model sampling approaches:
one-shot and few-shot API queries. We observe significant variations in
distributions of outputs between specific models, and model sub versions, with
GPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes
responses) compared to GPT-4-0125-preview's extreme bias (98-99% yes
responses). We show that sampling methods and model sub-versions significantly
impact results: repeated independent API calls produce different distributions
compared to batch sampling within a single call. While no current GPT model can
simultaneously achieve a uniform distribution and Markovian properties in
one-shot testing, few-shot sampling can approach uniform distributions under
certain conditions. We explore the Temperature parameter, providing a
definition and comparative results. We further compare our results to true
random binary series and test specifically for the common human bias of
Negative Recency - finding LLMs have a mixed ability to 'beat' humans in this
one regard. These findings emphasise the critical importance of careful LLM
integration into ABMs for financial markets and more broadly.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM)  zunehmend zur Simulation
menschlicher Entscheidungsfindung in agentenbasierten Finanzmarktmodellen (ABM) eingesetzt. Da
Modelle leistungsfÃ¤higer und zugÃ¤nglicher werden, kÃ¶nnen Forscher jetzt
einzelne LLM-Entscheidungen in ABM-Umgebungen integrieren. Die Integration kann jedoch
inhÃ¤rente Verzerrungen einfÃ¼hren, die sorgfÃ¤ltig bewertet werden mÃ¼ssen. In diesem Artikel testen wir
drei hochmoderne GPT-Modelle auf Verzerrungen unter Verwendung zweier Modell-Sampling-AnsÃ¤tze:
One-Shot- und Few-Shot-API-Abfragen. Wir beobachten signifikante Unterschiede in
Verteilungen von Ausgaben zwischen bestimmten Modellen und Modellunterversionen, wobei
GPT-4o-Mini-2024-07-18 eine deutlich bessere Leistung zeigt (32-43 % Ja-Antworten) im Vergleich zu GPT-4-0125-Vorschau extreme Verzerrung (98-99 % Ja-Antworten). Wir zeigen, dass Sampling-Methoden und Modellunterversionen die Ergebnisse erheblich beeinflussen: Wiederholte unabhÃ¤ngige API-Aufrufe erzeugen unterschiedliche Verteilungen im Vergleich zur Batch-Abtastung innerhalb eines einzelnen Aufrufs. Zwar kann kein aktuelles GPT-Modell gleichzeitig eine gleichmÃ¤Ãige Verteilung und markovsche Eigenschaften im One-Shot-Test erreichen, aber Few-Shot-Sampling kann sich unter bestimmten Bedingungen gleichmÃ¤Ãigen Verteilungen annÃ¤hern. Wir untersuchen den Temperaturparameter und liefern eine
Definition und Vergleichsergebnisse. DarÃ¼ber hinaus vergleichen wir unsere Ergebnisse mit echten
zufÃ¤lligen BinÃ¤rreihen und testen speziell auf die hÃ¤ufige menschliche Verzerrung von
Negative Rezenz - Feststellung, dass LLMs eine gemischte FÃ¤higkeit haben, Menschen in diesem Fall zu âschlagenâ.
eine Hinsicht. Diese Ergebnisse unterstreichen die entscheidende Bedeutung einer sorgfÃ¤ltigen LLM-Integration in ABMs fÃ¼r FinanzmÃ¤rkte und darÃ¼ber hinaus.

##### **RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?**
2501.11284v1 by Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, Debing Zhang

Can scaling transform reasoning? In this work, we explore the untapped
potential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples,
pioneering the development of a slow-thinking model, RedStar. Through extensive
experiments with various LLMs and different sizes, we uncover the ingredients
for specialization and scale for Long-CoT training. Surprisingly, even smaller
models show significant performance gains with limited data, revealing the
sample efficiency of Long-CoT and the critical role of sample difficulty in the
learning process. Our findings demonstrate that Long-CoT reasoning can be
effectively triggered with just a few thousand examples, while larger models
achieve unparalleled improvements. We also introduce reinforcement learning
(RL)-scale training as a promising direction for advancing slow-thinking
systems. RedStar shines across domains: on the MATH-Hard benchmark,
RedStar-code-math boosts performance from 66.2\% to 81.6\%, and on the USA Math
Olympiad (AIME), it solves 46.7\% of problems using only 21k mixed-code-math
datasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo
achieves competitive results with minimal Long-CoT data, outperforming other
slow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the
perfect balance between reasoning and generalizability. Our work highlights
that, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning
capabilities-even with limited dataset and set a new standard for slow-thinking
models across diverse challenges. Our data and models are released at
https://huggingface.co/RedStar-Reasoning.

æè¦ï¼<paragraph>ç¸®æ¾å¯ä»¥è½ææ¨çåï¼å¨éé å·¥ä½ä¸­ï¼æåæ¢ç´¢å°é·éæèï¼Long-CoTï¼è³æç¸®æ¾å° 1000k ç¯ä¾çæªéç¼æ½åï¼çåéç¼æ¢æèæ¨¡å RedStarãééä½¿ç¨åç¨® LLM åä¸åå¤§å°é²è¡å»£æ³å¯¦é©ï¼æåæ­ç¤ºäº Long-CoT è¨ç·´çå°æ¥­ååè¦æ¨¡è¦ç´ ãä»¤äººé©è¨çæ¯ï¼å³ä½¿è¼å°çæ¨¡åå¨è³ææéçææ³ä¸ä¹å±ç¾åºé¡¯èçæè½æåï¼æ­ç¤ºäº Long-CoT çç¯ä¾æçåç¯ä¾é£åº¦å¨å­¸ç¿éç¨ä¸­æ®æ¼çééµè§è²ãæåçç¼ç¾è­æï¼åªè¦ææ¸ååç¯ä¾ï¼å°±å¯ä»¥ææè§¸ç¼ Long-CoT æ¨çï¼èè¼å¤§çæ¨¡ååå¯ç²å¾ç¡èå«æ¯çæ¹é²ãæåéå°å¥å¼·åå­¸ç¿ (RL) è¦æ¨¡è¨ç·´ï¼ä½çºæ¨é²æ¢æèç³»çµ±çä¸åæåéçæ¹åãRedStar å¨ååé åä¸­è¡¨ç¾åºè²ï¼å¨ MATH-Hard åºæºæ¸¬è©¦ä¸­ï¼RedStar-code-math å°æè½å¾ 66.2% æåè³ 81.6%ï¼èå¨ç¾åæ¸å­¸å¥§æå¹åï¼AIMEï¼ä¸­ï¼å®åä½¿ç¨ 21k åæ··åç¨å¼ç¢¼æ¸å­¸è³æéå°±è§£æ±ºäº 46.7% çåé¡ãå¨ GeoQA å MathVista-GEO ç­å¤æ¨¡æä»»åä¸­ï¼RedStar-Geo å¨ Long-CoT è³ææå°çææ³ä¸åå¾ç«¶ç­åççµæï¼åªæ¼å¶ä»æ¢æèç³»çµ±ï¼ä¾å¦ QvQ-Previewãè QwQ ç¸æ¯ï¼RedStar å¨æ¨çåæ¦æ¬æ§ä¹éåå¾äºå®ç¾çå¹³è¡¡ãæåçç ç©¶éé»å¨æ¼ï¼ééä»ç´°èª¿æ´ï¼ç¸®æ¾ Long-CoT å¯ä»¥è§£ééå¡çæ¨çè½åï¼å³ä½¿å¨è³æéæéçææ³ä¸ï¼ä¹è½çºåç¨®ææ°è¨­å®æ¢æèæ¨¡åçæ°æ¨æºãæåçè³æåæ¨¡åå·²æ¼ https://huggingface.co/RedStar-Reasoning ç¼å¸ã</paragraph>

##### **Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features**
2501.11270v1 by Osama Ahmad, Zubair Khalid, Muhammad Tahir, Momin Uppal

Monitoring air pollution is crucial for protecting human health from exposure
to harmful substances. Traditional methods of air quality monitoring, such as
ground-based sensors and satellite-based remote sensing, face limitations due
to high deployment costs, sparse sensor coverage, and environmental
interferences. To address these challenges, this paper proposes a framework for
high-resolution spatiotemporal Air Quality Index (AQI) mapping using sparse
sensor data, satellite imagery, and various spatiotemporal factors. By
leveraging Graph Neural Networks (GNNs), we estimate AQI values at unmonitored
locations based on both spatial and temporal dependencies. The framework
incorporates a wide range of environmental features, including meteorological
data, road networks, points of interest (PoIs), population density, and urban
green spaces, which enhance prediction accuracy. We illustrate the use of our
approach through a case study in Lahore, Pakistan, where multi-resolution data
is used to generate the air quality index map at a fine spatiotemporal scale.

æè¦ï¼ç£æ§ç©ºæ°£æ±¡æå°æ¼ä¿è­·äººé¡å¥åº·åæ¼æ¥è§¸æå®³ç©è³ªè³ééè¦ãå³çµ±çç©ºæ°£åè³ªç£æ¸¬æ¹æ³ï¼ä¾å¦å°é¢ææ¸¬å¨åè¡æéæ¸¬ï¼ç±æ¼é¨ç½²ææ¬é«ãææ¸¬å¨è¦èç¯åç¨çä»¥åç°å¢å¹²æ¾èé¢è¨éå¶ãçºäºæå°éäºææ°ï¼æ¬ææåºäºä¸åä½¿ç¨ç¨çææ¸¬å¨è³æãè¡æå½±åååç¨®æç©ºå å­ä¾ç¹ªè£½é«è§£æåº¦æç©ºç©ºæ°£åè³ªææ¸ (AQI) çæ¶æ§ãééå©ç¨åå½¢ç¥ç¶ç¶²è·¯ (GNN)ï¼æåæ ¹æç©ºéåæéä¾è³´æ§ä¾ä¼°è¨æªç£æ§å°é»ç AQI å¼ãè©²æ¶æ§çµåäºå»£æ³çç°å¢ç¹å¾µï¼åæ¬æ°£è±¡è³æãéè·¯ç¶²è·¯ãèè¶£é» (PoI)ãäººå£å¯åº¦ååå¸ç¶ å°ï¼éäºç¹å¾µå¢å¼·äºé æ¸¬æºç¢ºåº¦ãæåééå·´åºæ¯å¦æåç¾çä¸åæ¡ä¾ç ç©¶ä¾èªªææåæ¹æ³çä½¿ç¨ï¼å¶ä¸­ä½¿ç¨å¤è§£æåº¦è³æä¾çæç²¾ç´°æç©ºå°ºåº¦çç©ºæ°£åè³ªææ¸å°åã

