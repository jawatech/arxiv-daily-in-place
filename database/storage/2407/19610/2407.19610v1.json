{"2407.19610": {"publish_time": "2024-07-28", "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models", "paper_summary": "This research combines Knowledge Distillation (KD) and Mixture of Experts\n(MoE) to develop modular, efficient multilingual language models. Key\nobjectives include evaluating adaptive versus fixed alpha methods in KD and\ncomparing modular MoE architectures for handling multi-domain inputs and\npreventing catastrophic forgetting. KD compresses large language models (LLMs)\ninto smaller, efficient models, while MoE enhances modularity with specialized\ntasks. Experiments showed similar performance for both KD methods, with\nmarginal improvements from adaptive alpha. A combined loss approach provided\nmore stable learning. The router, trained to classify input sequences into\nEnglish, French, German, or Python, achieved 99.95% precision, recall, and F1\nscore, with Logistic Regression being the most effective classifier.\nEvaluations of modular MoE architectures revealed that Pre-trained Language\nExperts (PLE) and Joint Expert Embedding Training (JEET) performed similarly,\nwhile the MoE with Common Expert (MoE-CE) setup showed slightly lower\nperformance. Including a common expert in MoE-CE improved its performance.\nStudies on catastrophic forgetting indicated that sequential training led to\nsignificant forgetting, while single-session training with balanced batches and\nthe MoE approach mitigated this issue. The MoE architecture preserved knowledge\nacross multiple languages effectively.\n  The research contributes open-sourced resources including the dataset\n(https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation\ntool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the\nresearch codebase (https://github.com/ModMaamari/mixture-modular-experts).", "paper_summary_zh": "<paragraph>\u672c\u7814\u7a76\u7d50\u5408\u77e5\u8b58\u84b8\u993e (KD) \u548c\u5c08\u5bb6\u6df7\u5408 (MoE) \u4f86\u958b\u767c\u6a21\u7d44\u5316\u3001\u9ad8\u6548\u7684\u591a\u8a9e\u8a00\u8a9e\u8a00\u6a21\u578b\u3002\u4e3b\u8981\u76ee\u6a19\u5305\u62ec\u8a55\u4f30 KD \u4e2d\u7684\u9069\u61c9\u6027\u8207\u56fa\u5b9a alpha \u65b9\u6cd5\uff0c\u4ee5\u53ca\u6bd4\u8f03\u6a21\u7d44\u5316 MoE \u67b6\u69cb\u4ee5\u8655\u7406\u591a\u9818\u57df\u8f38\u5165\u4e26\u9632\u6b62\u707d\u96e3\u6027\u907a\u5fd8\u3002KD \u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u58d3\u7e2e\u6210\u66f4\u5c0f\u3001\u66f4\u6709\u6548\u7387\u7684\u6a21\u578b\uff0c\u800c MoE \u5247\u900f\u904e\u5c08\u9580\u4efb\u52d9\u589e\u5f37\u6a21\u7d44\u5316\u3002\u5be6\u9a57\u986f\u793a\u5169\u7a2e KD \u65b9\u6cd5\u7684\u6548\u80fd\u76f8\u4f3c\uff0c\u9069\u61c9\u6027 alpha \u7684\u908a\u969b\u6539\u5584\u3002\u7d50\u5408\u640d\u5931\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7a69\u5b9a\u7684\u5b78\u7fd2\u3002\u7d93\u904e\u8a13\u7df4\u4ee5\u5c07\u8f38\u5165\u5e8f\u5217\u5206\u985e\u70ba\u82f1\u8a9e\u3001\u6cd5\u8a9e\u3001\u5fb7\u8a9e\u6216 Python \u7684\u8def\u7531\u5668\u9054\u5230\u4e86 99.95% \u7684\u7cbe\u78ba\u5ea6\u3001\u53ec\u56de\u7387\u548c F1 \u5206\u6578\uff0c\u5176\u4e2d\u908f\u8f2f\u8ff4\u6b78\u662f\u6700\u6709\u6548\u7684\u5206\u985e\u5668\u3002\u5c0d\u6a21\u7d44\u5316 MoE \u67b6\u69cb\u7684\u8a55\u4f30\u986f\u793a\uff0c\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u5c08\u5bb6 (PLE) \u548c\u806f\u5408\u5c08\u5bb6\u5d4c\u5165\u8a13\u7df4 (JEET) \u7684\u8868\u73fe\u76f8\u4f3c\uff0c\u800c\u5177\u6709\u5171\u540c\u5c08\u5bb6 (MoE-CE) \u8a2d\u5b9a\u7684 MoE \u986f\u793a\u51fa\u7565\u4f4e\u7684\u8868\u73fe\u3002\u5728 MoE-CE \u4e2d\u52a0\u5165\u4e00\u500b\u5171\u540c\u5c08\u5bb6\u6539\u5584\u4e86\u5176\u8868\u73fe\u3002\u5c0d\u707d\u96e3\u6027\u907a\u5fd8\u7684\u7814\u7a76\u8868\u660e\uff0c\u9806\u5e8f\u8a13\u7df4\u5c0e\u81f4\u986f\u8457\u7684\u907a\u5fd8\uff0c\u800c\u4f7f\u7528\u5e73\u8861\u6279\u6b21\u548c MoE \u65b9\u6cd5\u7684\u55ae\u4e00\u8ab2\u7a0b\u8a13\u7df4\u6e1b\u8f15\u4e86\u9019\u500b\u554f\u984c\u3002MoE \u67b6\u69cb\u6709\u6548\u5730\u4fdd\u7559\u4e86\u591a\u7a2e\u8a9e\u8a00\u7684\u77e5\u8b58\u3002\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u958b\u653e\u539f\u59cb\u78bc\u8cc7\u6e90\uff0c\u5305\u62ec\u8cc7\u6599\u96c6 (https://zenodo.org/doi/10.5281/zenodo.12677631)\u3001\u5e73\u8861\u8cc7\u6599\u96c6\u5efa\u7acb\u5de5\u5177 (https://github.com/padas-lab-de/multi-language-dataset-creator) \u548c\u7814\u7a76\u7a0b\u5f0f\u78bc\u5eab (https://github.com/ModMaamari/mixture-modular-experts)\u3002</paragraph>", "author": "Mohammed Al-Maamari et.al.", "authors": "Mohammed Al-Maamari, Mehdi Ben Amor, Michael Granitzer", "id": "2407.19610v1", "paper_url": "http://arxiv.org/abs/2407.19610v1", "repo": "https://github.com/padas-lab-de/multi-language-dataset-creator"}}