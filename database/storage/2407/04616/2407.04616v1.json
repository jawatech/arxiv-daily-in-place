{"2407.04616": {"publish_time": "2024-07-05", "title": "Isomorphic Pruning for Vision Models", "paper_summary": "Structured pruning reduces the computational overhead of deep neural networks\nby removing redundant sub-structures. However, assessing the relative\nimportance of different sub-structures remains a significant challenge,\nparticularly in advanced vision models featuring novel mechanisms and\narchitectures like self-attention, depth-wise convolutions, or residual\nconnections. These heterogeneous substructures usually exhibit diverged\nparameter scales, weight distributions, and computational topology, introducing\nconsiderable difficulty to importance comparison. To overcome this, we present\nIsomorphic Pruning, a simple approach that demonstrates effectiveness across a\nrange of network architectures such as Vision Transformers and CNNs, and\ndelivers competitive performance across different model sizes. Isomorphic\nPruning originates from an observation that, when evaluated under a pre-defined\nimportance criterion, heterogeneous sub-structures demonstrate significant\ndivergence in their importance distribution, as opposed to isomorphic\nstructures that present similar importance patterns. This inspires us to\nperform isolated ranking and comparison on different types of sub-structures\nfor more reliable pruning. Our empirical results on ImageNet-1K demonstrate\nthat Isomorphic Pruning surpasses several pruning baselines dedicatedly\ndesigned for Transformers or CNNs. For instance, we improve the accuracy of\nDeiT-Tiny from 74.52% to 77.50% by pruning an off-the-shelf DeiT-Base model.\nAnd for ConvNext-Tiny, we enhanced performance from 82.06% to 82.18%, while\nreducing the number of parameters and memory usage. Code is available at\n\\url{https://github.com/VainF/Isomorphic-Pruning}.", "paper_summary_zh": "<paragraph>\u7d50\u69cb\u5316\u526a\u679d\u900f\u904e\u79fb\u9664\u5197\u9918\u7684\u5b50\u7d50\u69cb\uff0c\u4f86\u964d\u4f4e\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7684\u904b\u7b97\u8ca0\u64d4\u3002\u7136\u800c\uff0c\u8a55\u4f30\u4e0d\u540c\u5b50\u7d50\u69cb\u7684\u76f8\u5c0d\u91cd\u8981\u6027\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u7684\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u5177\u5099\u81ea\u6ce8\u610f\u529b\u3001\u6df1\u5ea6\u5377\u7a4d\u6216\u6b98\u5dee\u9023\u63a5\u7b49\u65b0\u6a5f\u5236\u548c\u67b6\u69cb\u7684\u5148\u9032\u8996\u89ba\u6a21\u578b\u4e2d\u3002\u9019\u4e9b\u7570\u8cea\u5b50\u7d50\u69cb\u901a\u5e38\u5c55\u73fe\u51fa\u4e0d\u540c\u7684\u53c3\u6578\u898f\u6a21\u3001\u6b0a\u91cd\u5206\u914d\u548c\u904b\u7b97\u62d3\u64b2\uff0c\u70ba\u91cd\u8981\u6027\u6bd4\u8f03\u5e36\u4f86\u4e86\u76f8\u7576\u5927\u7684\u56f0\u96e3\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u540c\u69cb\u526a\u679d\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u7684\u65b9\u6cd5\uff0c\u8b49\u660e\u4e86\u5728\u5404\u7a2e\u7db2\u8def\u67b6\u69cb\uff08\u4f8b\u5982 Vision Transformers \u548c CNN\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e26\u5728\u4e0d\u540c\u7684\u6a21\u578b\u898f\u6a21\u4e2d\u63d0\u4f9b\u4e86\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\u540c\u69cb\u526a\u679d\u6e90\u65bc\u4e00\u500b\u89c0\u5bdf\uff0c\u5373\u5728\u9810\u5b9a\u7fa9\u7684\u91cd\u8981\u6027\u6e96\u5247\u4e0b\u9032\u884c\u8a55\u4f30\u6642\uff0c\u7570\u8cea\u5b50\u7d50\u69cb\u5728\u5176\u91cd\u8981\u6027\u5206\u4f48\u4e0a\u8868\u73fe\u51fa\u986f\u8457\u7684\u5dee\u7570\uff0c\u800c\u540c\u69cb\u7d50\u69cb\u5247\u5448\u73fe\u51fa\u76f8\u4f3c\u7684\u91cd\u8981\u6027\u6a21\u5f0f\u3002\u9019\u555f\u767c\u6211\u5011\u5c0d\u4e0d\u540c\u985e\u578b\u7684\u5b50\u7d50\u69cb\u57f7\u884c\u5b64\u7acb\u7684\u6392\u540d\u548c\u6bd4\u8f03\uff0c\u4ee5\u9032\u884c\u66f4\u53ef\u9760\u7684\u526a\u679d\u3002\u6211\u5011\u5728 ImageNet-1K \u4e0a\u7684\u5be6\u8b49\u7d50\u679c\u8b49\u660e\uff0c\u540c\u69cb\u526a\u679d\u8d85\u8d8a\u4e86\u5c08\u9580\u70ba Transformer \u6216 CNN \u8a2d\u8a08\u7684\u5e7e\u500b\u526a\u679d\u57fa\u6e96\u3002\u4f8b\u5982\uff0c\u6211\u5011\u5c07\u73fe\u6210\u7684 DeiT-Base \u6a21\u578b\u9032\u884c\u526a\u679d\uff0c\u5c07 DeiT-Tiny \u7684\u6e96\u78ba\u5ea6\u5f9e 74.52% \u63d0\u5347\u81f3 77.50%\u3002\u5c0d\u65bc ConvNext-Tiny\uff0c\u6211\u5011\u5c07\u6548\u80fd\u5f9e 82.06% \u63d0\u5347\u81f3 82.18%\uff0c\u540c\u6642\u6e1b\u5c11\u4e86\u53c3\u6578\u6578\u91cf\u548c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/VainF/Isomorphic-Pruning} \u53d6\u5f97\u3002</paragraph>", "author": "Gongfan Fang et.al.", "authors": "Gongfan Fang, Xinyin Ma, Michael Bi Mi, Xinchao Wang", "id": "2407.04616v1", "paper_url": "http://arxiv.org/abs/2407.04616v1", "repo": "https://github.com/vainf/isomorphic-pruning"}}