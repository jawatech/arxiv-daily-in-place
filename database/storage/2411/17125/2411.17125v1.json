{"2411.17125": {"publish_time": "2024-11-26", "title": "DOGE: Towards Versatile Visual Document Grounding and Referring", "paper_summary": "In recent years, Multimodal Large Language Models (MLLMs) have increasingly\nemphasized grounding and referring capabilities to achieve detailed\nunderstanding and flexible user interaction. However, in the realm of visual\ndocument understanding, these capabilities lag behind due to the scarcity of\nfine-grained datasets and comprehensive benchmarks. To fill this gap, we\npropose the DOcument Grounding and Eferring data engine (DOGE-Engine), which\nproduces two types of high-quality fine-grained document data: multi-granular\nparsing data for enhancing fundamental text localization and recognition\ncapabilities; and instruction-tuning data to activate MLLM's grounding and\nreferring capabilities during dialogue and reasoning. Additionally, using our\nengine, we construct DOGE-Bench, which encompasses 7 grounding and referring\ntasks across 3 document types (chart, poster, PDF document), providing\ncomprehensive evaluations for fine-grained document understanding. Furthermore,\nleveraging the data generated by our engine, we develop a strong baseline\nmodel, DOGE. This pioneering MLLM is capable of accurately referring and\ngrounding texts at multiple granularities within document images. Our code,\ndata, and model will be open-sourced for community development.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u6108\u4f86\u6108\u5f37\u8abf\u57fa\u790e\u548c\u6307\u6d89\u80fd\u529b\uff0c\u4ee5\u9054\u6210\u8a73\u7d30\u7406\u89e3\u548c\u5f48\u6027\u7684\u4f7f\u7528\u8005\u4e92\u52d5\u3002\u7136\u800c\uff0c\u5728\u8996\u89ba\u6587\u4ef6\u7406\u89e3\u9818\u57df\u4e2d\uff0c\u9019\u4e9b\u80fd\u529b\u7531\u65bc\u7d30\u7c92\u5ea6\u8cc7\u6599\u96c6\u548c\u7d9c\u5408\u57fa\u6e96\u7684\u7a00\u5c11\u800c\u843d\u5f8c\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6587\u4ef6\u57fa\u790e\u548c\u6307\u6d89\u8cc7\u6599\u5f15\u64ce (DOGE-Engine)\uff0c\u5b83\u7522\u751f\u4e86\u5169\u7a2e\u9ad8\u54c1\u8cea\u7684\u7d30\u7c92\u5ea6\u6587\u4ef6\u8cc7\u6599\uff1a\u591a\u7c92\u5ea6\u89e3\u6790\u8cc7\u6599\uff0c\u7528\u65bc\u589e\u5f37\u57fa\u672c\u7684\u6587\u5b57\u5b9a\u4f4d\u548c\u8fa8\u8b58\u80fd\u529b\uff1b\u4ee5\u53ca\u6307\u4ee4\u5fae\u8abf\u8cc7\u6599\uff0c\u7528\u65bc\u5728\u5c0d\u8a71\u548c\u63a8\u7406\u671f\u9593\u555f\u52d5 MLLM \u7684\u57fa\u790e\u548c\u6307\u6d89\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u6211\u5011\u7684\u5f15\u64ce\uff0c\u6211\u5011\u5efa\u69cb\u4e86 DOGE-Bench\uff0c\u5b83\u5305\u542b\u4e86\u8de8\u8d8a 3 \u7a2e\u6587\u4ef6\u985e\u578b\uff08\u5716\u8868\u3001\u6d77\u5831\u3001PDF \u6587\u4ef6\uff09\u7684 7 \u500b\u57fa\u790e\u548c\u6307\u6d89\u4efb\u52d9\uff0c\u63d0\u4f9b\u7d30\u7c92\u5ea6\u6587\u4ef6\u7406\u89e3\u7684\u7d9c\u5408\u8a55\u4f30\u3002\u6b64\u5916\uff0c\u5229\u7528\u6211\u5011\u7684\u5f15\u64ce\u7522\u751f\u7684\u8cc7\u6599\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u5f37\u5927\u7684\u57fa\u6e96\u6a21\u578b DOGE\u3002\u9019\u500b\u958b\u5275\u6027\u7684 MLLM \u80fd\u5920\u6e96\u78ba\u5730\u6307\u6d89\u548c\u57fa\u790e\u6587\u4ef6\u5f71\u50cf\u4e2d\u7684\u591a\u91cd\u7c92\u5ea6\u6587\u5b57\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u548c\u6a21\u578b\u5c07\u6703\u958b\u653e\u539f\u59cb\u78bc\uff0c\u4ee5\u4f9b\u793e\u7fa4\u958b\u767c\u3002", "author": "Yinan Zhou et.al.", "authors": "Yinan Zhou, Yuxin Chen, Haokun Lin, Shuyu Yang, Li Zhu, Zhongang Qi, Chen Ma, Ying Shan", "id": "2411.17125v1", "paper_url": "http://arxiv.org/abs/2411.17125v1", "repo": "null"}}