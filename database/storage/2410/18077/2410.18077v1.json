{"2410.18077": {"publish_time": "2024-10-23", "title": "ALTA: Compiler-Based Analysis of Transformers", "paper_summary": "We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba ALTA \u7684\u65b0\u7a0b\u5f0f\u8a9e\u8a00\uff0c\u4ee5\u53ca\u4e00\u500b\u53ef\u4ee5\u5c07 ALTA \u7a0b\u5f0f\u5c0d\u61c9\u5230 Transformer \u6b0a\u91cd\u7684\u7de8\u8b6f\u5668\u3002ALTA \u7684\u9748\u611f\u4f86\u81ea RASP\uff0c\u4e00\u7a2e\u7531 Weiss \u7b49\u4eba (2021) \u63d0\u51fa\u7684\u8a9e\u8a00\uff0c\u4ee5\u53ca Tracr (Lindner \u7b49\u4eba\uff0c2023)\uff0c\u4e00\u500b\u5c07 RASP \u7a0b\u5f0f\u7de8\u8b6f\u5230 Transformer \u6b0a\u91cd\u7684\u7de8\u8b6f\u5668\u3002ALTA \u88dc\u5145\u4e26\u5ef6\u4f38\u4e86\u9019\u9805\u5148\u524d\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u8868\u9054\u8ff4\u5708\u548c\u5c07\u7a0b\u5f0f\u7de8\u8b6f\u5230\u901a\u7528 Transformer \u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u4ed6\u512a\u9ede\u3002ALTA \u8b93\u6211\u5011\u80fd\u5920\u5efa\u69cb\u6027\u5730\u5c55\u793a Transformer \u5982\u4f55\u8868\u793a\u7528\u65bc\u8a08\u7b97\u5947\u5076\u6821\u9a57\u548c\u52a0\u6cd5\u7684\u9577\u5ea6\u4e0d\u8b8a\u6f14\u7b97\u6cd5\uff0c\u4ee5\u53ca SCAN \u7d44\u5408\u6982\u5316\u4efb\u52d9\u57fa\u6e96\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u800c\u4e0d\u9700\u8981\u4e2d\u9593\u66ab\u5b58\u5340\u89e3\u78bc\u6b65\u9a5f\u3002\u6211\u5011\u4e5f\u63d0\u51fa\u4e86\u5de5\u5177\u4f86\u5206\u6790\u6f14\u7b97\u6cd5\u7684\u53ef\u8868\u9054\u6027\u5df2\u5efa\u7acb\uff0c\u4f46\u7d66\u5b9a\u8a13\u7df4\u96c6\u4e0a\u7684\u7aef\u5230\u7aef\u8a13\u7df4\u7121\u6cd5\u8a98\u5c0e\u8207\u6240\u9700\u6f14\u7b97\u6cd5\u4e00\u81f4\u7684\u884c\u70ba\u7684\u60c5\u6cc1\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63a2\u7d22\u5f9e ALTA \u57f7\u884c\u8ffd\u8e64\u8a13\u7df4\uff0c\u4f5c\u70ba\u66f4\u7d30\u7dfb\u7684\u76e3\u7763\u8a0a\u865f\u3002\u9019\u555f\u7528\u4e86\u984d\u5916\u7684\u5be6\u9a57\u548c\u7406\u8ad6\u5206\u6790\uff0c\u5c07\u5404\u7a2e\u6f14\u7b97\u6cd5\u7684\u53ef\u5b78\u7fd2\u6027\u8207\u8cc7\u6599\u53ef\u7528\u6027\u548c\u5efa\u6a21\u6c7a\u7b56\uff08\u4f8b\u5982\u4f4d\u7f6e\u7de8\u78bc\uff09\u95dc\u806f\u8d77\u4f86\u3002\u6211\u5011\u5c07 ALTA \u6846\u67b6\uff08\u8a9e\u8a00\u898f\u7bc4\u3001\u7b26\u865f\u89e3\u91cb\u5668\u548c\u6b0a\u91cd\u7de8\u8b6f\u5668\uff09\u63d0\u4f9b\u7d66\u793e\u7fa4\uff0c\u4ee5\u555f\u7528\u9032\u4e00\u6b65\u7684\u61c9\u7528\u548c\u898b\u89e3\u3002</paragraph>", "author": "Peter Shaw et.al.", "authors": "Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, Kristina Toutanova", "id": "2410.18077v1", "paper_url": "http://arxiv.org/abs/2410.18077v1", "repo": "https://github.com/google-deepmind/alta"}}