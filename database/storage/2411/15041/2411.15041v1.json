{"2411.15041": {"publish_time": "2024-11-22", "title": "mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA", "paper_summary": "Advanced Multimodal Large Language Models (MLLMs) struggle with recent\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\neffectively expanding the knowledge scope. However, current mRAG methods have\ninherent drawbacks, including: 1) Performing retrieval even when external\nknowledge is not needed. 2) Lacking of identification of evidence that supports\nthe query. 3) Increasing model complexity due to additional information\nfiltering modules or rules. To address these shortcomings, we propose a novel\ngeneralized framework called \\textbf{m}ultimodal\n\\textbf{R}etrieval-\\textbf{R}eflection-\\textbf{A}ugmented \\textbf{G}eneration\n(mR$^2$AG), which achieves adaptive retrieval and useful information\nlocalization to enable answers through two easy-to-implement reflection\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\nis designed to distinguish different user queries and avoids redundant\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\nlocating beneficial evidence of the retrieved content and generating answers\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\nVisual-dependent tasks.", "paper_summary_zh": "\u5148\u9032\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5728\u57fa\u65bc\u77e5\u8b58\u7684 VQA \u4efb\u52d9\u4e2d\u9047\u5230\u56f0\u96e3\uff0c\u4f8b\u5982 INFOSEEK \u548c\u767e\u79d1\u5168\u66f8 VQA\uff0c\u539f\u56e0\u5728\u65bc\u5b83\u5011\u7684\u77e5\u8b58\u7bc4\u570d\u6709\u9650\u4e14\u5df2\u51cd\u7d50\uff0c\u9019\u901a\u5e38\u6703\u5c0e\u81f4\u6a21\u68f1\u5169\u53ef\u4e14\u4e0d\u6e96\u78ba\u7684\u56de\u61c9\u3002\u56e0\u6b64\uff0c\u591a\u6a21\u614b\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (mRAG) \u81ea\u7136\u800c\u7136\u5730\u88ab\u5f15\u5165\uff0c\u4ee5\u5411 MLLM \u63d0\u4f9b\u5168\u9762\u4e14\u6700\u65b0\u7684\u77e5\u8b58\uff0c\u6709\u6548\u5730\u64f4\u5c55\u4e86\u77e5\u8b58\u7bc4\u570d\u3002\u7136\u800c\uff0c\u7576\u524d\u7684 mRAG \u65b9\u6cd5\u6709\u5176\u56fa\u6709\u7684\u7f3a\u9ede\uff0c\u5305\u62ec\uff1a1) \u5373\u4f7f\u4e0d\u9700\u8981\u5916\u90e8\u77e5\u8b58\uff0c\u4e5f\u6703\u57f7\u884c\u6aa2\u7d22\u30022) \u7f3a\u4e4f\u5c0d\u652f\u6301\u67e5\u8a62\u8b49\u64da\u7684\u8b58\u5225\u30023) \u7531\u65bc\u984d\u5916\u7684\u8cc7\u8a0a\u904e\u6ffe\u6a21\u7d44\u6216\u898f\u5247\uff0c\u5c0e\u81f4\u6a21\u578b\u8907\u96dc\u5ea6\u589e\u52a0\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u901a\u7528\u67b6\u69cb\uff0c\u7a31\u70ba **m**ultimodal **R**etrieval-**R**eflection-**A**ugmented **G**eneration (mR$^2$AG)\uff0c\u5b83\u901a\u904e\u5169\u500b\u6613\u65bc\u5be6\u4f5c\u7684\u53cd\u5c04\u64cd\u4f5c\u5be6\u73fe\u4e86\u9069\u61c9\u6027\u6aa2\u7d22\u548c\u6709\u7528\u7684\u8cc7\u8a0a\u5b9a\u4f4d\uff0c\u4ee5\u555f\u7528\u7b54\u6848\uff0c\u9632\u6b62\u6a21\u578b\u8907\u96dc\u5ea6\u9ad8\u3002\u5728 mR$^2$AG \u4e2d\uff0c\u6aa2\u7d22\u53cd\u5c04\u88ab\u8a2d\u8a08\u70ba\u5340\u5206\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u67e5\u8a62\uff0c\u4e26\u907f\u514d\u91cd\u8907\u7684\u6aa2\u7d22\u547c\u53eb\uff0c\u800c\u76f8\u95dc\u6027\u53cd\u5c04\u88ab\u5f15\u5165\u4ee5\u6307\u5c0e MLLM \u5b9a\u4f4d\u6aa2\u7d22\u5167\u5bb9\u7684\u6709\u76ca\u8b49\u64da\uff0c\u4e26\u64da\u6b64\u7522\u751f\u7b54\u6848\u3002\u6b64\u5916\uff0cmR$^2$AG \u53ef\u4ee5\u6574\u5408\u5230\u4efb\u4f55\u8a13\u7df4\u6709\u7d20\u7684 MLLM \u4e2d\uff0c\u4e26\u5728\u63d0\u8b70\u7684 mR$^2$AG \u6307\u4ee4\u5fae\u8abf\u8cc7\u6599\u96c6 (mR$^2$AG-IT) \u4e0a\u9032\u884c\u6709\u6548\u7684\u5fae\u8abf\u3002mR$^2$AG \u5728 INFOSEEK \u548c\u767e\u79d1\u5168\u66f8 VQA \u4e0a\u660e\u986f\u512a\u65bc\u6700\u5148\u9032\u7684 MLLM\uff08\u4f8b\u5982 GPT-4v/o\uff09\u548c\u57fa\u65bc RAG \u7684 MLLM\uff0c\u540c\u6642\u5728\u5ee3\u6cdb\u7684\u8996\u89ba\u4f9d\u8cf4\u4efb\u52d9\u4e2d\u4fdd\u6301\u4e86\u57fa\u790e MLLM \u7684\u5353\u8d8a\u80fd\u529b\u3002", "author": "Tao Zhang et.al.", "authors": "Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, Weiming Hu", "id": "2411.15041v1", "paper_url": "http://arxiv.org/abs/2411.15041v1", "repo": "null"}}