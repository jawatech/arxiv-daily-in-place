{"2411.15982": {"publish_time": "2024-11-24", "title": "Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format", "paper_summary": "The widely-used, weight-only quantized large language models (LLMs), which\nleverage low-bit integer (INT) weights and retain floating-point (FP)\nactivations, reduce storage requirements while maintaining accuracy. However,\nthis shifts the energy and latency bottlenecks towards the FP activations that\nare associated with costly memory accesses and computations. Existing LLM\naccelerators focus primarily on computation optimizations, overlooking the\npotential of jointly optimizing FP computations and data movement, particularly\nfor the dominant FP-INT GeMM operations in LLM inference.\n  To address these challenges, we investigate the sensitivity of activation\nprecision across various LLM modules and its impact on overall model accuracy.\nBased on our findings, we first propose the Anda data type: an adaptive data\nformat with group-shared exponent bits and dynamic mantissa bit allocation.\nSecondly, we develop an iterative post-training adaptive precision search\nalgorithm that optimizes the bit-width for different LLM modules to balance\nmodel accuracy, energy efficiency, and inference speed. Lastly, a suite of\nhardware optimization techniques is proposed to maximally exploit the benefits\nof the Anda format. These include a bit-plane-based data organization scheme,\nAnda-enhanced processing units with bit-serial computation, and a runtime\nbit-plane Anda compressor to simultaneously optimize storage, computation, and\nmemory footprints. Our evaluations on FPINT GeMM operations show that Anda\nachieves a 2.4x speedup, 4.0x area efficiency, and 3.1x energy efficiency\nimprovement on average for popular LLMs including OPT, LLaMA, and LLaMA-2\nseries over the GPU-like FP-FP baseline. Anda demonstrates strong adaptability\nacross various application scenarios, accuracy requirements, and system\nperformance, enabling efficient LLM inference across a wide range of deployment\nscenarios.", "paper_summary_zh": "\u5ee3\u6cdb\u4f7f\u7528\u7684\u3001\u50c5\u6b0a\u91cd\u91cf\u5316\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u5229\u7528\u4f4e\u4f4d\u5143\u6574\u6578 (INT) \u6b0a\u91cd\u4e26\u4fdd\u7559\u6d6e\u9ede (FP) \u6fc0\u6d3b\uff0c\u5728\u7dad\u6301\u7cbe\u6e96\u5ea6\u7684\u540c\u6642\u964d\u4f4e\u5132\u5b58\u9700\u6c42\u3002\u7136\u800c\uff0c\u9019\u5c07\u80fd\u91cf\u548c\u5ef6\u9072\u74f6\u9838\u8f49\u79fb\u5230\u8207\u6602\u8cb4\u7684\u8a18\u61b6\u9ad4\u5b58\u53d6\u548c\u904b\u7b97\u76f8\u95dc\u7684 FP \u6fc0\u6d3b\u3002\u73fe\u6709\u7684 LLM \u52a0\u901f\u5668\u4e3b\u8981\u5c08\u6ce8\u65bc\u904b\u7b97\u6700\u4f73\u5316\uff0c\u5ffd\u8996\u4e86\u806f\u5408\u6700\u4f73\u5316 FP \u904b\u7b97\u548c\u8cc7\u6599\u79fb\u52d5\u7684\u6f5b\u529b\uff0c\u7279\u5225\u662f\u91dd\u5c0d LLM \u63a8\u8ad6\u4e2d\u4f54\u4e3b\u5c0e\u5730\u4f4d\u7684 FP-INT GeMM \u904b\u7b97\u3002\n\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5404\u7a2e LLM \u6a21\u7d44\u4e2d\u6fc0\u6d3b\u7cbe\u5ea6\u7684\u654f\u611f\u6027\u53ca\u5176\u5c0d\u6574\u9ad4\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u97ff\u3002\u6839\u64da\u6211\u5011\u7684\u767c\u73fe\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa Anda \u8cc7\u6599\u985e\u578b\uff1a\u4e00\u7a2e\u5177\u6709\u7fa4\u7d44\u5171\u7528\u6307\u6578\u4f4d\u5143\u548c\u52d5\u614b\u5c3e\u6578\u4f4d\u5143\u914d\u7f6e\u7684\u81ea\u9069\u61c9\u8cc7\u6599\u683c\u5f0f\u3002\u5176\u6b21\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u53cd\u8986\u8a13\u7df4\u5f8c\u81ea\u9069\u61c9\u7cbe\u5ea6\u7684\u641c\u5c0b\u6f14\u7b97\u6cd5\uff0c\u91dd\u5c0d\u4e0d\u540c\u7684 LLM \u6a21\u7d44\u6700\u4f73\u5316\u4f4d\u5143\u5bec\u5ea6\uff0c\u4ee5\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u3001\u80fd\u6e90\u6548\u7387\u548c\u63a8\u8ad6\u901f\u5ea6\u3002\u6700\u5f8c\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u786c\u9ad4\u6700\u4f73\u5316\u6280\u8853\uff0c\u4ee5\u6700\u5927\u7a0b\u5ea6\u5730\u5229\u7528 Anda \u683c\u5f0f\u7684\u512a\u9ede\u3002\u9019\u4e9b\u6280\u8853\u5305\u62ec\u57fa\u65bc\u4f4d\u5143\u5e73\u9762\u7684\u8cc7\u6599\u7d44\u7e54\u67b6\u69cb\u3001\u5177\u6709\u4f4d\u5143\u5e8f\u5217\u904b\u7b97\u7684 Anda \u589e\u5f37\u8655\u7406\u55ae\u5143\uff0c\u4ee5\u53ca\u540c\u6642\u6700\u4f73\u5316\u5132\u5b58\u3001\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u4f54\u7528\u7a7a\u9593\u7684\u57f7\u884c\u6642\u671f\u4f4d\u5143\u5e73\u9762 Anda \u58d3\u7e2e\u5668\u3002\u6211\u5011\u5c0d FPINT GeMM \u904b\u7b97\u7684\u8a55\u4f30\u986f\u793a\uff0c\u8207\u985e GPU \u7684 FP-FP \u57fa\u6e96\u76f8\u6bd4\uff0cAnda \u5c0d\u5305\u62ec OPT\u3001LLaMA \u548c LLaMA-2 \u7cfb\u5217\u5728\u5167\u7684\u71b1\u9580 LLM \u5e73\u5747\u53ef\u5be6\u73fe 2.4 \u500d\u7684\u901f\u5ea6\u63d0\u5347\u30014.0 \u500d\u7684\u9762\u7a4d\u6548\u7387\u548c 3.1 \u500d\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\u3002Anda \u5728\u5404\u7a2e\u61c9\u7528\u5834\u666f\u3001\u7cbe\u5ea6\u8981\u6c42\u548c\u7cfb\u7d71\u6548\u80fd\u4e2d\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u9069\u61c9\u6027\uff0c\u53ef\u5728\u5ee3\u6cdb\u7684\u90e8\u7f72\u5834\u666f\u4e2d\u5be6\u73fe\u9ad8\u6548\u7684 LLM \u63a8\u8ad6\u3002", "author": "Chao Fang et.al.", "authors": "Chao Fang, Man Shi, Robin Geens, Arne Symons, Zhongfeng Wang, Marian Verhelst", "id": "2411.15982v1", "paper_url": "http://arxiv.org/abs/2411.15982v1", "repo": "null"}}