{"2407.19672": {"publish_time": "2024-07-29", "title": "SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages", "paper_summary": "Large Language Models (LLMs) have shown remarkable abilities across various\ntasks, yet their development has predominantly centered on high-resource\nlanguages like English and Chinese, leaving low-resource languages underserved.\nTo address this disparity, we present SeaLLMs 3, the latest iteration of the\nSeaLLMs model family, tailored for Southeast Asian languages. This region,\ncharacterized by its rich linguistic diversity, has lacked adequate language\ntechnology support. SeaLLMs 3 aims to bridge this gap by covering a\ncomprehensive range of languages spoken in this region, including English,\nChinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao,\nTamil, and Javanese. Leveraging efficient language enhancement techniques and a\nspecially constructed instruction tuning dataset, SeaLLMs 3 significantly\nreduces training costs while maintaining high performance and versatility. Our\nmodel excels in tasks such as world knowledge, mathematical reasoning,\ntranslation, and instruction following, achieving state-of-the-art performance\namong similarly sized models. Additionally, we prioritized safety and\nreliability by addressing both general and culture-specific considerations and\nincorporated mechanisms to reduce hallucinations. This work underscores the\nimportance of inclusive AI, showing that advanced LLM capabilities can benefit\nunderserved linguistic and cultural communities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u4f46\u5176\u767c\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8a9e\u548c\u4e2d\u6587\u7b49\u9ad8\u8cc7\u6e90\u8a9e\u8a00\u4e0a\uff0c\u5c0e\u81f4\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7121\u6cd5\u7372\u5f97\u670d\u52d9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u7a2e\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SeaLLMs 3\uff0c\u9019\u662f SeaLLMs \u6a21\u578b\u7cfb\u5217\u7684\u6700\u65b0\u7248\u672c\uff0c\u5c08\u9580\u91dd\u5c0d\u6771\u5357\u4e9e\u8a9e\u8a00\u8a2d\u8a08\u3002\u9019\u500b\u5730\u5340\u4ee5\u5176\u8c50\u5bcc\u7684\u8a9e\u8a00\u591a\u6a23\u6027\u70ba\u7279\u5fb5\uff0c\u4f46\u7f3a\u4e4f\u8db3\u5920\u7684\u8a9e\u8a00\u6280\u8853\u652f\u63f4\u3002SeaLLMs 3 \u65e8\u5728\u6db5\u84cb\u8a72\u5730\u5340\u6240\u4f7f\u7528\u7684\u5404\u7a2e\u8a9e\u8a00\uff0c\u5305\u62ec\u82f1\u8a9e\u3001\u4e2d\u6587\u3001\u5370\u5c3c\u8a9e\u3001\u8d8a\u5357\u8a9e\u3001\u6cf0\u8a9e\u3001\u4ed6\u52a0\u797f\u8a9e\u3001\u99ac\u4f86\u8a9e\u3001\u7dec\u7538\u8a9e\u3001\u9ad8\u68c9\u8a9e\u3001\u5bee\u8a9e\u3001\u6cf0\u7c73\u723e\u8a9e\u548c\u722a\u54c7\u8a9e\uff0c\u4ee5\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\u3002\u5229\u7528\u9ad8\u6548\u7684\u8a9e\u8a00\u589e\u5f37\u6280\u8853\u548c\u4e00\u500b\u7279\u5225\u5efa\u69cb\u7684\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u96c6\uff0cSeaLLMs 3 \u5927\u5e45\u964d\u4f4e\u4e86\u8a13\u7df4\u6210\u672c\uff0c\u540c\u6642\u7dad\u6301\u9ad8\u6027\u80fd\u548c\u591a\u529f\u80fd\u6027\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u4e16\u754c\u77e5\u8b58\u3001\u6578\u5b78\u63a8\u7406\u3001\u7ffb\u8b6f\u548c\u6307\u4ee4\u9075\u5faa\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u5728\u540c\u7b49\u898f\u6a21\u7684\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u512a\u5148\u8003\u616e\u5b89\u5168\u6027\u8207\u53ef\u9760\u6027\uff0c\u89e3\u6c7a\u4e86\u4e00\u822c\u6027\u548c\u7279\u5b9a\u6587\u5316\u7684\u8003\u91cf\uff0c\u4e26\u7d0d\u5165\u4e86\u6e1b\u5c11\u5e7b\u89ba\u7684\u6a5f\u5236\u3002\u9019\u9805\u5de5\u4f5c\u5f37\u8abf\u4e86\u5305\u5bb9\u6027 AI \u7684\u91cd\u8981\u6027\uff0c\u986f\u793a\u5148\u9032\u7684 LLM \u80fd\u529b\u53ef\u4ee5\u4f7f\u670d\u52d9\u4e0d\u8db3\u7684\u8a9e\u8a00\u548c\u6587\u5316\u793e\u7fa4\u53d7\u76ca\u3002", "author": "Wenxuan Zhang et.al.", "authors": "Wenxuan Zhang, Hou Pong Chan, Yiran Zhao, Mahani Aljunied, Jianyu Wang, Chaoqun Liu, Yue Deng, Zhiqiang Hu, Weiwen Xu, Yew Ken Chia, Xin Li, Lidong Bing", "id": "2407.19672v1", "paper_url": "http://arxiv.org/abs/2407.19672v1", "repo": "null"}}