{"2410.08661": {"publish_time": "2024-10-11", "title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "paper_summary": "With the rapid growth in the use of fine-tuning for large language models\n(LLMs), optimizing fine-tuning while keeping inference efficient has become\nhighly important. However, this is a challenging task as it requires\nimprovements in all aspects, including inference speed, fine-tuning speed,\nmemory consumption, and, most importantly, model quality. Previous studies have\nattempted to achieve this by combining quantization with fine-tuning, but they\nhave failed to enhance all four aspects simultaneously. In this study, we\npropose a new lightweight technique called Quantization for Efficient\nFine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is\nsupported by robust theoretical foundations, offers high flexibility, and\nmaintains good hardware compatibility. Our extensive experiments demonstrate\nthat QEFT matches the quality and versatility of full-precision\nparameter-efficient fine-tuning, while using fewer resources. Our code is\navailable at https://github.com/xvyaward/qeft.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8abf\u4f7f\u7528\u7684\u5feb\u901f\u589e\u9577\uff0c\u5728\u4fdd\u6301\u63a8\u8ad6\u6548\u7387\u7684\u540c\u6642\u512a\u5316\u5fae\u8abf\u5df2\u8b8a\u5f97\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u9019\u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u56e0\u70ba\u5b83\u9700\u8981\u5728\u6240\u6709\u65b9\u9762\u9032\u884c\u6539\u9032\uff0c\u5305\u62ec\u63a8\u8ad6\u901f\u5ea6\u3001\u5fae\u8abf\u901f\u5ea6\u3001\u8a18\u61b6\u9ad4\u6d88\u8017\uff0c\u6700\u91cd\u8981\u7684\u662f\u6a21\u578b\u54c1\u8cea\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u5617\u8a66\u900f\u904e\u5c07\u91cf\u5316\u8207\u5fae\u8abf\u76f8\u7d50\u5408\u4f86\u9054\u6210\u6b64\u76ee\u6a19\uff0c\u4f46\u5b83\u5011\u672a\u80fd\u540c\u6642\u63d0\u5347\u6240\u6709\u56db\u500b\u65b9\u9762\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u9ad8\u6548\u5fae\u8abf\u91cf\u5316\uff08QEFT\uff09\u7684\u65b0\u8f15\u91cf\u7d1a\u6280\u8853\u3002QEFT \u52a0\u901f\u4e86\u63a8\u8ad6\u548c\u5fae\u8abf\uff0c\u4e26\u53d7\u5230\u7a69\u5065\u7684\u7406\u8ad6\u57fa\u790e\u652f\u6301\uff0c\u63d0\u4f9b\u9ad8\u5ea6\u7684\u9748\u6d3b\u6027\uff0c\u4e26\u7dad\u6301\u826f\u597d\u7684\u786c\u9ad4\u76f8\u5bb9\u6027\u3002\u6211\u5011\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0cQEFT \u5728\u4f7f\u7528\u8f03\u5c11\u8cc7\u6e90\u7684\u60c5\u6cc1\u4e0b\uff0c\u7b26\u5408\u5168\u7cbe\u5ea6\u53c3\u6578\u6709\u6548\u5fae\u8abf\u7684\u54c1\u8cea\u548c\u591a\u529f\u80fd\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/xvyaward/qeft \u53d6\u5f97\u3002", "author": "Changhun Lee et.al.", "authors": "Changhun Lee, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park", "id": "2410.08661v1", "paper_url": "http://arxiv.org/abs/2410.08661v1", "repo": "https://github.com/xvyaward/qeft"}}