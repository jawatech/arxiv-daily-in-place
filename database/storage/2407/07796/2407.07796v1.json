{"2407.07796": {"publish_time": "2024-07-10", "title": "Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard", "paper_summary": "We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.", "paper_summary_zh": "<paragraph>\u6211\u5011\u900f\u904e\u4e95\u5b57\u904a\u6232\u3001\u56db\u9023\u7dda\u548c\u4e94\u5b50\u68cb\u7b49\u683c\u7dda\u904a\u6232\uff0c\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u4e14\u53ef\u64f4\u5c55\u7684\u57fa\u6e96\u3002\u53ef\u4ee5\u5728 GitHub \u4e0a\u53d6\u5f97\u7684\u958b\u653e\u539f\u59cb\u78bc\u904a\u6232\u6a21\u64ec\u7a0b\u5f0f\u78bc\uff0c\u5141\u8a31 LLM \u7af6\u722d\u4e26\u7522\u751f JSON\u3001CSV\u3001TXT \u548c PNG \u683c\u5f0f\u7684\u8a73\u7d30\u8cc7\u6599\u6a94\uff0c\u4ee5\u9032\u884c\u6392\u884c\u699c\u6392\u540d\u548c\u9032\u4e00\u6b65\u5206\u6790\u3002\u6211\u5011\u5c55\u793a\u4e86\u5305\u62ec Anthropic \u7684 Claude 3.5 Sonnet \u548c Claude 3 Sonnet\u3001Google \u7684 Gemini 1.5 Pro \u548c Gemini 1.5 Flash\u3001OpenAI \u7684 GPT-4 Turbo \u548c GPT-4o\uff0c\u4ee5\u53ca Meta \u7684 Llama3-70B \u7b49\u9818\u5148 LLM \u4e4b\u9593\u7684\u904a\u6232\u7d50\u679c\u3002\u6211\u5011\u4e5f\u9f13\u52f5\u63d0\u4ea4\u5176\u4ed6 LLM \u7684\u7d50\u679c\u3002\u7e3d\u8a08\u6211\u5011\u6a21\u64ec\u4e86 2,310 \u5834\u6bd4\u8cfd\uff087 \u500b LLM \u548c\u4e00\u500b\u96a8\u6a5f\u73a9\u5bb6\u4e4b\u9593\u7684\u6bcf\u5c0d 5 \u5834\u8cfd\u4e8b\uff09\uff0c\u6a6b\u8de8\u4e09\u7a2e\u985e\u578b\u7684\u904a\u6232\uff0c\u4f7f\u7528\u4e09\u7a2e\u4e0d\u540c\u7684\u63d0\u793a\u985e\u578b\uff1a\u6e05\u55ae\u3001\u63d2\u5716\u548c\u5716\u50cf\u3002\u7d50\u679c\u63ed\u793a\u4e86 LLM \u5728\u4e0d\u540c\u904a\u6232\u548c\u63d0\u793a\u985e\u578b\u4e2d\u7684\u6548\u80fd\u6709\u986f\u8457\u5dee\u7570\uff0c\u5206\u6790\u6db5\u84cb\u4e86\u7372\u52dd\u548c\u53d6\u6d88\u8cc7\u683c\u7387\u3001\u932f\u5931\u6a5f\u6703\u5206\u6790\u548c\u7121\u6548\u79fb\u52d5\u5206\u6790\u3002\u6392\u884c\u699c\u548c\u7d50\u679c\u77e9\u9663\u8cc7\u6599\u7684\u8a73\u7d30\u8cc7\u8a0a\u53ef\u5728 GitHub \u4e0a\u53d6\u5f97\u958b\u653e\u5b58\u53d6\u8cc7\u6599\u3002\u9019\u9805\u7814\u7a76\u589e\u9032\u4e86\u6211\u5011\u5c0d LLM \u5728\u672a\u7d93\u7279\u5225\u8a13\u7df4\u7684\u904a\u6232\u4e2d\u9032\u884c\u904a\u6232\u7684\u80fd\u529b\u7684\u4e86\u89e3\uff0c\u6709\u52a9\u65bc\u8a55\u4f30\u5b83\u5011\u7684\u898f\u5247\u7406\u89e3\u548c\u7b56\u7565\u601d\u8003\u3002\u5728\u901a\u5f80\u4eba\u5de5\u901a\u7528\u667a\u6167 (AGI) \u7684\u9053\u8def\u4e0a\uff0c\u9019\u9805\u7814\u7a76\u70ba\u672a\u4f86\u63a2\u7d22\u5b83\u5011\u5728\u8907\u96dc\u6c7a\u7b56\u60c5\u5883\u4e2d\u7684\u6548\u7528\u5960\u5b9a\u4e86\u57fa\u790e\uff0c\u95e1\u660e\u4e86\u5b83\u5011\u7684\u7b56\u7565\u601d\u8003\u80fd\u529b\uff0c\u4e26\u70ba\u9032\u4e00\u6b65\u63a2\u8a0e LLM \u5728\u57fa\u65bc\u904a\u6232\u7684\u67b6\u69cb\u4e2d\u7684\u9650\u5236\u63d0\u4f9b\u4e86\u65b9\u5411\u3002</paragraph>", "author": "Oguzhan Topsakal et.al.", "authors": "Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper", "id": "2407.07796v1", "paper_url": "http://arxiv.org/abs/2407.07796v1", "repo": "https://github.com/research-outcome/llm-game-benchmark"}}