
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-12**|**Doe-1: Closed-Loop Autonomous Driving with Large World Model**|Wenzhao Zheng et.al.|[2412.09627v1](http://arxiv.org/abs/2412.09627v1)|[link](https://github.com/wzzheng/doe)|
|**2024-12-12**|**Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG**|Kavana Venkatesh et.al.|[2412.09614v1](http://arxiv.org/abs/2412.09614v1)|null|
|**2024-12-12**|**Olympus: A Universal Task Router for Computer Vision Tasks**|Yuanze Lin et.al.|[2412.09612v1](http://arxiv.org/abs/2412.09612v1)|null|
|**2024-12-12**|**AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials**|Yiheng Xu et.al.|[2412.09605v1](http://arxiv.org/abs/2412.09605v1)|null|
|**2024-12-12**|**Hidden Biases of End-to-End Driving Datasets**|Julian Zimmerlin et.al.|[2412.09602v1](http://arxiv.org/abs/2412.09602v1)|[link](https://github.com/autonomousvision/carla_garage)|
|**2024-12-12**|**TimeRefine: Temporal Grounding with Time Refining Video LLM**|Xizi Wang et.al.|[2412.09601v1](http://arxiv.org/abs/2412.09601v1)|null|
|**2024-12-12**|**Owl-1: Omni World Model for Consistent Long Video Generation**|Yuanhui Huang et.al.|[2412.09600v1](http://arxiv.org/abs/2412.09600v1)|[link](https://github.com/huang-yh/owl)|
|**2024-12-12**|**InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**|Pan Zhang et.al.|[2412.09596v1](http://arxiv.org/abs/2412.09596v1)|[link](https://github.com/internlm/internlm-xcomposer)|
|**2024-12-12**|**OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages**|Chester Palen-Michel et.al.|[2412.09587v1](http://arxiv.org/abs/2412.09587v1)|null|
|**2024-12-12**|**Neptune: The Long Orbit to Benchmarking Long Video Understanding**|Arsha Nagrani et.al.|[2412.09582v1](http://arxiv.org/abs/2412.09582v1)|[link](https://github.com/google-deepmind/neptune)|
|**2024-12-12**|**A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks**|Saptarshi Mandal et.al.|[2412.09579v1](http://arxiv.org/abs/2412.09579v1)|null|
|**2024-12-12**|**DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial NEtworks and Semantic Topic classification**|Caleb Stam et.al.|[2412.09578v1](http://arxiv.org/abs/2412.09578v1)|null|
|**2024-12-12**|**DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction**|Yu Feng et.al.|[2412.09572v1](http://arxiv.org/abs/2412.09572v1)|null|
|**2024-12-12**|**JuStRank: Benchmarking LLM Judges for System Ranking**|Ariel Gera et.al.|[2412.09569v1](http://arxiv.org/abs/2412.09569v1)|null|
|**2024-12-12**|**Does Representation Matter? Exploring Intermediate Layers in Large Language Models**|Oscar Skean et.al.|[2412.09563v1](http://arxiv.org/abs/2412.09563v1)|null|
|**2024-12-12**|**Foundational Large Language Models for Materials Research**|Vaibhav Mishra et.al.|[2412.09560v1](http://arxiv.org/abs/2412.09560v1)|null|
|**2024-12-12**|**Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking**|Paria Rashidinejad et.al.|[2412.09544v1](http://arxiv.org/abs/2412.09544v1)|null|
|**2024-12-12**|**Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis**|Shengxuming Zhang et.al.|[2412.09521v1](http://arxiv.org/abs/2412.09521v1)|null|
|**2024-12-12**|**Video Seal: Open and Efficient Video Watermarking**|Pierre Fernandez et.al.|[2412.09492v1](http://arxiv.org/abs/2412.09492v1)|[link](https://github.com/facebookresearch/videoseal)|
|**2024-12-12**|**Regression and Classification with Single-Qubit Quantum Neural Networks**|Leandro C. Souza et.al.|[2412.09486v1](http://arxiv.org/abs/2412.09486v1)|null|
|**2024-12-12**|**The Parameters of Educability**|Leslie G. Valiant et.al.|[2412.09480v1](http://arxiv.org/abs/2412.09480v1)|null|
|**2024-12-12**|**New keypoint-based approach for recognising British Sign Language (BSL) from sequences**|Oishi Deb et.al.|[2412.09475v1](http://arxiv.org/abs/2412.09475v1)|null|
|**2024-12-12**|**Audios Don't Lie: Multi-Frequency Channel Attention Mechanism for Audio Deepfake Detection**|Yangguang Feng et.al.|[2412.09467v1](http://arxiv.org/abs/2412.09467v1)|null|
|**2024-12-12**|**STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading**|Yilei Zhao et.al.|[2412.09468v1](http://arxiv.org/abs/2412.09468v1)|null|
|**2024-12-12**|**The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective**|Javier de la Rosa et.al.|[2412.09460v1](http://arxiv.org/abs/2412.09460v1)|null|
|**2024-12-12**|**From Intention To Implementation: Automating Biomedical Research via LLMs**|Yi Luo et.al.|[2412.09429v1](http://arxiv.org/abs/2412.09429v1)|null|
|**2024-12-12**|**Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer**|Adam Labiosa et.al.|[2412.09417v1](http://arxiv.org/abs/2412.09417v1)|null|
|**2024-12-12**|**Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors**|Kaushal Kumar Maurya et.al.|[2412.09416v1](http://arxiv.org/abs/2412.09416v1)|null|
|**2024-12-12**|**Text Generation Models for Luxembourgish with Limited Data: A Balanced Multilingual Strategy**|Alistair Plum et.al.|[2412.09415v1](http://arxiv.org/abs/2412.09415v1)|null|
|**2024-12-12**|**Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems**|Yingqian Min et.al.|[2412.09413v1](http://arxiv.org/abs/2412.09413v1)|null|
|**2024-12-12**|**Uncommon Belief in Rationality**|Qi Shi et.al.|[2412.09407v1](http://arxiv.org/abs/2412.09407v1)|null|
|**2024-12-12**|**UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer**|Delong Liu et.al.|[2412.09389v1](http://arxiv.org/abs/2412.09389v1)|[link](https://github.com/delong-liu-bupt/ufo)|
|**2024-12-12**|**All You Need in Knowledge Distillation Is a Tailored Coordinate System**|Junjie Zhou et.al.|[2412.09388v1](http://arxiv.org/abs/2412.09388v1)|null|
|**2024-12-12**|**AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities**|Fabrizio Davide et.al.|[2412.09385v1](http://arxiv.org/abs/2412.09385v1)|null|
|**2024-12-12**|**Neural Text Normalization for Luxembourgish using Real-Life Variation Data**|Anne-Marie Lutgen et.al.|[2412.09383v1](http://arxiv.org/abs/2412.09383v1)|null|
|**2024-12-12**|**Diffusion Model with Representation Alignment for Protein Inverse Folding**|Chenglin Wang et.al.|[2412.09380v1](http://arxiv.org/abs/2412.09380v1)|null|
|**2024-12-12**|**Word Sense Linking: Disambiguating Outside the Sandbox**|Andrei Stefan Bejgu et.al.|[2412.09370v1](http://arxiv.org/abs/2412.09370v1)|null|
|**2024-12-12**|**Falcon-UI: Understanding GUI Before Following User Instructions**|Huawen Shen et.al.|[2412.09362v1](http://arxiv.org/abs/2412.09362v1)|null|
|**2024-12-12**|**Causal Graphical Models for Vision-Language Compositional Understanding**|Fiorenzo Parascandolo et.al.|[2412.09353v1](http://arxiv.org/abs/2412.09353v1)|null|
|**2024-12-12**|**Training LayoutLM from Scratch for Efficient Named-Entity Recognition in the Insurance Domain**|Benno Uthayasooriyar et.al.|[2412.09341v1](http://arxiv.org/abs/2412.09341v1)|null|
|**2024-12-12**|**Does Low Spoilage Under Cold Conditions Foster Cultural Complexity During the Foraging Era? -- A Theoretical and Computational Inquiry**|Minhyeok Lee et.al.|[2412.09335v1](http://arxiv.org/abs/2412.09335v1)|null|
|**2024-12-12**|**Towards Open-Vocabulary Video Semantic Segmentation**|Xinhao Li et.al.|[2412.09329v1](http://arxiv.org/abs/2412.09329v1)|null|
|**2024-12-12**|**Auto-Regressive Moving Diffusion Models for Time Series Forecasting**|Jiaxin Gao et.al.|[2412.09328v1](http://arxiv.org/abs/2412.09328v1)|[link](https://github.com/daxin007/armd)|
|**2024-12-12**|**Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction**|Jing Liu et.al.|[2412.09318v1](http://arxiv.org/abs/2412.09318v1)|null|
|**2024-12-12**|**Multimodal Sentiment Analysis based on Video and Audio Inputs**|Antonio Fernandez et.al.|[2412.09317v1](http://arxiv.org/abs/2412.09317v1)|null|
|**2024-12-12**|**Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation**|Davor Vukadin et.al.|[2412.09311v1](http://arxiv.org/abs/2412.09311v1)|[link](https://github.com/davor10105/relative-absolute-magnitude-propagation)|
|**2024-12-12**|**Learning Novel Skills from Language-Generated Demonstrations**|Ao-Qun Jin et.al.|[2412.09286v1](http://arxiv.org/abs/2412.09286v1)|null|
|**2024-12-12**|**InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption**|Tiehan Fan et.al.|[2412.09283v1](http://arxiv.org/abs/2412.09283v1)|null|
|**2024-12-12**|**CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of LLMs**|Yuzhuang Xu et.al.|[2412.09282v1](http://arxiv.org/abs/2412.09282v1)|null|
|**2024-12-12**|**Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator**|Chengyuan Liu et.al.|[2412.09280v1](http://arxiv.org/abs/2412.09280v1)|null|
|**2024-12-12**|**Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine**|Xiaoshuang Huang et.al.|[2412.09278v1](http://arxiv.org/abs/2412.09278v1)|[link](https://github.com/shawnhuang497/medplib)|
|**2024-12-12**|**Towards Understanding the Robustness of LLM-based Evaluations under Perturbations**|Manav Chaudhary et.al.|[2412.09269v1](http://arxiv.org/abs/2412.09269v1)|null|
|**2024-12-12**|**First Train to Generate, then Generate to Train: UnitedSynT5 for Few-Shot NLI**|Sourav Banerjee et.al.|[2412.09263v1](http://arxiv.org/abs/2412.09263v1)|null|
|**2024-12-12**|**Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by Utilizing Generative LLMs**|Asli Umay Ozturk et.al.|[2412.09247v1](http://arxiv.org/abs/2412.09247v1)|null|
|**2024-12-12**|**VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation with Unsupervised Domain Adaptation**|Roberto Alcover-Couso et.al.|[2412.09240v1](http://arxiv.org/abs/2412.09240v1)|null|
|**2024-12-12**|**LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation**|Yijun Liu et.al.|[2412.09237v1](http://arxiv.org/abs/2412.09237v1)|null|
|**2024-12-12**|**Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering**|Sai Bhargav Rongali et.al.|[2412.09230v1](http://arxiv.org/abs/2412.09230v1)|null|
|**2024-12-12**|**CSSDH: An Ontology for Social Determinants of Health to Operational Continuity of Care Data Interoperability**|Subhashis Das et.al.|[2412.09223v1](http://arxiv.org/abs/2412.09223v1)|null|
|**2024-12-12**|**CleanComedy: Creating Friendly Humor through Generative Techniques**|Dmitry Vikhorev et.al.|[2412.09203v1](http://arxiv.org/abs/2412.09203v1)|null|
|**2024-12-12**|**ReFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks**|Jiashu Yao et.al.|[2412.09173v1](http://arxiv.org/abs/2412.09173v1)|null|
|**2024-12-12**|**When Text Embedding Meets Large Language Model: A Comprehensive Survey**|Zhijie Nie et.al.|[2412.09165v1](http://arxiv.org/abs/2412.09165v1)|null|
|**2024-12-12**|**Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning**|Meng Shen et.al.|[2412.09126v1](http://arxiv.org/abs/2412.09126v1)|null|
|**2024-12-12**|**Goal-Driven Query Answering over First- and Second-Order Dependencies with Equality**|Efthymia Tsamoura et.al.|[2412.09125v1](http://arxiv.org/abs/2412.09125v1)|null|
|**2024-12-12**|**In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning**|Songjun Tu et.al.|[2412.09104v1](http://arxiv.org/abs/2412.09104v1)|null|
|**2024-12-12**|**PolyIPA -- Multilingual Phoneme-to-Grapheme Conversion Model**|Davor Lauc et.al.|[2412.09102v1](http://arxiv.org/abs/2412.09102v1)|null|
|**2024-12-12**|**Temporal Numeric Planning with Patterns**|Matteo Cardellini et.al.|[2412.09101v1](http://arxiv.org/abs/2412.09101v1)|null|
|**2024-12-12**|**Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion**|Ben Liu et.al.|[2412.09094v1](http://arxiv.org/abs/2412.09094v1)|[link](https://github.com/lb0828/ftg)|
|**2024-12-12**|**Evaluating Pixel Language Models on Non-Standardized Languages**|Alberto Muñoz-Ortiz et.al.|[2412.09084v1](http://arxiv.org/abs/2412.09084v1)|null|
|**2024-12-12**|**Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning**|Zhenni Bi et.al.|[2412.09078v1](http://arxiv.org/abs/2412.09078v1)|null|
|**2024-12-12**|**EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems**|Huanqi Yang et.al.|[2412.09058v1](http://arxiv.org/abs/2412.09058v1)|null|
|**2024-12-12**|**Dial-In LLM: Human-Aligned Dialogue Intent Clustering with LLM-in-the-loop**|Mengze Hong et.al.|[2412.09049v1](http://arxiv.org/abs/2412.09049v1)|null|
|**2024-12-12**|**Multi-Task Learning with LLMs for Implicit Sentiment Analysis: Data-level and Task-level Automatic Weight Learning**|Wenna Lai et.al.|[2412.09046v1](http://arxiv.org/abs/2412.09046v1)|null|
|**2024-12-12**|**Motif Guided Graph Transformer with Combinatorial Skeleton Prototype Learning for Skeleton-Based Person Re-Identification**|Haocong Rao et.al.|[2412.09044v1](http://arxiv.org/abs/2412.09044v1)|[link](https://github.com/kali-hac/mocos)|
|**2024-12-12**|**ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty**|Meizhi Zhong et.al.|[2412.09036v1](http://arxiv.org/abs/2412.09036v1)|null|
|**2024-12-12**|**Dialogue Language Model with Large-Scale Persona Data Engineering**|Mengze Hong et.al.|[2412.09034v1](http://arxiv.org/abs/2412.09034v1)|null|
|**2024-12-12**|**Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis**|Zhoulin Ji et.al.|[2412.09032v1](http://arxiv.org/abs/2412.09032v1)|null|
|**2024-12-12**|**RingFormer: A Ring-Enhanced Graph Transformer for Organic Solar Cell Property Prediction**|Zhihao Ding et.al.|[2412.09030v1](http://arxiv.org/abs/2412.09030v1)|null|
|**2024-12-12**|**Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages**|Advait Joglekar et.al.|[2412.09025v1](http://arxiv.org/abs/2412.09025v1)|null|
|**2024-12-12**|**Improvement in Sign Language Translation Using Text CTC Alignment**|Sihan Tan et.al.|[2412.09014v1](http://arxiv.org/abs/2412.09014v1)|null|
|**2024-12-12**|**What Makes Cryptic Crosswords Challenging for LLMs?**|Abdelrahman Sadallah et.al.|[2412.09012v1](http://arxiv.org/abs/2412.09012v1)|[link](https://github.com/bodasadallah/decrypting-crosswords)|
|**2024-12-12**|**Assessing the Robustness of Retrieval-Augmented Generation Systems in K-12 Educational Question Answering with Knowledge Discrepancies**|Tianshi Zheng et.al.|[2412.08985v1](http://arxiv.org/abs/2412.08985v1)|null|
|**2024-12-12**|**Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?**|Yifan Zhang et.al.|[2412.08973v1](http://arxiv.org/abs/2412.08973v1)|null|
|**2024-12-12**|**RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios**|Ruiwen Zhou et.al.|[2412.08972v1](http://arxiv.org/abs/2412.08972v1)|[link](https://github.com/skyriver-2000/rulearena)|
|**2024-12-12**|**Reasoning-Aware Query-Focused Summarization over Multi-Table Data**|Xiaochuan Lin et.al.|[2412.08970v1](http://arxiv.org/abs/2412.08970v1)|null|
|**2024-12-12**|**AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection**|Zihan Ji et.al.|[2412.08965v1](http://arxiv.org/abs/2412.08965v1)|null|
|**2024-12-12**|**Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning**|Mateo Alejandro Rojas et.al.|[2412.08955v1](http://arxiv.org/abs/2412.08955v1)|null|
|**2024-12-12**|**Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning**|Zhongyang Zhang et.al.|[2412.08950v1](http://arxiv.org/abs/2412.08950v1)|null|
|**2024-12-12**|**Mojito: Motion Trajectory and Intensity Control for Video Generation**|Xuehai He et.al.|[2412.08948v1](http://arxiv.org/abs/2412.08948v1)|null|
|**2024-12-12**|**Selective Visual Prompting in Vision Mamba**|Yifeng Yao et.al.|[2412.08947v1](http://arxiv.org/abs/2412.08947v1)|null|
|**2024-12-12**|**MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning**|Lulu Zhao et.al.|[2412.08946v1](http://arxiv.org/abs/2412.08946v1)|null|
|**2024-12-12**|**Multi-Scale Heterogeneous Text-Attributed Graph Datasets From Diverse Domains**|Yunhui Liu et.al.|[2412.08937v1](http://arxiv.org/abs/2412.08937v1)|null|
|**2024-12-12**|**From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning**|Pusen Dong et.al.|[2412.08920v1](http://arxiv.org/abs/2412.08920v1)|null|
|**2024-12-12**|**Goal-Conditioned Supervised Learning for Multi-Objective Recommendation**|Shijun Li et.al.|[2412.08911v1](http://arxiv.org/abs/2412.08911v1)|null|
|**2024-12-12**|**Phi-4 Technical Report**|Marah Abdin et.al.|[2412.08905v1](http://arxiv.org/abs/2412.08905v1)|null|
|**2024-12-12**|**Radiology Report Generation via Multi-objective Preference Optimization**|Ting Xiao et.al.|[2412.08901v1](http://arxiv.org/abs/2412.08901v1)|null|
|**2024-12-12**|**AI-assisted Knowledge Discovery in Biomedical Literature to Support Decision-making in Precision Oncology**|Ting He et.al.|[2412.08900v1](http://arxiv.org/abs/2412.08900v1)|null|
|**2024-12-12**|**Neural Interactive Proofs**|Lewis Hammond et.al.|[2412.08897v1](http://arxiv.org/abs/2412.08897v1)|null|
|**2024-12-12**|**SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization**|Kwangryeol Park et.al.|[2412.08894v1](http://arxiv.org/abs/2412.08894v1)|null|
|**2024-12-12**|**Residual Channel Boosts Contrastive Learning for Radio Frequency Fingerprint Identification**|Rui Pan et.al.|[2412.08885v1](http://arxiv.org/abs/2412.08885v1)|null|
|**2024-12-12**|**Towards modeling evolving longitudinal health trajectories with a transformer-based deep learning model**|Hans Moen et.al.|[2412.08873v1](http://arxiv.org/abs/2412.08873v1)|null|

#### Abstracts
##### **Doe-1: Closed-Loop Autonomous Driving with Large World Model**
2412.09627v1 by Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, Jiwen Lu

End-to-end autonomous driving has received increasing attention due to its
potential to learn from large amounts of data. However, most existing methods
are still open-loop and suffer from weak scalability, lack of high-order
interactions, and inefficient decision-making. In this paper, we explore a
closed-loop framework for autonomous driving and propose a large Driving wOrld
modEl (Doe-1) for unified perception, prediction, and planning. We formulate
autonomous driving as a next-token generation problem and use multi-modal
tokens to accomplish different tasks. Specifically, we use free-form texts
(i.e., scene descriptions) for perception and generate future predictions
directly in the RGB space with image tokens. For planning, we employ a
position-aware tokenizer to effectively encode action into discrete tokens. We
train a multi-modal transformer to autoregressively generate perception,
prediction, and planning tokens in an end-to-end and unified manner.
Experiments on the widely used nuScenes dataset demonstrate the effectiveness
of Doe-1 in various tasks including visual question-answering,
action-conditioned video generation, and motion planning. Code:
https://github.com/wzzheng/Doe.

摘要：端到端自動駕駛由於其從大量資料中學習的潛力而備受關注。然而，大多數現有方法仍然是開迴路，並受到可擴充性弱、缺乏高階互動以及決策效率低下的困擾。在本文中，我們探討了一個用於自動駕駛的閉迴路架構，並提出了一個大型駕駛世界模型 (Doe-1) 以實現統一的感知、預測和規劃。我們將自動駕駛表述為一個下一個符號生成問題，並使用多模態符號來完成不同的任務。具體來說，我們使用自由形式文本（即場景描述）進行感知，並直接在 RGB 空間中使用影像符號生成未來的預測。對於規劃，我們採用一個位置感知符號化器，將動作有效編碼成離散符號。我們訓練一個多模態Transformer，以自迴歸方式生成感知、預測和規劃符號，並以端到端和統一的方式進行。在廣泛使用的 nuScenes 資料集上進行的實驗證明了 Doe-1 在各種任務中的有效性，包括視覺問答、動作條件影片生成和動作規劃。程式碼：https://github.com/wzzheng/Doe。

##### **Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG**
2412.09614v1 by Kavana Venkatesh, Yusuf Dalva, Ismini Lourentzou, Pinar Yanardag

We introduce a novel approach to enhance the capabilities of text-to-image
models by incorporating a graph-based RAG. Our system dynamically retrieves
detailed character information and relational data from the knowledge graph,
enabling the generation of visually accurate and contextually rich images. This
capability significantly improves upon the limitations of existing T2I models,
which often struggle with the accurate depiction of complex or culturally
specific subjects due to dataset constraints. Furthermore, we propose a novel
self-correcting mechanism for text-to-image models to ensure consistency and
fidelity in visual outputs, leveraging the rich context from the graph to guide
corrections. Our qualitative and quantitative experiments demonstrate that
Context Canvas significantly enhances the capabilities of popular models such
as Flux, Stable Diffusion, and DALL-E, and improves the functionality of
ControlNet for fine-grained image editing tasks. To our knowledge, Context
Canvas represents the first application of graph-based RAG in enhancing T2I
models, representing a significant advancement for producing high-fidelity,
context-aware multi-faceted images.

摘要：我們提出了一種新的方法來增強文字轉圖像模型的能力，方法是加入一個基於圖形的 RAG。我們的系統從知識圖譜中動態檢索詳細的角色資訊和關係資料，能夠產生視覺上準確且內容豐富的圖像。這種能力顯著地改善了現有 T2I 模型的限制，這些模型由於資料集的限制，通常難以準確描繪複雜或特定文化的題材。此外，我們提出了一種新的文字轉圖像模型自我修正機制，以確保視覺輸出的連貫性和保真度，利用圖形中的豐富內容來指導修正。我們的定性和定量實驗表明，Context Canvas 大幅增強了 Flux、Stable Diffusion 和 DALL-E 等熱門模型的能力，並改善了 ControlNet 在精細影像編輯任務中的功能。據我們所知，Context Canvas 代表了基於圖形的 RAG 在增強 T2I 模型中的首次應用，這代表了產生高保真、具有脈絡感知的多面向圖像的重大進展。

##### **Olympus: A Universal Task Router for Computer Vision Tasks**
2412.09612v1 by Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip H. S. Torr

We introduce Olympus, a new approach that transforms Multimodal Large
Language Models (MLLMs) into a unified framework capable of handling a wide
array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates
over 20 specialized tasks across images, videos, and 3D objects to dedicated
modules. This instruction-based routing enables complex workflows through
chained actions without the need for training heavy generative models. Olympus
easily integrates with existing MLLMs, expanding their capabilities with
comparable performance. Experimental results demonstrate that Olympus achieves
an average routing accuracy of 94.75% across 20 tasks and precision of 91.82%
in chained action scenarios, showcasing its effectiveness as a universal task
router that can solve a diverse range of computer vision tasks. Project page:
https://github.com/yuanze-lin/Olympus_page

摘要：我們介紹 Olympus，這是一種新方法，可以將多模態大型語言模型 (MLLM) 轉換為一個統一的框架，能夠處理各種電腦視覺任務。Olympus 利用控制器 MLLM，將超過 20 項針對影像、影片和 3D 物件的專門任務委派給專用模組。這種基於指令的路由透過串聯動作啟用複雜的工作流程，而無需訓練大量生成模型。Olympus 可以輕鬆與現有的 MLLM 整合，以具有可比較的效能來擴充其功能。實驗結果證明，Olympus 在 20 項任務中達成平均路由準確度 94.75%，在串聯動作場景中達成 91.82% 的精確度，展示其作為通用任務路由器的有效性，可以解決各種電腦視覺任務。專案頁面：https://github.com/yuanze-lin/Olympus_page

##### **AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials**
2412.09605v1 by Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu

Graphical User Interface (GUI) agents hold great potential for automating
complex tasks across diverse digital environments, from web applications to
desktop software. However, the development of such agents is hindered by the
lack of high-quality, multi-step trajectory data required for effective
training. Existing approaches rely on expensive and labor-intensive human
annotation, making them unsustainable at scale. To address this challenge, we
propose AgentTrek, a scalable data synthesis pipeline that generates
high-quality GUI agent trajectories by leveraging web tutorials. Our method
automatically gathers tutorial-like texts from the internet, transforms them
into task goals with step-by-step instructions, and employs a visual-language
model agent to simulate their execution in a real digital environment. A
VLM-based evaluator ensures the correctness of the generated trajectories. We
demonstrate that training GUI agents with these synthesized trajectories
significantly improves their grounding and planning performance over the
current models. Moreover, our approach is more cost-efficient compared to
traditional human annotation methods. This work underscores the potential of
guided replay with web tutorials as a viable strategy for large-scale GUI agent
training, paving the way for more capable and autonomous digital agents.

摘要：圖形使用者介面 (GUI) 代理程式在自動化各種數位環境中的複雜任務方面具有極大的潛力，從網路應用程式到桌上型電腦軟體皆是如此。然而，此類代理程式的開發受到缺乏高品質、多步驟軌跡資料的阻礙，而這對於有效的訓練是必要的。現有方法依賴昂貴且勞力密集的人工註解，這使得它們在規模上無法持續。為了應對這項挑戰，我們提出 AgentTrek，這是一個可擴充的資料合成管線，它透過利用網路教學課程產生高品質的 GUI 代理程式軌跡。我們的程式自動從網路上收集類似教學課程的文字，將它們轉換為具有逐步說明的任務目標，並使用視覺語言模型代理程式在真實的數位環境中模擬它們的執行。基於 VLM 的評估器確保產生軌跡的正確性。我們證明使用這些合成軌跡訓練 GUI 代理程式可以顯著改善它們的基礎和規劃效能，優於目前的模型。此外，與傳統的人工註解方法相比，我們的方法更具成本效益。這項工作強調了以網路教學課程為指導的重播作為大規模 GUI 代理程式訓練的可行策略的潛力，為更強大且自主的數位代理程式鋪路。

##### **Hidden Biases of End-to-End Driving Datasets**
2412.09602v1 by Julian Zimmerlin, Jens Beißwenger, Bernhard Jaeger, Andreas Geiger, Kashyap Chitta

End-to-end driving systems have made rapid progress, but have so far not been
applied to the challenging new CARLA Leaderboard 2.0. Further, while there is a
large body of literature on end-to-end architectures and training strategies,
the impact of the training dataset is often overlooked. In this work, we make a
first attempt at end-to-end driving for Leaderboard 2.0. Instead of
investigating architectures, we systematically analyze the training dataset,
leading to new insights: (1) Expert style significantly affects downstream
policy performance. (2) In complex data sets, the frames should not be weighted
on the basis of simplistic criteria such as class frequencies. (3) Instead,
estimating whether a frame changes the target labels compared to previous
frames can reduce the size of the dataset without removing important
information. By incorporating these findings, our model ranks first and second
respectively on the map and sensors tracks of the 2024 CARLA Challenge, and
sets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover
a design flaw in the current evaluation metrics and propose a modification for
future challenges. Our dataset, code, and pre-trained models are publicly
available at https://github.com/autonomousvision/carla_garage.

摘要：端到端驾驶系統已取得快速進展，但迄今尚未應用於具有挑戰性的新 CARLA Leaderboard 2.0。此外，儘管有大量關於端到端架構和訓練策略的文獻，但訓練資料集的影響往往被忽視。在這項工作中，我們首次嘗試為 Leaderboard 2.0 進行端到端駕駛。我們沒有研究架構，而是系統性地分析訓練資料集，從而獲得新的見解：(1) 專家風格顯著影響下游策略效能。(2) 在複雜的資料集中，不應根據類別頻率等簡化標準對幀進行加權。(3) 相反，估計與先前的幀相比，幀是否改變目標標籤可以減少資料集的大小，而不會移除重要資訊。透過納入這些發現，我們的模型分別在 2024 CARLA 挑戰的地圖和感測器軌道上排名第一和第二，並在 Bench2Drive 測試路線上創下新的技術水準。最後，我們發現當前評估指標中的設計缺陷，並提出針對未來挑戰的修改。我們的資料集、程式碼和預先訓練的模型已公開發布於 https://github.com/autonomousvision/carla_garage。

##### **TimeRefine: Temporal Grounding with Time Refining Video LLM**
2412.09601v1 by Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, David Crandall

Video temporal grounding aims to localize relevant temporal boundaries in a
video given a textual prompt. Recent work has focused on enabling Video LLMs to
perform video temporal grounding via next-token prediction of temporal
timestamps. However, accurately localizing timestamps in videos remains
challenging for Video LLMs when relying solely on temporal token prediction.
Our proposed TimeRefine addresses this challenge in two ways. First, instead of
directly predicting the start and end timestamps, we reformulate the temporal
grounding task as a temporal refining task: the model first makes rough
predictions and then refines them by predicting offsets to the target segment.
This refining process is repeated multiple times, through which the model
progressively self-improves its temporal localization accuracy. Second, to
enhance the model's temporal perception capabilities, we incorporate an
auxiliary prediction head that penalizes the model more if a predicted segment
deviates further from the ground truth, thus encouraging the model to make
closer and more accurate predictions. Our plug-and-play method can be
integrated into most LLM-based temporal grounding approaches. The experimental
results demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on
the ActivityNet and Charades-STA datasets, respectively. Code and pretrained
models will be released.

摘要：影片時序定位旨在根據文字提示在影片中找出相關的時間邊界。近期研究專注於讓影片大型語言模型 (LLM) 透過下一個時序標記預測來執行影片時序定位。然而，在僅依賴時序標記預測時，影片大型語言模型要精準定位影片中的時序標記仍是一大挑戰。我們提出的 TimeRefine 以兩種方式解決這個挑戰。首先，我們並未直接預測開始和結束時序標記，而是將時序定位任務重新定義為時序精煉任務：模型先做出粗略預測，然後透過預測目標片段的偏移量來精煉這些預測。這個精煉程序會重複多次，模型透過此程序逐步自行改善其時序定位準確度。其次，為了增強模型的時序感知能力，我們整合了一個輔助預測頭，如果預測片段與實際情況相差越大，這個輔助預測頭就會對模型施加更多懲罰，因此鼓勵模型做出更接近且更精準的預測。我們的即插即用方法可以整合到大多數基於大型語言模型的時序定位方法中。實驗結果顯示，TimeRefine 在 ActivityNet 和 Charades-STA 資料集上分別達到了 3.6% 和 5.0% 的 mIoU 提升。程式碼和預先訓練的模型將會釋出。

##### **Owl-1: Omni World Model for Consistent Long Video Generation**
2412.09600v1 by Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, Jiwen Lu

Video generation models (VGMs) have received extensive attention recently and
serve as promising candidates for general-purpose large vision models. While
they can only generate short videos each time, existing methods achieve long
video generation by iteratively calling the VGMs, using the last-frame output
as the condition for the next-round generation. However, the last frame only
contains short-term fine-grained information about the scene, resulting in
inconsistency in the long horizon. To address this, we propose an Omni World
modeL (Owl-1) to produce long-term coherent and comprehensive conditions for
consistent long video generation. As videos are observations of the underlying
evolving world, we propose to model the long-term developments in a latent
space and use VGMs to film them into videos. Specifically, we represent the
world with a latent state variable which can be decoded into explicit video
observations. These observations serve as a basis for anticipating temporal
dynamics which in turn update the state variable. The interaction between
evolving dynamics and persistent state enhances the diversity and consistency
of the long videos. Extensive experiments show that Owl-1 achieves comparable
performance with SOTA methods on VBench-I2V and VBench-Long, validating its
ability to generate high-quality video observations. Code:
https://github.com/huang-yh/Owl.

摘要：影片生成模型（VGM）最近受到广泛关注，并成为通用大型视觉模型的有力候选者。虽然它们每次只能生成短影片，但现有方法通过迭代调用 VGM，使用最后一帧输出作为下一轮生成的条件来实现长影片生成。然而，最后一帧仅包含关于场景的短期细粒度信息，导致长期视界不一致。为了解决这个问题，我们提出了一个全景世界模型 (Owl-1) 来产生长期连贯且全面的条件，以实现一致的长影片生成。由于影片是对底层演化世界的观察，我们提议在潜在空间中对长期发展进行建模，并使用 VGM 将其拍摄成影片。具体来说，我们用一个潜在状态变量表示世界，可以将其解码为明确的影片观察。这些观察作为预测时间动态的基础，进而更新状态变量。演化动态和持久状态之间的相互作用增强了长影片的多样性和一致性。大量实验表明，Owl-1 在 VBench-I2V 和 VBench-Long 上实现了与 SOTA 方法相当的性能，验证了其生成高质量影片观察的能力。代码：https://github.com/huang-yh/Owl。

##### **InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**
2412.09596v1 by Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, Jiaqi Wang

Creating AI systems that can interact with environments over long periods,
similar to human cognition, has been a longstanding research goal. Recent
advancements in multimodal large language models (MLLMs) have made significant
strides in open-world understanding. However, the challenge of continuous and
simultaneous streaming perception, memory, and reasoning remains largely
unexplored. Current MLLMs are constrained by their sequence-to-sequence
architecture, which limits their ability to process inputs and generate
responses simultaneously, akin to being unable to think while perceiving.
Furthermore, relying on long contexts to store historical data is impractical
for long-term interactions, as retaining all information becomes costly and
inefficient. Therefore, rather than relying on a single foundation model to
perform all functions, this project draws inspiration from the concept of the
Specialized Generalist AI and introduces disentangled streaming perception,
reasoning, and memory mechanisms, enabling real-time interaction with streaming
video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive
(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:
Processes multimodal information in real-time, storing key details in memory
and triggering reasoning in response to user queries. (2) Multi-modal Long
Memory Module: Integrates short-term and long-term memory, compressing
short-term memories into long-term ones for efficient retrieval and improved
accuracy. (3) Reasoning Module: Responds to queries and executes reasoning
tasks, coordinating with the perception and memory modules. This project
simulates human-like cognition, enabling multimodal large language models to
provide continuous and adaptive service over time.

摘要：<paragraph>建立能與環境進行長時間互動的人工智慧系統，就像人類的認知一樣，一直是長久以來的研究目標。最近多模態大型語言模型 (MLLM) 的進展在開放世界的理解上取得了顯著進展。然而，持續且同時串流感知、記憶和推理的挑戰在很大程度上仍未被探索。目前的 MLLM 受到其序列到序列架構的限制，這限制了它們同時處理輸入和產生回應的能力，就像在感知時無法思考一樣。此外，依賴於長期的脈絡來儲存歷史資料對於長期互動來說是不切實際的，因為保留所有資訊會變得昂貴且低效。因此，這個專案並非依賴單一的基礎模型來執行所有功能，而是從專業通才 AI 的概念中汲取靈感，並引入了分離的串流感知、推理和記憶機制，從而實現與串流視訊和音訊輸入的即時互動。所提出的架構 InternLM-XComposer2.5-OmniLive (IXC2.5-OL) 包含三個關鍵模組：(1) 串流感知模組：即時處理多模態資訊，將關鍵細節儲存在記憶體中，並針對使用者的查詢觸發推理。(2) 多模態長期記憶模組：整合短期和長期記憶，將短期記憶壓縮成長期記憶，以利於有效地擷取和提高準確性。(3) 推理模組：回應查詢並執行推理任務，與感知和記憶模組協調。這個專案模擬了類人的認知，使多模態大型語言模型能夠隨著時間的推移提供持續且適應性的服務。</paragraph>

##### **OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages**
2412.09587v1 by Chester Palen-Michel, Maxwell Pickering, Maya Kruse, Jonne Sälevä, Constantine Lignos

We present OpenNER 1.0, a standardized collection of openly available named
entity recognition (NER) datasets. OpenNER contains 34 datasets spanning 51
languages, annotated in varying named entity ontologies. We correct annotation
format issues, standardize the original datasets into a uniform representation,
map entity type names to be more consistent across corpora, and provide the
collection in a structure that enables research in multilingual and
multi-ontology NER. We provide baseline models using three pretrained
multilingual language models to compare the performance of recent models and
facilitate future research in NER.

摘要：我們提出 OpenNER 1.0，一個標準化的公開命名實體辨識 (NER) 資料集集合。OpenNER 包含 34 個資料集，涵蓋 51 種語言，並以不同的命名實體知識庫進行標註。我們修正標註格式問題，將原始資料集標準化為統一的表示，將實體類型名稱對應到語料庫中更一致的類型，並以一個結構提供集合，以利於多語言和多知識庫 NER 的研究。我們提供基線模型，使用三個預訓練的多語言語言模型，以比較近期模型的效能，並促進 NER 的未來研究。

##### **Neptune: The Long Orbit to Benchmarking Long Video Understanding**
2412.09582v1 by Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, Tobias Weyand

This paper describes a semi-automatic pipeline to generate challenging
question-answer-decoy sets for understanding long videos. Many existing video
datasets and models are focused on short clips (10s-30s). While some long video
datasets do exist, they can often be solved by powerful image models applied
per frame (and often to very few frames) in a video, and are usually manually
annotated at high cost. In order to mitigate both these problems, we propose a
scalable dataset creation pipeline which leverages large models (VLMs and
LLMs), to automatically generate dense, time-aligned video captions, as well as
tough question answer decoy sets for video segments (up to 15 minutes in
length). Our dataset Neptune covers a broad range of long video reasoning
abilities and consists of a subset that emphasizes multimodal reasoning. Since
existing metrics for open-ended question answering are either rule-based or may
rely on proprietary models, we provide a new open source model-based metric GEM
to score open-ended responses on Neptune. Benchmark evaluations reveal that
most current open-source long video models perform poorly on Neptune,
particularly on questions testing temporal ordering, counting and state
changes. Through Neptune, we aim to spur the development of more advanced
models capable of understanding long videos. The dataset is available at
https://github.com/google-deepmind/neptune

摘要：本文介紹了一個半自動化的管道，用於產生具有挑戰性的問題-答案-誘餌組，以了解長影片。許多現有的影片資料集和模型都專注於短片（10 秒至 30 秒）。雖然確實存在一些長影片資料集，但它們通常可以用強大的影像模型解決，這些模型會套用在影片的每個畫格（而且通常只套用在極少數畫格）上，而且通常是以高成本人工標註。為了減輕這兩個問題，我們提出了一個可擴充的資料集建立管道，它利用大型模型（VLM 和 LLM）自動產生密集的時間對齊影片字幕，以及針對影片片段（長達 15 分鐘）的困難問題答案誘餌組。我們的資料集 Neptune 涵蓋了廣泛的長影片推理能力，並包含一個強調多模態推理的子集。由於現有的開放式問題解答指標不是基於規則，就是可能依賴專有模型，因此我們提供了一個新的開源模型式指標 GEM，以評分 Neptune 上的開放式回應。基準評估顯示，目前大多數開源長影片模型在 Neptune 上的表現不佳，特別是在測試時間順序、計數和狀態變化的問題上。透過 Neptune，我們旨在促進開發更先進的模型，以便了解長影片。資料集可在 https://github.com/google-deepmind/neptune 取得

##### **A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks**
2412.09579v1 by Saptarshi Mandal, Xiaojun Lin, R. Srikant

Knowledge distillation, where a small student model learns from a pre-trained
large teacher model, has achieved substantial empirical success since the
seminal work of \citep{hinton2015distilling}. Despite prior theoretical studies
exploring the benefits of knowledge distillation, an important question remains
unanswered: why does soft-label training from the teacher require significantly
fewer neurons than directly training a small neural network with hard labels?
To address this, we first present motivating experimental results using simple
neural network models on a binary classification problem. These results
demonstrate that soft-label training consistently outperforms hard-label
training in accuracy, with the performance gap becoming more pronounced as the
dataset becomes increasingly difficult to classify. We then substantiate these
observations with a theoretical contribution based on two-layer neural network
models. Specifically, we show that soft-label training using gradient descent
requires only $O\left(\frac{1}{\gamma^2 \epsilon}\right)$ neurons to achieve a
classification loss averaged over epochs smaller than some $\epsilon > 0$,
where $\gamma$ is the separation margin of the limiting kernel. In contrast,
hard-label training requires $O\left(\frac{1}{\gamma^4} \cdot
\ln\left(\frac{1}{\epsilon}\right)\right)$ neurons, as derived from an adapted
version of the gradient descent analysis in \citep{ji2020polylogarithmic}. This
implies that when $\gamma \leq \epsilon$, i.e., when the dataset is challenging
to classify, the neuron requirement for soft-label training can be
significantly lower than that for hard-label training. Finally, we present
experimental results on deep neural networks, further validating these
theoretical findings.

摘要：<paragraph>知識蒸餾，其中一個小型學生模型從一個預先訓練的大型教師模型中學習，自\citep{hinton2015distilling}的開創性工作以來，已經取得了大量的經驗成功。儘管之前有理論研究探討了知識蒸餾的好處，但一個重要問題仍然沒有得到解答：為什麼來自教師的軟標籤訓練需要的，顯著少於直接使用硬標籤訓練一個小型神經網路？為了解決這個問題，我們首先使用一個二元分類問題上的簡單神經網路模型，展示了有力的實驗結果。這些結果表明，軟標籤訓練在準確度上始終優於硬標籤訓練，隨著數據集變得越來越難以分類，兩者之間的效能差距變得更加明顯。然後，我們使用基於兩層神經網路模型的理論貢獻，證實了這些觀察結果。具體來說，我們表明，使用梯度下降的軟標籤訓練只需要$O\left(\frac{1}{\gamma^2 \epsilon}\right)$個神經元，就可以實現分類損失，平均來說，比某個$\epsilon > 0$還要小，其中$\gamma$是極限核的分離邊界。相比之下，硬標籤訓練需要$O\left(\frac{1}{\gamma^4} \cdot\ln\left(\frac{1}{\epsilon}\right)\right)$個神經元，這源自於\citep{ji2020polylogarithmic}中梯度下降分析的改編版本。這意味著，當$\gamma \leq \epsilon$時，即當數據集難以分類時，軟標籤訓練的神經元需求可以顯著低於硬標籤訓練。最後，我們展示了深度神經網路的實驗結果，進一步驗證了這些理論發現。</paragraph>

##### **DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial NEtworks and Semantic Topic classification**
2412.09578v1 by Caleb Stam, Emily Saldanha, Mahantesh Halappanavar, Anurag Acharya

The emergence of the COVID-19 pandemic resulted in a significant rise in the
spread of misinformation on online platforms such as Twitter. Oftentimes this
growth is blamed on the idea of the "echo chamber." However, the behavior said
to characterize these echo chambers exists in two dimensions. The first is in a
user's social interactions, where they are said to stick with the same clique
of like-minded users. The second is in the content of their posts, where they
are said to repeatedly espouse homogeneous ideas. In this study, we link the
two by using Twitter's network of retweets to study social interactions and
topic modeling to study tweet content. In order to measure the diversity of a
user's interactions over time, we develop a novel metric to track the speed at
which they travel through the social network. The application of these analysis
methods to misinformation-focused data from the pandemic demonstrates
correlation between social behavior and tweet content. We believe this
correlation supports the common intuition about how antisocial users behave,
and further suggests that it holds even in subcommunities already rife with
misinformation.

摘要：COVID-19 疫情的出現導致 Twitter 等線上平台上的錯誤訊息大幅增加。這種增長通常歸咎於「同溫層」的概念。然而，據說表徵這些同溫層的行為存在於兩個面向。第一個面向在於使用者的社交互動，他們據說會與志同道合的使用者群體保持聯繫。第二個面向在於他們貼文的內容，他們據說會重複擁護同質化的想法。在本研究中，我們透過使用 Twitter 的轉推網路來研究社交互動，並透過主題建模來研究推文內容，將這兩者連結起來。為了衡量使用者互動的多樣性，我們開發了一種新穎的指標來追蹤他們在社群網路中移動的速度。將這些分析方法應用於疫情期間以錯誤訊息為主的資料，證明了社交行為與推文內容之間存在關聯性。我們相信這種關聯性支持了關於反社會使用者行為的普遍直覺，並進一步表明，即使在已經充斥錯誤訊息的次社群中，這種關聯性依然存在。

##### **DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction**
2412.09572v1 by Yu Feng, Phu Mon Htut, Zheng Qi, Wei Xiao, Manuel Mager, Nikolaos Pappas, Kishaloy Halder, Yang Li, Yassine Benajiba, Dan Roth

Quantifying the uncertainty in the factual parametric knowledge of Large
Language Models (LLMs), especially in a black-box setting, poses a significant
challenge. Existing methods, which gauge a model's uncertainty through
evaluating self-consistency in responses to the original query, do not always
capture true uncertainty. Models might respond consistently to the origin query
with a wrong answer, yet respond correctly to varied questions from different
perspectives about the same query, and vice versa. In this paper, we propose a
novel method, DiverseAgentEntropy, for evaluating a model's uncertainty using
multi-agent interaction under the assumption that if a model is certain, it
should consistently recall the answer to the original query across a diverse
collection of questions about the same original query. We further implement an
abstention policy to withhold responses when uncertainty is high. Our method
offers a more accurate prediction of the model's reliability and further
detects hallucinations, outperforming other self-consistency-based methods.
Additionally, it demonstrates that existing models often fail to consistently
retrieve the correct answer to the same query under diverse varied questions
even when knowing the correct answer.

摘要：量化大型语言模型 (LLM) 中事实参数知识的不确定性，尤其是在黑盒设置中，构成了重大挑战。现有的方法通过评估对原始查询的响应中的一致性来衡量模型的不确定性，并不总是能捕捉到真正的不确定性。模型可能会对原始查询一致地做出错误的回答，但对同一查询的不同角度提出的各种问题做出正确的回答，反之亦然。在本文中，我们提出了一种新方法 DiverseAgentEntropy，用于在多主体交互下评估模型的不确定性，假设如果模型确定，它应该在关于同一原始查询的各种问题中一致地回忆原始查询的答案。我们进一步实施了一项弃权策略，以在不确定性较高时不做出回应。我们的方法提供了对模型可靠性的更准确预测，并进一步检测幻觉，优于其他基于自洽性的方法。此外，它表明，即使知道了正确答案，现有模型也常常无法在各种不同的问题下始终如一地检索到同一查询的正确答案。

##### **JuStRank: Benchmarking LLM Judges for System Ranking**
2412.09569v1 by Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai

Given the rapid progress of generative AI, there is a pressing need to
systematically compare and choose between the numerous models and
configurations available. The scale and versatility of such evaluations make
the use of LLM-based judges a compelling solution for this challenge.
Crucially, this approach requires first to validate the quality of the LLM
judge itself. Previous work has focused on instance-based assessment of LLM
judges, where a judge is evaluated over a set of responses, or response pairs,
while being agnostic to their source systems. We argue that this setting
overlooks critical factors affecting system-level ranking, such as a judge's
positive or negative bias towards certain systems. To address this gap, we
conduct the first large-scale study of LLM judges as system rankers. System
scores are generated by aggregating judgment scores over multiple system
outputs, and the judge's quality is assessed by comparing the resulting system
ranking to a human-based ranking. Beyond overall judge assessment, our analysis
provides a fine-grained characterization of judge behavior, including their
decisiveness and bias.

摘要：鉴于生成式 AI 的快速发展，迫切需要系统地比较和选择众多可用的模型和配置。此类评估的规模和多功能性使得基于 LLM 的评委的使用成为这一挑战的令人信服的解决方案。至关重要的是，这种方法首先需要验证 LLM 评委本身的质量。以前的工作重点是基于实例的 LLM 评委评估，其中评委在响应集合或响应对上进行评估，同时对它们的源系统不可知。我们认为，这种设置忽略了影响系统级排名的关键因素，例如评委对某些系统的正面或负面偏见。为了解决这一差距，我们对 LLM 评委作为系统排名者进行了首次大规模研究。系统分数是通过汇总对多个系统输出的判断分数生成的，评委的质量通过将生成的系统排名与基于人的排名进行比较来评估。除了整体评委评估之外，我们的分析还提供了对评委行为的细粒度表征，包括他们的果断性和偏见。

##### **Does Representation Matter? Exploring Intermediate Layers in Large Language Models**
2412.09563v1 by Oscar Skean, Md Rifat Arefin, Yann LeCun, Ravid Shwartz-Ziv

Understanding what defines a good representation in large language models
(LLMs) is fundamental to both theoretical understanding and practical
applications. In this paper, we investigate the quality of intermediate
representations in various LLM architectures, including Transformers and State
Space Models (SSMs). We find that intermediate layers often yield more
informative representations for downstream tasks than the final layers. To
measure the representation quality, we adapt and apply a suite of metrics -
such as prompt entropy, curvature, and augmentation-invariance - originally
proposed in other contexts. Our empirical study reveals significant
architectural differences, how representations evolve throughout training, and
how factors like input randomness and prompt length affect each layer. Notably,
we observe a bimodal pattern in the entropy of some intermediate layers and
consider potential explanations tied to training data. Overall, our results
illuminate the internal mechanics of LLMs and guide strategies for
architectural optimization and training.

摘要：了解大型語言模型 (LLM) 中良好表徵的定義，對於理論理解和實際應用至關重要。在本文中，我們探討了各種 LLM 架構（包括 Transformer 和狀態空間模型 (SSM)）中中間表徵的品質。我們發現，中間層通常會產生比最後一層更具資訊性的表徵，以供下游任務使用。為了衡量表徵品質，我們調整並套用了一組量度，例如提示熵、曲率和擴充不變性，這些量度最初是在其他背景中提出的。我們的實證研究揭示了顯著的架構差異、表徵在整個訓練過程中如何演變，以及輸入隨機性和提示長度如何影響每一層。值得注意的是，我們觀察到某些中間層的熵呈現雙峰模式，並考慮了與訓練資料相關的潛在解釋。總體而言，我們的結果闡明了 LLM 的內部機制，並為架構最佳化和訓練策略提供指導。

##### **Foundational Large Language Models for Materials Research**
2412.09560v1 by Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret, Mausam, N. M. Anoop Krishnan

Materials discovery and development are critical for addressing global
challenges. Yet, the exponential growth in materials science literature
comprising vast amounts of textual data has created significant bottlenecks in
knowledge extraction, synthesis, and scientific reasoning. Large Language
Models (LLMs) offer unprecedented opportunities to accelerate materials
research through automated analysis and prediction. Still, their effective
deployment requires domain-specific adaptation for understanding and solving
domain-relevant tasks. Here, we present LLaMat, a family of foundational models
for materials science developed through continued pretraining of LLaMA models
on an extensive corpus of materials literature and crystallographic data.
Through systematic evaluation, we demonstrate that LLaMat excels in
materials-specific NLP and structured information extraction while maintaining
general linguistic capabilities. The specialized LLaMat-CIF variant
demonstrates unprecedented capabilities in crystal structure generation,
predicting stable crystals with high coverage across the periodic table.
Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,
we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific
performance across diverse materials science tasks, including structured
information extraction from text and tables, more particularly in crystal
structure generation, a potential adaptation rigidity in overtrained LLMs.
Altogether, the present work demonstrates the effectiveness of domain
adaptation towards developing practically deployable LLM copilots for materials
research. Beyond materials science, our findings reveal important
considerations for domain adaptation of LLMs, such as model selection, training
methodology, and domain-specific performance, which may influence the
development of specialized scientific AI systems.

摘要：材料的發現與開發對於因應全球挑戰至關重要。然而，材料科學文獻的指數型成長包含大量的文字資料，已在知識萃取、綜合與科學推理方面造成重大的瓶頸。大型語言模型 (LLM) 提供了前所未有的機會，可透過自動化分析與預測來加速材料研究。然而，其有效部署需要特定領域的適應，才能理解並解決與領域相關的任務。在此，我們提出 LLaMat，這是一個基礎模型系列，用於材料科學，透過持續預訓練 LLaMA 模型於廣泛的材料文獻與晶體學資料中而開發。透過系統性評估，我們證明 LLaMat 在特定材料的自然語言處理與結構化資訊萃取方面表現優異，同時維持一般語言能力。專門的 LLaMat-CIF 變體在晶體結構生成方面展現前所未有的能力，預測週期表中具有高覆蓋率的穩定晶體。有趣的是，儘管 LLaMA-3 的效能優於 LLaMA-2，我們觀察到 LLaMat-2 在不同的材料科學任務中展現出預期之外的增強領域特定效能，包括從文字與表格中萃取結構化資訊，更特別的是在晶體結構生成中，這是一種過度訓練 LLM 中潛在的適應僵化。總而言之，目前的研究證明了領域適應在開發實務上可部署的 LLM 副駕駛對於材料研究的有效性。除了材料科學之外，我們的發現揭示了 LLM 領域適應的重要考量，例如模型選擇、訓練方法與特定領域的效能，這可能會影響專門科學 AI 系統的開發。

##### **Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking**
2412.09544v1 by Paria Rashidinejad, Yuandong Tian

Aligning AI systems with human preferences typically suffers from the
infamous reward hacking problem, where optimization of an imperfect reward
model leads to undesired behaviors. In this paper, we investigate reward
hacking in offline preference optimization, which aims to improve an initial
model using a preference dataset. We identify two types of reward hacking
stemming from statistical fluctuations in the dataset: Type I Reward Hacking
due to subpar choices appearing more favorable, and Type II Reward Hacking due
to decent choices appearing less favorable. We prove that many (mainstream or
theoretical) preference optimization methods suffer from both types of reward
hacking. To mitigate Type I Reward Hacking, we propose POWER, a new preference
optimization method that combines Guiasu's weighted entropy with a robust
reward maximization objective. POWER enjoys finite-sample guarantees under
general function approximation, competing with the best covered policy in the
data. To mitigate Type II Reward Hacking, we analyze the learning dynamics of
preference optimization and develop a novel technique that dynamically updates
preference labels toward certain "stationary labels", resulting in diminishing
gradients for untrustworthy samples. Empirically, POWER with dynamic labels
(POWER-DL) consistently outperforms state-of-the-art methods on alignment
benchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and
11.5 points on Arena-Hard over DPO, while also improving or maintaining
performance on downstream tasks such as mathematical reasoning. Strong
theoretical guarantees and empirical results demonstrate the promise of
POWER-DL in mitigating reward hacking.

摘要：對齊人工智慧系統與人類偏好通常會遭遇惡名昭彰的獎勵破解問題，其中不完美的獎勵模型的最佳化會導致不想要的行為。在本論文中，我們探討離線偏好最佳化中的獎勵破解，其目標是使用偏好資料集改善初始模型。我們找出源自資料集中統計波動的兩種獎勵破解類型：由於較差的選擇看起來較有利而產生的第一型獎勵破解，以及由於較好的選擇看起來較不利而產生的第二型獎勵破解。我們證明許多（主流或理論）偏好最佳化方法都會遭遇這兩種類型的獎勵破解。為了減輕第一型獎勵破解，我們提出 POWER，這是一種結合了 Guiasu 的加權熵與穩健獎勵最大化目標的新偏好最佳化方法。POWER 在一般函數逼近下享有有限樣本保證，與資料中涵蓋最廣的政策競爭。為了減輕第二型獎勵破解，我們分析偏好最佳化的學習動態，並開發一種新技術，將偏好標籤動態更新為某些「固定標籤」，導致不可信樣本的梯度遞減。根據經驗，具有動態標籤的 POWER（POWER-DL）在對齊基準上始終優於最先進的方法，在 AlpacaEval 2.0 上的改進幅度高達 13.0 個百分點，在 Arena-Hard 上比 DPO 高 11.5 個百分點，同時也改善或維持了下游任務（例如數學推理）的效能。強大的理論保證和經驗結果證明了 POWER-DL 在減輕獎勵破解方面的潛力。

##### **Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis**
2412.09521v1 by Shengxuming Zhang, Weihan Li, Tianhong Gao, Jiacong Hu, Haoming Luo, Mingli Song, Xiuming Zhang, Zunlei Feng

Pathological diagnosis is vital for determining disease characteristics,
guiding treatment, and assessing prognosis, relying heavily on detailed,
multi-scale analysis of high-resolution whole slide images (WSI). However,
traditional pure vision models face challenges of redundant feature extraction,
whereas existing large vision-language models (LVLMs) are limited by input
resolution constraints, hindering their efficiency and accuracy. To overcome
these issues, we propose two innovative strategies: the mixed task-guided
feature enhancement, which directs feature extraction toward lesion-related
details across scales, and the prompt-guided detail feature completion, which
integrates coarse- and fine-grained features from WSI based on specific prompts
without compromising inference speed. Leveraging a comprehensive dataset of
490,000 samples from diverse pathology tasks-including cancer detection,
grading, vascular and neural invasion identification, and so on-we trained the
pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that
this model significantly outperforms existing methods in diagnostic accuracy
and efficiency, offering an interactive, clinically aligned approach for
auxiliary diagnosis in a wide range of pathology applications.

摘要：病理诊断对于确定疾病特征、指导治疗和评估预后至关重要，它严重依赖于对高分辨率全玻片图像 (WSI) 的详细、多尺度分析。然而，传统的纯视觉模型面临冗余特征提取的挑战，而现有的大型视觉语言模型 (LVLMs) 受到输入分辨率约束的限制，阻碍了它们的效率和准确性。为了克服这些问题，我们提出了两种创新策略：混合任务引导的特征增强，它将特征提取引导到跨尺度的病变相关细节上；以及提示引导的细节特征完成，它基于特定提示将 WSI 中的粗粒度和细粒度特征集成在一起，而不会影响推理速度。利用来自不同病理任务的 490,000 个样本的综合数据集，包括癌症检测、分级、血管和神经侵袭识别等，我们训练了病理学专业 LVLM，即 OmniPath。大量的实验表明，该模型在诊断准确性和效率方面明显优于现有方法，为广泛的病理学应用中的辅助诊断提供了一种交互式、临床上一致的方法。

##### **Video Seal: Open and Efficient Video Watermarking**
2412.09492v1 by Pierre Fernandez, Hady Elsahar, I. Zeki Yalniz, Alexandre Mourachko

The proliferation of AI-generated content and sophisticated video editing
tools has made it both important and challenging to moderate digital platforms.
Video watermarking addresses these challenges by embedding imperceptible
signals into videos, allowing for identification. However, the rare open tools
and methods often fall short on efficiency, robustness, and flexibility. To
reduce these gaps, this paper introduces Video Seal, a comprehensive framework
for neural video watermarking and a competitive open-sourced model. Our
approach jointly trains an embedder and an extractor, while ensuring the
watermark robustness by applying transformations in-between, e.g., video
codecs. This training is multistage and includes image pre-training, hybrid
post-training and extractor fine-tuning. We also introduce temporal watermark
propagation, a technique to convert any image watermarking model to an
efficient video watermarking model without the need to watermark every
high-resolution frame. We present experimental results demonstrating the
effectiveness of the approach in terms of speed, imperceptibility, and
robustness. Video Seal achieves higher robustness compared to strong baselines
especially under challenging distortions combining geometric transformations
and video compression. Additionally, we provide new insights such as the impact
of video compression during training, and how to compare methods operating on
different payloads. Contributions in this work - including the codebase,
models, and a public demo - are open-sourced under permissive licenses to
foster further research and development in the field.

摘要：人工智能生成內容和精密的影片編輯工具的激增，讓審核數位平台變得既重要又具挑戰性。影片浮水印透過將難以察覺的訊號嵌入影片中，讓影片得以識別，來因應這些挑戰。但少見的開放式工具和方法通常在效率、健全性和彈性上有所不足。為了縮小這些差距，本文介紹 Video Seal，一個神經影片浮水印的全面架構和一個有競爭力的開源模型。我們的做法是同時訓練一個嵌入器和一個萃取器，並透過在中間套用轉換（例如影片編解碼器）來確保浮水印的健全性。此訓練是多階段的，包括影像預訓練、混合後訓練和萃取器微調。我們也介紹了時間浮水印傳播，一種將任何影像浮水印模型轉換成高效能影片浮水印模型的技術，無需對每個高解析度畫格加上浮水印。我們提出實驗結果，證明了此方法在速度、難以察覺性和健全性方面的效能。與強大的基準線相比，Video Seal 達到了更高的健全性，特別是在結合幾何轉換和影片壓縮的具挑戰性失真下。此外，我們提供了新的見解，例如訓練過程中影片壓縮的影響，以及如何比較處理不同負載的方法。本研究中的貢獻（包括程式碼庫、模型和公開展示）在寬鬆的授權下開源，以促進該領域的進一步研究和開發。

##### **Regression and Classification with Single-Qubit Quantum Neural Networks**
2412.09486v1 by Leandro C. Souza, Bruno C. Guingo, Gilson Giraldi, Renato Portugal

Since classical machine learning has become a powerful tool for developing
data-driven algorithms, quantum machine learning is expected to similarly
impact the development of quantum algorithms. The literature reflects a
mutually beneficial relationship between machine learning and quantum
computing, where progress in one field frequently drives improvements in the
other. Motivated by the fertile connection between machine learning and quantum
computing enabled by parameterized quantum circuits, we use a
resource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for
both regression and classification tasks. The SQQNN leverages parameterized
single-qubit unitary operators and quantum measurements to achieve efficient
learning. To train the model, we use gradient descent for regression tasks. For
classification, we introduce a novel training method inspired by the Taylor
series, which can efficiently find a global minimum in a single step. This
approach significantly accelerates training compared to iterative methods.
Evaluated across various applications, the SQQNN exhibits virtually error-free
and strong performance in regression and classification tasks, including the
MNIST dataset. These results demonstrate the versatility, scalability, and
suitability of the SQQNN for deployment on near-term quantum devices.

摘要：由於經典機器學習已成為開發資料驅動演算法的強大工具，量子機器學習預計也會對量子演算法的開發產生類似的影響。文獻反映出機器學習和量子計算之間的互惠關係，其中一個領域的進展經常推動另一個領域的改善。受到參數化量子電路所啟用的機器學習和量子計算之間的豐碩連結所激勵，我們使用資源有效率且可擴充的單量子位元量子神經網路 (SQQNN) 來進行迴歸和分類任務。SQQNN 透過參數化單量子位元酉算子和量子測量來達成有效率的學習。為了訓練模型，我們對迴歸任務使用梯度下降。對於分類，我們引入一種新穎的訓練方法，其靈感來自泰勒級數，它可以在單一步驟中有效率地找到全局最小值。與反覆運算方法相比，這種方法顯著地加速了訓練。在各種應用中進行評估，SQQNN 在迴歸和分類任務中表現出幾乎沒有錯誤且強大的效能，包括 MNIST 資料集。這些結果證明了 SQQNN 的多功能性、可擴充性和適用性，可用於部署在近期量子裝置上。

##### **The Parameters of Educability**
2412.09480v1 by Leslie G. Valiant

The educability model is a computational model that has been recently
proposed to describe the cognitive capability that makes humans unique among
existing biological species on Earth in being able to create advanced
civilizations. Educability is defined as a capability for acquiring and
applying knowledge. It is intended both to describe human capabilities and,
equally, as an aspirational description of what can be usefully realized by
machines. While the intention is to have a mathematically well-defined
computational model, in constructing an instance of the model there are a
number of decisions to make. We call these decisions {\it parameters}. In a
standard computer, two parameters are the memory capacity and clock rate. There
is no universally optimal choice for either one, or even for their ratio.
Similarly, in a standard machine learning system, two parameters are the
learning algorithm and the dataset used for training. Again, there are no
universally optimal choices known for either. An educable system has many more
parameters than either of these two kinds of system. This short paper discusses
some of the main parameters of educable systems, and the broader implications
of their existence.

摘要：可教育性模型是一个计算模型，最近被提出用于描述认知能力，这种能力使人类在能够创造先进文明方面在现有的地球生物物种中独一无二。可教育性被定义为获取和应用知识的能力。它的目的是描述人类的能力，同样也是对机器可以有用地实现什么的一种理想描述。虽然目的是拥有一个数学上定义良好的计算模型，但在构建模型的一个实例时，需要做出许多决策。我们称这些决策为{\it 参数}。在标准计算机中，两个参数是内存容量和时钟速率。对于这两个参数，甚至对于它们的比率，都没有普遍最优的选择。类似地，在标准机器学习系统中，两个参数是学习算法和用于训练的数据集。同样，对于这两个参数，也没有已知的普遍最优选择。可教育系统比这两种系统中的任何一种系统都有更多参数。这篇短文讨论了可教育系统的一些主要参数，以及它们存在的更广泛的含义。

##### **New keypoint-based approach for recognising British Sign Language (BSL) from sequences**
2412.09475v1 by Oishi Deb, KR Prajwal, Andrew Zisserman

In this paper, we present a novel keypoint-based classification model
designed to recognise British Sign Language (BSL) words within continuous
signing sequences. Our model's performance is assessed using the BOBSL dataset,
revealing that the keypoint-based approach surpasses its RGB-based counterpart
in computational efficiency and memory usage. Furthermore, it offers expedited
training times and demands fewer computational resources. To the best of our
knowledge, this is the inaugural application of a keypoint-based model for BSL
word classification, rendering direct comparisons with existing works
unavailable.

摘要：在本文中，我們提出了一個新穎的基於關鍵點的分類模型，旨在識別連續手語序列中的英國手語 (BSL) 詞彙。我們使用 BOBSL 資料集評估模型的效能，結果顯示基於關鍵點的方法在運算效率和記憶體使用率上優於基於 RGB 的方法。此外，它提供更快的訓練時間並需要較少的運算資源。據我們所知，這是基於關鍵點的模型首次應用於 BSL 詞彙分類，無法直接與現有作品進行比較。

##### **Audios Don't Lie: Multi-Frequency Channel Attention Mechanism for Audio Deepfake Detection**
2412.09467v1 by Yangguang Feng

With the rapid development of artificial intelligence technology, the
application of deepfake technology in the audio field has gradually increased,
resulting in a wide range of security risks. Especially in the financial and
social security fields, the misuse of deepfake audios has raised serious
concerns. To address this challenge, this study proposes an audio deepfake
detection method based on multi-frequency channel attention mechanism (MFCA)
and 2D discrete cosine transform (DCT). By processing the audio signal into a
melspectrogram, using MobileNet V2 to extract deep features, and combining it
with the MFCA module to weight different frequency channels in the audio
signal, this method can effectively capture the fine-grained frequency domain
features in the audio signal and enhance the Classification capability of fake
audios. Experimental results show that compared with traditional methods, the
model proposed in this study shows significant advantages in accuracy,
precision,recall, F1 score and other indicators. Especially in complex audio
scenarios, this method shows stronger robustness and generalization
capabilities and provides a new idea for audio deepfake detection and has
important practical application value. In the future, more advanced audio
detection technologies and optimization strategies will be explored to further
improve the accuracy and generalization capabilities of audio deepfake
detection.

摘要：隨著人工智慧技術的快速發展，深度偽造技術在音訊領域的應用逐漸增多，導致安全性風險範圍廣泛。特別是在金融、社會安全領域，深度偽造音訊的濫用引發了嚴重的關注。為了應對這一挑戰，本研究提出了一種基於多頻道頻道注意力機制 (MFCA) 和 2D 離散餘弦變換 (DCT) 的音訊深度偽造檢測方法。通過將音訊訊號處理成梅爾頻譜圖，使用 MobileNet V2 提取深度特徵，並結合 MFCA 模組對音訊訊號中的不同頻率通道進行加權，此方法可以有效捕捉音訊訊號中的細粒度頻域特徵，並增強偽造音訊的分類能力。實驗結果表明，與傳統方法相比，本研究提出的模型在準確度、精確度、召回率、F1 分數等指標上表現出顯著的優勢。特別是在複雜的音訊場景中，此方法展現出更強的魯棒性和泛化能力，為音訊深度偽造檢測提供了新思路，具有重要的實務應用價值。未來將探索更多進階的音訊檢測技術和最佳化策略，以進一步提升音訊深度偽造檢測的準確度和泛化能力。

##### **STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading**
2412.09468v1 by Yilei Zhao, Wentao Zhang, Tingran Yang, Yong Jiang, Fei Huang, Wei Yang Bryan Lim

In financial trading, factor models are widely used to price assets and
capture excess returns from mispricing. Recently, we have witnessed the rise of
variational autoencoder-based latent factor models, which learn latent factors
self-adaptively. While these models focus on modeling overall market
conditions, they often fail to effectively capture the temporal patterns of
individual stocks. Additionally, representing multiple factors as single values
simplifies the model but limits its ability to capture complex relationships
and dependencies. As a result, the learned factors are of low quality and lack
diversity, reducing their effectiveness and robustness across different trading
periods. To address these issues, we propose a Spatio-Temporal factOR Model
based on dual vector quantized variational autoencoders, named STORM, which
extracts features of stocks from temporal and spatial perspectives, then fuses
and aligns these features at the fine-grained and semantic level, and
represents the factors as multi-dimensional embeddings. The discrete codebooks
cluster similar factor embeddings, ensuring orthogonality and diversity, which
helps distinguish between different factors and enables factor selection in
financial trading. To show the performance of the proposed factor model, we
apply it to two downstream experiments: portfolio management on two stock
datasets and individual trading tasks on six specific stocks. The extensive
experiments demonstrate STORM's flexibility in adapting to downstream tasks and
superior performance over baseline models.

摘要：在金融交易中，因子模型被广泛用于对资产定价并从定价错误中获取超额收益。最近，我们见证了基于变分自动编码器的潜在因子模型的兴起，它可以自适应地学习潜在因子。虽然这些模型专注于对整体市场状况进行建模，但它们通常无法有效地捕捉个股的时间模式。此外，将多个因子表示为单个值简化了模型，但限制了其捕捉复杂关系和依赖性的能力。因此，学习到的因子质量低且缺乏多样性，降低了它们在不同交易时段的有效性和鲁棒性。为了解决这些问题，我们提出了一种基于双向量量化变分自动编码器的时空因子模型，名为 STORM，它从时间和空间角度提取股票特征，然后在细粒度和语义级别融合和对齐这些特征，并将因子表示为多维嵌入。离散码本对相似的因子嵌入进行聚类，确保正交性和多样性，这有助于区分不同的因子并支持金融交易中的因子选择。为了展示所提出的因子模型的性能，我们将其应用于两个下游实验：两个股票数据集上的投资组合管理和六个特定股票上的个股交易任务。大量的实验表明，STORM 在适应下游任务方面具有灵活性，并且比基线模型具有更好的性能。

##### **The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective**
2412.09460v1 by Javier de la Rosa, Vladislav Mikhailov, Lemei Zhang, Freddy Wetjen, David Samuel, Peng Liu, Rolv-Arild Braaten, Petter Mæhlum, Magnus Breder Birkenes, Andrey Kutuzov, Tita Enstad, Svein Arne Brygfjeld, Jon Atle Gulla, Stephan Oepen, Erik Velldal, Wilfred Østgulen, Liljia Øvrelid, Aslak Sira Myhre

The use of copyrighted materials in training generative language models
raises critical legal and ethical questions. This paper presents a framework
for and the results of empirically assessing the impact of copyrighted
materials on the performance of large language models (LLMs) for Norwegian. We
found that both books and newspapers contribute positively when the models are
evaluated on a diverse set of Norwegian benchmarks, while fiction works
possibly lead to decreased performance. Our experiments could inform the
creation of a compensation scheme for authors whose works contribute to AI
development.

摘要：在訓練生成式語言模型中使用受版權保護的材料會引發重大的法律和倫理問題。本文提出了一個架構，並根據經驗評估受版權保護的材料對挪威大型語言模型 (LLM) 效能的影響。我們發現，當這些模型在各種挪威基準上進行評估時，書籍和報紙都有正面的貢獻，而虛構作品可能會導致效能下降。我們的實驗可以為作者制定補償計畫，而他們的作品有助於 AI 發展。

##### **From Intention To Implementation: Automating Biomedical Research via LLMs**
2412.09429v1 by Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Lin Chen

Conventional biomedical research is increasingly labor-intensive due to the
exponential growth of scientific literature and datasets. Artificial
intelligence (AI), particularly Large Language Models (LLMs), has the potential
to revolutionize this process by automating various steps. Still, significant
challenges remain, including the need for multidisciplinary expertise,
logicality of experimental design, and performance measurements. This paper
introduces BioResearcher, the first end-to-end automated system designed to
streamline the entire biomedical research process involving dry lab
experiments. BioResearcher employs a modular multi-agent architecture,
integrating specialized agents for search, literature processing, experimental
design, and programming. By decomposing complex tasks into logically related
sub-tasks and utilizing a hierarchical learning approach, BioResearcher
effectively addresses the challenges of multidisciplinary requirements and
logical complexity. Furthermore, BioResearcher incorporates an LLM-based
reviewer for in-process quality control and introduces novel evaluation metrics
to assess the quality and automation of experimental protocols. BioResearcher
successfully achieves an average execution success rate of 63.07% across eight
previously unmet research objectives. The generated protocols averagely
outperform typical agent systems by 22.0% on five quality metrics. The system
demonstrates significant potential to reduce researchers' workloads and
accelerate biomedical discoveries, paving the way for future innovations in
automated research systems.

摘要：傳統生物醫學研究由於科學文獻和數據集的指數級增長，正變得越來越依賴人力。人工智慧 (AI)，特別是大語言模型 (LLM)，有潛力透過自動化各種步驟來革新此流程。儘管如此，仍存在重大挑戰，包括對跨領域專業知識、實驗設計的邏輯性以及效能測量方面的需求。本文介紹了 BioResearcher，這是第一個端到端自動化系統，旨在簡化涉及乾式實驗室實驗的整個生物醫學研究流程。BioResearcher 採用模組化多主體架構，整合了專門用於搜尋、文獻處理、實驗設計和程式編寫的主體。透過將複雜任務分解成邏輯相關的子任務，並利用階層式學習方法，BioResearcher 有效地應對了跨領域需求和邏輯複雜性的挑戰。此外，BioResearcher 結合了基於 LLM 的審查員，用於進行中的品質控管，並引入了新的評估指標來評估實驗協定的品質和自動化。BioResearcher 成功地在八個先前未達成的研究目標中達到了平均執行成功率 63.07%。所產生的協定在五項品質指標上平均優於典型的代理系統 22.0%。該系統展現了顯著的潛力，可減少研究人員的工作量並加速生物醫學發現，為自動化研究系統的未來創新鋪路。

##### **Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer**
2412.09417v1 by Adam Labiosa, Zhihan Wang, Siddhant Agarwal, William Cong, Geethika Hemkumar, Abhinav Narayan Harish, Benjamin Hong, Josh Kelle, Chen Li, Yuhao Li, Zisen Shao, Peter Stone, Josiah P. Hanna

Robot decision-making in partially observable, real-time, dynamic, and
multi-agent environments remains a difficult and unsolved challenge. Model-free
reinforcement learning (RL) is a promising approach to learning decision-making
in such domains, however, end-to-end RL in complex environments is often
intractable. To address this challenge in the RoboCup Standard Platform League
(SPL) domain, we developed a novel architecture integrating RL within a
classical robotics stack, while employing a multi-fidelity sim2real approach
and decomposing behavior into learned sub-behaviors with heuristic selection.
Our architecture led to victory in the 2024 RoboCup SPL Challenge Shield
Division. In this work, we fully describe our system's architecture and
empirically analyze key design decisions that contributed to its success. Our
approach demonstrates how RL-based behaviors can be integrated into complete
robot behavior architectures.

摘要：機器人在部分可觀察、即時、動態和多主體環境中的決策制定仍然是一個困難且尚未解決的挑戰。無模型強化學習 (RL) 是一種在這種領域中學習決策制定很有前景的方法，然而，在複雜環境中的端到端 RL 通常難以處理。為了應對 RoboCup 標準平台聯盟 (SPL) 領域中的這一挑戰，我們開發了一種新穎的架構，將 RL 整合到經典機器人堆疊中，同時採用多保真度 sim2real 方法，並將行為分解為具有啟發式選擇的學習子行為。我們的架構在 2024 年 RoboCup SPL 挑戰盾牌部門中取得了勝利。在這項工作中，我們充分描述了我們系統的架構，並根據經驗分析了有助於其成功的關鍵設計決策。我們的做法展示了基於 RL 的行為如何整合到完整的機器人行為架構中。

##### **Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors**
2412.09416v1 by Kaushal Kumar Maurya, KV Aditya Srivatsa, Kseniia Petukhova, Ekaterina Kochmar

In this paper, we investigate whether current state-of-the-art large language
models (LLMs) are effective as AI tutors and whether they demonstrate
pedagogical abilities necessary for good AI tutoring in educational dialogues.
Previous efforts towards evaluation have been limited to subjective protocols
and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy
with eight pedagogical dimensions based on key learning sciences principles,
which is designed to assess the pedagogical value of LLM-powered AI tutor
responses grounded in student mistakes or confusion in the mathematical domain.
We release MRBench -- a new evaluation benchmark containing 192 conversations
and 1,596 responses from seven state-of-the-art LLM-based and human tutors,
providing gold annotations for eight pedagogical dimensions. We assess
reliability of the popular Prometheus2 LLM as an evaluator and analyze each
tutor's pedagogical abilities, highlighting which LLMs are good tutors and
which ones are more suitable as question-answering systems. We believe that the
presented taxonomy, benchmark, and human-annotated labels will streamline the
evaluation process and help track the progress in AI tutors' development.

摘要：在本文中，我們探討目前最先進的大語言模型 (LLM) 是否能有效作為 AI 導師，以及它們是否展現出在教育對話中進行良好 AI 輔導所需的教學能力。以前在評估方面的努力僅限於主觀協定和基準。為了彌補這個差距，我們提出了統一的評估分類法，其中包含八個基於關鍵學習科學原則的教學面向，旨在評估以學生在數學領域的錯誤或困惑為基礎的 LLM 驅動 AI 導師回應的教學價值。我們發布了 MRBench——一個新的評估基準，其中包含來自七個最先進的基於 LLM 和人類導師的 192 次對話和 1,596 個回應，為八個教學面向提供了黃金註解。我們評估了流行的 Prometheus2 LLM 作為評估者的可靠性，並分析了每位導師的教學能力，重點說明了哪些 LLM 是好的導師，哪些更適合作為問答系統。我們相信，所提出的分類法、基準和人工註解標籤將簡化評估過程，並有助於追蹤 AI 導師發展的進度。

##### **Text Generation Models for Luxembourgish with Limited Data: A Balanced Multilingual Strategy**
2412.09415v1 by Alistair Plum, Tharindu Ranasinghe, Christoph Purschke

This paper addresses the challenges in developing language models for
less-represented languages, with a focus on Luxembourgish. Despite its active
development, Luxembourgish faces a digital data scarcity, exacerbated by
Luxembourg's multilingual context. We propose a novel text generation model
based on the T5 architecture, combining limited Luxembourgish data with equal
amounts, in terms of size and type, of German and French data. We hypothesise
that a model trained on Luxembourgish, German, and French will improve the
model's cross-lingual transfer learning capabilities and outperform monolingual
and large multilingual models. To verify this, the study at hand explores
whether multilingual or monolingual training is more beneficial for
Luxembourgish language generation. For the evaluation, we introduce LuxGen, a
text generation benchmark that is the first of its kind for Luxembourgish.

摘要：本文探討了開發低資源語言語言模型的挑戰，重點關注盧森堡語。儘管盧森堡語積極發展，但仍面臨數位資料短缺的問題，而盧森堡的多語言環境更讓情況雪上加霜。我們提出了基於 T5 架構的新穎文字生成模型，將有限的盧森堡語資料與德語和法語資料在大小和類型上等量結合。我們假設以盧森堡語、德語和法語訓練的模型將提升模型的跨語言傳輸學習能力，並優於單語和大型多語言模型。為了驗證這一點，本研究探討了多語言或單語言訓練對於盧森堡語生成更為有利。在評估方面，我們引入了 LuxGen，這是盧森堡語的第一個同類型文字生成基準。

##### **Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems**
2412.09413v1 by Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen

Recently, slow-thinking reasoning systems, such as o1, have demonstrated
remarkable capabilities in solving complex reasoning tasks. These systems
typically engage in an extended thinking process before responding to a query,
allowing them to generate more thorough, accurate, and well-reasoned solutions.
These systems are primarily developed and maintained by industry, with their
core techniques not publicly disclosed. In response, an increasing number of
studies from the research community aim to explore the technical foundations
underlying these powerful reasoning systems. Building on these prior efforts,
this paper presents a reproduction report on implementing o1-like reasoning
systems. We introduce an "imitate, explore, and self-improve" framework as our
primary technical approach to train the reasoning model. In the initial phase,
we use distilled long-form thought data to fine-tune the reasoning model,
enabling it to invoke a slow-thinking mode. The model is then encouraged to
explore challenging problems by generating multiple rollouts, which can result
in increasingly more high-quality trajectories that lead to correct answers.
Furthermore, the model undergoes self-improvement by iteratively refining its
training dataset. To verify the effectiveness of this approach, we conduct
extensive experiments on three challenging benchmarks. The experimental results
demonstrate that our approach achieves competitive performance compared to
industry-level reasoning systems on these benchmarks.

摘要：<paragraph>最近，慢思考推理系统（例如 o1）已展现出在解决复杂推理任务方面的非凡能力。这些系统通常在回答查询之前进行长时间的思考过程，从而能够生成更全面、准确且经过充分推理的解决方案。这些系统主要由行业开发和维护，其核心技术并未公开披露。作为回应，越来越多的研究界研究旨在探索这些强大的推理系统背后的技术基础。在此前工作的基础上，本文介绍了关于实施类似 o1 的推理系统的复制报告。我们引入了一个“模仿、探索和自我完善”框架作为我们训练推理模型的主要技术方法。在初始阶段，我们使用精馏的长格式思维数据对推理模型进行微调，使其能够调用慢思考模式。然后鼓励模型通过生成多个滚动来探索具有挑战性的问题，这可能导致产生更多高质量的轨迹，从而得出正确的答案。此外，该模型通过迭代优化其训练数据集来自我完善。为了验证此方法的有效性，我们在三个具有挑战性的基准上进行了广泛的实验。实验结果表明，与这些基准上的行业级推理系统相比，我们的方法取得了具有竞争力的性能。</paragraph>

##### **Uncommon Belief in Rationality**
2412.09407v1 by Qi Shi, Pavel Naumov

Common knowledge/belief in rationality is the traditional standard assumption
in analysing interaction among agents. This paper proposes a graph-based
language for capturing significantly more complicated structures of
higher-order beliefs that agents might have about the rationality of the other
agents. The two main contributions are a solution concept that captures the
reasoning process based on a given belief structure and an efficient algorithm
for compressing any belief structure into a unique minimal form.

摘要：在分析代理之間的互動時，理性中的常識/信念是傳統的標準假設。本文提出了一種基於圖形的語言，用於捕捉代理人可能對其他代理人的理性具有顯著更複雜的高階信念結構。兩項主要貢獻是捕捉基於給定信念結構的推理過程的解決方案概念，以及將任何信念結構壓縮成唯一最小形式的有效演算法。

##### **UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer**
2412.09389v1 by Delong Liu, Zhaohui Hou, Mingjie Zhan, Shihao Han, Zhicheng Zhao, Fei Su

Recently, diffusion-based video generation models have achieved significant
success. However, existing models often suffer from issues like weak
consistency and declining image quality over time. To overcome these
challenges, inspired by aesthetic principles, we propose a non-invasive plug-in
called Uniform Frame Organizer (UFO), which is compatible with any
diffusion-based video generation model. The UFO comprises a series of adaptive
adapters with adjustable intensities, which can significantly enhance the
consistency between the foreground and background of videos and improve image
quality without altering the original model parameters when integrated. The
training for UFO is simple, efficient, requires minimal resources, and supports
stylized training. Its modular design allows for the combination of multiple
UFOs, enabling the customization of personalized video generation models.
Furthermore, the UFO also supports direct transferability across different
models of the same specification without the need for specific retraining. The
experimental results indicate that UFO effectively enhances video generation
quality and demonstrates its superiority in public video generation benchmarks.
The code will be publicly available at https://github.com/Delong-liu-bupt/UFO.

摘要：最近，基于扩散的视频生成模型取得了重大成功。然而，现有模型通常存在一致性弱和图像质量随时间下降等问题。为了克服这些挑战，我们受到美学原理的启发，提出了一种名为统一帧组织器 (UFO) 的非侵入式插件，它与任何基于扩散的视频生成模型兼容。UFO 包含一系列具有可调节强度的自适应适配器，它可以显著增强视频的前景和背景之间的一致性，并在集成时提高图像质量，而无需改变原始模型参数。UFO 的训练简单、高效、需要的资源最少，并支持风格化训练。其模块化设计允许组合多个 UFO，从而实现个性化视频生成模型的定制。此外，UFO 还支持在相同规范的不同模型之间直接转移，而无需进行特定再训练。实验结果表明，UFO 有效地增强了视频生成质量，并在公开视频生成基准测试中展示了其优越性。代码将在 https://github.com/Delong-liu-bupt/UFO 上公开。

##### **All You Need in Knowledge Distillation Is a Tailored Coordinate System**
2412.09388v1 by Junjie Zhou, Ke Zhu, Jianxin Wu

Knowledge Distillation (KD) is essential in transferring dark knowledge from
a large teacher to a small student network, such that the student can be much
more efficient than the teacher but with comparable accuracy. Existing KD
methods, however, rely on a large teacher trained specifically for the target
task, which is both very inflexible and inefficient. In this paper, we argue
that a SSL-pretrained model can effectively act as the teacher and its dark
knowledge can be captured by the coordinate system or linear subspace where the
features lie in. We then need only one forward pass of the teacher, and then
tailor the coordinate system (TCS) for the student network. Our TCS method is
teacher-free and applies to diverse architectures, works well for KD and
practical few-shot learning, and allows cross-architecture distillation with
large capacity gap. Experiments show that TCS achieves significantly higher
accuracy than state-of-the-art KD methods, while only requiring roughly half of
their training time and GPU memory costs.

摘要：知識蒸餾 (KD) 在將黑暗知識從大型教師傳輸到小型學生網路時至關重要，這樣學生可以比教師有效率得多，但準確度卻相當。然而，現有的 KD 方法依賴於專門針對目標任務訓練的大型教師，這既不靈活又低效。在本文中，我們認為 SSL 預訓練模型可以有效地充當教師，其黑暗知識可以被特徵所在的坐標系或線性子空間所捕獲。然後我們只需要教師的前向傳遞一次，然後為學生網路調整坐標系 (TCS)。我們的 TCS 方法是無教師的，適用於各種架構，適用於 KD 和實際的 few-shot 學習，並允許跨架構蒸餾具有較大的容量差距。實驗表明，TCS 的準確度顯著高於最先進的 KD 方法，同時只需要大約一半的訓練時間和 GPU 記憶體成本。

##### **AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities**
2412.09385v1 by Fabrizio Davide, Pietro Torre, Andrea Gaggioli

We tasked 16 state-of-the-art large language models (LLMs) with estimating
the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To
assess the quality of these forecasts, we implemented an automated peer review
process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-
Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align
with a recent expert survey that projected a 10% likelihood of AGI by 2027,
underscoring the relevance of LLMs in forecasting complex, speculative
scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a
high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable
consistency in scoring across the models. Among the models, Pplx-70b-online
emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A
cross-comparison with external benchmarks, such as LMSYS Chatbot Arena,
revealed that LLM rankings remained consistent across different evaluation
methods, suggesting that existing benchmarks may not encapsulate some of the
skills relevant for AGI prediction. We further explored the use of weighting
schemes based on external benchmarks, optimizing the alignment of LLMs'
predictions with human expert forecasts. This analysis led to the development
of a new, 'AGI benchmark' designed to highlight performance differences in
AGI-related tasks. Our findings offer insights into LLMs' capabilities in
speculative, interdisciplinary forecasting tasks and emphasize the growing need
for innovative evaluation frameworks for assessing AI performance in complex,
uncertain real-world scenarios.

摘要：<paragraph>我們委託 16 個最先進的大型語言模型 (LLM) 估計人工通用智慧 (AGI) 在 2030 年出現的可能性。為了評估這些預測的品質，我們實施了一項自動化的同儕審查流程 (LLM-PR)。LLM 的估計差異很大，從 3% (Reka-Core) 到 47.6% (GPT-4o) 不等，中位數為 12.5%。這些估計與最近專家調查密切一致，該調查預測 AGI 在 2027 年出現的可能性為 10%，這強調了 LLM 在預測複雜、推測性場景中的相關性。LLM-PR 流程表現出很強的可靠性，這由高類內相關係數 (ICC = 0.79) 證明，反映出模型之間評分的顯著一致性。在這些模型中，Pplx-70b-online 成為表現最佳者，而 Gemini-1.5-pro-api 排名最低。與外部基準（例如 LMSYS Chatbot Arena）的交叉比較顯示，LLM 排名在不同的評估方法中保持一致，這表明現有的基準可能無法概括與 AGI 預測相關的一些技能。我們進一步探討了基於外部基準的加權方案的使用，優化了 LLM 預測與人類專家預測的一致性。此分析導致開發了一個新的「AGI 基準」，旨在強調 AGI 相關任務中的效能差異。我們的研究結果提供了對 LLM 在推測性、跨學科預測任務中的能力的見解，並強調了在複雜、不確定的現實世界場景中評估 AI 效能時，對創新評估框架日益增長的需求。</paragraph>

##### **Neural Text Normalization for Luxembourgish using Real-Life Variation Data**
2412.09383v1 by Anne-Marie Lutgen, Alistair Plum, Christoph Purschke, Barbara Plank

Orthographic variation is very common in Luxembourgish texts due to the
absence of a fully-fledged standard variety. Additionally, developing NLP tools
for Luxembourgish is a difficult task given the lack of annotated and parallel
data, which is exacerbated by ongoing standardization. In this paper, we
propose the first sequence-to-sequence normalization models using the ByT5 and
mT5 architectures with training data obtained from word-level real-life
variation data. We perform a fine-grained, linguistically-motivated evaluation
to test byte-based, word-based and pipeline-based models for their strengths
and weaknesses in text normalization. We show that our sequence model using
real-life variation data is an effective approach for tailor-made normalization
in Luxembourgish.

摘要：由於完全成熟的標準變體不存在，拼寫變異在盧森堡文本中非常常見。此外，由於缺乏註釋和並行數據，開發盧森堡語的 NLP 工具是一項艱鉅的任務，而持續的標準化更使問題惡化。在本文中，我們使用從字元級真實變異數據中獲得的訓練數據，提出了第一個使用 ByT5 和 mT5 架構的序列對序列正規化模型。我們執行細緻的、語言學動機評估，以測試基於字元、基於字詞和基於管線的模型在文字正規化中的優缺點。我們展示使用真實變異數據的序列模型是針對盧森堡語量身打造的正規化的有效方法。

##### **Diffusion Model with Representation Alignment for Protein Inverse Folding**
2412.09380v1 by Chenglin Wang, Yucheng Zhou, Zijie Zhai, Jianbing Shen, Kai Zhang

Protein inverse folding is a fundamental problem in bioinformatics, aiming to
recover the amino acid sequences from a given protein backbone structure.
Despite the success of existing methods, they struggle to fully capture the
intricate inter-residue relationships critical for accurate sequence
prediction. We propose a novel method that leverages diffusion models with
representation alignment (DMRA), which enhances diffusion-based inverse folding
by (1) proposing a shared center that aggregates contextual information from
the entire protein structure and selectively distributes it to each residue;
and (2) aligning noisy hidden representations with clean semantic
representations during the denoising process. This is achieved by predefined
semantic representations for amino acid types and a representation alignment
method that utilizes type embeddings as semantic feedback to normalize each
residue. In experiments, we conduct extensive evaluations on the CATH4.2
dataset to demonstrate that DMRA outperforms leading methods, achieving
state-of-the-art performance and exhibiting strong generalization capabilities
on the TS50 and TS500 datasets.

摘要：蛋白質逆向摺疊是生物資訊學中的基本問題，旨在從給定的蛋白質骨架結構中恢復胺基酸序列。儘管現有方法已經成功，但它們仍難以完全捕捉到對準確序列預測至關重要的複雜殘基間關係。我們提出了一種新方法，該方法利用具有表示對齊（DMRA）的擴散模型，通過（1）提出一個共享中心，從整個蛋白質結構中彙總上下文資訊並將其選擇性地分發到每個殘基；以及（2）在去噪過程中將雜訊隱藏表示與乾淨語義表示對齊來增強基於擴散的逆向摺疊。這是通過針對胺基酸類型預先定義的語義表示和一種將類型嵌入作為語義回饋以標準化每個殘基的表示對齊方法來實現的。在實驗中，我們對 CATH4.2 資料集進行了廣泛的評估，以證明 DMRA 優於領先的方法，在 TS50 和 TS500 資料集上實現了最先進的效能並展現出強大的泛化能力。

##### **Word Sense Linking: Disambiguating Outside the Sandbox**
2412.09370v1 by Andrei Stefan Bejgu, Edoardo Barba, Luigi Procopio, Alberte Fernández-Castro, Roberto Navigli

Word Sense Disambiguation (WSD) is the task of associating a word in a given
context with its most suitable meaning among a set of possible candidates.
While the task has recently witnessed renewed interest, with systems achieving
performances above the estimated inter-annotator agreement, at the time of
writing it still struggles to find downstream applications. We argue that one
of the reasons behind this is the difficulty of applying WSD to plain text.
Indeed, in the standard formulation, models work under the assumptions that a)
all the spans to disambiguate have already been identified, and b) all the
possible candidate senses of each span are provided, both of which are
requirements that are far from trivial. In this work, we present a new task
called Word Sense Linking (WSL) where, given an input text and a reference
sense inventory, systems have to both identify which spans to disambiguate and
then link them to their most suitable meaning.We put forward a
transformer-based architecture for the task and thoroughly evaluate both its
performance and those of state-of-the-art WSD systems scaled to WSL,
iteratively relaxing the assumptions of WSD. We hope that our work will foster
easier integration of lexical semantics into downstream applications.

摘要：詞義消歧 (WSD) 的任務是將特定語境中的詞彙與一組可能的候選詞彙中最合適的意義關聯起來。儘管這項任務最近引起新的興趣，系統達成的效能高於估計的標註者間一致性，但在撰寫本文時，它仍難以找出下游應用程式。我們認為，原因之一是將 WSD 應用於純文字的難度。事實上，在標準制定中，模型在以下假設下運作：a) 所有要消歧的範圍都已識別出來，以及 b) 提供每個範圍的所有可能的候選詞義，這兩個都是遠非微不足道的需求。在這項工作中，我們提出一個稱為詞義連結 (WSL) 的新任務，在給定輸入文字和參考詞義清單的情況下，系統必須同時識別要消歧的範圍，然後將它們連結到最合適的意義。我們提出一個基於轉換器的架構來執行這項任務，並徹底評估其效能和縮放到 WSL 的最先進 WSD 系統，反覆放寬 WSD 的假設。我們希望我們的這項工作能促進將詞彙語義更輕鬆地整合到下游應用程式中。

##### **Falcon-UI: Understanding GUI Before Following User Instructions**
2412.09362v1 by Huawen Shen, Chang Liu, Gengluo Li, Xinlong Wang, Yu Zhou, Can Ma, Xiangyang Ji

Pursuing human-like interaction for Graphical User Interface (GUI) agents
requires understanding the GUI context and following user instructions.
However, existing works typically couple these two aspects and focus more on
instruct-following abilities, while ignoring the importance of understanding
the GUI context. In this paper, we introduce an instruction-free GUI navigation
dataset, termed Insight-UI Dataset, to enhance model comprehension of GUI
environments. Insight-UI Dataset is automatically generated from the Common
Crawl corpus, simulating various platforms -- including iOS, Android, Windows,
and Linux -- across multiple resolutions on 312K domains. Although GUI
interactions vary by context, diverse interfaces share common internal
patterns, such as clicking an item to view its details. It implies the
feasibility of independent GUI operation learning, followed by joint
optimization with instruction tuning. Thereby, we develop the GUI agent model
Falcon-UI, which is initially pretrained on Insight-UI Dataset and subsequently
fine-tuned on Android and Web GUI datasets, including AITW, AITZ, Android
Control, and Mind2Web. With 7 billion parameters, Falcon-UI achieves accuracy
comparable to the 72 billion-parameter Qwen2VL on AITZ, validating the
alignment between GUI context comprehension and agent performance. Our code and
dataset will be open-sourced.

摘要：為了追求圖形使用者介面 (GUI) 代理人的類人互動，
需要了解 GUI 背景和遵循使用者指示。
然而，現有的作品通常結合這兩個方面，並更專注於
遵循指示的能力，同時忽略了解 GUI 背景的重要性。
在本文中，我們引入了一個無需指示的 GUI 導航
資料集，稱為 Insight-UI 資料集，以增強模型對 GUI
環境的理解。Insight-UI 資料集是從 Common
Crawl 語料庫自動生成的，模擬了各種平台 -- 包括 iOS、Android、Windows
和 Linux -- 在 312K 個網域上採用多種解析度。雖然 GUI
互動會因背景而異，但不同的介面有共同的內部
模式，例如按一下項目以檢視其詳細資料。這表示
獨立 GUI 操作學習的可行性，接著透過指示調整進行聯合
最佳化。因此，我們開發了 GUI 代理人模型
Falcon-UI，它最初在 Insight-UI 資料集上進行預訓練，並隨後
在 Android 和 Web GUI 資料集上進行微調，包括 AITW、AITZ、Android
Control 和 Mind2Web。Falcon-UI 擁有 70 億個參數，在 AITZ 上達到的準確度
可與擁有 720 億個參數的 Qwen2VL 相媲美，驗證了 GUI 背景理解和代理人效能之間的一致性。我們的程式碼和
資料集將會開放原始碼。

##### **Causal Graphical Models for Vision-Language Compositional Understanding**
2412.09353v1 by Fiorenzo Parascandolo, Nicholas Moratelli, Enver Sangineto, Lorenzo Baraldi, Rita Cucchiara

Recent work has empirically shown that Vision-Language Models (VLMs) struggle
to fully understand the compositional properties of the human language, usually
modeling an image caption as a "bag of words". As a result, they perform poorly
on compositional tasks, which require a deeper understanding of the different
entities of a sentence (subject, verb, etc.) jointly with their mutual
relationships in order to be solved. In this paper, we model the dependency
relations among textual and visual tokens using a Causal Graphical Model (CGM),
built using a dependency parser, and we train a decoder conditioned by the VLM
visual encoder. Differently from standard autoregressive or parallel
predictions, our decoder's generative process is partially-ordered following
the CGM structure. This structure encourages the decoder to learn only the main
causal dependencies in a sentence discarding spurious correlations. Using
extensive experiments on five compositional benchmarks, we show that our method
significantly outperforms all the state-of-the-art compositional approaches by
a large margin, and it also improves over methods trained using much larger
datasets.

摘要：最近的研究實證顯示，視覺語言模型 (VLM) 難以完全理解人類語言的組合特性，通常將影像標題建模為「詞彙袋」。因此，它們在組合任務上的表現不佳，而這些任務需要更深入地理解句子中不同實體（主詞、動詞等）以及它們之間的相互關係才能解決。在本文中，我們使用因果圖形模型 (CGM) 對文本和視覺符號之間的依賴關係進行建模，該模型是使用依賴解析器建立的，並且我們訓練了一個由 VLM 視覺編碼器條件化的解碼器。與標準的自動迴歸或並行預測不同，我們的解碼器的生成過程是部分排序的，遵循 CGM 結構。此結構鼓勵解碼器僅學習句子中的主要因果依賴關係，並捨棄虛假的相關性。透過在五個組合基準上進行廣泛的實驗，我們展示了我們的模型大幅優於所有最先進的組合方法，而且也優於使用更大資料集訓練的方法。

##### **Training LayoutLM from Scratch for Efficient Named-Entity Recognition in the Insurance Domain**
2412.09341v1 by Benno Uthayasooriyar, Antoine Ly, Franck Vermet, Caio Corro

Generic pre-trained neural networks may struggle to produce good results in
specialized domains like finance and insurance. This is due to a domain
mismatch between training data and downstream tasks, as in-domain data are
often scarce due to privacy constraints. In this work, we compare different
pre-training strategies for LayoutLM. We show that using domain-relevant
documents improves results on a named-entity recognition (NER) problem using a
novel dataset of anonymized insurance-related financial documents called
Payslips. Moreover, we show that we can achieve competitive results using a
smaller and faster model.

摘要：通用預訓練神經網路在金融和保險等專業領域可能難以產生良好的結果。這是因為訓練資料和下游任務之間的領域不匹配，因為領域內資料由於隱私限制而經常稀缺。在這項工作中，我們比較了 LayoutLM 的不同預訓練策略。我們展示了使用與領域相關的文件改進了使用稱為 Payslips 的匿名化保險相關財務文件的全新資料集的命名實體識別 (NER) 問題的結果。此外，我們展示了我們可以使用更小、更快的模型來取得有競爭力的結果。

##### **Does Low Spoilage Under Cold Conditions Foster Cultural Complexity During the Foraging Era? -- A Theoretical and Computational Inquiry**
2412.09335v1 by Minhyeok Lee

Human cultural complexity did not arise in a vacuum. Scholars in the
humanities and social sciences have long debated how ecological factors, such
as climate and resource availability, enabled early hunter-gatherers to
allocate time and energy beyond basic subsistence tasks. This paper presents a
formal, interdisciplinary approach that integrates theoretical modeling with
computational methods to examine whether conditions that allow lower spoilage
of stored food, often associated with colder climates and abundant large fauna,
could indirectly foster the emergence of cultural complexity. Our contribution
is twofold. First, we propose a mathematical framework that relates spoilage
rates, yield levels, resource management skills, and cultural activities. Under
this framework, we prove that lower spoilage and adequate yields reduce the
frequency of hunting, thus freeing substantial time for cultural pursuits.
Second, we implement a reinforcement learning simulation, inspired by
engineering optimization techniques, to validate the theoretical predictions.
By training agents in different $(Y,p)$ environments, where $Y$ is yield and
$p$ is the probability of daily spoilage, we observe patterns consistent with
the theoretical model: stable conditions with lower spoilage strongly correlate
with increased cultural complexity. While we do not claim to replicate
prehistoric social realities directly, our results suggest that ecologically
stable niches provided a milieu in which cultural forms could germinate and
evolve. This study, therefore, offers an integrative perspective that unites
humanistic inquiries into the origins of culture with the formal rigor and
exploratory power of computational modeling.

摘要：人類文化複雜性並非憑空產生。人文學科和社會科學學者長期以來一直爭論生態因素，例如氣候和資源可用性，如何使早期狩獵採集者能夠將時間和精力分配在基本的維持生計任務之外。本文提出了一種形式化的跨學科方法，將理論建模與計算方法相結合，以檢驗允許儲存食品腐敗較低的條件，通常與較冷的氣候和豐富的大型動物有關，是否能間接促進文化複雜性的出現。我們的貢獻有兩方面。首先，我們提出了一個數學框架，它將腐敗率、產量水準、資源管理技能和文化活動聯繫起來。在此框架下，我們證明了較低的腐敗率和充足的產量會減少狩獵的頻率，從而為文化活動騰出大量時間。其次，我們實施了一種強化學習模擬，靈感來自工程優化技術，以驗證理論預測。通過在不同的 $(Y,p)$ 環境中訓練代理，其中 $Y$ 是產量，$p$ 是每日腐敗的機率，我們觀察到與理論模型一致的模式：腐敗較低的穩定條件與文化複雜性的增加密切相關。雖然我們並未聲稱直接複製史前的社會現實，但我們的結果表明生態穩定的生態位提供了文化形式得以萌芽和演化的環境。因此，本研究提供了一個整合觀點，將對文化起源的人文主義探究與計算建模的形式嚴謹性和探索性力量結合在一起。

##### **Towards Open-Vocabulary Video Semantic Segmentation**
2412.09329v1 by Xinhao Li, Yun Liu, Guolei Sun, Min Wu, Le Zhang, Ce Zhu

Semantic segmentation in videos has been a focal point of recent research.
However, existing models encounter challenges when faced with unfamiliar
categories. To address this, we introduce the Open Vocabulary Video Semantic
Segmentation (OV-VSS) task, designed to accurately segment every pixel across a
wide range of open-vocabulary categories, including those that are novel or
previously unexplored. To enhance OV-VSS performance, we propose a robust
baseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing
the model to utilize temporal relationships across consecutive frames.
Additionally, we incorporate a random frame enhancement module, broadening the
model's understanding of semantic context throughout the entire video sequence.
Our approach also includes video text encoding, which strengthens the model's
capability to interpret textual information within the video context.
Comprehensive evaluations on benchmark datasets such as VSPW and Cityscapes
highlight OV-VSS's zero-shot generalization capabilities, especially in
handling novel categories. The results validate OV2VSS's effectiveness,
demonstrating improved performance in semantic segmentation tasks across
diverse video datasets.

摘要：影片中的语意分割一直是近期研究的重点。
然而，现有的模型在面对不熟悉的类别时会遇到挑战。为了解决这个问题，我们引入了开放词汇视频语义分割 (OV-VSS) 任务，旨在准确分割广泛开放词汇类别中的每个像素，包括那些新颖或以前未探索的类别。为了增强 OV-VSS 的性能，我们提出了一个稳健的基准 OV2VSS，它集成了一个时空融合模块，允许模型利用连续帧中的时间关系。
此外，我们还纳入了随机帧增强模块，扩大了模型对整个视频序列中语义上下文的理解。我们的方法还包括视频文本编码，这加强了模型在视频上下文中解释文本信息的能力。
在 VSPW 和 Cityscapes 等基准数据集上的全面评估突出了 OV-VSS 的零次泛化能力，尤其是在处理新颖类别方面。结果验证了 OV2VSS 的有效性，证明了在各种视频数据集中的语义分割任务中性能得到提升。

##### **Auto-Regressive Moving Diffusion Models for Time Series Forecasting**
2412.09328v1 by Jiaxin Gao, Qinglong Cao, Yuntian Chen

Time series forecasting (TSF) is essential in various domains, and recent
advancements in diffusion-based TSF models have shown considerable promise.
However, these models typically adopt traditional diffusion patterns, treating
TSF as a noise-based conditional generation task. This approach neglects the
inherent continuous sequential nature of time series, leading to a fundamental
misalignment between diffusion mechanisms and the TSF objective, thereby
severely impairing performance. To bridge this misalignment, and inspired by
the classic Auto-Regressive Moving Average (ARMA) theory, which views time
series as continuous sequential progressions evolving from previous data
points, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to
first achieve the continuous sequential diffusion-based TSF. Unlike previous
methods that start from white Gaussian noise, our model employs chain-based
diffusion with priors, accurately modeling the evolution of time series and
leveraging intermediate state information to improve forecasting accuracy and
stability. Specifically, our approach reinterprets the diffusion process by
considering future series as the initial state and historical series as the
final state, with intermediate series generated using a sliding-based technique
during the forward process. This design aligns the diffusion model's sampling
procedure with the forecasting objective, resulting in an unconditional,
continuous sequential diffusion TSF model. Extensive experiments conducted on
seven widely used datasets demonstrate that our model achieves state-of-the-art
performance, significantly outperforming existing diffusion-based TSF models.
Our code is available on GitHub: https://github.com/daxin007/ARMD.

摘要：時間序列預測 (TSF) 在各種領域中至關重要，而基於擴散的 TSF 模型最近的進展已展現出相當大的前景。
然而，這些模型通常採用傳統的擴散模式，將 TSF 視為基於雜訊的條件生成任務。這種方法忽略了時間序列固有的連續序列性質，導致擴散機制與 TSF 目標之間的基本錯位，從而嚴重損害了效能。為了彌合這種錯位，並受到經典自迴歸移動平均線 (ARMA) 理論的啟發，該理論將時間序列視為從先前資料點演變而來的連續序列進程，我們提出了一種新穎的自迴歸移動擴散 (ARMD) 模型，以首先實現連續序列基於擴散的 TSF。與從白色高斯雜訊開始的先前方法不同，我們的模型採用具有先驗的基於鏈的擴散，準確地模擬時間序列的演變，並利用中間狀態資訊來提高預測準確性和穩定性。具體來說，我們的做法透過將未來序列視為初始狀態，將歷史序列視為最終狀態，並在前進過程中使用基於滑動的技術產生中間序列，重新詮釋擴散過程。此設計將擴散模型的取樣程序與預測目標對齊，產生無條件、連續序列擴散 TSF 模型。在七個廣泛使用的資料集上進行的廣泛實驗表明，我們的模型達到了最先進的效能，顯著優於現有的基於擴散的 TSF 模型。我們的程式碼可在 GitHub 上取得：https://github.com/daxin007/ARMD。

##### **Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction**
2412.09318v1 by Jing Liu, Abdellah Fourtassi

LLMs can generate human-like dialogues, yet their ability to simulate early
child-adult interactions remains largely unexplored. In this paper, we examined
how effectively LLMs can capture the distinctive features of child-caregiver
language in interaction, using both static and interactive benchmarking
methods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can
approximate child-caregiver dialogues at the word and utterance level, but they
struggle to reproduce the child and caregiver's discursive patterns, exaggerate
alignment, and fail to reach the level of diversity shown by humans. The
broader goal of this work is to initiate the development of a comprehensive
benchmark for LLMs in child-oriented applications.

摘要：大型語言模型可以產生類似人類的對話，但它們模擬兒童與成人早期互動的能力在很大程度上仍未被探索。在本文中，我們探討了大型語言模型如何有效地捕捉兒童與照顧者在互動中語言的獨特特徵，並使用靜態和互動基準方法。我們發現，像 Llama 3 和 GPT-4o 等最先進的大型語言模型可以在詞彙和話語層面上近似兒童與照顧者的對話，但它們難以複製兒童和照顧者的論述模式，誇大了對齊，並且無法達到人類表現出的多樣性水平。這項工作的更廣泛目標是啟動針對兒童導向應用中大型語言模型的全面基準開發。

##### **Multimodal Sentiment Analysis based on Video and Audio Inputs**
2412.09317v1 by Antonio Fernandez, Suzan Awinat

Despite the abundance of current researches working on the sentiment analysis
from videos and audios, finding the best model that gives the highest accuracy
rate is still considered a challenge for researchers in this field. The main
objective of this paper is to prove the usability of emotion recognition models
that take video and audio inputs. The datasets used to train the models are the
CREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned
models that been used are: Facebook/wav2vec2-large for audio and the
Google/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for
each emotion generated by the two previous models is utilized in the decision
making framework. After disparity in the results, if one of the models gets
much higher accuracy, another test framework is created. The methods used are
the Weighted Average method, the Confidence Level Threshold method, the Dynamic
Weighting Based on Confidence method, and the Rule-Based Logic method. This
limited approach gives encouraging results that make future research into these
methods viable.

摘要：儘管目前有許多研究致力於影片和音訊的情感分析，但找到能提供最高準確度率的最佳模型，在這個領域中對研究人員來說仍被視為一項挑戰。本文的主要目標是證明使用影片和音訊輸入的情緒辨識模型的可行性。用於訓練模型的資料集是音訊的 CREMA-D 資料集和影片的 RAVDESS 資料集。已經使用的微調模型有：音訊的 Facebook/wav2vec2-large 和影片的 Google/vivit-b-16x2-kinetics400。在決策架構中，使用了前兩個模型產生的每個情緒的機率平均值。在結果出現差異後，如果其中一個模型獲得更高的準確度，則會建立另一個測試架構。所使用的方法有加權平均法、信心水準閾值法、基於信心的動態加權法和基於規則的邏輯法。這種受限的方法提供令人鼓舞的結果，讓未來對這些方法的研究具有可行性。

##### **Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation**
2412.09311v1 by Davor Vukadin, Petar Afrić, Marin Šilić, Goran Delač

Recent advancement in deep-neural network performance led to the development
of new state-of-the-art approaches in numerous areas. However, the black-box
nature of neural networks often prohibits their use in areas where model
explainability and model transparency are crucial. Over the years, researchers
proposed many algorithms to aid neural network understanding and provide
additional information to the human expert. One of the most popular methods
being Layer-Wise Relevance Propagation (LRP). This method assigns local
relevance based on the pixel-wise decomposition of nonlinear classifiers. With
the rise of attribution method research, there has emerged a pressing need to
assess and evaluate their performance. Numerous metrics have been proposed,
each assessing an individual property of attribution methods such as
faithfulness, robustness or localization. Unfortunately, no single metric is
deemed optimal for every case, and researchers often use several metrics to
test the quality of the attribution maps. In this work, we address the
shortcomings of the current LRP formulations and introduce a novel method for
determining the relevance of input neurons through layer-wise relevance
propagation. Furthermore, we apply this approach to the recently developed
Vision Transformer architecture and evaluate its performance against existing
methods on two image classification datasets, namely ImageNet and PascalVOC.
Our results clearly demonstrate the advantage of our proposed method.
Furthermore, we discuss the insufficiencies of current evaluation metrics for
attribution-based explainability and propose a new evaluation metric that
combines the notions of faithfulness, robustness and contrastiveness. We
utilize this new metric to evaluate the performance of various
attribution-based methods. Our code is available at:
https://github.com/davor10105/relative-absolute-magnitude-propagation

摘要：<paragraph>深度神经网络性能的最新进展导致了众多领域中新的最先进方法的发展。然而，神经网络的黑盒性质通常禁止在模型可解释性和模型透明性至关重要的领域中使用它们。多年来，研究人员提出了许多算法来帮助神经网络理解并向人类专家提供附加信息。最流行的方法之一是层级相关性传播 (LRP)。此方法基于非线性分类器的逐像素分解分配局部相关性。随着归因方法研究的兴起，迫切需要评估和评估其性能。已经提出了许多指标，每个指标都评估归因方法的单个属性，例如忠实度、鲁棒性或局部化。不幸的是，没有一个指标被认为在每种情况下都是最优的，研究人员通常使用多个指标来测试归因图的质量。在这项工作中，我们解决了当前 LRP 公式的缺点，并引入了一种通过层级相关性传播确定输入神经元相关性的新方法。此外，我们将此方法应用于最近开发的视觉变压器架构，并评估其在两个图像分类数据集（即 ImageNet 和 PascalVOC）上与现有方法的性能。我们的结果清楚地证明了我们提出的方法的优势。此外，我们讨论了当前归因解释的评估指标的不足，并提出了一个新的评估指标，该指标结合了忠实度、鲁棒性和对比度的概念。我们利用这个新指标来评估各种基于归因的方法的性能。我们的代码可在以下位置获取：
https://github.com/davor10105/relative-absolute-magnitude-propagation</paragraph>

##### **Learning Novel Skills from Language-Generated Demonstrations**
2412.09286v1 by Ao-Qun Jin, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Yue Cao, Sheng-Bin Duan, Fu-Chao Xie, Zeng-Guang Hou

Current robot learning algorithms for acquiring novel skills often rely on
demonstration datasets or environment interactions, resulting in high labor
costs and potential safety risks. To address these challenges, this study
proposes a skill-learning framework that enables robots to acquire novel skills
from natural language instructions. The proposed pipeline leverages
vision-language models to generate demonstration videos of novel skills, which
are processed by an inverse dynamics model to extract actions from the
unlabeled demonstrations. These actions are subsequently mapped to
environmental contexts via imitation learning, enabling robots to learn new
skills effectively. Experimental evaluations in the MetaWorld simulation
environments demonstrate the pipeline's capability to generate high-fidelity
and reliable demonstrations. Using the generated demonstrations, various skill
learning algorithms achieve an accomplishment rate three times the original on
novel tasks. These results highlight a novel approach to robot learning,
offering a foundation for the intuitive and intelligent acquisition of novel
robotic skills.

摘要：當前機器人學習演算法用於獲取新技能時，通常依賴於示範資料集或環境互動，導致高昂的人力成本和潛在的安全風險。為了應對這些挑戰，本研究提出了一個技能學習架構，使機器人能夠從自然語言指令中獲取新技能。所提出的管道利用視覺語言模型來產生新技能的示範影片，這些影片由逆動力學模型處理，以從未標記的示範中提取動作。這些動作隨後透過模仿學習映射到環境背景，使機器人能夠有效學習新技能。在 MetaWorld 模擬環境中的實驗評估證明了該管道產生高保真度和可靠示範的能力。使用產生的示範，各種技能學習演算法在新的任務上實現了三倍於原來的完成率。這些結果突顯了機器人學習的一種新方法，為直覺和智慧地獲取新的機器人技能奠定了基礎。

##### **InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption**
2412.09283v1 by Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, Ying Tai

Text-to-video generation has evolved rapidly in recent years, delivering
remarkable results. Training typically relies on video-caption paired data,
which plays a crucial role in enhancing generation performance. However,
current video captions often suffer from insufficient details, hallucinations
and imprecise motion depiction, affecting the fidelity and consistency of
generated videos. In this work, we propose a novel instance-aware structured
caption framework, termed InstanceCap, to achieve instance-level and
fine-grained video caption for the first time. Based on this scheme, we design
an auxiliary models cluster to convert original video into instances to enhance
instance fidelity. Video instances are further used to refine dense prompts
into structured phrases, achieving concise yet precise descriptions.
Furthermore, a 22K InstanceVid dataset is curated for training, and an
enhancement pipeline that tailored to InstanceCap structure is proposed for
inference. Experimental results demonstrate that our proposed InstanceCap
significantly outperform previous models, ensuring high fidelity between
captions and videos while reducing hallucinations.

摘要：文字转视频生成近年来发展迅速，取得了显著的成果。训练通常依赖于视频字幕配对数据，这对提高生成性能起着至关重要的作用。然而，当前的视频字幕常常存在细节不足、出现幻觉和动作描述不精确的问题，影响了生成视频的保真度和一致性。在这项工作中，我们提出了一种新颖的实例感知结构化字幕框架，称为 InstanceCap，首次实现了实例级和细粒度的视频字幕。基于此方案，我们设计了一个辅助模型集群，将原始视频转换为实例，以增强实例保真度。视频实例进一步用于将密集提示精炼为结构化短语，从而实现简洁而精确的描述。此外，还整理了一个 22K InstanceVid 数据集用于训练，并针对 InstanceCap 结构提出了一条增强的管道用于推理。实验结果表明，我们提出的 InstanceCap 明显优于以前的模型，确保了字幕和视频之间的高保真度，同时减少了幻觉。

##### **CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of LLMs**
2412.09282v1 by Yuzhuang Xu, Shiyu Ji, Qingfu Zhu, Wanxiang Che

Powerful large language models (LLMs) are increasingly expected to be
deployed with lower computational costs, enabling their capabilities on
resource-constrained devices. Post-training quantization (PTQ) has emerged as a
star approach to achieve this ambition, with best methods compressing weights
to less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector
Quantization (CRVQ), a novel technique that significantly improves the
performance of PTQ baselines at the cost of only minimal additional bits. This
state-of-the-art extreme compression method achieves its results through two
key innovations: (1) carefully selecting and reordering a very small subset of
critical weight channels, and (2) leveraging multiple codebooks to relax the
constraint of critical channels. With our method, we demonstrate a 38.9%
improvement over the current strongest sub-2-bit PTQ baseline, enabling nearer
lossless 1-bit compression. Furthermore, our approach offers flexible
customization of quantization bit-width and performance, providing a wider
range of deployment options for diverse hardware platforms.

摘要：強大的大型語言模型 (LLM) 愈來愈有望以較低的運算成本部署，讓它們能夠在資源受限的裝置上發揮功能。訓練後量化 (PTQ) 已成為實現此目標的明星方法，其中最佳方法將權重平均壓縮到不到 2 位元。在本文中，我們提出通道放鬆向量量化 (CRVQ)，這是一種新技術，僅以極少的額外位元為代價，就能顯著提升 PTQ 基準的效能。此最先進的極端壓縮方法透過兩項關鍵創新來達成其成果：(1) 仔細選取並重新排序極少數的關鍵權重通道，以及 (2) 採用多個碼本放寬關鍵通道的約束。透過我們的方法，我們展示出比目前最強的次 2 位元 PTQ 基準高出 38.9% 的提升，實現更接近無失真的 1 位元壓縮。此外，我們的做法提供量化位元寬度和效能的彈性自訂，為各種硬體平台提供更廣泛的部署選項。

##### **Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator**
2412.09280v1 by Chengyuan Liu, Shihang Wang, Lizhi Qing, Jun Lin, Ji Zhang, Fei Wu, Kun Kuang

Domain Large Language Models (LLMs) are developed for domain-specific tasks
based on general LLMs. But it still requires professional knowledge to
facilitate the expertise for some domain-specific tasks. In this paper, we
investigate into knowledge-intensive calculation problems. We find that the
math problems to be challenging for LLMs, when involving complex
domain-specific rules and knowledge documents, rather than simple formulations
of terminologies. Therefore, we propose a pipeline to solve the domain-specific
calculation problems with Knowledge-Intensive Programs Generator more
effectively, named as KIPG. It generates knowledge-intensive programs according
to the domain-specific documents. For each query, key variables are extracted,
then outcomes which are dependent on domain knowledge are calculated with the
programs. By iterative preference alignment, the code generator learns to
improve the logic consistency with the domain knowledge. Taking legal domain as
an example, we have conducted experiments to prove the effectiveness of our
pipeline, and extensive analysis on the modules. We also find that the code
generator is also adaptable to other domains, without training on the new
knowledge.

摘要：領域大型語言模型 (LLM) 是根據一般 LLM 開發用於特定領域任務。但仍需專業知識，才能促進某些特定領域任務的專業知識。在本文中，我們探討知識密集型計算問題。我們發現，當涉及複雜的特定領域規則和知識文件（而非術語的簡單表述）時，數學問題對 LLM 而言具有挑戰性。因此，我們提出了一個管道，以更有效地使用知識密集型程式產生器解決特定領域的計算問題，稱為 KIPG。它根據特定領域的文件產生知識密集型程式。對於每個查詢，會萃取出關鍵變數，然後使用程式計算依賴於領域知識的結果。透過反覆偏好比對，程式產生器學會改善與領域知識的邏輯一致性。以法律領域為例，我們已進行實驗證明我們管道的有效性，並對模組進行廣泛分析。我們還發現，程式產生器也可以適應其他領域，而不需要對新知識進行訓練。

##### **Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine**
2412.09278v1 by Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang

In recent years, Multimodal Large Language Models (MLLM) have achieved
notable advancements, demonstrating the feasibility of developing an
intelligent biomedical assistant. However, current biomedical MLLMs
predominantly focus on image-level understanding and restrict interactions to
textual commands, thus limiting their capability boundaries and the flexibility
of usage. In this paper, we introduce a novel end-to-end multimodal large
language model for the biomedical domain, named MedPLIB, which possesses
pixel-level understanding. Excitingly, it supports visual question answering
(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form
shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)
multi-stage training strategy, which divides MoE into separate training phases
for a visual-language expert model and a pixel-grounding expert model, followed
by fine-tuning using MoE. This strategy effectively coordinates multitask
learning while maintaining the computational cost at inference equivalent to
that of a single expert model. To advance the research of biomedical MLLMs, we
introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),
which comprises an array of 8 modalities for complex medical imaging question
answering and image region understanding. Experimental results indicate that
MedPLIB has achieved state-of-the-art outcomes across multiple medical visual
language tasks. More importantly, in zero-shot evaluations for the pixel
grounding task, MedPLIB leads the best small and large models by margins of
19.7 and 15.6 respectively on the mDice metric. The codes, data, and model
checkpoints will be made publicly available at
https://github.com/ShawnHuang497/MedPLIB.

摘要：<paragraph>近年来，多模态大型语言模型 (MLLM) 已取得显著进展，证明了开发智能生物医学助理的可行性。然而，当前的生物医学 MLLM 主要专注于图像级理解，并将交互限制在文本命令中，从而限制了它们的能力边界和使用灵活性。在本文中，我们介绍了一个用于生物医学领域的全新端到端多模态大型语言模型，名为 MedPLIB，它具有像素级理解能力。令人兴奋的是，它支持视觉问答 (VQA)、任意像素级提示（点、边界框和自由形式形状）以及像素级接地。我们提出了一种新颖的专家混合 (MoE) 多阶段训练策略，该策略将 MoE 分为视觉语言专家模型和像素接地专家模型的单独训练阶段，然后使用 MoE 进行微调。该策略有效地协调了多任务学习，同时将推理时的计算成本保持在与单个专家模型相当的水平。为了推进生物医学 MLLM 的研究，我们引入了医学复杂视觉问答数据集 (MeCoVQA)，它包含一系列 8 种用于复杂医学影像问答和图像区域理解的模态。实验结果表明，MedPLIB 在多个医学视觉语言任务中取得了最先进的成果。更重要的是，在像素接地任务的零样本评估中，MedPLIB 在 mDice 指标上分别以 19.7 和 15.6 的优势领先于最好的小型和大型模型。代码、数据和模型检查点将在 https://github.com/ShawnHuang497/MedPLIB 上公开。
</paragraph>

##### **Towards Understanding the Robustness of LLM-based Evaluations under Perturbations**
2412.09269v1 by Manav Chaudhary, Harshit Gupta, Savita Bhat, Vasudeva Varma

Traditional evaluation metrics like BLEU and ROUGE fall short when capturing
the nuanced qualities of generated text, particularly when there is no single
ground truth. In this paper, we explore the potential of Large Language Models
(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for
non-standardized metrics in summarization and dialog-based tasks. We conduct
experiments across multiple prompting strategies to examine how LLMs fare as
quality evaluators when compared with human judgments on the SummEval and USR
datasets, asking the model to generate both a score as well as a justification
for the score. Furthermore, we explore the robustness of the LLM evaluator by
using perturbed inputs. Our findings suggest that while LLMs show promise,
their alignment with human evaluators is limited, they are not robust against
perturbations and significant improvements are required for their standalone
use as reliable evaluators for subjective metrics.

摘要：傳統評估指標，例如 BLEU 和 ROUGE，在捕捉生成文字的細微品質時有所不足，特別是在沒有單一真實情況時。在本文中，我們探討大型語言模型 (LLM) 的潛力，特別是 Google Gemini 1，作為摘要和基於對話任務中非標準化指標的自動評估器。我們在多種提示策略中進行實驗，以檢驗 LLM 作為品質評估器的表現，並與人類對 SummEval 和 USR 資料集的判斷進行比較，要求模型生成一個分數以及對分數的說明。此外，我們透過使用擾動輸入來探討 LLM 評估器的穩健性。我們的研究結果表明，儘管 LLM 顯示出前景，但它們與人類評估者的對齊是有限的，它們無法抵抗擾動，並且需要顯著改進才能獨立用作主觀指標的可靠評估器。

##### **First Train to Generate, then Generate to Train: UnitedSynT5 for Few-Shot NLI**
2412.09263v1 by Sourav Banerjee, Anush Mahajan, Ayushi Agarwal, Eishkaran Singh

Natural Language Inference (NLI) tasks require identifying the relationship
between sentence pairs, typically classified as entailment, contradiction, or
neutrality. While the current state-of-the-art (SOTA) model, Entailment
Few-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural
Language Inference (SNLI) dataset, further advancements are constrained by the
dataset's limitations. To address this, we propose a novel approach leveraging
synthetic data augmentation to enhance dataset diversity and complexity. We
present UnitedSynT5, an advanced extension of EFL that leverages a T5-based
generator to synthesize additional premise-hypothesis pairs, which are
rigorously cleaned and integrated into the training data. These augmented
examples are processed within the EFL framework, embedding labels directly into
hypotheses for consistency. We train a GTR-T5-XL model on this expanded
dataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset,
94.01% accuracy on the E-SNLI dataset, and 92.57% accuracy on the MultiNLI
dataset, surpassing the previous SOTA models. This research demonstrates the
potential of synthetic data augmentation in improving NLI models, offering a
path forward for further advancements in natural language understanding tasks.

摘要：自然語言推論 (NLI) 任務需要辨識句子對之間的關係，通常分類為蘊涵、矛盾或中立。雖然當前最先進 (SOTA) 的模型，蘊涵少發學習 (EFL)，在史丹佛自然語言推論 (SNLI) 資料集上達到了 93.1% 的準確度，但進一步的進展受到資料集限制的約束。為了解決這個問題，我們提出了一種新方法，利用合成資料擴充來增強資料集的多樣性和複雜性。我們提出了 UnitedSynT5，這是一種 EFL 的進階擴充，它利用基於 T5 的生成器來合成額外的前提假設對，這些對經過嚴格的清理並整合到訓練資料中。這些擴充的範例在 EFL 架構中進行處理，將標籤直接嵌入假設中以確保一致性。我們在這個擴充的資料集上訓練了一個 GTR-T5-XL 模型，在 SNLI 資料集上達到了 94.7% 的準確度新基準，在 E-SNLI 資料集上達到了 94.01% 的準確度，在 MultiNLI 資料集上達到了 92.57% 的準確度，超越了之前的 SOTA 模型。這項研究展示了合成資料擴充在改善 NLI 模型中的潛力，為自然語言理解任務的進一步進展提供了前進的道路。

##### **Make Satire Boring Again: Reducing Stylistic Bias of Satirical Corpus by Utilizing Generative LLMs**
2412.09247v1 by Asli Umay Ozturk, Recep Firat Cekinel, Pinar Karagoz

Satire detection is essential for accurately extracting opinions from textual
data and combating misinformation online. However, the lack of diverse corpora
for satire leads to the problem of stylistic bias which impacts the models'
detection performances. This study proposes a debiasing approach for satire
detection, focusing on reducing biases in training data by utilizing generative
large language models. The approach is evaluated in both cross-domain (irony
detection) and cross-lingual (English) settings. Results show that the
debiasing method enhances the robustness and generalizability of the models for
satire and irony detection tasks in Turkish and English. However, its impact on
causal language models, such as Llama-3.1, is limited. Additionally, this work
curates and presents the Turkish Satirical News Dataset with detailed human
annotations, with case studies on classification, debiasing, and
explainability.

摘要：諷刺偵測對於從文本資料中準確提取意見和打擊網路上錯誤訊息至關重要。然而，缺乏多樣化的諷刺語料庫導致風格偏誤的問題，這會影響模型的偵測效能。本研究提出了一種諷刺偵測的去偏誤方法，重點在於透過利用生成式大型語言模型減少訓練資料中的偏誤。這種方法在跨領域（諷刺偵測）和跨語言（英語）設定中都得到評估。結果顯示，去偏誤方法增強了模型在土耳其語和英語的諷刺和反諷偵測任務中的穩健性和概括性。然而，它對因果語言模型（例如 Llama-3.1）的影響有限。此外，這項工作策劃並呈現了土耳其諷刺新聞資料集，其中包含詳細的人工註解，以及分類、去偏誤和可解釋性的案例研究。

##### **VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation with Unsupervised Domain Adaptation**
2412.09240v1 by Roberto Alcover-Couso, Marcos Escudero-Viñolo, Juan C. SanMiguel, Jesus Bescos

Segmentation models are typically constrained by the categories defined
during training. To address this, researchers have explored two independent
approaches: adapting Vision-Language Models (VLMs) and leveraging synthetic
data. However, VLMs often struggle with granularity, failing to disentangle
fine-grained concepts, while synthetic data-based methods remain limited by the
scope of available datasets.
  This paper proposes enhancing segmentation accuracy across diverse domains by
integrating Vision-Language reasoning with key strategies for Unsupervised
Domain Adaptation (UDA). First, we improve the fine-grained segmentation
capabilities of VLMs through multi-scale contextual data, robust text
embeddings with prompt augmentation, and layer-wise fine-tuning in our proposed
Foundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS)
framework. Next, we incorporate these enhancements into a UDA framework by
employing distillation to stabilize training and cross-domain mixed sampling to
boost adaptability without compromising generalization. The resulting
UDA-FROVSS framework is the first UDA approach to effectively adapt across
domains without requiring shared categories.

摘要：分段模型通常會受到訓練期間所定義類別的限制。為了解決這個問題，研究人員探索了兩種獨立的方法：調整視覺語言模型 (VLM) 和利用合成資料。然而，VLM 經常會在細微差別上遇到困難，無法解開細緻的概念，而基於合成資料的方法仍然受到可用資料集範圍的限制。
本文提出透過將視覺語言推理與無監督域適應 (UDA) 的關鍵策略整合，來提升不同領域的分割精度。首先，我們透過多尺度脈絡資料、穩健文字嵌入與提示擴充，以及在我們提出的基礎保留開放詞彙語義分割 (FROVSS) 框架中進行逐層微調，來提升 VLM 的細緻分割能力。接著，我們透過採用蒸餾法來穩定訓練，以及跨域混合取樣來提升適應性，同時不損害泛化能力，將這些增強功能整合到 UDA 框架中。產生的 UDA-FROVSS 框架是第一個 UDA 方法，可以在不需共享類別的情況下，有效地跨域適應。

##### **LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation**
2412.09237v1 by Yijun Liu, Wu Liu, Xiaoyan Gu, Yong Rui, Xiaodong He, Yongdong Zhang

The believable simulation of multi-user behavior is crucial for understanding
complex social systems. Recently, large language models (LLMs)-based AI agents
have made significant progress, enabling them to achieve human-like
intelligence across various tasks. However, real human societies are often
dynamic and complex, involving numerous individuals engaging in multimodal
interactions. In this paper, taking e-commerce scenarios as an example, we
present LMAgent, a very large-scale and multimodal agents society based on
multimodal LLMs. In LMAgent, besides freely chatting with friends, the agents
can autonomously browse, purchase, and review products, even perform live
streaming e-commerce. To simulate this complex system, we introduce a
self-consistency prompting mechanism to augment agents' multimodal
capabilities, resulting in significantly improved decision-making performance
over the existing multi-agent system. Moreover, we propose a fast memory
mechanism combined with the small-world model to enhance system efficiency,
which supports more than 10,000 agent simulations in a society. Experiments on
agents' behavior show that these agents achieve comparable performance to
humans in behavioral indicators. Furthermore, compared with the existing
LLMs-based multi-agent system, more different and valuable phenomena are
exhibited, such as herd behavior, which demonstrates the potential of LMAgent
in credible large-scale social behavior simulations.

摘要：多用戶行為的可信模擬對於理解複雜的社會系統至關重要。最近，大型語言模型 (LLM) 為基礎的人工智慧代理已取得顯著進展，讓它們能夠在各種任務中實現類人智慧。然而，真實的人類社會通常是動態且複雜的，涉及許多從事多模態互動的個人。在本文中，我們以電子商務場景為例，展示了 LMAgent，一個基於多模態 LLM 的非常大規模且多模態的代理社會。在 LMAgent 中，除了可以自由地與朋友聊天之外，代理還可以自主瀏覽、購買和評論產品，甚至進行直播電子商務。為了模擬這個複雜的系統，我們引入了一種自洽提示機制來擴充代理的多模態能力，從而顯著改善了現有多代理系統的決策制定性能。此外，我們提出了一種結合小世界模型的快速記憶機制來提高系統效率，這支持在一個社會中模擬 10,000 多個代理。代理行為的實驗表明，這些代理在行為指標上達到了與人類相當的表現。此外，與現有的基於 LLM 的多代理系統相比，展示了更多不同且有價值的現象，例如從眾行為，這證明了 LMAgent 在可信的大規模社會行為模擬中的潛力。

##### **Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering**
2412.09230v1 by Sai Bhargav Rongali, Mohamad Hassan N C, Ankit Jha, Neha Bhargava, Saurabh Prasad, Biplab Banerjee

This paper tackles the intricate challenge of video question-answering
(VideoQA). Despite notable progress, current methods fall short of effectively
integrating questions with video frames and semantic object-level abstractions
to create question-aware video representations. We introduce Local-Global
Question Aware Video Embedding (LGQAVE), which incorporates three major
innovations to integrate multi-modal knowledge better and emphasize semantic
visual concepts relevant to specific questions. LGQAVE moves beyond traditional
ad-hoc frame sampling by utilizing a cross-attention mechanism that precisely
identifies the most relevant frames concerning the questions. It captures the
dynamics of objects within these frames using distinct graphs, grounding them
in question semantics with the miniGPT model. These graphs are processed by a
question-aware dynamic graph transformer (Q-DGT), which refines the outputs to
develop nuanced global and local video representations. An additional
cross-attention module integrates these local and global embeddings to generate
the final video embeddings, which a language model uses to generate answers.
Extensive evaluations across multiple benchmarks demonstrate that LGQAVE
significantly outperforms existing models in delivering accurate multi-choice
and open-ended answers.

摘要：本文探討了影片問答 (VideoQA) 的複雜挑戰。儘管取得顯著進展，但目前的技術仍無法有效結合問題、影片畫面和語義物件層級抽象，以建立問題感知的影片表徵。我們引進了局部-全域問題感知影片嵌入 (LGQAVE)，它包含三項重大創新，以更好地整合多模式知識，並強調與特定問題相關的語義視覺概念。LGQAVE 超越了傳統的臨時畫面取樣，利用跨注意力機制精確找出與問題最相關的畫面。它使用不同的圖形捕捉這些畫面中物件的動態，並透過 miniGPT 模型將它們奠基於問題語義中。這些圖形由問題感知動態圖形轉換器 (Q-DGT) 處理，它會改善輸出，以開發細緻的全局和局部影片表徵。額外的跨注意力模組整合這些局部和全局嵌入，以產生最終的影片嵌入，語言模型使用這些嵌入來產生答案。跨多個基準的廣泛評估證明，LGQAVE 在提供準確的多選和開放式答案方面，明顯優於現有模型。

##### **CSSDH: An Ontology for Social Determinants of Health to Operational Continuity of Care Data Interoperability**
2412.09223v1 by Subhashis Das, Debashis Naskar, Sara Rodriguez Gonzalez

The rise of digital platforms has led to an increasing reliance on
technology-driven, home-based healthcare solutions, enabling individuals to
monitor their health and share information with healthcare professionals as
needed. However, creating an efficient care plan management system requires
more than just analyzing hospital summaries and Electronic Health Records
(EHRs). Factors such as individual user needs and social determinants of
health, including living conditions and the flow of healthcare information
between different settings, must also be considered. Challenges in this complex
healthcare network involve schema diversity (in EHRs, personal health records,
etc.) and terminology diversity (e.g., ICD, SNOMED-CT) across ancillary
healthcare operations. Establishing interoperability among various systems and
applications is crucial, with the European Interoperability Framework (EIF)
emphasizing the need for patient-centric access and control of healthcare data.
In this paper, we propose an integrated ontological model, the Common Semantic
Data Model for Social Determinants of Health (CSSDH), by combining ISO/DIS
13940:2024 ContSys with WHO Social Determinants of Health. CSSDH aims to
achieve interoperability within the Continuity of Care Network.

摘要：數位平台的興起導致愈來愈依賴科技驅動、居家醫療保健解決方案，讓個人得以監測自己的健康，並視需要與醫療保健專業人員分享資訊。然而，建立一個有效的照護計畫管理系統，需要的可不僅僅是分析醫院摘要和電子健康紀錄 (EHR) 而已。還必須考量個人使用者需求和健康的社會決定因素，包括生活條件和不同環境之間的醫療保健資訊流動。這個複雜的醫療保健網路中的挑戰，包括架構多樣性 (在 EHR、個人健康紀錄等) 和術語多樣性 (例如 ICD、SNOMED-CT) 等輔助醫療保健作業。在各種系統和應用程式之間建立互通性至關重要，歐洲互通性架構 (EIF) 強調需要以病人為中心存取和控制醫療保健資料。在本文中，我們提出一個整合的本體論模型，即結合 ISO/DIS 13940:2024 ContSys 與 WHO 健康社會決定因素的社會決定因素健康共同語義資料模型 (CSSDH)。CSSDH 旨在在照護連續性網路中達成互通性。

##### **CleanComedy: Creating Friendly Humor through Generative Techniques**
2412.09203v1 by Dmitry Vikhorev, Daria Galimzianova, Svetlana Gorovaia, Elizaveta Zhemchuzhina, Ivan P. Yamshchikov

Humor generation is a challenging task in natural language processing due to
limited resources and the quality of existing datasets. Available humor
language resources often suffer from toxicity and duplication, limiting their
effectiveness for training robust models. This paper proposes CleanComedy, a
specialized, partially annotated toxicity-filtered corpus of English and
Russian jokes collected from various sources. We study the effectiveness of our
data filtering approach through a survey on humor and toxicity levels in
various joke groups. In addition, we study advances in computer humor
generation by comparing jokes written by humans with various groups of
generative jokes, including our baseline models trained on the CleanComedy
datasets.

摘要：幽默生成是自然語言處理中的一項具有挑戰性的任務，原因在於資源有限且現有資料集的品質不佳。現有的幽默語言資源經常受到毒性和重複性的影響，限制了它們在訓練穩健模型方面的效用。本文提出 CleanComedy，一個由各種來源收集的英語和俄語笑話所組成的專業且部分註解的非毒性過濾語料庫。我們透過針對各種笑話群組進行幽默和毒性程度的調查，來研究我們的資料過濾方法的效用。此外，我們透過比較人類撰寫的笑話與各種生成笑話的群組（包括我們在 CleanComedy 資料集上訓練的基線模型）來研究電腦幽默生成的進展。

##### **ReFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks**
2412.09173v1 by Jiashu Yao, Heyan Huang, Zeming Liu, Haoyu Wen, Wei Su, Boao Qian, Yuhang Guo

Following formatting instructions to generate well-structured content is a
fundamental yet often unmet capability for large language models (LLMs). To
study this capability, which we refer to as format faithfulness, we present
FormatBench, a comprehensive format-related benchmark. Compared to previous
format-related benchmarks, FormatBench involves a greater variety of tasks in
terms of application scenes (traditional NLP tasks, creative works, autonomous
agency tasks), human-LLM interaction styles (single-turn instruction,
multi-turn chat), and format types (inclusion, wrapping, length, coding).
Moreover, each task in FormatBench is attached with a format checker program.
Extensive experiments on the benchmark reveal that state-of-the-art open- and
closed-source LLMs still suffer from severe deficiency in format faithfulness.
By virtue of the decidable nature of formats, we propose to Reinforce Format
Faithfulness (ReFF) to help LLMs generate formatted output as instructed
without compromising general quality. Without any annotated data, ReFF can
substantially improve the format faithfulness rate (e.g., from 21.6% in
original LLaMA3 to 95.0% on caption segmentation task), while keep the general
quality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with
labeled training data, ReFF can simultaneously improve both format faithfulness
(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from
47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to
explain how ReFF improves both format faithfulness and general quality.

摘要：<paragraph>遵循格式化指令來產生結構良好的內容，是大型語言模型 (LLM) 的基本功能，但通常無法滿足。為了研究這種我們稱為格式忠實度的功能，我們提出了 FormatBench，一個全面的格式相關基準測試。與之前的格式相關基準測試相比，FormatBench 涉及更多樣化的任務，包括應用場景（傳統 NLP 任務、創意作品、自主代理任務）、人機互動風格（單回合指令、多回合聊天）和格式類型（包含、換行、長度、編碼）。此外，FormatBench 中的每個任務都附帶一個格式檢查器程式。在基準測試上進行的廣泛實驗表明，最先進的開源和閉源 LLM 仍然存在嚴重的格式忠實度缺陷。由於格式的可決定性，我們提出加強格式忠實度 (ReFF) 來幫助 LLM 按照指示產生格式化的輸出，而不會損害一般品質。在沒有任何註解資料的情況下，ReFF 可以大幅提高格式忠實度（例如，從 LLaMA3 中的 21.6% 提高到標題分段任務中的 95.0%），同時保持一般品質相當（例如，F1 分數從 47.3 提高到 46.4）。結合標籤訓練資料，ReFF 可以同時提高格式忠實度（例如，從 LLaMA3 中的 21.6% 提高到 75.5%）和一般品質（例如，F1 分數從 47.3 提高到 61.6）。我們進一步提供了可解釋性分析，說明 ReFF 如何同時提高格式忠實度和一般品質。</paragraph>

##### **When Text Embedding Meets Large Language Model: A Comprehensive Survey**
2412.09165v1 by Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang, Dingkun Long, Richong Zhang

Text embedding has become a foundational technology in natural language
processing (NLP) during the deep learning era, driving advancements across a
wide array of downstream tasks. While many natural language understanding
challenges can now be modeled using generative paradigms and leverage the
robust generative and comprehension capabilities of large language models
(LLMs), numerous practical applications, such as semantic matching, clustering,
and information retrieval, continue to rely on text embeddings for their
efficiency and effectiveness. In this survey, we categorize the interplay
between LLMs and text embeddings into three overarching themes: (1)
LLM-augmented text embedding, enhancing traditional embedding methods with
LLMs; (2) LLMs as text embedders, utilizing their innate capabilities for
embedding generation; and (3) Text embedding understanding with LLMs,
leveraging LLMs to analyze and interpret embeddings. By organizing these
efforts based on interaction patterns rather than specific downstream
applications, we offer a novel and systematic overview of contributions from
various research and application domains in the era of LLMs. Furthermore, we
highlight the unresolved challenges that persisted in the pre-LLM era with
pre-trained language models (PLMs) and explore the emerging obstacles brought
forth by LLMs. Building on this analysis, we outline prospective directions for
the evolution of text embedding, addressing both theoretical and practical
opportunities in the rapidly advancing landscape of NLP.

摘要：文本嵌入已成為深度學習時代自然語言處理 (NLP) 中的基礎技術，推動了廣泛下游任務的進展。雖然現在可以使用生成範例對許多自然語言理解挑戰進行建模，並利用大型語言模型 (LLM) 強大的生成和理解能力，但諸如語義匹配、分群和資訊檢索等許多實用應用仍依賴文本嵌入以發揮其效率和效能。在本次調查中，我們將 LLM 和文本嵌入之間的交互作用分為三個主要的類別：(1) LLM 增強文本嵌入，使用 LLM 增強傳統嵌入方法；(2) LLM 作為文本嵌入器，利用其內在能力進行嵌入生成；以及 (3) 使用 LLM 理解文本嵌入，利用 LLM 分析和詮釋嵌入。透過根據互動模式而非特定下游應用來組織這些工作，我們提供了 LLM 時代來自各個研究和應用領域的貢獻的創新且系統性的概觀。此外，我們強調了在使用預訓練語言模型 (PLM) 的 LLM 前時代持續存在且尚未解決的挑戰，並探討了 LLM 帶來的全新障礙。根據此分析，我們概述了文本嵌入演變的未來方向，探討了 NLP 快速發展領域中的理論和實務機會。

##### **Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning**
2412.09126v1 by Meng Shen, Yake Wei, Jianxiong Yin, Deepu Rajan, Di Hu, Simon See

Training multimodal models requires a large amount of labeled data. Active
learning (AL) aim to reduce labeling costs. Most AL methods employ warm-start
approaches, which rely on sufficient labeled data to train a well-calibrated
model that can assess the uncertainty and diversity of unlabeled data. However,
when assembling a dataset, labeled data are often scarce initially, leading to
a cold-start problem. Additionally, most AL methods seldom address multimodal
data, highlighting a research gap in this field. Our research addresses these
issues by developing a two-stage method for Multi-Modal Cold-Start Active
Learning (MMCSAL).
  Firstly, we observe the modality gap, a significant distance between the
centroids of representations from different modalities, when only using
cross-modal pairing information as self-supervision signals. This modality gap
affects data selection process, as we calculate both uni-modal and cross-modal
distances. To address this, we introduce uni-modal prototypes to bridge the
modality gap. Secondly, conventional AL methods often falter in multimodal
scenarios where alignment between modalities is overlooked. Therefore, we
propose enhancing cross-modal alignment through regularization, thereby
improving the quality of selected multimodal data pairs in AL. Finally, our
experiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs
across three multimodal datasets.

摘要：訓練多模態模型需要大量的標籤資料。主動學習 (AL) 旨在降低標籤成本。大多數 AL 方法採用熱啟動方法，依賴於足夠的標籤資料來訓練一個校準良好的模型，該模型可以評估未標籤資料的不確定性和多樣性。然而，在組裝資料集時，標籤資料通常一開始很稀少，導致冷啟動問題。此外，大多數 AL 方法很少處理多模態資料，凸顯了該領域的研究空白。我們的研究透過開發多模態冷啟動主動學習 (MMCSAL) 的兩階段方法來解決這些問題。
首先，當僅使用跨模態配對資訊作為自我監督訊號時，我們觀察到模態差距，這是來自不同模態的表示之重心的顯著距離。此模態差距會影響資料選取流程，因為我們計算單模態和跨模態距離。為了解決此問題，我們引入單模態原型來彌合模態差距。其次，傳統的 AL 方法通常在忽略模態之間對齊的多模態場景中失敗。因此，我們建議透過正則化增強跨模態對齊，從而提高 AL 中所選多模態資料對的品質。最後，我們的實驗證明了 MMCSAL 在三個多模態資料集中選取多模態資料對的效能。

##### **Goal-Driven Query Answering over First- and Second-Order Dependencies with Equality**
2412.09125v1 by Efthymia Tsamoura, Boris Motik

Query answering over data with dependencies plays a central role in most
applications of dependencies. The problem is commonly solved by using a
suitable variant of the chase algorithm to compute a universal model of the
dependencies and the data and thus explicate all knowledge implicit in the
dependencies. After this preprocessing step, an arbitrary conjunctive query
over the dependencies and the data can be answered by evaluating it the
computed universal model. If, however, the query to be answered is fixed and
known in advance, computing the universal model is often inefficient as many
inferences made during this process can be irrelevant to a given query. In such
cases, a goal-driven approach, which avoids drawing unnecessary inferences,
promises to be more efficient and thus preferable in practice.
  In this paper we present what we believe to be the first technique for
goal-driven query answering over first- and second-order dependencies with
equality reasoning. Our technique transforms the input dependencies so that
applying the chase to the output avoids many inferences that are irrelevant to
the query. The transformation proceeds in several steps, which comprise the
following three novel techniques. First, we present a variant of the
singularisation technique by Marnette [60] that is applicable to second-order
dependencies and that corrects an incompleteness of a related formulation by
ten Cate et al. [74]. Second, we present a relevance analysis technique that
can eliminate from the input dependencies that provably do not contribute to
query answers. Third, we present a variant of the magic sets algorithm [19]
that can handle second-order dependencies with equality reasoning. We also
present the results of an extensive empirical evaluation, which show that
goal-driven query answering can be orders of magnitude faster than computing
the full universal model.

摘要：<paragraph>在大多数依赖项应用中，对具有依赖关系的数据进行查询回答都扮演着核心角色。该问题通常通过使用追逐算法的合适变体来计算依赖关系和数据的通用模型，从而阐明依赖关系中所有隐含的知识来解决。在此预处理步骤之后，可以通过在计算的通用模型中对其进行评估来回答对依赖关系和数据的任意合取查询。但是，如果要回答的查询是固定的并且已知，则计算通用模型通常效率低下，因为在此过程中进行的许多推理可能与给定的查询无关。在这种情况下，一种避免得出不必要的推理的目标驱动方法有望在实践中更有效，因此更可取。
在本文中，我们展示了我们认为是对具有相等推理的一阶和二阶依赖关系进行目标驱动查询回答的第一种技术。我们的技术转换输入依赖关系，以便将追逐应用于输出可避免许多与查询无关的推理。转换分几个步骤进行，其中包括以下三种新技术。首先，我们展示了 Marnette [60] 的奇异化技术的变体，该变体适用于二阶依赖关系，并纠正了 ten Cate 等人 [74] 相关表述的不完整性。其次，我们提出了一种相关性分析技术，该技术可以从输入依赖关系中消除那些明显不会有助于查询答案的依赖关系。第三，我们提出了魔术集算法 [19] 的变体，该变体可以处理具有相等推理的二阶依赖关系。我们还展示了广泛的实证评估结果，该结果表明目标驱动的查询回答比计算完整的通用模型快几个数量级。</paragraph>

##### **In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning**
2412.09104v1 by Songjun Tu, Jingbo Sun, Qichao Zhang, Yaocheng Zhang, Jia Liu, Ke Chen, Dongbin Zhao

Offline preference-based reinforcement learning (PbRL) typically operates in
two phases: first, use human preferences to learn a reward model and annotate
rewards for a reward-free offline dataset; second, learn a policy by optimizing
the learned reward via offline RL. However, accurately modeling step-wise
rewards from trajectory-level preference feedback presents inherent challenges.
The reward bias introduced, particularly the overestimation of predicted
rewards, leads to optimistic trajectory stitching, which undermines the
pessimism mechanism critical to the offline RL phase. To address this
challenge, we propose In-Dataset Trajectory Return Regularization (DTR) for
offline PbRL, which leverages conditional sequence modeling to mitigate the
risk of learning inaccurate trajectory stitching under reward bias.
Specifically, DTR employs Decision Transformer and TD-Learning to strike a
balance between maintaining fidelity to the behavior policy with high
in-dataset trajectory returns and selecting optimal actions based on high
reward labels. Additionally, we introduce an ensemble normalization technique
that effectively integrates multiple reward models, balancing the tradeoff
between reward differentiation and accuracy. Empirical evaluations on various
benchmarks demonstrate the superiority of DTR over other state-of-the-art
baselines

摘要：離線基於偏好的強化學習 (PbRL) 通常分為兩個階段：首先，使用人類偏好學習獎勵模型，並為無獎勵離線資料集註解獎勵；其次，通過離線 RL 最佳化學習的獎勵來學習策略。然而，從軌跡級別偏好回饋中準確建模逐步獎勵會帶來固有的挑戰。引入的獎勵偏差，特別是對預測獎勵的高估，會導致樂觀的軌跡拼接，這會破壞對離線 RL 階段至關重要的悲觀機制。為了應對這一挑戰，我們提出了離線 PbRL 的資料集內軌跡回報正則化 (DTR)，它利用條件序列建模來降低在獎勵偏差下學習不準確軌跡拼接的風險。具體來說，DTR 使用決策轉換器和 TD 學習來在保持對行為策略的忠誠度與高資料集軌跡回報之間取得平衡，並根據高獎勵標籤選擇最佳行動。此外，我們引入了一種集合正規化技術，它有效地整合了多個獎勵模型，平衡了獎勵區分度和準確性之間的權衡。在各種基準上的經驗評估證明了 DTR 優於其他最先進的基線

##### **PolyIPA -- Multilingual Phoneme-to-Grapheme Conversion Model**
2412.09102v1 by Davor Lauc

This paper presents PolyIPA, a novel multilingual phoneme-to-grapheme
conversion model designed for multilingual name transliteration, onomastic
research, and information retrieval. The model leverages two helper models
developed for data augmentation: IPA2vec for finding soundalikes across
languages, and similarIPA for handling phonetic notation variations. Evaluated
on a test set that spans multiple languages and writing systems, the model
achieves a mean Character Error Rate of 0.055 and a character-level BLEU score
of 0.914, with particularly strong performance on languages with shallow
orthographies. The implementation of beam search further improves practical
utility, with top-3 candidates reducing the effective error rate by 52.7\% (to
CER: 0.026), demonstrating the model's effectiveness for cross-linguistic
applications.

摘要：本文提出了 PolyIPA，一種新穎的多語言音素轉換為書寫素模型，專為多語言名稱音譯、命名學研究和資訊檢索而設計。此模型利用為資料擴充而開發的兩個輔助模型：IPA2vec 用於尋找跨語言的音似字，而 similarIPA 則用於處理音標符號的變化。在跨越多種語言和書寫系統的測試集中評估，該模型的平均字元錯誤率為 0.055，字元層級 BLEU 分數為 0.914，在正寫法較淺的語言中表現特別出色。波束搜尋的實作進一步提升了實用性，前三名候選人將有效錯誤率降低了 52.7%（至 CER：0.026），證明了該模型在跨語言應用中的有效性。

##### **Temporal Numeric Planning with Patterns**
2412.09101v1 by Matteo Cardellini, Enrico Giunchiglia

We consider temporal numeric planning problems $\Pi$ expressed in PDDL2.1
level 3, and show how to produce SMT formulas $(i)$ whose models correspond to
valid plans of $\Pi$, and $(ii)$ that extend the recently proposed planning
with patterns approach from the numeric to the temporal case. We prove the
correctness and completeness of the approach and show that it performs very
well on 10 domains with required concurrency.

摘要：我們考慮以 PDDL2.1 表達的時間數值規劃問題 $\Pi$ 等級 3，並展示如何產生 SMT 公式 $(i)$，其模型對應於 $\Pi$ 的有效計畫，以及 $(ii)$ 將最近提出的規劃模式方法從數值擴展到時間案例。我們證明了此方法的正確性和完整性，並展示它在 10 個需要並發性的網域上表現得非常好。

##### **Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion**
2412.09094v1 by Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng

Large Language Models (LLMs) present massive inherent knowledge and superior
semantic comprehension capability, which have revolutionized various tasks in
natural language processing. Despite their success, a critical gap remains in
enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence
suggests that LLMs consistently perform worse than conventional KGC approaches,
even through sophisticated prompt design or tailored instruction-tuning.
Fundamentally, applying LLMs on KGC introduces several critical challenges,
including a vast set of entity candidates, hallucination issue of LLMs, and
under-exploitation of the graph structure. To address these challenges, we
propose a novel instruction-tuning-based method, namely FtG. Specifically, we
present a \textit{filter-then-generate} paradigm and formulate the KGC task
into a multiple-choice question format. In this way, we can harness the
capability of LLMs while mitigating the issue casused by hallucinations.
Moreover, we devise a flexible ego-graph serialization prompt and employ a
structure-text adapter to couple structure and text information in a
contextualized manner. Experimental results demonstrate that FtG achieves
substantial performance gain compared to existing state-of-the-art methods. The
instruction dataset and code are available at
\url{https://github.com/LB0828/FtG}.

摘要：大型語言模型 (LLM) 具有龐大的內部知識和卓越的語義理解能力，這徹底改變了自然語言處理中的各種任務。儘管它們成功，但在使 LLM 能執行知識圖譜完成 (KGC) 方面仍存在一個關鍵差距。經驗證據表明，即使透過精密的提示設計或量身打造的指令調整，LLM 的表現也始終不如傳統的 KGC 方法。從根本上來說，在 KGC 上應用 LLM 會帶來幾個關鍵挑戰，包括大量的實體候選、LLM 的幻覺問題以及圖形結構的利用不足。為了應對這些挑戰，我們提出了一種新的基於指令調整的方法，即 FtG。具體來說，我們提出了「先過濾再生成」的範例，並將 KGC 任務制定為多選題格式。這樣，我們就能利用 LLM 的能力，同時減輕幻覺所造成的問題。此外，我們設計了一個靈活的自圖序列化提示，並採用結構文本適配器，以情境化的方式結合結構和文本資訊。實驗結果表明，與現有的最先進方法相比，FtG 獲得了顯著的效能提升。指令資料集和程式碼可在
\url{https://github.com/LB0828/FtG} 取得。

##### **Evaluating Pixel Language Models on Non-Standardized Languages**
2412.09084v1 by Alberto Muñoz-Ortiz, Verena Blaschke, Barbara Plank

We explore the potential of pixel-based models for transfer learning from
standard languages to dialects. These models convert text into images that are
divided into patches, enabling a continuous vocabulary representation that
proves especially useful for out-of-vocabulary words common in dialectal data.
Using German as a case study, we compare the performance of pixel-based models
to token-based models across various syntactic and semantic tasks. Our results
show that pixel-based models outperform token-based models in part-of-speech
tagging, dependency parsing and intent detection for zero-shot dialect
evaluation by up to 26 percentage points in some scenarios, though not in
Standard German. However, pixel-based models fall short in topic
classification. These findings emphasize the potential of pixel-based models
for handling dialectal data, though further research should be conducted to
assess their effectiveness in various linguistic contexts.

摘要：我們探討了基於像素的模型從標準語言轉移學習到方言的潛力。這些模型將文字轉換為圖像，並將其分割成區塊，實現連續的詞彙表徵，這對於方言資料中常見的生僻字特別有用。我們以德語為案例研究，比較了基於像素的模型與基於符號的模型在各種句法和語義任務中的表現。我們的結果表明，在某些情況下，基於像素的模型在詞性標註、依存句法分析和意圖偵測中優於基於符號的模型，在零次方言評估中最多可達 26 個百分點，但標準德語除外。然而，基於像素的模型在主題分類方面表現不佳。這些發現強調了基於像素的模型在處理方言資料方面的潛力，儘管應進行進一步的研究以評估其在各種語言環境中的有效性。

##### **Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning**
2412.09078v1 by Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang

Large Language Models (LLMs) have shown remarkable abilities across various
language tasks, but solving complex reasoning problems remains a challenge.
While existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)
enhance reasoning by decomposing problems or structuring prompts, they
typically perform a single pass of reasoning and may fail to revisit flawed
paths, compromising accuracy. To address this, we propose a novel reasoning
framework called Forest-of-Thought (FoT), which integrates multiple reasoning
trees to leverage collective decision-making for solving complex logical
problems. FoT utilizes sparse activation strategies to select the most relevant
reasoning paths, improving both efficiency and accuracy. Additionally, we
introduce a dynamic self-correction strategy that enables real-time error
correction and learning from past mistakes, as well as consensus-guided
decision making strategies to optimize correctness and computational resources.
Experimental results demonstrate that the FoT framework, combined with these
strategies, significantly enhances the reasoning capabilities of LLMs, enabling
them to solve complex tasks with greater precision and efficiency.

摘要：大型語言模型 (LLM) 已在各種語言任務中展現出非凡的能力，但解決複雜推理問題仍然是一項挑戰。雖然現有方法（例如思考鏈 (CoT) 和思考樹 (ToT)）透過分解問題或結構化提示來增強推理能力，但它們通常執行一次推理，可能無法重新審視有缺陷的路徑，進而影響準確性。為了解決這個問題，我們提出了一種名為思考森林 (FoT) 的創新推理框架，它整合了多個推理樹，以利用集體決策來解決複雜的邏輯問題。FoT 利用稀疏激活策略來選擇最相關的推理路徑，同時提升效率和準確性。此外，我們引入了一種動態自我修正策略，它能進行即時錯誤修正，並從過去的錯誤中學習，以及共識引導的決策制定策略，以最佳化正確性和運算資源。實驗結果表明，FoT 框架結合這些策略，顯著增強了 LLM 的推理能力，讓它們能夠以更高的準確性和效率解決複雜任務。

##### **EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems**
2412.09058v1 by Huanqi Yang, Mingzhe Li, Mingda Han, Zhenjiang Li, Weitao Xu

Embedded IoT system development is crucial for enabling seamless connectivity
and functionality across a wide range of applications. However, such a complex
process requires cross-domain knowledge of hardware and software and hence
often necessitates direct developer involvement, making it labor-intensive,
time-consuming, and error-prone. To address this challenge, this paper
introduces EmbedGenius, the first fully automated software development platform
for general-purpose embedded IoT systems. The key idea is to leverage the
reasoning ability of Large Language Models (LLMs) and embedded system expertise
to automate the hardware-in-the-loop development process. The main methods
include a component-aware library resolution method for addressing hardware
dependencies, a library knowledge generation method that injects utility domain
knowledge into LLMs, and an auto-programming method that ensures successful
deployment. We evaluate EmbedGenius's performance across 71 modules and four
mainstream embedded development platforms with over 350 IoT tasks. Experimental
results show that EmbedGenius can generate codes with an accuracy of 95.7% and
complete tasks with a success rate of 86.5%, surpassing human-in-the-loop
baselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show
EmbedGenius's potential through case studies in environmental monitoring and
remote control systems development.

摘要：嵌入式物聯網系統開發對於在各種應用中實現無縫連線和功能至關重要。然而，如此複雜的流程需要跨領域的硬體和軟體知識，因此通常需要開發人員直接參與，這使得它既費力又費時，而且容易出錯。為了應對這一挑戰，本文介紹了 EmbedGenius，這是第一個針對通用嵌入式物聯網系統的全自動軟體開發平台。關鍵思想是利用大型語言模型 (LLM) 的推理能力和嵌入式系統專業知識來自動化硬體迴路開發流程。主要方法包括：用於解決硬體依賴性的元件感知函式庫解析方法、將效用領域知識注入 LLM 的函式庫知識生成方法以及確保成功部署的自動程式設計方法。我們評估了 EmbedGenius 在 71 個模組和四個主流嵌入式開發平台上的效能，其中包含超過 350 項物聯網任務。實驗結果顯示，EmbedGenius 可以產生準確度達 95.7% 的程式碼，並以 86.5% 的成功率完成任務，分別比人類迴路基準高出 15.6%--37.7% 和 25.5%--53.4%。我們也透過環境監控和遠端控制系統開發的案例研究展示了 EmbedGenius 的潛力。

##### **Dial-In LLM: Human-Aligned Dialogue Intent Clustering with LLM-in-the-loop**
2412.09049v1 by Mengze Hong, Yuanfeng Song, Di Jiang, Wailing Ng, Yanjie Sun, Chen Jason Zhang

The discovery of customer intention from dialogue plays an important role in
automated support system. However, traditional text clustering methods are
poorly aligned with human perceptions due to the shift from embedding distance
to semantic distance, and existing quantitative metrics for text clustering may
not accurately reflect the true quality of intent clusters. In this paper, we
leverage the superior language understanding capabilities of Large Language
Models (LLMs) for designing better-calibrated intent clustering algorithms. We
first establish the foundation by verifying the robustness of fine-tuned LLM
utility in semantic coherence evaluation and cluster naming, resulting in an
accuracy of 97.50% and 94.40%, respectively, when compared to the human-labeled
ground truth. Then, we propose an iterative clustering algorithm that
facilitates cluster-level refinement and the continuous discovery of
high-quality intent clusters. Furthermore, we present several LLM-in-the-loop
semi-supervised clustering techniques tailored for intent discovery from
customer service dialogue. Experiments on a large-scale industrial dataset
comprising 1,507 intent clusters demonstrate the effectiveness of the proposed
techniques. The methods outperformed existing counterparts, achieving 6.25%
improvement in quantitative metrics and 12% enhancement in application-level
performance when constructing an intent classifier.

摘要：從對話中發現客戶意圖在自動化支援系統中扮演重要角色。然而，傳統的文字分群方法由於從嵌入距離轉換為語義距離，與人類的認知不符，而現有的文字分群量化指標可能無法準確反映意圖分群的真實品質。在本文中，我們利用大型語言模型 (LLM) 優異的語言理解能力，來設計校準更好的意圖分群演算法。我們首先透過驗證微調後的 LLM 實用程式在語義一致性評估和分群命名中的穩健性，建立基礎，與人為標記的地面實況相比，分別得到 97.50% 和 94.40% 的準確率。接著，我們提出一個反覆運算的分群演算法，促進分群層級的精進和持續發現高品質的意圖分群。此外，我們提出數種 LLM in the loop 半監督式分群技術，專門用於從客戶服務對話中發現意圖。在包含 1,507 個意圖分群的大規模產業資料集上進行的實驗，證明了所提出技術的有效性。這些方法優於現有的對應方法，在建構意圖分類器時，量化指標提升了 6.25%，應用層級效能提升了 12%。

##### **Multi-Task Learning with LLMs for Implicit Sentiment Analysis: Data-level and Task-level Automatic Weight Learning**
2412.09046v1 by Wenna Lai, Haoran Xie, Guandong Xu, Qing Li

Implicit sentiment analysis (ISA) presents significant challenges due to the
absence of salient cue words. Previous methods have struggled with insufficient
data and limited reasoning capabilities to infer underlying opinions.
Integrating multi-task learning (MTL) with large language models (LLMs) offers
the potential to enable models of varying sizes to reliably perceive and
recognize genuine opinions in ISA. However, existing MTL approaches are
constrained by two sources of uncertainty: data-level uncertainty, arising from
hallucination problems in LLM-generated contextual information, and task-level
uncertainty, stemming from the varying capacities of models to process
contextual information. To handle these uncertainties, we introduce MT-ISA, a
novel MTL framework that enhances ISA by leveraging the generation and
reasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA
constructs auxiliary tasks using generative LLMs to supplement sentiment
elements and incorporates automatic MTL to fully exploit auxiliary data. We
introduce data-level and task-level automatic weight learning (AWL), which
dynamically identifies relationships and prioritizes more reliable data and
critical tasks, enabling models of varying sizes to adaptively learn
fine-grained weights based on their reasoning capabilities. We investigate
three strategies for data-level AWL, while also introducing homoscedastic
uncertainty for task-level AWL. Extensive experiments reveal that models of
varying sizes achieve an optimal balance between primary prediction and
auxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability
of our approach.

摘要：隱含情緒分析 (ISA) 由於缺少顯著提示詞，因此提出了重大挑戰。先前的研究方法一直苦於資料不足，且推理能力有限，無法推論出潛在的觀點。將多任務學習 (MTL) 與大型語言模型 (LLM) 結合，讓不同規模的模型能夠可靠地感知和識別 ISA 中的真實觀點。然而，現有的 MTL 方法受到兩個不確定性來源的限制：資料層級的不確定性，源自於 LLM 生成的脈絡資訊中的幻覺問題，以及任務層級的不確定性，源自於模型處理脈絡資訊的能力差異。為了處理這些不確定性，我們引入了 MT-ISA，一個新穎的 MTL 框架，透過自動 MTL，利用 LLM 的生成和推理能力來增強 ISA。具體來說，MT-ISA 使用生成式 LLM 建構輔助任務，以補充情緒元素，並結合自動 MTL 來充分利用輔助資料。我們引入了資料層級和任務層級的自動權重學習 (AWL)，它動態識別關係，並優先考慮更可靠的資料和關鍵任務，讓不同規模的模型能夠根據其推理能力自適應地學習細粒度的權重。我們探討了資料層級 AWL 的三種策略，同時也引入了同質變異數的不確定性，用於任務層級 AWL。大量的實驗表明，不同規模的模型在 MT-ISA 中達到了主要預測和輔助任務之間的最佳平衡。這強調了我們方法的有效性和適應性。

##### **Motif Guided Graph Transformer with Combinatorial Skeleton Prototype Learning for Skeleton-Based Person Re-Identification**
2412.09044v1 by Haocong Rao, Chunyan Miao

Person re-identification (re-ID) via 3D skeleton data is a challenging task
with significant value in many scenarios. Existing skeleton-based methods
typically assume virtual motion relations between all joints, and adopt average
joint or sequence representations for learning. However, they rarely explore
key body structure and motion such as gait to focus on more important body
joints or limbs, while lacking the ability to fully mine valuable
spatial-temporal sub-patterns of skeletons to enhance model learning. This
paper presents a generic Motif guided graph transformer with Combinatorial
skeleton prototype learning (MoCos) that exploits structure-specific and
gait-related body relations as well as combinatorial features of skeleton
graphs to learn effective skeleton representations for person re-ID. In
particular, motivated by the locality within joints' structure and the
body-component collaboration in gait, we first propose the motif guided graph
transformer (MGT) that incorporates hierarchical structural motifs and gait
collaborative motifs, which simultaneously focuses on multi-order local joint
correlations and key cooperative body parts to enhance skeleton relation
learning. Then, we devise the combinatorial skeleton prototype learning (CSP)
that leverages random spatial-temporal combinations of joint nodes and skeleton
graphs to generate diverse sub-skeleton and sub-tracklet representations, which
are contrasted with the most representative features (prototypes) of each
identity to learn class-related semantics and discriminative skeleton
representations. Extensive experiments validate the superior performance of
MoCos over existing state-of-the-art models. We further show its generality
under RGB-estimated skeletons, different graph modeling, and unsupervised
scenarios.

摘要：基於 3D 骨架資料的人員再辨識 (re-ID) 是一項具有挑戰性的任務，在許多情境中具有顯著的價值。現有的基於骨架的方法通常假設所有關節之間的虛擬運動關係，並採用平均關節或序列表示進行學習。然而，它們很少探索關鍵的身體結構和運動（例如步態），以專注於更重要的身體關節或肢體，同時缺乏充分挖掘骨架的寶貴時空子模式以增強模型學習的能力。本文提出一個通用動機導引圖形轉換器，結合組合骨架原型學習 (MoCos)，利用骨架圖形的結構特定和步態相關的身體關係以及組合特徵，以學習有效的人員再辨識骨架表示。特別是，受到關節結構中的局部性和步態中身體組成協作的啟發，我們首先提出動機導引圖形轉換器 (MGT)，它結合了分層結構動機和步態協作動機，同時專注於多階局部關節相關性和關鍵協作身體部位，以增強骨架關係學習。然後，我們設計組合骨架原型學習 (CSP)，利用關節節點和骨架圖形的隨機時空組合，產生多樣化的子骨架和子軌跡表示，這些表示與每個身分的代表性特徵（原型）形成對比，以學習類相關語義和區分性的骨架表示。廣泛的實驗驗證了 MoCos 優於現有最先進模型的卓越效能。我們進一步展示了它在 RGB 估計骨架、不同的圖形建模和非監督場景下的普遍性。

##### **ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty**
2412.09036v1 by Meizhi Zhong, Xikai Liu, Chen Zhang, Yikun Lei, Yan Gao, Yao Hu, Kehai Chen, Min Zhang

Large Language models (LLMs) have become a research hotspot. To accelerate
the inference of LLMs, storing computed caches in memory has become the
standard technique. However, as the inference length increases, growing KV
caches might lead to out-of-memory issues. Many existing methods address this
issue through KV cache compression, primarily by preserving key tokens
throughout all layers to reduce information loss. Most of them allocate a
uniform budget size for each layer to retain. However, we observe that the
minimum budget sizes needed to retain essential information vary across layers
and models based on the perspectives of attention and hidden state output.
Building on this observation, this paper proposes a simple yet effective KV
cache compression method that leverages layer uncertainty to allocate budget
size for each layer. Experimental results show that the proposed method can
reduce memory usage of the KV caches to only $\sim$20\% when compared to Full
KV inference while achieving nearly lossless performance.

摘要：大型語言模型 (LLM) 已成為研究熱點。為了加速 LLM 的推論，將計算好的快取儲存在記憶體中已成為標準技術。然而，隨著推論長度的增加，不斷增長的 KV 快取可能會導致記憶體不足的問題。許多現有方法透過 KV 快取壓縮來解決這個問題，主要是透過在所有層中保留關鍵代碼，以減少資訊遺失。它們大多數會為每個層分配一個均勻的預算大小來保留。然而，我們觀察到，基於注意力和隱藏狀態輸出的觀點，保留必要資訊所需的最小預算大小會因層和模型而異。基於此觀察，本文提出一個簡單但有效的 KV 快取壓縮方法，該方法利用層不確定性為每個層分配預算大小。實驗結果顯示，與完整 KV 推論相比，所提出的方法可以將 KV 快取的記憶體使用量減少到僅約 20%，同時實現幾乎無損失的效能。

##### **Dialogue Language Model with Large-Scale Persona Data Engineering**
2412.09034v1 by Mengze Hong, Chen Zhang, Chaotao Chen, Rongzhong Lian, Di Jiang

Maintaining persona consistency is paramount in the application of
open-domain dialogue systems, as exemplified by models like ChatGPT. Despite
significant advancements, the limited scale and diversity of current persona
dialogue datasets remain challenges to achieving robust persona-consistent
dialogue models. In this study, drawing inspiration from the success of
large-scale pre-training, we introduce PPDS, an open-domain persona dialogue
system that employs extensive generative pre-training on a persona dialogue
dataset to enhance persona consistency. Specifically, we present a persona
extraction model designed to autonomously and precisely generate vast persona
dialogue datasets. Additionally, we unveil a pioneering persona augmentation
technique to address the invalid persona bias inherent in the constructed
dataset. Both quantitative and human evaluations consistently highlight the
superior response quality and persona consistency of our proposed model,
underscoring its effectiveness.

摘要：在開放領域對話系統的應用中，維持角色一致性至關重要，如同 ChatGPT 等模型所例證的。儘管有顯著的進展，但當前角色對話資料集的規模和多樣性有限，這仍然是實現穩健的角色一致對話模型的挑戰。在本研究中，從大規模預訓練的成功中汲取靈感，我們引入了 PPDS，這是一個開放領域角色對話系統，它採用廣泛的生成式預訓練，在角色對話資料集上增強角色一致性。具體來說，我們提出了一個角色萃取模型，旨在自主且精確地生成大量的角色對話資料集。此外，我們揭示了一種創新的角色擴充技術，以解決建構資料集中固有的無效角色偏差。定量和人工評估持續強調我們提出的模型的優異回應品質和角色一致性，強調其有效性。

##### **Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis**
2412.09032v1 by Zhoulin Ji, Chenhao Lin, Hang Wang, Chao Shen

Detecting synthetic from real speech is increasingly crucial due to the risks
of misinformation and identity impersonation. While various datasets for
synthetic speech analysis have been developed, they often focus on specific
areas, limiting their utility for comprehensive research. To fill this gap, we
propose the Speech-Forensics dataset by extensively covering authentic,
synthetic, and partially forged speech samples that include multiple segments
synthesized by different high-quality algorithms. Moreover, we propose a
TEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously
performing authenticity detection, multiple fake segments localization, and
synthesis algorithms recognition, without any complex post-processing. TEST
effectively integrates LSTM and Transformer to extract more powerful temporal
speech representations and utilizes dense prediction on multi-scale pyramid
features to estimate the synthetic spans. Our model achieves an average mAP of
83.55% and an EER of 5.25% at the utterance level. At the segment level, it
attains an EER of 1.07% and a 92.19% F1 score. These results highlight the
model's robust capability for a comprehensive analysis of synthetic speech,
offering a promising avenue for future research and practical applications in
this field.

摘要：由於錯誤資訊和身分冒用的風險，偵測合成語音和真實語音變得越來越重要。雖然已經開發出各種合成語音分析的資料集，但它們通常專注於特定領域，限制了它們在綜合研究中的效用。為了填補這個缺口，我們提出 Speech-Forensics 資料集，廣泛涵蓋真實、合成和部分偽造的語音樣本，其中包括由不同高品質演算法合成的多個區段。此外，我們提出了一個稱為 TEST 的時間語音定位網路，旨在同時執行真實性偵測、多個偽造區段定位和合成演算法辨識，而無需任何複雜的後處理。TEST 有效地整合 LSTM 和 Transformer 以提取更強大的時間語音表示，並利用多尺度金字塔特徵上的密集預測來估計合成跨度。我們的模型在語句層級達到了 83.55% 的平均 mAP 和 5.25% 的 EER。在區段層級，它達到了 1.07% 的 EER 和 92.19% 的 F1 分數。這些結果突顯了該模型在合成語音綜合分析方面的強大功能，為此領域未來的研究和實際應用提供了有前景的途徑。

##### **RingFormer: A Ring-Enhanced Graph Transformer for Organic Solar Cell Property Prediction**
2412.09030v1 by Zhihao Ding, Ting Zhang, Yiran Li, Jieming Shi, Chen Jason Zhang

Organic Solar Cells (OSCs) are a promising technology for sustainable energy
production. However, the identification of molecules with desired OSC
properties typically involves laborious experimental research. To accelerate
progress in the field, it is crucial to develop machine learning models capable
of accurately predicting the properties of OSC molecules. While graph
representation learning has demonstrated success in molecular property
prediction, it remains underexplored for OSC-specific tasks. Existing methods
fail to capture the unique structural features of OSC molecules, particularly
the intricate ring systems that critically influence OSC properties, leading to
suboptimal performance. To fill the gap, we present RingFormer, a novel graph
transformer framework specially designed to capture both atom and ring level
structural patterns in OSC molecules. RingFormer constructs a hierarchical
graph that integrates atomic and ring structures and employs a combination of
local message passing and global attention mechanisms to generate expressive
graph representations for accurate OSC property prediction. We evaluate
RingFormer's effectiveness on five curated OSC molecule datasets through
extensive experiments. The results demonstrate that RingFormer consistently
outperforms existing methods, achieving a 22.77% relative improvement over the
nearest competitor on the CEPDB dataset.

摘要：有機太陽能電池 (OSC) 是一種有望實現永續能源生產的技術。然而，找出具有所需 OSC 特性的分子通常需要進行繁瑣的實驗研究。為了加速該領域的進展，開發出能夠準確預測 OSC 分子特性的機器學習模型至關重要。儘管圖表表示學習已在分子特性預測中展現成功，但對於 OSC 特定任務的探索仍不足。現有方法無法捕捉 OSC 分子的獨特結構特徵，特別是對 OSC 特性產生重大影響的複雜環狀系統，導致效能不佳。為了填補這一空白，我們提出了 RingFormer，這是一個新穎的圖形轉換器框架，專門設計用於捕捉 OSC 分子中的原子和環級結構模式。RingFormer 建構了一個整合原子和環狀結構的階層式圖形，並採用局部訊息傳遞和全局注意力機制的組合，為準確的 OSC 特性預測產生富有表現力的圖形表示。我們透過廣泛的實驗，評估了 RingFormer 在五個精選的 OSC 分子資料集上的有效性。結果表明，RingFormer 持續優於現有方法，在 CEPDB 資料集上較最接近的競爭者提升了 22.77% 的相對改進。

##### **Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages**
2412.09025v1 by Advait Joglekar, Srinivasan Umesh

Neural Machine Translation (NMT) models are typically trained on datasets
with limited exposure to Scientific, Technical and Educational domains.
Translation models thus, in general, struggle with tasks that involve
scientific understanding or technical jargon. Their performance is found to be
even worse for low-resource Indian languages. Finding a translation dataset
that tends to these domains in particular, poses a difficult challenge. In this
paper, we address this by creating a multilingual parallel corpus containing
more than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality
translation pairs across 8 Indian languages. We achieve this by bitext mining
human-translated transcriptions of NPTEL video lectures. We also finetune and
evaluate NMT models using this corpus and surpass all other publicly available
models at in-domain tasks. We also demonstrate the potential for generalizing
to out-of-domain translation tasks by improving the baseline by over 2 BLEU on
average for these Indian languages on the Flores+ benchmark. We are pleased to
release our model and dataset via this link: https://huggingface.co/SPRINGLab.

摘要：神經機器翻譯 (NMT) 模型通常在對科學、技術和教育領域接觸有限的資料集上進行訓練。因此，翻譯模型通常難以應付涉及科學理解或技術術語的任務。對於資源較少的印度語言，其表現更為糟糕。找到特別針對這些領域的翻譯資料集是一個艱難的挑戰。在本文中，我們通過建立一個多語言平行語料庫來解決這個問題，其中包含超過 280 萬列高品質的英印和印印翻譯對，涵蓋 8 種印度語言。我們通過對 NPTEL 視頻課程的人工翻譯轉錄進行雙語文本挖掘來實現這一點。我們還使用這個語料庫對 NMT 模型進行微調和評估，並在領域內任務中超越所有其他公開可用的模型。我們還展示了通過在 Flores+ 基準上平均提高這些印度語言的 BLEU 超過 2 個點來概括到領域外翻譯任務的潛力。我們很榮幸通過以下連結發布我們的模型和資料集：https://huggingface.co/SPRINGLab。

##### **Improvement in Sign Language Translation Using Text CTC Alignment**
2412.09014v1 by Sihan Tan, Taro Miyazaki, Nabeela Khan, Kazuhiro Nakadai

Current sign language translation (SLT) approaches often rely on gloss-based
supervision with Connectionist Temporal Classification (CTC), limiting their
ability to handle non-monotonic alignments between sign language video and
spoken text. In this work, we propose a novel method combining joint
CTC/Attention and transfer learning. The joint CTC/Attention introduces
hierarchical encoding and integrates CTC with the attention mechanism during
decoding, effectively managing both monotonic and non-monotonic alignments.
Meanwhile, transfer learning helps bridge the modality gap between vision and
language in SLT. Experimental results on two widely adopted benchmarks,
RWTH-PHOENIX-Weather 2014 T and CSL-Daily, show that our method achieves
results comparable to state-of-the-art and outperforms the pure-attention
baseline. Additionally, this work opens a new door for future research into
gloss-free SLT using text-based CTC alignment.

摘要：當前的符號語言翻譯 (SLT) 方法通常依賴於基於語彙的監督，並使用連接主義時序分類 (CTC)，這限制了它們處理符號語言影片和口語文字之間非單調對齊的能力。在這項工作中，我們提出了一種結合聯合 CTC/注意力和遷移學習的新方法。聯合 CTC/注意力引入了階層編碼，並在解碼過程中將 CTC 與注意力機制整合，有效管理單調和非單調對齊。同時，遷移學習有助於彌合 SLT 中視覺和語言之間的模態差距。在兩個廣泛採用的基準 RWTH-PHOENIX-Weather 2014 T 和 CSL-Daily 上的實驗結果顯示，我們的模型達到了與最先進技術相當的結果，並優於純注意力基線。此外，這項工作為未來使用基於文字的 CTC 對齊的無語彙 SLT 研究開啟了一扇新門。

##### **What Makes Cryptic Crosswords Challenging for LLMs?**
2412.09012v1 by Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar

Cryptic crosswords are puzzles that rely on general knowledge and the
solver's ability to manipulate language on different levels, dealing with
various types of wordplay. Previous research suggests that solving such puzzles
is challenging even for modern NLP models, including Large Language Models
(LLMs). However, there is little to no research on the reasons for their poor
performance on this task. In this paper, we establish the benchmark results for
three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance
on this task is still significantly below that of humans. We also investigate
why these models struggle to achieve superior performance. We release our code
and introduced datasets at
https://github.com/bodasadallah/decrypting-crosswords.

摘要：隱晦的填字遊戲是一種謎題，它依賴於常識以及解謎者在不同層面上操縱語言的能力，並處理各種文字遊戲。先前的研究表明，即使對於現代的 NLP 模型，包括大型語言模型 (LLM)，解開此類謎題也具有挑戰性。然而，對於它們在這個任務上表現不佳的原因，幾乎沒有研究。在本文中，我們為三個流行的 LLM：Gemma2、LLaMA3 和 ChatGPT 建立了基準結果，表明它們在這個任務上的表現仍然顯著低於人類。我們還研究了為什麼這些模型難以實現卓越的表現。我們發布了我們的程式碼，並在 https://github.com/bodasadallah/decrypting-crosswords 上介紹了資料集。

##### **Assessing the Robustness of Retrieval-Augmented Generation Systems in K-12 Educational Question Answering with Knowledge Discrepancies**
2412.08985v1 by Tianshi Zheng, Weihan Li, Jiaxin Bai, Weiqi Wang, Yangqiu Song

Retrieval-Augmented Generation (RAG) systems have demonstrated remarkable
potential as question answering systems in the K-12 Education domain, where
knowledge is typically queried within the restricted scope of authoritative
textbooks. However, the discrepancy between textbooks and the parametric
knowledge in Large Language Models (LLMs) could undermine the effectiveness of
RAG systems. To systematically investigate the robustness of RAG systems under
such knowledge discrepancies, we present EduKDQA, a question answering dataset
that simulates knowledge discrepancies in real applications by applying
hypothetical knowledge updates in answers and source documents. EduKDQA
includes 3,005 questions covering five subjects, under a comprehensive question
typology from the perspective of context utilization and knowledge integration.
We conducted extensive experiments on retrieval and question answering
performance. We find that most RAG systems suffer from a substantial
performance drop in question answering with knowledge discrepancies, while
questions that require integration of contextual knowledge and parametric
knowledge pose a challenge to LLMs.

摘要：檢索增強生成 (RAG) 系統已展現出驚人的潛力，可用作 K-12 教育領域的問題解答系統，其中知識通常在權威教科書的受限範圍內查詢。然而，教科書與大型語言模型 (LLM) 中的參數化知識之間的差異可能會損害 RAG 系統的效能。為了系統性地探討 RAG 系統在這種知識差異下的穩健性，我們提出了 EduKDQA，這是一個問題解答資料集，透過在答案和原始文件套用假設的知識更新，模擬實際應用中的知識差異。EduKDQA 包含 3,005 個問題，涵蓋五個科目，從情境利用和知識整合的角度來看，是一個全面的問題類型學。我們對檢索和問題解答效能進行了廣泛的實驗。我們發現，大多數 RAG 系統在有知識差異的問題解答中效能大幅下降，而需要整合情境知識和參數化知識的問題對 LLM 構成挑戰。

##### **Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?**
2412.08973v1 by Yifan Zhang, Junhui Hou

Cross-modal contrastive distillation has recently been explored for learning
effective 3D representations. However, existing methods focus primarily on
modality-shared features, neglecting the modality-specific features during the
pre-training process, which leads to suboptimal representations. In this paper,
we theoretically analyze the limitations of current contrastive methods for 3D
representation learning and propose a new framework, namely CMCR, to address
these shortcomings. Our approach improves upon traditional methods by better
integrating both modality-shared and modality-specific features. Specifically,
we introduce masked image modeling and occupancy estimation tasks to guide the
network in learning more comprehensive modality-specific features. Furthermore,
we propose a novel multi-modal unified codebook that learns an embedding space
shared across different modalities. Besides, we introduce geometry-enhanced
masked image modeling to further boost 3D representation learning. Extensive
experiments demonstrate that our method mitigates the challenges faced by
traditional approaches and consistently outperforms existing image-to-LiDAR
contrastive distillation methods in downstream tasks. Code will be available at
https://github.com/Eaphan/CMCR.

摘要：跨模态对比蒸馏最近已被探索用于学习有效的 3D 表征。然而，现有方法主要关注模态共享特征，在预训练过程中忽略了模态特定特征，这导致了次优表征。在本文中，我们从理论上分析了当前对比方法在 3D 表征学习方面的局限性，并提出了一个名为 CMCR 的新框架来解决这些缺点。我们的方法通过更好地整合模态共享和模态特定特征来改进传统方法。具体来说，我们引入了掩码图像建模和占用估计任务来指导网络学习更全面的模态特定特征。此外，我们提出了一个新颖的多模态统一码本，该码本学习了一个跨不同模态共享的嵌入空间。此外，我们引入了几何增强的掩码图像建模以进一步提升 3D 表征学习。大量实验表明，我们的方法减轻了传统方法面临的挑战，并且在图像到激光雷达对比蒸馏方法的下游任务中始终优于现有的方法。代码将可在 https://github.com/Eaphan/CMCR 获得。

##### **RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios**
2412.08972v1 by Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, William Yang Wang

This paper introduces RuleArena, a novel and challenging benchmark designed
to evaluate the ability of large language models (LLMs) to follow complex,
real-world rules in reasoning. Covering three practical domains -- airline
baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'
proficiency in handling intricate natural language instructions that demand
long-context understanding, logical reasoning, and accurate mathematical
computation. Two key attributes distinguish RuleArena from traditional
rule-based reasoning benchmarks: (1) it extends beyond standard first-order
logic representations, and (2) it is grounded in authentic, practical
scenarios, providing insights into the suitability and reliability of LLMs for
real-world applications. Our findings reveal several notable limitations in
LLMs: (1) they struggle to identify and apply the appropriate rules, frequently
becoming confused by similar but distinct regulations, (2) they cannot
consistently perform accurate mathematical computations, even when they
correctly identify the relevant rules, and (3) in general, they perform poorly
in the benchmark. These results highlight significant challenges in advancing
LLMs' rule-guided reasoning capabilities in real-life applications.

摘要：這篇論文介紹了 RuleArena，這是一個新穎且具有挑戰性的基準，旨在評估大型語言模型 (LLM) 在推理中遵循複雜的現實規則的能力。涵蓋三個實用領域——航空公司行李費、NBA 交易和稅務法規——RuleArena 評估了 LLM 在處理複雜的自然語言指令方面的能力，這些指令要求長語境理解、邏輯推理和準確的數學計算。RuleArena 與傳統基於規則的推理基準的兩個關鍵區別在於：(1) 它超越了標準的一階邏輯表示，(2) 它基於真實的實用場景，提供了對 LLM 適用性和可靠性的見解，以用於現實世界應用。我們的研究結果揭示了 LLM 的幾個顯著限制：(1) 它們難以識別和應用適當的規則，經常會對相似但不同的法規感到困惑，(2) 它們無法始終執行準確的數學計算，即使它們正確識別了相關規則，(3) 通常，它們在基準測試中的表現不佳。這些結果突出了在現實生活應用中推進 LLM 的規則指導推理能力方面的重大挑戰。

##### **Reasoning-Aware Query-Focused Summarization over Multi-Table Data**
2412.08970v1 by Xiaochuan Lin, Xiangyong Chen

Query-focused summarization over multi-table data is a challenging yet
critical task for extracting precise and relevant information from structured
data. Existing methods often rely on complex preprocessing steps and struggle
to generalize across domains or handle the logical reasoning required for
multi-table queries. In this paper, we propose QueryTableSummarizer++, an
end-to-end generative framework leveraging large language models (LLMs)
enhanced with table-aware pre-training, query-aligned fine-tuning, and
reinforcement learning with feedback. Our method eliminates the need for
intermediate serialization steps and directly generates query-relevant
summaries. Experiments on a benchmark dataset demonstrate that
QueryTableSummarizer++ significantly outperforms state-of-the-art baselines in
terms of BLEU, ROUGE, and F1-score. Additional analyses highlight its
scalability, generalization across domains, and robust handling of complex
queries. Human evaluation further validates the superior quality and practical
applicability of the generated summaries, establishing QueryTableSummarizer++
as a highly effective solution for multi-table summarization tasks.

摘要：針對多表格資料的查詢重點摘要是一項具有挑戰性但對於從結構化資料中萃取精確且相關資訊至關重要的任務。現有方法通常依賴複雜的預處理步驟，並且難以跨網域概括或處理多表格查詢所需的邏輯推理。在本文中，我們提出 QueryTableSummarizer++，一個端對端的生成架構，利用大型語言模型 (LLM)，並透過考量表格的預訓練、與查詢比對的微調，以及帶有回饋的強化學習進行強化。我們的模型不需要中間序列化步驟，並直接產生與查詢相關的摘要。在基準資料集上的實驗顯示，QueryTableSummarizer++ 在 BLEU、ROUGE 和 F1 分數方面顯著優於現有的基準。額外的分析強調了它的可擴充性、跨網域的概括性，以及對複雜查詢的穩健處理。人工評估進一步驗證了所產生摘要的優異品質和實用性，確立了 QueryTableSummarizer++ 作為多表格摘要任務的高效解決方案。

##### **AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection**
2412.08965v1 by Zihan Ji, Xuetao Tian, Ye Liu

The scarcity of high-quality large-scale labeled datasets poses a huge
challenge for employing deep learning models in video deception detection. To
address this issue, inspired by the psychological theory on the relation
between deception and expressions, we propose a novel method called AFFAKT in
this paper, which enhances the classification performance by transferring
useful and correlated knowledge from a large facial expression dataset. Two key
challenges in knowledge transfer arise: 1) \textit{how much} knowledge of
facial expression data should be transferred and 2) \textit{how to} effectively
leverage transferred knowledge for the deception classification model during
inference. Specifically, the optimal relation mapping between facial expression
classes and deception samples is firstly quantified using proposed H-OTKT
module and then transfers knowledge from the facial expression dataset to
deception samples. Moreover, a correlation prototype within another proposed
module SRKB is well designed to retain the invariant correlations between
facial expression classes and deception classes through momentum updating.
During inference, the transferred knowledge is fine-tuned with the correlation
prototype using a sample-specific re-weighting strategy. Experimental results
on two deception detection datasets demonstrate the superior performance of our
proposed method. The interpretability study reveals high associations between
deception and negative affections, which coincides with the theory in
psychology.

摘要：高品質大規模標籤資料集的稀少性對在影片欺騙偵測中採用深度學習模型構成了巨大的挑戰。為了解決這個問題，本文受到心理學理論中關於欺騙與表情之間關係的啟發，提出了一種名為 AFFAKT 的新方法，它透過從大型面部表情資料集中轉移有用且相關的知識來增強分類效能。知識轉移中有兩個關鍵的挑戰：1) 應轉移多少面部表情資料的知識，以及 2) 如何在推論期間有效地利用轉移的知識來進行欺騙分類模型。具體來說，首先使用所提出的 H-OTKT 模組量化面部表情類別與欺騙樣本之間的最佳關係對應，然後將知識從面部表情資料集轉移到欺騙樣本。此外，在另一個所提出的 SRKB 模組中，一個關聯原型經過精心設計，透過動量更新來保留面部表情類別與欺騙類別之間的恆定關聯。在推論期間，使用樣本特定的重新加權策略，針對關聯原型微調轉移的知識。在兩個欺騙偵測資料集上的實驗結果證明了我們所提出的方法具有優異的效能。可解釋性研究揭示了欺騙與負面情感之間的高度關聯，這與心理學理論相符。

##### **Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning**
2412.08955v1 by Mateo Alejandro Rojas, Rafael Carranza

Cross-lingual in-context learning (XICL) has emerged as a transformative
paradigm for leveraging large language models (LLMs) to tackle multilingual
tasks, especially for low-resource languages. However, existing approaches
often rely on external retrievers or task-specific fine-tuning, limiting their
scalability and generalizability. In this paper, we propose a novel
self-supervised framework that harnesses the generative capabilities of LLMs to
internally select and utilize task-relevant examples. Our method introduces two
key objectives: a retrieval-generation alignment loss to optimize the quality
of selected examples and a semantic coherence loss to ensure cross-lingual
consistency. Through extensive experiments on multilingual benchmarks, our
approach achieves state-of-the-art performance, significantly outperforming
existing baselines. Further analysis highlights its robustness across diverse
language families and its ability to generalize to unseen tasks. Human
evaluations confirm the superior fluency, relevance, and semantic correctness
of outputs generated by our method. This work provides a scalable, effective,
and generalizable solution for cross-lingual in-context learning.

摘要：跨語言情境學習 (XICL) 已成為利用大型語言模型 (LLM) 來處理多語言任務的轉型範例，特別是對於低資源語言。然而，現有方法通常依賴於外部檢索器或特定任務的微調，這限制了它們的可擴充性和概括性。在本文中，我們提出了一個新穎的自我監督框架，利用 LLM 的生成能力來內部選擇和利用與任務相關的範例。我們的模型引入了兩個關鍵目標：檢索生成對齊損失，以最佳化所選範例的品質；以及語義一致性損失，以確保跨語言一致性。透過對多語言基準進行廣泛的實驗，我們的模型達到了最先進的效能，顯著優於現有的基準。進一步的分析突顯了它在不同語言系列中的穩健性，以及它對未見任務的概括能力。人工評估確認了我們的方法所產生的輸出的優越流暢性、相關性和語義正確性。這項工作提供了一個可擴充、有效且可概括的跨語言情境學習解決方案。

##### **Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning**
2412.08950v1 by Zhongyang Zhang, Jinhe Wen, Zixi Chen, Dara Arbab, Sruti Sahani, Bijan Arbab, Haojian Jin, Tauhidur Rahman

Frames Per Second (FPS) significantly affects the gaming experience.
Providing players with accurate FPS estimates prior to purchase benefits both
players and game developers. However, we have a limited understanding of how to
predict a game's FPS performance on a specific device. In this paper, we first
conduct a comprehensive analysis of a wide range of factors that may affect
game FPS on a global-scale dataset to identify the determinants of FPS. This
includes player-side and game-side characteristics, as well as country-level
socio-economic statistics. Furthermore, recognizing that accurate FPS
predictions require extensive user data, which raises privacy concerns, we
propose a federated learning-based model to ensure user privacy. Each player
and game is assigned a unique learnable knowledge kernel that gradually
extracts latent features for improved accuracy. We also introduce a novel
training and prediction scheme that allows these kernels to be dynamically
plug-and-play, effectively addressing cold start issues. To train this model
with minimal bias, we collected a large telemetry dataset from 224 countries
and regions, 100,000 users, and 835 games. Our model achieved a mean
Wasserstein distance of 0.469 between predicted and ground truth FPS
distributions, outperforming all baseline methods.

摘要：每秒幀數 (FPS) 會顯著影響遊戲體驗。
在購買前為玩家提供準確的 FPS 估計值對玩家和遊戲開發者都有利。然而，我們對於如何預測遊戲在特定裝置上的 FPS 效能了解有限。在本文中，我們首先對可能影響全球規模資料集上遊戲 FPS 的各種因素進行全面分析，以找出 FPS 的決定因素。這包括玩家端和遊戲端特徵，以及國家層級的社會經濟統計資料。此外，我們認知到準確的 FPS 預測需要大量的使用者資料，這會引發隱私問題，因此我們提出一個基於聯邦學習的模型來確保使用者隱私。每個玩家和遊戲都分配一個獨特的可學習知識核心，它會逐步提取潛在特徵以提高準確度。我們還引入一個新穎的訓練和預測方案，讓這些核心可以動態地即插即用，有效地解決冷啟動問題。為了以最小的偏差訓練此模型，我們從 224 個國家和地區、100,000 名使用者和 835 款遊戲收集了一個大型遙測資料集。我們的模型在預測 FPS 分布和真實 FPS 分布之間達到了 0.469 的平均 Wasserstein 距離，優於所有基準方法。

##### **Mojito: Motion Trajectory and Intensity Control for Video Generation**
2412.08948v1 by Xuehai He, Shuohang Wang, Jianwei Yang, Xiaoxia Wu, Yiping Wang, Kuan Wang, Zheng Zhan, Olatunji Ruwase, Yelong Shen, Xin Eric Wang

Recent advancements in diffusion models have shown great promise in producing
high-quality video content. However, efficiently training diffusion models
capable of integrating directional guidance and controllable motion intensity
remains a challenging and under-explored area. This paper introduces Mojito, a
diffusion model that incorporates both \textbf{Mo}tion tra\textbf{j}ectory and
\textbf{i}ntensi\textbf{t}y contr\textbf{o}l for text to video generation.
Specifically, Mojito features a Directional Motion Control module that
leverages cross-attention to efficiently direct the generated object's motion
without additional training, alongside a Motion Intensity Modulator that uses
optical flow maps generated from videos to guide varying levels of motion
intensity. Extensive experiments demonstrate Mojito's effectiveness in
achieving precise trajectory and intensity control with high computational
efficiency, generating motion patterns that closely match specified directions
and intensities, providing realistic dynamics that align well with natural
motion in real-world scenarios.

摘要：最近在扩散模型的进步显示出在制作高品质影片内容方面有很大的希望。然而，有效地训练扩散模型来整合方向引导和可控的运动强度仍然是一个具有挑战性和尚未探索的领域。本文介绍了 Mojito，这是一种扩散模型，它结合了文字到影片生成的运动轨迹和强度控制。具体来说，Mojito 具有一个方向运动控制模块，它利用交叉注意力来有效地引导生成的物体的运动，而不需要额外的训练，同时还有一个运动强度调节器，它使用从影片生成的视流图来引导不同级别的运动强度。广泛的实验表明了 Mojito 在实现精确的轨迹和强度控制方面的有效性，具有很高的计算效率，生成的运动模式与指定的方向和强度非常匹配，提供了与现实世界场景中的自然运动非常吻合的逼真动态。

##### **Selective Visual Prompting in Vision Mamba**
2412.08947v1 by Yifeng Yao, Zichen Liu, Zhenyu Cui, Yuxin Peng, Jiahuan Zhou

Pre-trained Vision Mamba (Vim) models have demonstrated exceptional
performance across various computer vision tasks in a computationally efficient
manner, attributed to their unique design of selective state space models. To
further extend their applicability to diverse downstream vision tasks, Vim
models can be adapted using the efficient fine-tuning technique known as visual
prompting. However, existing visual prompting methods are predominantly
tailored for Vision Transformer (ViT)-based models that leverage global
attention, neglecting the distinctive sequential token-wise compression and
propagation characteristics of Vim. Specifically, existing prompt tokens
prefixed to the sequence are insufficient to effectively activate the input and
forget gates across the entire sequence, hindering the extraction and
propagation of discriminative information. To address this limitation, we
introduce a novel Selective Visual Prompting (SVP) method specifically for the
efficient fine-tuning of Vim. To prevent the loss of discriminative information
during state space propagation, SVP employs lightweight selective prompters for
token-wise prompt generation, ensuring adaptive activation of the update and
forget gates within Mamba blocks to promote discriminative information
propagation. Moreover, considering that Vim propagates both shared cross-layer
information and specific inner-layer information, we further refine SVP with a
dual-path structure: Cross-Prompting and Inner-Prompting. Cross-Prompting
utilizes shared parameters across layers, while Inner-Prompting employs
distinct parameters, promoting the propagation of both shared and specific
information, respectively. Extensive experimental results on various
large-scale benchmarks demonstrate that our proposed SVP significantly
outperforms state-of-the-art methods. Our code is available at
https://github.com/zhoujiahuan1991/AAAI2025-SVP.

摘要：預訓練的 Vision Mamba (Vim) 模型已展現出在各種電腦視覺任務中的出色效能，且運算效率高，這要歸功於它們獨特的選擇性狀態空間模型設計。為了進一步擴展其對各種下游視覺任務的適用性，Vim 模型可以使用稱為視覺提示的有效微調技術進行調整。然而，現有的視覺提示方法主要針對利用全局注意力的 Vision Transformer (ViT) 模型量身打造，忽略了 Vim 獨特的順序式代幣壓縮和傳播特性。具體來說，前置於序列的現有提示代幣不足以有效激活整個序列中的輸入和遺忘閘，阻礙了區辨資訊的提取和傳播。為了解決這個限制，我們針對 Vim 的有效微調引入了創新的選擇性視覺提示 (SVP) 方法。為了防止在狀態空間傳播期間遺失區辨資訊，SVP 使用輕量級選擇性提示器進行代幣提示生成，確保 Mamba 區塊中更新和遺忘閘的自適應激活，以促進區辨資訊的傳播。此外，考慮到 Vim 傳播共享跨層資訊和特定內層資訊，我們進一步使用雙路徑結構對 SVP 進行優化：交叉提示和內部提示。交叉提示利用跨層共享參數，而內部提示使用不同的參數，分別促進共享和特定資訊的傳播。在各種大規模基準測試上的廣泛實驗結果表明，我們提出的 SVP 明顯優於最先進的方法。我們的程式碼可在 https://github.com/zhoujiahuan1991/AAAI2025-SVP 取得。

##### **MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning**
2412.08946v1 by Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou

Recently, LoRA has emerged as a crucial technique for fine-tuning large
pre-trained models, yet its performance in multi-task learning scenarios often
falls short. In contrast, the MoE architecture presents a natural solution to
this issue. However, it introduces challenges such as mutual interference of
data across multiple domains and knowledge forgetting of various tasks.
Additionally, MoE significantly increases the number of parameters, posing a
computational cost challenge. Therefore, in this paper, we propose MoSLD, a
mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these
challenges by sharing the upper projection matrix in LoRA among different
experts, encouraging the model to learn general knowledge across tasks, while
still allowing the lower projection matrix to focus on the unique features of
each task. The application of dropout alleviates the imbalanced update of
parameter matrix and mitigates parameter overfitting in LoRA. Extensive
experiments demonstrate that our model exhibits excellent performance in both
single-task and multi-task scenarios, with robust out-of-domain generalization
capabilities.

摘要：最近，LoRA 已成为微调大型预训练模型的关键技术，但其在多任务学习场景中的表现往往不足。相比之下，MoE 架构为这个问题提供了一个自然的解决方案。然而，它带来了诸如跨多个域的数据相互干扰和各种任务知识遗忘等挑战。此外，MoE 大大增加了参数的数量，带来了计算成本挑战。因此，在本文中，我们提出了 MoSLD，一种具有 dropout 策略的混合共享 LoRA 模型。MoSLD 通过在不同专家之间共享 LoRA 中的上层投影矩阵来应对这些挑战，鼓励模型跨任务学习一般知识，同时仍然允许下层投影矩阵专注于每个任务的独特特征。dropout 的应用减轻了参数矩阵的不平衡更新，并减轻了 LoRA 中的参数过拟合。大量实验表明，我们的模型在单任务和多任务场景中都表现出优异的性能，具有强大的域外泛化能力。

##### **Multi-Scale Heterogeneous Text-Attributed Graph Datasets From Diverse Domains**
2412.08937v1 by Yunhui Liu, Qizhuo Xie, Jinwei Shi, Jiaxu Shen, Tieke He

Heterogeneous Text-Attributed Graphs (HTAGs), where different types of
entities are not only associated with texts but also connected by diverse
relationships, have gained widespread popularity and application across various
domains. However, current research on text-attributed graph learning
predominantly focuses on homogeneous graphs, which feature a single node and
edge type, thus leaving a gap in understanding how methods perform on HTAGs.
One crucial reason is the lack of comprehensive HTAG datasets that offer
original textual content and span multiple domains of varying sizes. To this
end, we introduce a collection of challenging and diverse benchmark datasets
for realistic and reproducible evaluation of machine learning models on HTAGs.
Our HTAG datasets are multi-scale, span years in duration, and cover a wide
range of domains, including movie, community question answering, academic,
literature, and patent networks. We further conduct benchmark experiments on
these datasets with various graph neural networks. All source data, dataset
construction codes, processed HTAGs, data loaders, benchmark codes, and
evaluation setup are publicly available at GitHub and Hugging Face.

摘要：異質文本屬性圖 (HTAG)，其中不同類型的實體不僅與文本相關聯，而且還通過不同的關係連接，在各個領域獲得了廣泛的普及和應用。然而，當前關於文本屬性圖學習的研究主要集中在同質圖上，同質圖特徵是單個節點和邊緣類型，因此在理解方法如何在 HTAG 上執行時留下了空白。一個關鍵原因是缺乏提供原始文本內容並跨越不同規模的多個域的綜合 HTAG 數據集。為此，我們引入了一組具有挑戰性和多樣性的基準數據集，用於對機器學習模型在 HTAG 上進行真實且可重現的評估。我們的 HTAG 數據集是多尺度的，跨越多年的持續時間，並涵蓋了廣泛的領域，包括電影、社區問答、學術、文學和專利網路。我們進一步在這些數據集上使用各種圖神經網路進行基準實驗。所有原始數據、數據集構造代碼、處理過的 HTAG、數據加載器、基準代碼和評估設置都可以在 GitHub 和 Hugging Face 上公開獲得。

##### **From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning**
2412.08920v1 by Pusen Dong, Tianchen Zhu, Yue Qiu, Haoyi Zhou, Jianxin Li

Safe reinforcement learning (RL) requires the agent to finish a given task
while obeying specific constraints. Giving constraints in natural language form
has great potential for practical scenarios due to its flexible transfer
capability and accessibility. Previous safe RL methods with natural language
constraints typically need to design cost functions manually for each
constraint, which requires domain expertise and lacks flexibility. In this
paper, we harness the dual role of text in this task, using it not only to
provide constraint but also as a training signal. We introduce the
Trajectory-level Textual Constraints Translator (TTCT) to replace the manually
designed cost function. Our empirical results demonstrate that TTCT effectively
comprehends textual constraint and trajectory, and the policies trained by TTCT
can achieve a lower violation rate than the standard cost function. Extra
studies are conducted to demonstrate that the TTCT has zero-shot transfer
capability to adapt to constraint-shift environments.

摘要：安全強化學習 (RL) 要求代理在遵守特定約束的同時完成既定任務。以自然語言形式給出約束由於其靈活的傳輸能力和可訪問性，對於實際場景具有巨大的潛力。具有自然語言約束的先前安全 RL 方法通常需要為每個約束手動設計成本函數，這需要領域專業知識且缺乏靈活性。在本文中，我們利用文本在這一任務中的雙重角色，不僅使用它來提供約束，還作為訓練信號。我們引入了軌跡級文本約束轉換器 (TTCT) 來替換手動設計的成本函數。我們的經驗結果表明，TTCT 有效地理解文本約束和軌跡，並且由 TTCT 訓練的策略可以實現比標準成本函數更低的違規率。進行了額外的研究以證明 TTCT 具有零次學習轉移能力，可以適應約束轉換環境。

##### **Goal-Conditioned Supervised Learning for Multi-Objective Recommendation**
2412.08911v1 by Shijun Li, Hilaf Hasson, Jing Hu, Joydeep Ghosh

Multi-objective learning endeavors to concurrently optimize multiple
objectives using a single model, aiming to achieve high and balanced
performance across these diverse objectives. However, it often involves a more
complex optimization problem, particularly when navigating potential conflicts
between objectives, leading to solutions with higher memory requirements and
computational complexity. This paper introduces a Multi-Objective
Goal-Conditioned Supervised Learning (MOGCSL) framework for automatically
learning to achieve multiple objectives from offline sequential data. MOGCSL
extends the conventional Goal-Conditioned Supervised Learning (GCSL) method to
multi-objective scenarios by redefining goals from one-dimensional scalars to
multi-dimensional vectors. The need for complex architectures and optimization
constraints can be naturally eliminated. MOGCSL benefits from filtering out
uninformative or noisy instances that do not achieve desirable long-term
rewards. It also incorporates a novel goal-choosing algorithm to model and
select "high" achievable goals for inference.
  While MOGCSL is quite general, we focus on its application to the next action
prediction problem in commercial-grade recommender systems. In this context,
any viable solution needs to be reasonably scalable and also be robust to large
amounts of noisy data that is characteristic of this application space. We show
that MOGCSL performs admirably on both counts. Specifically, extensive
experiments conducted on real-world recommendation datasets validate its
efficacy and efficiency. Also, analysis and experiments are included to explain
its strength in discounting the noisier portions of training data in
recommender systems.

摘要：<paragraph>多目標學習致力於使用單一模型同時最佳化多個目標，目標是在這些不同的目標中達成高且平衡的效能。然而，它通常涉及更複雜的最佳化問題，特別是在處理目標之間潛在衝突時，導致解決方案具有較高的記憶體需求和運算複雜度。本文介紹了一個多目標目標條件監督式學習 (MOGCSL) 架構，用於自動學習從離線順序資料達成多個目標。MOGCSL 將傳統的目標條件監督式學習 (GCSL) 方法擴充到多目標場景，方法是將目標重新定義為一維純量到多維向量。可以自然而然地消除對複雜架構和最佳化約束的需求。MOGCSL 受益於過濾掉無法達成理想長期回報的無資訊或雜訊實例。它還結合了一種新穎的目標選擇演算法，用於建模和選擇「高」可達成目標以進行推論。
雖然 MOGCSL 非常普遍，但我們專注於其在商業級推薦系統中的下一個動作預測問題的應用。在此脈絡中，任何可行的解決方案都需要具有合理的可擴充性，並且對於此應用空間特有的大量雜訊資料具有穩健性。我們證明 MOGCSL 在這兩方面都表現得很好。具體而言，在真實世界推薦資料集上進行的廣泛實驗驗證了其效能和效率。此外，還包括分析和實驗，以說明其在推薦系統中對訓練資料中較雜訊部分進行折扣的優勢。</paragraph>

##### **Phi-4 Technical Report**
2412.08905v1 by Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, Yi Zhang

We present phi-4, a 14-billion parameter language model developed with a
training recipe that is centrally focused on data quality. Unlike most language
models, where pre-training is based primarily on organic data sources such as
web content or code, phi-4 strategically incorporates synthetic data throughout
the training process. While previous models in the Phi family largely distill
the capabilities of a teacher model (specifically GPT-4), phi-4 substantially
surpasses its teacher model on STEM-focused QA capabilities, giving evidence
that our data-generation and post-training techniques go beyond distillation.
Despite minimal changes to the phi-3 architecture, phi-4 achieves strong
performance relative to its size -- especially on reasoning-focused benchmarks
-- due to improved data, training curriculum, and innovations in the
post-training scheme.

摘要：我們展示 phi-4，這是一個 140 億個參數的語言模型，其開發時採用了以資料品質為核心的訓練配方。與大多數語言模型不同，這些模型的預訓練主要基於自然資料來源，例如網路內容或程式碼，phi-4 在整個訓練過程中策略性地納入了合成資料。儘管 Phi 家族中的先前模型在很大程度上精煉了教師模型（特別是 GPT-4）的能力，但 phi-4 在以 STEM 為重點的 QA 能力方面顯著超越了其教師模型，這證明了我們的資料生成和訓練後技術超越了精煉。儘管對 phi-3 架構進行了最小的變更，但由於改進的資料、訓練課程和訓練後方案的創新，phi-4 相對於其大小（特別是在以推理為重點的基準測試中）達到了強勁的效能。

##### **Radiology Report Generation via Multi-objective Preference Optimization**
2412.08901v1 by Ting Xiao, Lei Shi, Peng Liu, Zhe Wang, Chenjia Bai

Automatic Radiology Report Generation (RRG) is an important topic for
alleviating the substantial workload of radiologists. Existing RRG approaches
rely on supervised regression based on different architectures or additional
knowledge injection,while the generated report may not align optimally with
radiologists' preferences. Especially, since the preferences of radiologists
are inherently heterogeneous and multidimensional, e.g., some may prioritize
report fluency, while others emphasize clinical accuracy. To address this
problem,we propose a new RRG method via Multi-objective Preference Optimization
(MPO) to align the pre-trained RRG model with multiple human preferences, which
can be formulated by multi-dimensional reward functions and optimized by
multi-objective reinforcement learning (RL). Specifically, we use a preference
vector to represent the weight of preferences and use it as a condition for the
RRG model. Then, a linearly weighed reward is obtained via a dot product
between the preference vector and multi-dimensional reward.Next,the RRG model
is optimized to align with the preference vector by optimizing such a reward
via RL. In the training stage,we randomly sample diverse preference vectors
from the preference space and align the model by optimizing the weighted
multi-objective rewards, which leads to an optimal policy on the entire
preference space. When inference,our model can generate reports aligned with
specific preferences without further fine-tuning. Extensive experiments on two
public datasets show the proposed method can generate reports that cater to
different preferences in a single model and achieve state-of-the-art
performance.

摘要：自動放射科報告生成 (RRG) 是減輕放射科醫師大量工作負擔的重要課題。現有的 RRG 方法仰賴基於不同架構或額外知識注入的監督式回歸，而產生的報告可能無法最佳地符合放射科醫師的偏好。特別是，由於放射科醫師的偏好本質上是異質且多面向的，例如，有些人可能會優先考慮報告流暢性，而另一些人則強調臨床準確性。為了解決這個問題，我們提出一個新的 RRG 方法，透過多目標偏好最佳化 (MPO) 來調整預訓練的 RRG 模型，以符合多個人偏好，這可以用多維度獎勵函數來制定，並透過多目標強化學習 (RL) 來最佳化。具體來說，我們使用偏好向量來表示偏好的權重，並將其用作 RRG 模型的條件。然後，透過偏好向量與多維度獎勵之間的點積，取得線性加權獎勵。接下來，透過 RL 最佳化此類獎勵，將 RRG 模型最佳化，以符合偏好向量。在訓練階段，我們從偏好空間中隨機取樣多樣化的偏好向量，並透過最佳化加權的多目標獎勵來調整模型，這會在整個偏好空間中產生最佳策略。在推理時，我們的模型可以在不進一步微調的情況下，產生符合特定偏好的報告。在兩個公開資料集上的廣泛實驗顯示，所提出的方法可以在單一模型中產生迎合不同偏好的報告，並達到最先進的效能。

##### **AI-assisted Knowledge Discovery in Biomedical Literature to Support Decision-making in Precision Oncology**
2412.08900v1 by Ting He, Kory Kreimeyer, Mimi Najjar, Jonathan Spiker, Maria Fatteh, Valsamo Anagnostou, Taxiarchis Botsis

The delivery of appropriate targeted therapies to cancer patients requires
the complete analysis of the molecular profiling of tumors and the patient's
clinical characteristics in the context of existing knowledge and recent
findings described in biomedical literature and several other sources. We
evaluated the potential contributions of specific natural language processing
solutions to support knowledge discovery from biomedical literature. Two models
from the Bidirectional Encoder Representations from Transformers (BERT) family,
two Large Language Models, and PubTator 3.0 were tested for their ability to
support the named entity recognition (NER) and the relation extraction (RE)
tasks. PubTator 3.0 and the BioBERT model performed best in the NER task (best
F1-score equal to 0.93 and 0.89, respectively), while BioBERT outperformed all
other solutions in the RE task (best F1-score 0.79) and a specific use case it
was applied to by recognizing nearly all entity mentions and most of the
relations.

摘要：適當標靶療法在癌症病患的應用，需要在現有知識和生物醫學文獻中所描述的最新發現的脈絡下，完整分析腫瘤的分子特徵和病患的臨床特徵。我們評估了特定自然語言處理解決方案在支援從生物醫學文獻中發現知識的潛在貢獻。我們測試了來自 Transformer 雙向編碼器表示法 (BERT) 家族的兩個模型、兩個大型語言模型和 PubTator 3.0，以評估它們支援命名實體辨識 (NER) 和關係萃取 (RE) 任務的能力。PubTator 3.0 和 BioBERT 模型在 NER 任務中表現最佳（最佳 F1 分數分別為 0.93 和 0.89），而 BioBERT 在 RE 任務中優於所有其他解決方案（最佳 F1 分數為 0.79），並且在一個特定的應用案例中，它幾乎辨識出所有實體提及和大部分關係。

##### **Neural Interactive Proofs**
2412.08897v1 by Lewis Hammond, Sam Adam-Day

We consider the problem of how a trusted, but computationally bounded agent
(a 'verifier') can learn to interact with one or more powerful but untrusted
agents ('provers') in order to solve a given task. More specifically, we study
the case in which agents are represented using neural networks and refer to
solutions of this problem as neural interactive proofs. First we introduce a
unifying framework based on prover-verifier games, which generalises previously
proposed interaction protocols. We then describe several new protocols for
generating neural interactive proofs, and provide a theoretical comparison of
both new and existing approaches. Finally, we support this theory with
experiments in two domains: a toy graph isomorphism problem that illustrates
the key ideas, and a code validation task using large language models. In so
doing, we aim to create a foundation for future work on neural interactive
proofs and their application in building safer AI systems.

摘要：<paragraph>我們考慮一個問題，說明一個受信任但計算受限的代理（「驗證者」）如何學會與一個或多個強大但不可信的代理（「證明者」）互動，以解決給定的任務。更具體地說，我們研究代理使用神經網路表示的情況，並將此問題的解決方案稱為神經互動證明。首先，我們引入一個基於證明者驗證者遊戲的統一框架，它概括了先前提出的互動協議。然後，我們描述了幾個生成神經互動證明的新協議，並對新舊方法進行了理論比較。最後，我們在兩個領域中用實驗支持了這個理論：一個玩具圖同構問題，說明了關鍵思想，以及使用大型語言模型的代碼驗證任務。這樣做，我們旨在為神經互動證明及其在構建更安全的 AI 系統中的應用奠定基礎。</paragraph>

##### **SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization**
2412.08894v1 by Kwangryeol Park, Seulki Lee

We propose SMMF (Square-Matricized Momentum Factorization), a
memory-efficient optimizer that reduces the memory requirement of the widely
used adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF
enables flexible and efficient factorization of an arbitrary rank (shape) of
the first and second momentum tensors during optimization, based on the
proposed square-matricization and one-time single matrix factorization. From
this, it becomes effectively applicable to any rank (shape) of momentum
tensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep
model architectures, such as CNNs (high rank) and Transformers (low rank), in
contrast to existing memory-efficient optimizers that applies only to a
particular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret
bound analysis of SMMF, which shows that it converges similarly to
non-memory-efficient adaptive learning rate optimizers, such as AdamNC,
providing a theoretical basis for its competitive optimization capability. In
our experiment, SMMF takes up to 96% less memory compared to state-of-the-art
memory efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving
comparable model performance on various CNN and Transformer tasks.

摘要：我們提出 SMMF（方陣動量分解），一種記憶體效率最佳化器，它可將廣泛使用的自適應學習率最佳化器（例如 Adam）的記憶體需求降低多達 96%。SMMF 能夠在最佳化過程中靈活且有效地對任意秩（形狀）的第一和第二動量張量進行分解，這基於所提出的方陣化和一次性單矩陣分解。因此，它可有效地應用於任何秩（形狀）的動量張量，即偏差、矩陣和任何秩 d 張量，這些張量在各種深度模型架構中很普遍，例如 CNN（高秩）和 Transformer（低秩），這與現有的記憶體效率最佳化器形成對比，後者僅適用於特定（秩 2）動量張量，例如線性層。我們對 SMMF 進行了遺憾邊界分析，這表明它的收斂方式與非記憶體效率自適應學習率最佳化器（例如 AdamNC）類似，為其競爭性的最佳化能力提供了理論基礎。在我們的實驗中，與最先進的記憶體效率最佳化器（例如 Adafactor、CAME 和 SM3）相比，SMMF 佔用的記憶體最多減少 96%，同時在各種 CNN 和 Transformer 任務上實現了相當的模型效能。

##### **Residual Channel Boosts Contrastive Learning for Radio Frequency Fingerprint Identification**
2412.08885v1 by Rui Pan, Hui Chen, Guanxiong Shen, Hongyang Chen

In order to address the issue of limited data samples for the deployment of
pre-trained models in unseen environments, this paper proposes a residual
channel-based data augmentation strategy for Radio Frequency Fingerprint
Identification (RFFI), coupled with a lightweight SimSiam contrastive learning
framework. By applying least square (LS) and minimum mean square error (MMSE)
channel estimations followed by equalization, signals with different residual
channel effects are generated. These residual channels enable the model to
learn more effective representations. Then the pre-trained model is fine-tuned
with 1% samples in a novel environment for RFFI. Experimental results
demonstrate that our method significantly enhances both feature extraction
ability and generalization while requiring fewer samples and less time, making
it suitable for practical wireless security applications.

摘要：為了解決在未見環境中部署預訓練模型的數據樣本有限問題，本文提出了一種殘差通道數據擴充策略，用於射頻指紋辨識 (RFFI)，並結合輕量級的 SimSiam 對比學習架構。透過運用最小二乘 (LS) 和最小均方誤差 (MMSE) 通道估計，然後進行等化，產生具有不同殘差通道效應的訊號。這些殘差通道使模型能夠學習更有效的表示。然後，預訓練模型在 RFFI 的新環境中使用 1% 的樣本進行微調。實驗結果表明，我們的模型顯著提升了特徵提取能力和泛化能力，同時需要更少的樣本和時間，使其適用於實際的無線安全應用。

##### **Towards modeling evolving longitudinal health trajectories with a transformer-based deep learning model**
2412.08873v1 by Hans Moen, Vishnu Raj, Andrius Vabalas, Markus Perola, Samuel Kaski, Andrea Ganna, Pekka Marttinen

Health registers contain rich information about individuals' health
histories. Here our interest lies in understanding how individuals' health
trajectories evolve in a nationwide longitudinal dataset with coded features,
such as clinical codes, procedures, and drug purchases. We introduce a
straightforward approach for training a Transformer-based deep learning model
in a way that lets us analyze how individuals' trajectories change over time.
This is achieved by modifying the training objective and by applying a causal
attention mask. We focus here on a general task of predicting the onset of a
range of common diseases in a given future forecast interval. However, instead
of providing a single prediction about diagnoses that could occur in this
forecast interval, our approach enable the model to provide continuous
predictions at every time point up until, and conditioned on, the time of the
forecast period. We find that this model performs comparably to other models,
including a bi-directional transformer model, in terms of basic prediction
performance while at the same time offering promising trajectory modeling
properties. We explore a couple of ways to use this model for analyzing health
trajectories and aiding in early detection of events that forecast possible
later disease onsets. We hypothesize that this method may be helpful in
continuous monitoring of peoples' health trajectories and enabling
interventions in ongoing health trajectories, as well as being useful in
retrospective analyses.

摘要：健康登記包含個人健康史的豐富資訊。我們在此有興趣了解個人健康軌跡如何隨著編碼功能（例如臨床代碼、程序和藥物購買）在全國縱向資料集中演變。我們引入一種直接的方法，用於訓練 Transformer 為基礎的深度學習模型，讓我們分析個人軌跡如何隨著時間推移而改變。這是透過修改訓練目標並應用因果注意力遮罩來實現的。我們在此專注於預測在給定未來預測區間內一系列常見疾病發病的一般任務。然而，我們的做法並非提供關於可能在此預測區間內發生的診斷的單一預測，而是讓模型能夠在每個時間點提供連續預測，直到預測期間的時間，並以其為條件。我們發現此模型的表現與其他模型（包括雙向 Transformer 模型）相當，在基本預測效能方面如此，同時提供有希望的軌跡建模屬性。我們探索了幾種使用此模型分析健康軌跡並協助早期偵測預測可能後續發病事件的方法。我們假設此方法可能有助於持續監測個人健康軌跡，並讓干預措施得以在持續的健康軌跡中進行，且在回顧性分析中也很有用。

