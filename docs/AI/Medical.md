
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-17**|**Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**|Fatema-E- Jannat et.al.|[2409.11375v1](http://arxiv.org/abs/2409.11375v1)|null|
|**2024-09-17**|**Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**|Lauren M. Zuromski et.al.|[2409.11350v1](http://arxiv.org/abs/2409.11350v1)|null|
|**2024-09-17**|**TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**|Rong Zhou et.al.|[2409.11299v1](http://arxiv.org/abs/2409.11299v1)|null|
|**2024-09-17**|**EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**|Zeyi Liao et.al.|[2409.11295v1](http://arxiv.org/abs/2409.11295v1)|null|
|**2024-09-17**|**Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**|Eunhae Lee et.al.|[2409.11192v1](http://arxiv.org/abs/2409.11192v1)|null|
|**2024-09-17**|**Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**|Mehroush Banday et.al.|[2409.10932v1](http://arxiv.org/abs/2409.10932v1)|null|
|**2024-09-16**|**Self-supervised Speech Models for Word-Level Stuttered Speech Detection**|Yi-Jen Shih et.al.|[2409.10704v1](http://arxiv.org/abs/2409.10704v1)|null|
|**2024-09-16**|**A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**|Zhang Zheng et.al.|[2409.10403v1](http://arxiv.org/abs/2409.10403v1)|null|
|**2024-09-16**|**Robust image representations with counterfactual contrastive learning**|MÃ©lanie Roschewitz et.al.|[2409.10365v1](http://arxiv.org/abs/2409.10365v1)|[link](https://github.com/biomedia-mira/counterfactual-contrastive)|
|**2024-09-16**|**Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation between the United States and South Africa**|Hayoung Jung et.al.|[2409.10168v1](http://arxiv.org/abs/2409.10168v1)|null|
|**2024-09-16**|**DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion**|Yuchen Guo et.al.|[2409.10080v1](http://arxiv.org/abs/2409.10080v1)|null|
|**2024-09-16**|**MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM**|Sijie Ji et.al.|[2409.10064v1](http://arxiv.org/abs/2409.10064v1)|null|
|**2024-09-16**|**HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making**|Sumera Anjum et.al.|[2409.10011v1](http://arxiv.org/abs/2409.10011v1)|null|
|**2024-09-16**|**Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System**|Raffi Hagopian et.al.|[2409.09968v1](http://arxiv.org/abs/2409.09968v1)|null|
|**2024-09-15**|**GP-GPT: Large Language Model for Gene-Phenotype Mapping**|Yanjun Lyu et.al.|[2409.09825v1](http://arxiv.org/abs/2409.09825v1)|null|
|**2024-09-15**|**Veridical Data Science for Medical Foundation Models**|Ahmed Alaa et.al.|[2409.10580v1](http://arxiv.org/abs/2409.10580v1)|null|
|**2024-09-15**|**From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare**|Ming Li et.al.|[2409.09727v1](http://arxiv.org/abs/2409.09727v1)|null|
|**2024-09-15**|**ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models**|Inhwa Song et.al.|[2409.09662v2](http://arxiv.org/abs/2409.09662v2)|null|
|**2024-09-15**|**MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences**|Subigya Nepal et.al.|[2409.09570v1](http://arxiv.org/abs/2409.09570v1)|null|
|**2024-09-14**|**COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare**|Chia-Hao Li et.al.|[2409.09549v1](http://arxiv.org/abs/2409.09549v1)|null|
|**2024-09-14**|**Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment**|Xin Hu et.al.|[2409.09520v1](http://arxiv.org/abs/2409.09520v1)|null|
|**2024-09-14**|**Synthetic4Health: Generating Annotated Synthetic Clinical Letters**|Libo Ren et.al.|[2409.09501v1](http://arxiv.org/abs/2409.09501v1)|null|
|**2024-09-14**|**From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging**|Maximilian Rokuss et.al.|[2409.09478v1](http://arxiv.org/abs/2409.09478v1)|null|
|**2024-09-14**|**Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking**|Cong Kong et.al.|[2409.10570v1](http://arxiv.org/abs/2409.10570v1)|null|
|**2024-09-14**|**Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation**|Hui Yi Leong et.al.|[2409.09324v1](http://arxiv.org/abs/2409.09324v1)|null|
|**2024-09-14**|**On the limits of agency in agent-based models**|Ayush Chopra et.al.|[2409.10568v1](http://arxiv.org/abs/2409.10568v1)|[link](https://github.com/agenttorch/agenttorch)|
|**2024-09-13**|**Contextual Evaluation of Large Language Models for Classifying Tropical and Infectious Diseases**|Mercy Asiedu et.al.|[2409.09201v1](http://arxiv.org/abs/2409.09201v1)|null|
|**2024-09-13**|**Phikon-v2, A large and public feature extractor for biomarker prediction**|Alexandre Filiot et.al.|[2409.09173v1](http://arxiv.org/abs/2409.09173v1)|null|
|**2024-09-13**|**Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation**|Cheng Charles Ma et.al.|[2409.09135v1](http://arxiv.org/abs/2409.09135v1)|null|
|**2024-09-13**|**MAISI: Medical AI for Synthetic Imaging**|Pengfei Guo et.al.|[2409.11169v1](http://arxiv.org/abs/2409.11169v1)|null|
|**2024-09-13**|**SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**|Paloma Rabaey et.al.|[2409.08936v1](http://arxiv.org/abs/2409.08936v1)|[link](https://github.com/prabaey/synsum)|
|**2024-09-13**|**Recent Trends in Modelling the Continuous Time Series using Deep Learning: A Survey**|Mansura Habiba et.al.|[2409.09106v1](http://arxiv.org/abs/2409.09106v1)|null|
|**2024-09-13**|**A BERT-Based Summarization approach for depression detection**|Hossein Salahshoor Gavalan et.al.|[2409.08483v1](http://arxiv.org/abs/2409.08483v1)|null|
|**2024-09-12**|**Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**|Fuchen Zheng et.al.|[2409.07793v1](http://arxiv.org/abs/2409.07793v1)|[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)|
|**2024-09-12**|**ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**|Fuchen Zheng et.al.|[2409.07779v1](http://arxiv.org/abs/2409.07779v1)|[link](https://github.com/lzeeorno/assnet)|
|**2024-09-11**|**SoK: Security and Privacy Risks of Medical AI**|Yuanhaur Chang et.al.|[2409.07415v1](http://arxiv.org/abs/2409.07415v1)|null|
|**2024-09-11**|**Federated Impression for Learning with Distributed Heterogeneous Data**|Sana Ayromlou et.al.|[2409.07351v1](http://arxiv.org/abs/2409.07351v1)|null|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314v1](http://arxiv.org/abs/2409.07314v1)|null|
|**2024-09-11**|**Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**|Sheng Chen et.al.|[2409.07186v2](http://arxiv.org/abs/2409.07186v2)|null|
|**2024-09-11**|**CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**|Feiyang Jia et.al.|[2409.07092v1](http://arxiv.org/abs/2409.07092v1)|null|
|**2024-09-11**|**Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**|Daeun Kyung et.al.|[2409.07012v1](http://arxiv.org/abs/2409.07012v1)|null|
|**2024-09-11**|**Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**|Jianmei Jiang et.al.|[2409.06928v1](http://arxiv.org/abs/2409.06928v1)|[link](https://github.com/jjm1589/dstct)|
|**2024-09-10**|**Bifurcation Identification for Ultrasound-driven Robotic Cannulation**|Cecilia G. Morales et.al.|[2409.06817v1](http://arxiv.org/abs/2409.06817v1)|null|
|**2024-09-10**|**Personalized Federated Learning Techniques: Empirical Analysis**|Azal Ahmad Khan et.al.|[2409.06805v1](http://arxiv.org/abs/2409.06805v1)|null|
|**2024-09-10**|**Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**|Cristian Trout et.al.|[2409.06672v1](http://arxiv.org/abs/2409.06672v1)|null|
|**2024-09-10**|**EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**|Danli Shi et.al.|[2409.06644v2](http://arxiv.org/abs/2409.06644v2)|null|
|**2024-09-10**|**Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**|Zoe Hancox et.al.|[2409.06585v1](http://arxiv.org/abs/2409.06585v1)|[link](https://github.com/zoehancox/sparse_tgcnn)|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351v1](http://arxiv.org/abs/2409.06351v1)|null|
|**2024-09-10**|**Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**|Xin Zhang et.al.|[2409.06209v1](http://arxiv.org/abs/2409.06209v1)|[link](https://github.com/xinz0419/unisurv)|
|**2024-09-10**|**Can Large Language Models Unlock Novel Scientific Research Ideas?**|Sandeep Kumar et.al.|[2409.06185v1](http://arxiv.org/abs/2409.06185v1)|null|
|**2024-09-10**|**Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**|Georgios Chochlakis et.al.|[2409.06173v2](http://arxiv.org/abs/2409.06173v2)|[link](https://github.com/gchochla/cot-priors)|
|**2024-09-10**|**Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**|Dong Han et.al.|[2409.06147v1](http://arxiv.org/abs/2409.06147v1)|null|
|**2024-09-09**|**ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**|Zhaoyue Sun et.al.|[2409.05592v1](http://arxiv.org/abs/2409.05592v1)|null|
|**2024-09-09**|**Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**|Camilo Thorne et.al.|[2409.05486v2](http://arxiv.org/abs/2409.05486v2)|null|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-09**|**Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**|Javad Hassannataj Joloudari et.al.|[2409.07493v1](http://arxiv.org/abs/2409.07493v1)|null|
|**2024-09-09**|**Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**|Nirmalya Thakur et.al.|[2409.05292v2](http://arxiv.org/abs/2409.05292v2)|null|
|**2024-09-09**|**RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**|Quoc-Bao Nguyen-Le et.al.|[2409.05280v1](http://arxiv.org/abs/2409.05280v1)|[link](https://github.com/kyle-paul/RotCAtt-TransUNet-plusplus)|
|**2024-09-07**|**Activation Function Optimization Scheme for Image Classification**|Abdur Rahman et.al.|[2409.04915v1](http://arxiv.org/abs/2409.04915v1)|[link](https://github.com/abdurrahman1828/afos)|
|**2024-09-07**|**LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**|Yongxin Deng et.al.|[2409.04744v1](http://arxiv.org/abs/2409.04744v1)|null|
|**2024-09-07**|**NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**|Debaditya Shome et.al.|[2409.04723v1](http://arxiv.org/abs/2409.04723v1)|null|
|**2024-09-07**|**A Comprehensive Survey on Evidential Deep Learning and Its Applications**|Junyu Gao et.al.|[2409.04720v1](http://arxiv.org/abs/2409.04720v1)|[link](https://github.com/mengyuanchen21/awesome-evidential-deep-learning)|
|**2024-09-07**|**A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**|Cheng Wan et.al.|[2409.04704v1](http://arxiv.org/abs/2409.04704v1)|null|
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**|Yizhen Zheng et.al.|[2409.04481v1](http://arxiv.org/abs/2409.04481v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-PÃ©rez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v2](http://arxiv.org/abs/2409.02681v2)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**EEG-Language Modeling for Pathology Detection**|Sam Gijsen et.al.|[2409.07480v1](http://arxiv.org/abs/2409.07480v1)|null|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|[link](https://github.com/scslabisu/aggym)|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|

#### Abstracts
##### **Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**
2409.11375v1 by Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi

In the medical domain, acquiring large datasets poses significant challenges
due to privacy concerns. Nonetheless, the development of a robust deep-learning
model for retinal disease diagnosis necessitates a substantial dataset for
training. The capacity to generalize effectively on smaller datasets remains a
persistent challenge. The scarcity of data presents a significant barrier to
the practical implementation of scalable medical AI solutions. To address this
issue, we've combined a wide range of data sources to improve performance and
generalization to new data by giving it a deeper understanding of the data
representation from multi-modal datasets and developed a self-supervised
framework based on large language models (LLMs), SwinV2 to gain a deeper
understanding of multi-modal dataset representations, enhancing the model's
ability to extrapolate to new data for the detection of eye diseases using
optical coherence tomography (OCT) images. We adopt a two-phase training
methodology, self-supervised pre-training, and fine-tuning on a downstream
supervised classifier. An ablation study conducted across three datasets
employing various encoder backbones, without data fusion, with low data
availability setting, and without self-supervised pre-training scenarios,
highlights the robustness of our method. Our findings demonstrate consistent
performance across these diverse conditions, showcasing superior generalization
capabilities compared to the baseline model, ResNet-50.

æè¦ï¼å¨é«çé åï¼ç±æ¼é±ç§åé¡ï¼ç²åå¤§åè³æéæé æéå¤§ææ°ãåç®¡å¦æ­¤ï¼å°æ¼è¦ç¶²èç¾çè¨ºæ·çå¼·å¥æ·±åº¦å­¸ç¿æ¨¡åçéç¼éè¦ä¸åé¾å¤§çè³æéé²è¡è¨ç·´ãå°è¼å°çè³æéé²è¡æææ¦æ¬çè½åä»ç¶æ¯ä¸åæçºçææ°ãè³æçç¨ç¼ºæ§å°å¯æ´åé«ç AI è§£æ±ºæ¹æ¡çå¯¦éå¯¦æ½æ§æéå¤§éç¤ãçºäºè§£æ±ºæ­¤åé¡ï¼æåçµåäºåç¨®è³æä¾æºï¼ééè®å¶æ´æ·±å¥å°äºè§£å¤æ¨¡å¼è³æéçè³æè¡¨ç¤ºï¼ä¾æ¹åæè½åå°æ°è³æçæ¦æ¬æ§ï¼ä¸¦éç¼äºä¸ååºæ¼å¤§åèªè¨æ¨¡å (LLM) çèªç£ç£æ¡æ¶ï¼SwinV2ï¼ä»¥æ´æ·±å¥å°äºè§£å¤æ¨¡å¼è³æéè¡¨ç¤ºï¼å¢å¼·æ¨¡åæ¨æ·æ°è³æçè½åï¼ä»¥ä½¿ç¨åå­¸ç¸å¹²æ·å±¤ææ (OCT) å½±ååµæ¸¬ç¼ç¾ãæåæ¡ç¨å©éæ®µè¨ç·´æ¹æ³ï¼èªç£ç£é è¨ç·´åå°ä¸æ¸¸ç£ç£åé¡å¨é²è¡å¾®èª¿ãå¨ä¸åè³æéä¸é²è¡çæ¶èç ç©¶ï¼æ¡ç¨åç¨®ç·¨ç¢¼ä¸»å¹¹ï¼æ²æè³æèåï¼å¨è³æå¯ç¨æ§è¨­å®è¼ä½çææ³ä¸ï¼ä»¥åæ²æèªç£ç£é è¨ç·´å ´æ¯ï¼çªåºäºæåæ¹æ³çç©©å¥æ§ãæåçç ç©¶çµæè­æäºå¨éäºä¸åæ¢ä»¶ä¸çä¸è´æè½ï¼èåºæºæ¨¡å ResNet-50 ç¸æ¯ï¼å±ç¤ºäºåè¶çæ¦æ¬è½åã

##### **Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**
2409.11350v1 by Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng

Machine-learning (ML) models in flow cytometry have the potential to reduce
error rates, increase reproducibility, and boost the efficiency of clinical
labs. While numerous ML models for flow cytometry data have been proposed, few
studies have described the clinical deployment of such models. Realizing the
potential gains of ML models in clinical labs requires not only an accurate
model, but infrastructure for automated inference, error detection, analytics
and monitoring, and structured data extraction. Here, we describe an ML model
for detection of Acute Myeloid Leukemia (AML), along with the infrastructure
supporting clinical implementation. Our infrastructure leverages the resilience
and scalability of the cloud for model inference, a Kubernetes-based workflow
system that provides model reproducibility and resource management, and a
system for extracting structured diagnoses from full-text reports. We also
describe our model monitoring and visualization platform, an essential element
for ensuring continued model accuracy. Finally, we present a post-deployment
analysis of impacts on turn-around time and compare production accuracy to the
original validation statistics.

æè¦ï¼æ©å¨å­¸ç¿ (ML) æ¨¡åå¨æµå¼ç´°èè¡ä¸­å·æéä½é¯èª¤çãæé«å¯éç¾æ§åæåè¨åºå¯¦é©å®¤æççæ½åãéç¶å·²ç¶æåºè¨±å¤ç¨æ¼æµå¼ç´°èè¡æ¸æç ML æ¨¡åï¼ä½å¾å°æç ç©¶æè¿°æ­¤é¡æ¨¡åçè¨åºé¨ç½²ãè¦å¯¦ç¾ ML æ¨¡åå¨è¨åºå¯¦é©å®¤ä¸­çæ½å¨æ¶çï¼ä¸åéè¦æºç¢ºçæ¨¡åï¼ééè¦ç¨æ¼èªåæ¨çãé¯èª¤æª¢æ¸¬ãåæåç£æ§ä»¥åçµæ§åæ¸ææåçåºç¤è¨­æ½ãå¨éè£¡ï¼æåæè¿°äºä¸åç¨æ¼æª¢æ¸¬æ¥æ§é«æ§ç½è¡ç (AML) ç ML æ¨¡åï¼ä»¥åæ¯æè¨åºå¯¦æ½çåºç¤è¨­æ½ãæåçåºç¤è¨­æ½å©ç¨é²ç«¯çå¾©åååå¯æ´åæ§é²è¡æ¨¡åæ¨çï¼ä¸ååºæ¼ Kubernetes çå·¥ä½æµç¨ç³»çµ±æä¾æ¨¡åå¯éç¾æ§åè³æºç®¡çï¼ä»¥åä¸åå¾å¨æå ±åä¸­æåçµæ§åè¨ºæ·çç³»çµ±ãæåéæè¿°äºæåçæ¨¡åç£æ§åè¦è¦ºåå¹³å°ï¼éæ¯ç¢ºä¿æçºæ¨¡åæºç¢ºæ§çåºæ¬è¦ç´ ãæå¾ï¼æåæåºäºå°å¨è½æéå½±é¿çé¨ç½²å¾åæï¼ä¸¦å°çç¢æºç¢ºåº¦èåå§é©è­çµ±è¨æ¸æé²è¡æ¯è¼ã

##### **TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**
2409.11299v1 by Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun

Biomedical image segmentation is crucial for accurately diagnosing and
analyzing various diseases. However, Convolutional Neural Networks (CNNs) and
Transformers, the most commonly used architectures for this task, struggle to
effectively capture long-range dependencies due to the inherent locality of
CNNs and the computational complexity of Transformers. To address this
limitation, we introduce TTT-Unet, a novel framework that integrates Test-Time
Training (TTT) layers into the traditional U-Net architecture for biomedical
image segmentation. TTT-Unet dynamically adjusts model parameters during the
testing time, enhancing the model's ability to capture both local and
long-range features. We evaluate TTT-Unet on multiple medical imaging datasets,
including 3D abdominal organ segmentation in CT and MR images, instrument
segmentation in endoscopy images, and cell segmentation in microscopy images.
The results demonstrate that TTT-Unet consistently outperforms state-of-the-art
CNN-based and Transformer-based segmentation models across all tasks. The code
is available at https://github.com/rongzhou7/TTT-Unet.

æè¦ï¼çç©é«å­¸å½±ååå²å°æ¼æºç¢ºè¨ºæ·ååæåç¨®ç¾çè³ééè¦ãç¶èï¼å·ç©ç¥ç¶ç¶²è·¯ï¼CNNï¼å Transformerï¼ä½çºæ­¤ä»»åæå¸¸ç¨çæ¶æ§ï¼ç±æ¼ CNN çåºæå±é¨æ§å Transformer çè¨ç®è¤éæ§ï¼é£ä»¥æææ·åé·ç¨ä¾è³´æ§ãçºäºè§£æ±ºéåéå¶ï¼æåå¼å¥äº TTT-Unetï¼ä¸ååµæ°çæ¡æ¶ï¼å°æ¸¬è©¦æéè¨ç·´ï¼TTTï¼å±¤æ´åå°å³çµ±ç U-Net æ¶æ§ä¸­ï¼ç¨æ¼çç©é«å­¸å½±ååå²ãTTT-Unet å¨æ¸¬è©¦æéåæèª¿æ´æ¨¡ååæ¸ï¼å¢å¼·æ¨¡åæ·åå±é¨åé·ç¨ç¹å¾µçè½åãæåå¨å¤åé«å­¸å½±åè³æéä¸è©ä¼° TTT-Unetï¼åæ¬é»è¦æ·å±¤ææåç£æ¯é å½±ä¸­ç 3D è¹èå¨å®åå²ãå§è¦é¡å½±åä¸­çåå¨åå²ä»¥åé¡¯å¾®é¡å½±åä¸­çç´°èåå²ãçµæè¡¨æï¼TTT-Unet å¨ææä»»åä¸­é½æçºåªæ¼æåé²çåºæ¼ CNN ååºæ¼ Transformer çåå²æ¨¡åãç¨å¼ç¢¼å¯å¨ https://github.com/rongzhou7/TTT-Unet åå¾ã

##### **EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**
2409.11295v1 by Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun

Generalist web agents have evolved rapidly and demonstrated remarkable
potential. However, there are unprecedented safety risks associated with these
them, which are nearly unexplored so far. In this work, we aim to narrow this
gap by conducting the first study on the privacy risks of generalist web agents
in adversarial environments. First, we present a threat model that discusses
the adversarial targets, constraints, and attack scenarios. Particularly, we
consider two types of adversarial targets: stealing users' specific personally
identifiable information (PII) or stealing the entire user request. To achieve
these objectives, we propose a novel attack method, termed Environmental
Injection Attack (EIA). This attack injects malicious content designed to adapt
well to different environments where the agents operate, causing them to
perform unintended actions. This work instantiates EIA specifically for the
privacy scenario. It inserts malicious web elements alongside persuasive
instructions that mislead web agents into leaking private information, and can
further leverage CSS and JavaScript features to remain stealthy. We collect 177
actions steps that involve diverse PII categories on realistic websites from
the Mind2Web dataset, and conduct extensive experiments using one of the most
capable generalist web agent frameworks to date, SeeAct. The results
demonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.
Stealing full user requests is more challenging, but a relaxed version of EIA
can still achieve 16% ASR. Despite these concerning results, it is important to
note that the attack can still be detectable through careful human inspection,
highlighting a trade-off between high autonomy and security. This leads to our
detailed discussion on the efficacy of EIA under different levels of human
supervision as well as implications on defenses for generalist web agents.

æè¦ï¼<paragraph>éç¨ç¶²è·¯ä»£çå¿«éæ¼åï¼å±ç¾åºé©äººçæ½åãç¶èï¼éäºä»£çä¼´é¨èåææªæçå®å¨é¢¨éªï¼èéäºé¢¨éªç®åå¹¾ä¹å°æªè¢«æ¢ç´¢ãå¨éé å·¥ä½ä¸­ï¼æåæ¨å¨ééå·è¡éç¨ç¶²è·¯ä»£çå¨å°æç°å¢ä¸­çé±ç§é¢¨éªçç¬¬ä¸åç ç©¶ä¾ç¸®å°éåå·®è·ãé¦åï¼æåæåºä¸åå¨èæ¨¡åï¼è¨è«å°æç®æ¨ãéå¶åæ»ææå¢ãç¹å¥æ¯ï¼æåèæ®å©ç¨®é¡åçå°æç®æ¨ï¼ç«åä½¿ç¨èçç¹å®åäººå¯è­å¥è³è¨ (PII) æç«åæ´åä½¿ç¨èè¦æ±ãçºäºéæéäºç®æ¨ï¼æåæåºäºä¸ç¨®æ°ç©çæ»ææ¹æ³ï¼ç¨±çºç°å¢æ³¨å¥æ»æ (EIA)ãæ­¤æ»ææ³¨å¥æ¡æå§å®¹ï¼æ¨å¨é©æä»£çéä½çä¸åç°å¢ï¼å°è´ä»£çå·è¡éé æçåä½ãéé å·¥ä½ç¹å¥éå°é±ç§æå¢å¯¦ä¾å EIAãå®å¨å·æèªªæåçæä»¤ææå¥æ¡æç¶²è·¯åç´ ï¼èª¤å°ç¶²è·¯ä»£çæ´©é²ç§äººè³è¨ï¼ä¸¦å¯é²ä¸æ­¥å©ç¨ CSS å JavaScript åè½ä¿æé±å¯ãæåå¾ Mind2Web è³æéçç¾å¯¦ç¶²ç«æ¶éäº 177 åæ¶åä¸å PII é¡å¥çåä½æ­¥é©ï¼ä¸¦ä½¿ç¨è¿ä»çºæ­¢åè½æå¼·å¤§çéç¨ç¶²è·¯ä»£çæ¡æ¶ SeeAct é²è¡å»£æ³çå¯¦é©ãçµæè­æï¼EIA å¨ç«åä½¿ç¨èçç¹å® PII æ¹é¢éå°äº 70% ç ASRãç«åå®æ´çä½¿ç¨èè¦æ±æ´å·ææ°æ§ï¼ä½ EIA çæ¾å¯¬çæ¬ä»å¯éå° 16% ç ASRãåç®¡æéäºä»¤äººææççµæï¼ä½éè¦çæ¯è¦æ³¨æï¼æ»æä»ç¶å¯ä»¥ééä»ç´°çäººå·¥æª¢æ¥ä¾åµæ¸¬ï¼çªé¡¯äºé«åº¦èªä¸»æ§èå®å¨æ§ä¹éçæ¬è¡¡ãéå°è´æåè©³ç´°è¨è«äº EIA å¨ä¸åå±¤ç´çäººå·¥ç£ç£ä¸çæè½ï¼ä»¥åå°éç¨ç¶²è·¯ä»£çé²ç¦¦çå½±é¿ã</paragraph>

##### **Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**
2409.11192v1 by Eunhae Lee

One application area of long-term memory (LTM) capabilities with increasing
traction is personal AI companions and assistants. With the ability to retain
and contextualize past interactions and adapt to user preferences, personal AI
companions and assistants promise a profound shift in how we interact with AI
and are on track to become indispensable in personal and professional settings.
However, this advancement introduces new challenges and vulnerabilities that
require careful consideration regarding the deployment and widespread use of
these systems. The goal of this paper is to explore the broader implications of
building and deploying personal AI applications with LTM capabilities using a
holistic evaluation approach. This will be done in three ways: 1) reviewing the
technological underpinnings of LTM in Large Language Models, 2) surveying
current personal AI companions and assistants, and 3) analyzing critical
considerations and implications of deploying and using these applications.

æè¦ï¼é·æè¨æ¶ (LTM) è½åçæç¨é åä¹ä¸æ¯åäºº AI ä¼´ä¾¶åå©çï¼å¶å¸å¼åæ­£èæ¥ä¿±å¢ãåäºº AI ä¼´ä¾¶åå©çå·åä¿çåå°éå»äºåèçµ¡åï¼ä»¥åé©æä½¿ç¨èåå¥½çè½åï¼æ¿è«¾å°å¾¹åºæ¹è®æåè AI äºåçæ¹å¼ï¼ä¸¦ææå¨åäººåå°æ¥­é åä¸­è®å¾ä¸å¯æç¼ºãç¶èï¼éé é²å±å¸¶ä¾äºæ°çææ°åæ¼æ´ï¼éè¦ä»ç´°èééäºç³»çµ±çé¨ç½²åå»£æ³ä½¿ç¨ãæ¬æçç®æ¨æ¯å©ç¨æ´é«è©ä¼°æ¹æ³æ¢è¨å»ºæ§åé¨ç½²å·å LTM è½åçåäºº AI æç¨ç¨å¼çå»£æ³å½±é¿ãéå°ééä¸ç¨®æ¹å¼é²è¡ï¼1) æª¢è¦å¤§åèªè¨æ¨¡åä¸­ LTM çæè¡åºç¤ï¼2) èª¿æ¥ç®åçåäºº AI ä¼´ä¾¶åå©çï¼ä»¥å 3) åæé¨ç½²åä½¿ç¨éäºæç¨ç¨å¼çééµèéåå½±é¿ã

##### **Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**
2409.10932v1 by Mehroush Banday, Sherin Zafar, Parul Agarwal, M Afshar Alam, Abubeker K M

Coronary heart disease (CHD) is a severe cardiac disease, and hence, its
early diagnosis is essential as it improves treatment results and saves money
on medical care. The prevailing development of quantum computing and machine
learning (ML) technologies may bring practical improvement to the performance
of CHD diagnosis. Quantum machine learning (QML) is receiving tremendous
interest in various disciplines due to its higher performance and capabilities.
A quantum leap in the healthcare industry will increase processing power and
optimise multiple models. Techniques for QML have the potential to forecast
cardiac disease and help in early detection. To predict the risk of coronary
heart disease, a hybrid approach utilizing an ensemble machine learning model
based on QML classifiers is presented in this paper. Our approach, with its
unique ability to address multidimensional healthcare data, reassures the
method's robustness by fusing quantum and classical ML algorithms in a
multi-step inferential framework. The marked rise in heart disease and death
rates impacts worldwide human health and the global economy. Reducing cardiac
morbidity and mortality requires early detection of heart disease. In this
research, a hybrid approach utilizes techniques with quantum computing
capabilities to tackle complex problems that are not amenable to conventional
machine learning algorithms and to minimize computational expenses. The
proposed method has been developed in the Raspberry Pi 5 Graphics Processing
Unit (GPU) platform and tested on a broad dataset that integrates clinical and
imaging data from patients suffering from CHD and healthy controls. Compared to
classical machine learning models, the accuracy, sensitivity, F1 score, and
specificity of the proposed hybrid QML model used with CHD are manifold higher.

æè¦ï¼å çåèå¿èç (CHD) æ¯ä¸ç¨®å´éçç¾çï¼å æ­¤ï¼æ©æè¨ºæ·è³ééè¦ï¼å çºå®å¯ä»¥æ¹åæ²»ççµæä¸¦ç¯çé«çä¿å¥è²»ç¨ãéå­è¨ç®åæ©å¨å­¸ç¿ (ML) æè¡ççè¡ç¼å±å¯è½æå° CHD è¨ºæ·çæ§è½å¸¶ä¾å¯¦éæ¹åãéå­æ©å¨å­¸ç¿ (QML) ç±æ¼å¶æ´é«çæ§è½åè½åï¼å¨ååé åå¼èµ·äºæ¥µå¤§çèè¶£ãé«çä¿å¥è¡æ¥­çéå­é£èºå°å¢å èçè½åä¸¦åªåå¤åæ¨¡åãQML çæè¡ææ½åé æ¸¬å¿èçä¸¦å¹«å©æ©æç¼ç¾ãçºäºé æ¸¬å çåèå¿èççé¢¨éªï¼æ¬ææåºäºä¸ç¨®åºæ¼ QML åé¡å¨çæ··åæ©å¨å­¸ç¿æ¨¡åçæ··åæ¹æ³ãæåçéç¨®æ¹æ³å·åèçå¤ç¶­é«çä¿å¥æ¸æçç¨ç¹è½åï¼ééå¨å¤æ­¥é©æ¨çæ¡æ¶ä¸­èåéå­åç¶å¸ ML æ¼ç®æ³ï¼ç¢ºä¿äºè©²æ¹æ³çç©©å¥æ§ãå¿èçåæ­»äº¡ççé¡¯èä¸åå½±é¿äºå¨çäººé¡å¥åº·åå¨çç¶æ¿ãéä½å¿èç¼ççåæ­»äº¡çéè¦å°å¿èçé²è¡æ©æç¼ç¾ãå¨éé ç ç©¶ä¸­ï¼ä¸ç¨®æ··åæ¹æ³å©ç¨å·æéå­è¨ç®è½åçæè¡ä¾è§£æ±ºå³çµ±æ©å¨å­¸ç¿æ¼ç®æ³ç¡æ³è§£æ±ºçè¤éåé¡ï¼ä¸¦æå¤§ç¨åº¦å°æ¸å°è¨ç®éé·ãææåºçæ¹æ³å·²å¨ Raspberry Pi 5 ç¹ªåèçå®å (GPU) å¹³èºä¸éç¼ï¼ä¸¦å¨ä¸åå»£æ³çè³æéä¸é²è¡äºæ¸¬è©¦ï¼è©²è³æéæ´åäºæ£æ CHD åå¥åº·å°ç§èçè¨åºåå½±åæ¸æãèç¶å¸æ©å¨å­¸ç¿æ¨¡åç¸æ¯ï¼ææåºçæ··å QML æ¨¡åè CHD ä¸èµ·ä½¿ç¨çæºç¢ºæ§ãæææ§ãF1 åæ¸åç¹ç°æ§æ´é«ã

##### **Self-supervised Speech Models for Word-Level Stuttered Speech Detection**
2409.10704v1 by Yi-Jen Shih, Zoi Gkalitsiou, Alexandros G. Dimakis, David Harwath

Clinical diagnosis of stuttering requires an assessment by a licensed
speech-language pathologist. However, this process is time-consuming and
requires clinicians with training and experience in stuttering and fluency
disorders. Unfortunately, only a small percentage of speech-language
pathologists report being comfortable working with individuals who stutter,
which is inadequate to accommodate for the 80 million individuals who stutter
worldwide. Developing machine learning models for detecting stuttered speech
would enable universal and automated screening for stuttering, enabling speech
pathologists to identify and follow up with patients who are most likely to be
diagnosed with a stuttering speech disorder. Previous research in this area has
predominantly focused on utterance-level detection, which is not sufficient for
clinical settings where word-level annotation of stuttering is the norm. In
this study, we curated a stuttered speech dataset with word-level annotations
and introduced a word-level stuttering speech detection model leveraging
self-supervised speech models. Our evaluation demonstrates that our model
surpasses previous approaches in word-level stuttering speech detection.
Additionally, we conducted an extensive ablation analysis of our method,
providing insight into the most important aspects of adapting self-supervised
speech models for stuttered speech detection.

æè¦ï¼è¨åºå£åè¨ºæ·éè¦ç±å·ç§èªè¨ççå­¸å®¶è©ä¼°ãç¶èï¼éåéç¨å¾èæï¼éè¦åéå£ååæµå©éç¤è¨ç·´åç¶é©çè¨åºé«çãä¸å¹¸çæ¯ï¼åªæä¸å°é¨åèªè¨ççå­¸å®¶è¡¨ç¤ºé¡æèå£åèåä½ï¼éä¸è¶³ä»¥å®¹ç´å¨ç 8000 è¬åå£åèãéç¼ç¨æ¼æª¢æ¸¬å£åèªé³çæ©å¨å­¸ç¿æ¨¡åå°è½å°å£åé²è¡æ®éä¸èªååçç¯©æ¥ï¼ä½¿èªè¨ççå­¸å®¶è½å¤ è­å¥ä¸¦è¿½è¹¤ææå¯è½è¢«è¨ºæ·åºæ£æå£åè¨èªéç¤çæ£èãéæ¹é¢çååç ç©¶ä¸»è¦éä¸­æ¼è©±èªå±¤ç´çæª¢æ¸¬ï¼éä¸è¶³ä»¥ç¨æ¼å£åçå®å­å±¤ç´è¨»è§£çºå¸¸æçè¨åºç°å¢ãå¨æ¬ç ç©¶ä¸­ï¼æåç­åäºä¸åå¸¶æå®å­å±¤ç´è¨»è§£çå£åèªé³è³æéï¼ä¸¦å¼å¥äºä¸åå©ç¨èªæç£ç£èªé³æ¨¡åçå®å­å±¤ç´å£åèªé³æª¢æ¸¬æ¨¡åãæåçè©ä¼°è­æï¼æåçæ¨¡åå¨å®å­å±¤ç´å£åèªé³æª¢æ¸¬ä¸­åªæ¼ååçåæ³ãæ­¤å¤ï¼æåå°æåçæ¹æ³é²è¡äºå»£æ³çæ¶èåæï¼æä¾äºå°èª¿æ´èªæç£ç£èªé³æ¨¡åä»¥é²è¡å£åèªé³æª¢æ¸¬çæéè¦é¢åçè¦è§£ã

##### **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**
2409.10403v1 by Zhang Zheng

This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.

æè¦ï¼æ¬ææåºäºä¸ç§åºäºæç¤ºå­¦ä¹ æ¡æ¶çç¥è¯å¢å¼ºç¾çè¯æ­æ¹æ³ãè¯¥æ¹æ³ä»ä¸ä¸´åºçä¾ç¸å³çå¤é¨ç¥è¯å¾è°±ä¸­æ£ç´¢ç»æåç¥è¯ï¼å¯¹å¶è¿è¡ç¼ç ï¼å¹¶å°å¶æ³¨å¥å°æç¤ºæ¨¡æ¿ä¸­ï¼ä»¥å¢å¼ºè¯­è¨æ¨¡åå¯¹ä»»å¡ççè§£åæ¨çè½åãæä»¬å¨ä¸ä¸ªå¬å±æ°æ®éä¸è¿è¡äºå®éªï¼CHIP-CTCãIMCS-V2-NER å KUAKE-QTRãç»æè¡¨æï¼ææåºçæ¹æ³å¨å¤ä¸ªè¯ä¼°ææ ä¸ææ¾ä¼äºç°ææ¨¡åï¼å¨ CHIP-CTC æ°æ®éä¸ç F1 å¾åæé«äº 2.4%ï¼å¨ IMCS-V2-NER æ°æ®éä¸æé«äº 3.1%ï¼å¨ KUAKE-QTR æ°æ®éä¸æé«äº 4.2%ãæ­¤å¤ï¼æ¶èç ç©¶è¯å®äºç¥è¯æ³¨å¥æ¨¡åçå³é®ä½ç¨ï¼å ä¸ºç§»é¤æ­¤æ¨¡åä¼å¯¼è´ F1 å¾åæ¾çä¸éãå®éªç»æè¡¨æï¼ææåºçæ¹æ³ä¸ä»æææé«äºç¾çè¯æ­çåç¡®æ§ï¼èä¸å¢å¼ºäºé¢æµçå¯è§£éæ§ï¼ä¸ºä¸´åºè¯æ­æä¾äºæ´å¯é çæ¯æåè¯æ®ã

##### **Robust image representations with counterfactual contrastive learning**
2409.10365v1 by MÃ©lanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker

Contrastive pretraining can substantially increase model generalisation and
downstream performance. However, the quality of the learned representations is
highly dependent on the data augmentation strategy applied to generate positive
pairs. Positive contrastive pairs should preserve semantic meaning while
discarding unwanted variations related to the data acquisition domain.
Traditional contrastive pipelines attempt to simulate domain shifts through
pre-defined generic image transformations. However, these do not always mimic
realistic and relevant domain variations for medical imaging such as scanner
differences. To tackle this issue, we herein introduce counterfactual
contrastive learning, a novel framework leveraging recent advances in causal
image synthesis to create contrastive positive pairs that faithfully capture
relevant domain variations. Our method, evaluated across five datasets
encompassing both chest radiography and mammography data, for two established
contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive
learning in terms of robustness to acquisition shift. Notably, counterfactual
contrastive learning achieves superior downstream performance on both
in-distribution and on external datasets, especially for images acquired with
scanners under-represented in the training set. Further experiments show that
the proposed framework extends beyond acquisition shifts, with models trained
with counterfactual contrastive learning substantially improving subgroup
performance across biological sex.

æè¦ï¼å°æ¯é è¨ç·´å¯ä»¥å¤§å¹æåæ¨¡åçæ³åè½ååä¸æ¸¸æè½ãç¶èï¼å­¸ç¿å°çè¡¨å¾µåè³ªé«åº¦ä¾è³´æ¼ç¨ä¾ç¢çæ­£åéå°çè³ææ´åç­ç¥ãæ­£åå°æ¯éå°æç¶ä¿çèªææç¾©ï¼åææ¨æ£èè³ææ·åé åç¸éçä¸å¿è¦è®ç°ãå³çµ±çå°æ¯ç®¡ç·æåè©¦ééé åå®ç¾©çéç¨å½±åè½æä¾æ¨¡æ¬é åè½ç§»ãç¶èï¼éäºè½æä¸¦ä¸ç¸½æ¯è½æ¨¡ä»¿é«çå½±åçå¯¦éä¸ç¸éé åè®ç°ï¼ä¾å¦ææåçå·®ç°ãçºäºè§£æ±ºéååé¡ï¼æåå¨æ­¤æåºåäºå¯¦å°æ¯å­¸ç¿ï¼ä¸åå©ç¨å æå½±ååæè¿æé²å±ä¾å»ºç«å¿ å¯¦ææç¸éé åè®ç°çå°æ¯æ­£åéå°çæ°ç©æ¶æ§ãæåçåæ³å¨æ¶µèè¸é¨ X ååä¹³æ¿æå½±è³æçäºåè³æéä¸é²è¡è©ä¼°ï¼å°æ¼å©åå·²å»ºç«çå°æ¯ç®æ¨ï¼SimCLR å DINO-v2ï¼ï¼å¨å°æ¼æ·åè½ç§»çç©©å¥æ§æ¹é¢åªæ¼æ¨æºå°æ¯å­¸ç¿ãå¼å¾æ³¨æçæ¯ï¼åäºå¯¦å°æ¯å­¸ç¿å¨å§é¨åä½åå¤é¨è³æéä¸é½è½éæåªç°çä¸æ¸¸æè½ï¼ç¹å¥æ¯å°æ¼è¨ç·´éä¸­ä»£è¡¨æ§ä¸è¶³çææåææ·åçå½±åãé²ä¸æ­¥çå¯¦é©é¡¯ç¤ºï¼ææåºçæ¶æ§å»¶ä¼¸å°æ·åè½ç§»ä¹å¤ï¼ä½¿ç¨åäºå¯¦å°æ¯å­¸ç¿è¨ç·´çæ¨¡åå¤§å¹æåäºçç©æ§å¥çå­ç¾¤æè½ã

##### **Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation between the United States and South Africa**
2409.10168v1 by Hayoung Jung, Prerna Juneja, Tanushree Mitra

Despite being an integral tool for finding health-related information online,
YouTube has faced criticism for disseminating COVID-19 misinformation globally
to its users. Yet, prior audit studies have predominantly investigated YouTube
within the Global North contexts, often overlooking the Global South. To
address this gap, we conducted a comprehensive 10-day geolocation-based audit
on YouTube to compare the prevalence of COVID-19 misinformation in search
results between the United States (US) and South Africa (SA), the countries
heavily affected by the pandemic in the Global North and the Global South,
respectively. For each country, we selected 3 geolocations and placed
sock-puppets, or bots emulating "real" users, that collected search results for
48 search queries sorted by 4 search filters for 10 days, yielding a dataset of
915K results. We found that 31.55% of the top-10 search results contained
COVID-19 misinformation. Among the top-10 search results, bots in SA faced
significantly more misinformative search results than their US counterparts.
Overall, our study highlights the contrasting algorithmic behaviors of YouTube
search between two countries, underscoring the need for the platform to
regulate algorithmic behavior consistently across different regions of the
Globe.

æè¦ï¼åç®¡ YouTube æ¯å¨ç¶²è·¯ä¸å°æ¾èå¥åº·ç¸éè³è¨çä¸é éè¦å·¥å·ï¼ä½å®ä¹å çºåå¨çä½¿ç¨èæ£æ­ COVID-19 é¯èª¤è³è¨èåå°æ¹è©ãç¶èï¼ååçç¨½æ ¸ç ç©¶ä¸»è¦å¨å¨çåæ¹çèæ¯ä¸èª¿æ¥ YouTubeï¼å¸¸å¸¸å¿½ç¥äºå¨çåæ¹ãçºäºè§£æ±ºéåå·®è·ï¼æåå¨ YouTube ä¸é²è¡äºä¸é çºæ 10 å¤©çç¶åå°çä½ç½®ç¨½æ ¸ï¼ä»¥æ¯è¼ç¾åï¼ç¾åï¼ååéï¼åéï¼æå°çµæä¸­ COVID-19 é¯èª¤è³è¨ççè¡çï¼éå©ååå®¶åå¥æ¯å¨çåæ¹åå¨çåæ¹ä¸­åç«æå´éå½±é¿çåå®¶ãå°æ¼æ¯ååå®¶ï¼æåé¸æäº 3 åå°çä½ç½®ï¼ä¸¦æ¾ç½®äºæ¨¡æ¬ãçå¯¦ãä½¿ç¨èçè¥ªå­åå¡ææ©å¨äººï¼æ¶éäº 48 åæå°æ¥è©¢çæå°çµæï¼ä¸¦æ ¹æ 4 åæå°ç¯©é¸æ¢ä»¶é²è¡äº 10 å¤©çæåºï¼ç¢çäº 915K ç­çµæçè³æéãæåç¼ç¾ï¼31.55% çå 10 åæå°çµæåå« COVID-19 é¯èª¤è³è¨ãå¨æåå 10 åçæå°çµæä¸­ï¼åéçæ©å¨äººé¢è¨çé¯èª¤è³è¨æå°çµææé¡¯å¤æ¼ç¾åçæ©å¨äººãç¸½é«èè¨ï¼æåçç ç©¶çªåºäº YouTube æå°å¨å©ååå®¶ä¹éå°æ¯çæ¼ç®æ³è¡çºï¼å¼·èª¿äºè©²å¹³å°éè¦å¨å¨çä¸åå°åä¸è´å°è¦ç¯æ¼ç®æ³è¡çºã

##### **DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion**
2409.10080v1 by Yuchen Guo, Ruoxiang Xu, Rongcheng Li, Zhenghao Wu, Weifeng Su

Multi-modality image fusion aims to integrate complementary data information
from different imaging modalities into a single image. Existing methods often
generate either blurry fused images that lose fine-grained semantic information
or unnatural fused images that appear perceptually cropped from the inputs. In
this work, we propose a novel two-phase discriminative autoencoder framework,
termed DAE-Fuse, that generates sharp and natural fused images. In the
adversarial feature extraction phase, we introduce two discriminative blocks
into the encoder-decoder architecture, providing an additional adversarial loss
to better guide feature extraction by reconstructing the source images. While
the two discriminative blocks are adapted in the attention-guided
cross-modality fusion phase to distinguish the structural differences between
the fused output and the source inputs, injecting more naturalness into the
results. Extensive experiments on public infrared-visible, medical image
fusion, and downstream object detection datasets demonstrate our method's
superiority and generalizability in both quantitative and qualitative
evaluations.

æè¦ï¼å¤æ¨¡æå½±åèåæ¨å¨å°ä¾èªä¸åå½±åæ¨¡æçäºè£è³æè³è¨æ´åå°å®ä¸å½±åä¸­ãç¾ææ¹æ³éå¸¸æç¢çæ¨¡ç³çèåå½±åï¼å¤±å»ç´°ç·»çèªæè³è¨ï¼ææ¯ä¸èªç¶çèåå½±åï¼å¨æç¥ä¸çèµ·ä¾åæ¯å¾è¼¸å¥ä¸­è£ååºä¾çãå¨éé å·¥ä½ä¸­ï¼æåæåºä¸åæ°ç©çå©éæ®µå¤å¥å¼èªç·¨ç¢¼å¨æ¡æ¶ï¼ç¨±çº DAE-Fuseï¼å¯ç¢çæ¸æ°ä¸èªç¶çèåå½±åãå¨å°æç¹å¾µæåéæ®µï¼æåå¨ç·¨ç¢¼å¨-è§£ç¢¼å¨æ¶æ§ä¸­å¼å¥å©åå¤å¥å¼åå¡ï¼æä¾é¡å¤çå°ææå¤±ï¼èç±éå»ºåå§å½±åä¾æ´å¥½å°å¼å°ç¹å¾µæåãéç¶å©åå¤å¥å¼åå¡å¨æ³¨æåå¼å°çè·¨æ¨¡æèåéæ®µä¸­é²è¡èª¿æ´ï¼ä»¥ååèåè¼¸åºèåå§è¼¸å¥ä¹éççµæ§å·®ç°ï¼çºçµææ³¨å¥æ´å¤èªç¶æ§ãéå°å¬éç´å¤ç·å¯è¦åãé«å­¸å½±åèååä¸æ¸¸ç©ä»¶åµæ¸¬è³æéé²è¡çå»£æ³å¯¦é©è­æäºæåçæ¹æ³å¨éååå®æ§è©ä¼°ä¸­çåªè¶æ§åæ³åæ§ã

##### **MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM**
2409.10064v1 by Sijie Ji, Xinzhe Zheng, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava

Mental health disorders are among the most prevalent diseases worldwide,
affecting nearly one in four people. Despite their widespread impact, the
intervention rate remains below 25%, largely due to the significant cooperation
required from patients for both diagnosis and intervention. The core issue
behind this low treatment rate is stigma, which discourages over half of those
affected from seeking help. This paper presents MindGuard, an accessible,
stigma-free, and professional mobile mental healthcare system designed to
provide mental health first aid. The heart of MindGuard is an innovative edge
LLM, equipped with professional mental health knowledge, that seamlessly
integrates objective mobile sensor data with subjective Ecological Momentary
Assessment records to deliver personalized screening and intervention
conversations. We conduct a broad evaluation of MindGuard using open datasets
spanning four years and real-world deployment across various mobile devices
involving 20 subjects for two weeks. Remarkably, MindGuard achieves results
comparable to GPT-4 and outperforms its counterpart with more than 10 times the
model size. We believe that MindGuard paves the way for mobile LLM
applications, potentially revolutionizing mental healthcare practices by
substituting self-reporting and intervention conversations with passive,
integrated monitoring within daily life, thus ensuring accessible and
stigma-free mental health support.

æè¦ï¼å¿çå¥åº·ç¾çæ¯å¨çææ®éçç¾çä¹ä¸ï¼
å½±é¿äºè¿ååä¹ä¸çäººãåç®¡å¶å½±é¿å»£æ³ï¼
ä½ä»å¥çä»ä½æ¼ 25%ï¼å¾å¤§ç¨åº¦ä¸æ¯å çº
æ£èå¨è¨ºæ·åä»å¥æéè¦å¤§ééåãèå¾å°è´
æ²»ççä½ä¸çæ ¸å¿åé¡æ¯æ±¡ååï¼éè®è¶éä¸åç
åå½±é¿èä¸é¡æå°æ±å¹«å©ãæ¬ææåºäº MindGuardï¼ä¸å
ææ¼åå¾ãç¡æ±¡ååä¸å°æ¥­çææ©å¿çä¿å¥ç³»çµ±ï¼æ¨å¨
æä¾å¿çæ¥æãMindGuard çæ ¸å¿æ¯ä¸ååµæ°çéç·£
å¤§åèªè¨æ¨¡å (LLM)ï¼å·åå°æ¥­çå¿çå¥åº·ç¥è­ï¼å®è½ç¡ç¸«
æ´åå®¢è§çææ©ææ¸¬å¨è³æèä¸»è§ççæç¬æè©ä¼°è¨éï¼æä¾
åäººåçç¯©æª¢åä»å¥å°è©±ãæåä½¿ç¨æ©«è·¨åå¹´çéæ¾è³æé
å° MindGuard é²è¡å»£æ³çè©ä¼°ï¼ä¸¦å¨åç¨®è¡åè£ç½®ä¸é²è¡çºæ
å©é±ãæ¶å 20 ä½åè©¦èçå¯¦éé¨ç½²ãå¼å¾æ³¨æçæ¯ï¼MindGuard
éå°ççµæè GPT-4 ç¸ç¶ï¼ä¸¦ä¸åªæ¼æ¨¡åè¦æ¨¡å¤§æ¼å¶ 10 åä»¥ä¸ç
å°ææ¨¡åãæåç¸ä¿¡ MindGuard çºè¡å LLM æç¨éªå¹³äºéè·¯ï¼
æå¯è½ééå¨æ¥å¸¸çæ´»ä¸­é²è¡è¢«åãæ´åçç£æ§ä¾åä»£èªæå ±å
åä»å¥å°è©±ï¼å¾èå¾¹åºæ¹è®å¿çä¿å¥å¯¦åï¼é²èç¢ºä¿å¯åå¾ä¸
ç¡æ±¡ååçç²¾ç¥å¥åº·æ¯æã

##### **HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making**
2409.10011v1 by Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng

Large language models (LLMs) have significantly advanced natural language
processing tasks, yet they are susceptible to generating inaccurate or
unreliable responses, a phenomenon known as hallucination. In critical domains
such as health and medicine, these hallucinations can pose serious risks. This
paper introduces HALO, a novel framework designed to enhance the accuracy and
reliability of medical question-answering (QA) systems by focusing on the
detection and mitigation of hallucinations. Our approach generates multiple
variations of a given query using LLMs and retrieves relevant information from
external open knowledge bases to enrich the context. We utilize maximum
marginal relevance scoring to prioritize the retrieved context, which is then
provided to LLMs for answer generation, thereby reducing the risk of
hallucinations. The integration of LangChain further streamlines this process,
resulting in a notable and robust increase in the accuracy of both open-source
and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%
to 70%). This framework underscores the critical importance of addressing
hallucinations in medical QA systems, ultimately improving clinical
decision-making and patient care. The open-source HALO is available at:
https://github.com/ResponsibleAILab/HALO.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¤§å¹æåèªç¶èªè¨èçä»»åï¼ä½å®åå®¹æç¢çä¸æºç¢ºæä¸å¯é çåæï¼éç¾è±¡ç¨±çºå¹»è¦ºãå¨å¥åº·åé«å­¸ç­ééµé åï¼éäºå¹»è¦ºå¯è½æé æå´éçé¢¨éªãæ¬è«æä»ç´¹ HALOï¼ä¸ç¨®æ°ç©çæ¶æ§ï¼æ¨å¨ééå°æ³¨æ¼åµæ¸¬åæ¸è¼å¹»è¦ºï¼ä¾æåé«çåç­ (QA) ç³»çµ±çæºç¢ºæ§åå¯é æ§ãæåçåæ³æ¯ä½¿ç¨ LLM ç¢ççµ¦å®æ¥è©¢çå¤åè®é«ï¼ä¸¦å¾å¤é¨éæ¾ç¥è­åº«ä¸­æ·åç¸éè³è¨ï¼ä»¥è±å¯å§å®¹ãæåå©ç¨æå¤§ééç¸éæ§è©åä¾åªåèçæ·åçå§å®¹ï¼ç¶å¾æä¾çµ¦ LLM ä»¥ç¢çç­æ¡ï¼å¾èéä½å¹»è¦ºçé¢¨éªãLangChain çæ´åé²ä¸æ­¥ç°¡åäºéåæµç¨ï¼å°è´éæ¾åå§ç¢¼ååæ¥­ LLMï¼ä¾å¦ Llama-3.1ï¼å¾ 44% å° 65%ï¼å ChatGPTï¼å¾ 56% å° 70%ï¼çæºç¢ºæ§é¡¯èä¸ç©©å¥å°æåãéåæ¶æ§å¼·èª¿äºå¨é«çåç­ç³»çµ±ä¸­è§£æ±ºå¹»è¦ºçéè¦æ§ï¼æçµæ¹åäºè¨åºæ±ºç­å¶å®åæ£èç§è­·ãéæ¾åå§ç¢¼ HALO å¯å¨ä»¥ä¸ç¶²ååå¾ï¼https://github.com/ResponsibleAILab/HALOã

##### **Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System**
2409.09968v1 by Raffi Hagopian, Timothy Strebel, Simon Bernatz, Gregory A Myers, Erik Offerman, Eric Zuniga, Cy Y Kim, Angie T Ng, James A Iwaz, Sunny P Singh, Evan P Carey, Michael J Kim, R Spencer Schaefer, Jeannie Yu, Amilcare Gentili, Hugo JWL Aerts

Coronary artery calcium (CAC) is highly predictive of cardiovascular events.
While millions of chest CT scans are performed annually in the United States,
CAC is not routinely quantified from scans done for non-cardiac purposes. A
deep learning algorithm was developed using 446 expert segmentations to
automatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our
study differs from prior works as we leverage imaging data across the Veterans
Affairs national healthcare system, from 98 medical centers, capturing
extensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC
performance on non-gated scans was compared against clinical standard ECG-gated
CAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than
100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and
87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a
year of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year
all-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p <
0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group:
33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052
patients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%)
individuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT
images from a random sample of >400 AI-CAC patients and verified that 527/531
(99.2%) would benefit from lipid-lowering therapy. To the best of our
knowledge, this is the first non-gated CT CAC algorithm developed across a
national healthcare system, on multiple imaging protocols, without filtering
intra-cardiac hardware, and compared against a strong gated CT reference. We
report superior performance relative to previous CAC algorithms evaluated
against paired gated scans that included patients with intra-cardiac hardware.

æè¦ï¼å çåèé£å (CAC) æ¥µå·é æ¸¬å¿è¡ç®¡äºä»¶çè½åã
éç¶ç¾åæ¯å¹´é²è¡æ¸ç¾è¬æ¬¡è¸é¨é»è¦æ·å±¤ææï¼
ä½éå¿èç®çææéå¸¸ä¸æå° CAC é²è¡éåãä¸
åæ·±åº¦å­¸ç¿æ¼ç®æ³ä½¿ç¨ 446 åå°å®¶åæ®µéç¼ï¼ä»¥
å¨éå°æ¯ãééæ§é»è¦æ·å±¤ææ (AI-CAC) ä¸èªåéå CACãæåç
ç ç©¶èååçå·¥ä½ä¸åï¼å çºæåå©ç¨éä¼è»äººäºåé¨å¨åé«çä¿å¥ç³»çµ±ä¸­ä¾èª 98 åé«çä¸­å¿çå½±åè³æï¼ææ
å½±ååå®ãææå¨åæ£èçå»£æ³ç°è³ªæ§ãAI-CAC
å¨ééæ§ææä¸çè¡¨ç¾èè¨åºæ¨æº ECG éæ§
CAC è©åé²è¡æ¯è¼ãééæ§ AI-CAC ååé¶èéé¶ï¼ä»¥åä½æ¼
100 è 100 ææ´é«ç Agatston åæ¸ï¼å¨ä¸å¹´å§é²è¡éå°éæ§ææç 795 åæ£èä¸­æºç¢ºçåå¥çº 89.4% (F1 0.93) å
87.3% (F1 0.89)ãééæ§ AI-CAC å¯é æ¸¬ 10 å¹´å¨å æ­»äº¡ç (CAC 0 å°æ¯ >400 ç¾¤çµï¼25.4% å°æ¯ 60.2%ï¼Cox HR 3.49ï¼p <
0.005)ï¼ä»¥åé¦æ¬¡è¤åæ§ä¸­é¢¨ãå¿èæ¢å¡ææ­»äº¡ (CAC 0 å°æ¯ >400 ç¾¤çµï¼
33.5% å°æ¯ 63.8%ï¼Cox HR 3.00ï¼p < 0.005)ãå¨ 8,052 åæ¥åä½åéèºçç¯©æª¢é»è¦æ·å±¤ææ (LDCT) çæ£èçç¯©æª¢è³æéä¸­ï¼3,091/8,052 (38.4%)
åé«ç AI-CAC >400ãåä½å¿èçå°å®¶å°é¨æ©æ½åç >400 AI-CAC æ£èç LDCT
å½±åé²è¡è³ªæ§å¯©æ¥ï¼ä¸¦é©è­ 527/531
(99.2%) å°åçæ¼éè¡èæ²»çãææåæç¥ï¼éæ¯ç¬¬ä¸åééæ§é»è¦æ·å±¤ CAC æ¼ç®æ³ï¼å¨å¨åé«çä¿å¥ç³»çµ±ä¸­éç¼ï¼æ¡ç¨å¤ç¨®å½±ååå®ï¼ä¸ç¯©é¸
å¿å§ç¡¬é«ï¼ä¸¦èå¼·å¤§çéæ§é»è¦æ·å±¤åèé²è¡æ¯è¼ãæå
å ±åçæè½åªæ¼ååéå°åå«å¿å§ç¡¬é«æ£èçéå°éæ§ææé²è¡è©ä¼°ç CAC æ¼ç®æ³ã

##### **GP-GPT: Large Language Model for Gene-Phenotype Mapping**
2409.09825v1 by Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiaowei Yu, Chao Cao, Tong Chen, Minheng Chen, Yan Zhuang, Xiang Li, Rongjie Liu, Chao Huang, Wentao Li, Tianming Liu, Dajiang Zhu

Pre-trained large language models(LLMs) have attracted increasing attention
in biomedical domains due to their success in natural language processing.
However, the complex traits and heterogeneity of multi-sources genomics data
pose significant challenges when adapting these models to the bioinformatics
and biomedical field. To address these challenges, we present GP-GPT, the first
specialized large language model for genetic-phenotype knowledge representation
and genomics relation analysis. Our model is fine-tuned in two stages on a
comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,
and medical genetics, derived from multiple large-scale validated datasets and
scientific publications. GP-GPT demonstrates proficiency in accurately
retrieving medical genetics information and performing common genomics analysis
tasks, such as genomics information retrieval and relationship determination.
Comparative experiments across domain-specific tasks reveal that GP-GPT
outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These
results highlight GP-GPT's potential to enhance genetic disease relation
research and facilitate accurate and efficient analysis in the fields of
genomics and medical genetics. Our investigation demonstrated the subtle
changes of bio-factor entities' representations in the GP-GPT, which suggested
the opportunities for the application of LLMs to advancing gene-phenotype
research.

æè¦ï¼é åè¨ç·´å¥½çå¤§åèªè¨æ¨¡å (LLM) ç±æ¼å¨èªç¶èªè¨èçæ¹é¢åå¾æåï¼å æ­¤å¨çç©é«å­¸é åä¸­ååéæ³¨ã
ç¶èï¼å¤ä¾æºåºå çµæ¸æçè¤éç¹å¾µåç°è³ªæ§å¨å°éäºæ¨¡åæç¨æ¼çç©è³è¨å­¸åçç©é«å­¸é åæï¼æ§æäºéå¤§ææ°ãçºäºæå°éäºææ°ï¼æåæåºäº GP-GPTï¼éæ¯ç¬¬ä¸åç¨æ¼éºå³è¡¨åç¥è­è¡¨å¾µååºå çµéä¿åæçå°æ¥­å¤§åèªè¨æ¨¡åãæåçæ¨¡ååå©åéæ®µé²è¡å¾®èª¿ï¼ä¸åç¶åèªæåº«åå«è¶é 3,000,000 ååºå çµå­¸ãèç½è³ªçµå­¸åé«å­¸éºå³å­¸ä¸­çè¡èªï¼éäºè¡èªä¾èªå¤åç¶éé©è­çå¤§è¦æ¨¡æ¸æéåç§å­¸åºçç©ãGP-GPT é¡¯ç¤ºåºæºç¢ºæ·åé«å­¸éºå³å­¸è³è¨åå·è¡å¸¸è¦åºå çµåæä»»åï¼ä¾å¦åºå çµè³è¨æ·ååéä¿ç¢ºå®ï¼çè½åãè·¨é åç¹å®ä»»åçæ¯è¼å¯¦é©é¡¯ç¤ºï¼GP-GPT åªæ¼æåé²ç LLMï¼åæ¬ Llama2ãLlama3 å GPT-4ãéäºçµæçªåºäº GP-GPT å¨å å¼·éºå³ç¾çéä¿ç ç©¶ä»¥åä¿é²åºå çµå­¸åé«å­¸éºå³å­¸é åä¸­æºç¢ºèææåæçæ½åãæåçèª¿æ¥è­æäº GP-GPT ä¸­çç©å å­å¯¦é«è¡¨å¾µçç´°å¾®è®åï¼éè¡¨æäºå° LLM æç¨æ¼æ¨é²åºå è¡¨åç ç©¶çæ©æã

##### **Veridical Data Science for Medical Foundation Models**
2409.10580v1 by Ahmed Alaa, Bin Yu

The advent of foundation models (FMs) such as large language models (LLMs)
has led to a cultural shift in data science, both in medicine and beyond. This
shift involves moving away from specialized predictive models trained for
specific, well-defined domain questions to generalist FMs pre-trained on vast
amounts of unstructured data, which can then be adapted to various clinical
tasks and questions. As a result, the standard data science workflow in
medicine has been fundamentally altered; the foundation model lifecycle (FMLC)
now includes distinct upstream and downstream processes, in which computational
resources, model and data access, and decision-making power are distributed
among multiple stakeholders. At their core, FMs are fundamentally statistical
models, and this new workflow challenges the principles of Veridical Data
Science (VDS), hindering the rigorous statistical analysis expected in
transparent and scientifically reproducible data science practices. We
critically examine the medical FMLC in light of the core principles of VDS:
predictability, computability, and stability (PCS), and explain how it deviates
from the standard data science workflow. Finally, we propose recommendations
for a reimagined medical FMLC that expands and refines the PCS principles for
VDS including considering the computational and accessibility constraints
inherent to FMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç­åºç¤æ¨¡å (FM) çåºç¾ï¼å°è´äºè³æç§å­¸çæåè½è®ï¼ç¡è«æ¯å¨é«å­¸é åæå¶ä»é åãéåè½è®æ¶åå¾éå°ç¹å®ãå®ç¾©æç¢ºçé ååé¡è¨ç·´çå°éé æ¸¬æ¨¡åï¼è½ç§»å°é åå¨å¤§ééçµæ§åè³æä¸è¨ç·´çæ³ç¨å FMï¼ç¶å¾å¯ä»¥èª¿æ´éäº FM ä»¥é©æåç¨®è¨åºä»»åååé¡ãå æ­¤ï¼é«å­¸ä¸­çæ¨æºè³æç§å­¸å·¥ä½æµç¨å·²ç¶ç¼çäºæ ¹æ¬æ§çæ¹è®ï¼åºç¤æ¨¡åçå½é±æ (FMLC) ç¾å¨åæ¬ä¸åçä¸æ¸¸åä¸æ¸¸æµç¨ï¼å¶ä¸­éç®è³æºãæ¨¡ååè³æå­åï¼ä»¥åæ±ºç­æ¬åæåéçµ¦å¤åå©å®³éä¿äººãå¾æ¬è³ªä¸ä¾èªªï¼FM åºæ¬ä¸æ¯çµ±è¨æ¨¡åï¼èéåæ°çå·¥ä½æµç¨ææ°äºçå¯¦è³æç§å­¸ (VDS) çååï¼é»ç¤äºéæä¸ç§å­¸ä¸å¯è¤è£½çè³æç§å­¸å¯¦åä¸­æé æçå´è¬¹çµ±è¨åæãæåæ ¹æ VDS çæ ¸å¿ååï¼å¯é æ¸¬æ§ãå¯éç®æ§åç©©å®æ§ (PCS) ä¾æ¹å¤æ§å°æª¢è¦é«å­¸ FMLCï¼ä¸¦èªªæå®å¦ä½åé¢æ¨æºçè³æç§å­¸å·¥ä½æµç¨ãæå¾ï¼æåæåºäºéæ°æ§æ³é«å­¸ FMLC çå»ºè­°ï¼ä»¥æ´ååå®åé©ç¨æ¼ VDS ç PCS ååï¼åæ¬èé FM åºæçéç®åå¯å­åæ§éå¶ã

##### **From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare**
2409.09727v1 by Ming Li, Pengcheng Xu, Junjie Hu, Zeyu Tang, Guang Yang

Federated learning holds great potential for enabling large-scale healthcare
research and collaboration across multiple centres while ensuring data privacy
and security are not compromised. Although numerous recent studies suggest or
utilize federated learning based methods in healthcare, it remains unclear
which ones have potential clinical utility. This review paper considers and
analyzes the most recent studies up to May 2024 that describe federated
learning based methods in healthcare. After a thorough review, we find that the
vast majority are not appropriate for clinical use due to their methodological
flaws and/or underlying biases which include but are not limited to privacy
concerns, generalization issues, and communication costs. As a result, the
effectiveness of federated learning in healthcare is significantly compromised.
To overcome these challenges, we provide recommendations and promising
opportunities that might be implemented to resolve these problems and improve
the quality of model development in federated learning with healthcare.

æè¦ï¼è¯é¦å­¸ç¿å¨ç¢ºä¿è³æé±ç§åå®å¨ä¸è´åæçææ³ä¸ï¼çºå¤§åé«çä¿å¥ç ç©¶åè·¨å¤åä¸­å¿åä½æä¾äºå·¨å¤§æ½åãåç®¡è¨±å¤æè¿çç ç©¶å»ºè­°æå©ç¨åºæ¼è¯é¦å­¸ç¿çæ¹æ³é²è¡é«çä¿å¥ï¼ä½åªäºå·ææ½å¨çè¨åºæç¨ä»ä¸æ¸æ¥ãæ¬è©è«æç« èæ®ä¸¦åæäºæªè³ 2024 å¹´ 5 ææè¿°åºæ¼è¯é¦å­¸ç¿æ¹æ³çé«çä¿å¥çææ°ç ç©¶ãå¨å¾¹åºæª¢é±å¾ï¼æåç¼ç¾çµå¤§å¤æ¸ä¸é©åè¨åºä½¿ç¨ï¼å çºå®åå­å¨æ¹æ³è«ç¼ºé·å/ææ½å¨åå·®ï¼åæ¬ä½ä¸éæ¼é±ç§åé¡ãæ¦ååé¡åéè¨ææ¬ãå æ­¤ï¼è¯é¦å­¸ç¿å¨é«çä¿å¥ä¸­çæååå°é¡¯èå½±é¿ãçºäºåæéäºææ°ï¼æåæä¾äºå»ºè­°åæå¸æçæ©æï¼éäºæ©æå¯è½æè¢«å¯¦æ½ä»¥è§£æ±ºéäºåé¡ä¸¦æé«é«çä¿å¥ä¸­è¯é¦å­¸ç¿æ¨¡åéç¼çåè³ªã

##### **ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models**
2409.09662v2 by Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim

Expressing stressful experiences in words is proven to improve mental and
physical health, but individuals often disengage with writing interventions as
they struggle to organize their thoughts and emotions. Reflective prompts have
been used to provide direction, and large language models (LLMs) have
demonstrated the potential to provide tailored guidance. Current systems often
limit users' flexibility to direct their reflections. We thus present
ExploreSelf, an LLM-driven application designed to empower users to control
their reflective journey. ExploreSelf allows users to receive adaptive support
through dynamically generated questions. Through an exploratory study with 19
participants, we examine how participants explore and reflect on personal
challenges using ExploreSelf. Our findings demonstrate that participants valued
the balance between guided support and freedom to control their reflective
journey, leading to deeper engagement and insight. Building on our findings, we
discuss implications for designing LLM-driven tools that promote user
empowerment through effective reflective practices.

æè¦ï¼å·²è­å¯¦ç¨è¨èªè¡¨éå£åç¶é©æå©æ¼æ¹åå¿çåèº«é«å¥åº·ï¼ä½åäººå¸¸å¸¸æ¾æ£å¯«ä½ä»å¥ï¼å çºä»åå¨æ´çæç·åæç·ææéå°å°é£ãåææç¤ºå·²è¢«ç¨ä¾æä¾æ¹åï¼èå¤§åèªè¨æ¨¡å (LLM) å·²è­æææä¾å®¢è£½åæå°çæ½åãç®åçç³»çµ±éå¸¸æéå¶ä½¿ç¨èå¼å°å¶åæçéæ´»æ§ãå æ­¤ï¼æåæåºäº ExploreSelfï¼éæ¯ä¸åç± LLM é©åçæç¨ç¨å¼ï¼æ¨å¨ææ¬ä½¿ç¨èæ§å¶å¶åææç¨ãExploreSelf åè¨±ä½¿ç¨èééåæç¢ççåé¡ä¾æ¥æ¶é©ææ§æ¯æ´ãééä¸é è 19 ä½åèèé²è¡çæ¢ç´¢æ§ç ç©¶ï¼æåæ¢è¨åèèå¦ä½ä½¿ç¨ ExploreSelf ä¾æ¢ç´¢ååæåäººææ°ãæåçç ç©¶çµæè¡¨æï¼åèèéè¦å¼å°å¼æ¯æ´èæ§å¶å¶åææç¨çèªç±ä¹éçå¹³è¡¡ï¼éæå¸¶ä¾æ´æ·±å¥çåèåæ´å¯åãæ ¹ææåçç ç©¶çµæï¼æåè¨è«äºè¨­è¨ LLM é©åå·¥å·çå«æï¼éäºå·¥å·ééææçåæå¯¦åä¿é²ä½¿ç¨èè³¦æ¬ã

##### **MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences**
2409.09570v1 by Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Michael V. Heinz, Ashmita Kunwar, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Sarah M. Preum, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell

Mental health concerns are prevalent among college students, highlighting the
need for effective interventions that promote self-awareness and holistic
well-being. MindScape pioneers a novel approach to AI-powered journaling by
integrating passively collected behavioral patterns such as conversational
engagement, sleep, and location with Large Language Models (LLMs). This
integration creates a highly personalized and context-aware journaling
experience, enhancing self-awareness and well-being by embedding behavioral
intelligence into AI. We present an 8-week exploratory study with 20 college
students, demonstrating the MindScape app's efficacy in enhancing positive
affect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and
depression, with a significant week-over-week decrease in PHQ-4 scores (-0.25
coefficient), alongside improvements in mindfulness (7%) and self-reflection
(6%). The study highlights the advantages of contextual AI journaling, with
participants particularly appreciating the tailored prompts and insights
provided by the MindScape app. Our analysis also includes a comparison of
responses to AI-driven contextual versus generic prompts, participant feedback
insights, and proposed strategies for leveraging contextual AI journaling to
improve well-being on college campuses. By showcasing the potential of
contextual AI journaling to support mental health, we provide a foundation for
further investigation into the effects of contextual AI journaling on mental
health and well-being.

æè¦ï¼å¤§å­¸çæ®éæå¿çå¥åº·åé¡ï¼å¼·èª¿éè¦ææå¹²é æªæ½ä¾ä¿é²èªæè¦ºå¯åæ´é«ç¦ç¥ãMindScape éåµäº AI é©åæ¥èªçæ°æ¹æ³ï¼æ¹æ³æ¯å°è¢«åæ¶éçè¡çºæ¨¡å¼ï¼ä¾å¦å°è©±åèãç¡ç åä½ç½®ï¼èå¤§åèªè¨æ¨¡å (LLM) æ´åå¨ä¸èµ·ãéç¨®æ´ååµé äºé«åº¦åäººåä¸å·åæå¢æç¥è½åçæ¥èªé«é©ï¼ééå°è¡çºæºæ§åµå¥ AI ä¾å¢å¼·èªæè¦ºå¯åç¦ç¥ãæåæåºäºä¸é çºæ 8 é±çæ¢ç´¢æ§ç ç©¶ï¼æ 20 åå¤§å­¸çåèï¼è­æ MindScape æç¨ç¨å¼å¨å¢å¼·æ­£é¢å½±é¿ (7%)ãæ¸å°è² é¢å½±é¿ (11%)ãå­¤ç¨æ (6%) ä»¥åç¦æ®åæé¬±æ¹é¢ææï¼PHQ-4 åæ¸é±é±é¡¯èä¸é (-0.25 ä¿æ¸)ï¼åææ­£å¿µ (7%) åèªæåç (6%) ä¹æææ¹åãéé ç ç©¶å¼·èª¿äºæå¢ AI æ¥èªçåªé»ï¼åèèç¹å¥æ¬£è³ MindScape æç¨ç¨å¼æä¾çå®¢è£½åæç¤ºåè¦è§£ãæåçåæéåæ¬æ¯è¼å° AI é©åæå¢æç¤ºåä¸è¬æç¤ºçåæãåèèåé¥è¦è§£ï¼ä»¥åæåºå©ç¨æå¢ AI æ¥èªä¾æ¹åå¤§å­¸æ ¡åç¦ç¥çç­ç¥ãééå±ç¤ºæå¢ AI æ¥èªå¨æ¯æ´å¿çå¥åº·çæ½åï¼æåçºé²ä¸æ­¥æ¢è¨æå¢ AI æ¥èªå°å¿çå¥åº·åç¦ç¥çå½±é¿å¥ å®äºåºç¤ã

##### **COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare**
2409.09549v1 by Chia-Hao Li, Niraj K. Jha

Wearable medical sensors (WMSs) are revolutionizing smart healthcare by
enabling continuous, real-time monitoring of user physiological signals,
especially in the field of consumer healthcare. The integration of WMSs and
modern machine learning (ML) enables unprecedented solutions to efficient
early-stage disease detection. Despite the success of Transformers in various
fields, their application to sensitive domains, such as smart healthcare,
remains underexplored due to limited data accessibility and privacy concerns.
To bridge the gap between Transformer-based foundation models and WMS-based
disease detection, we propose COMFORT, a continual fine-tuning framework for
foundation models targeted at consumer healthcare. COMFORT introduces a novel
approach for pre-training a Transformer-based foundation model on a large
dataset of physiological signals exclusively collected from healthy individuals
with commercially available WMSs. We adopt a masked data modeling (MDM)
objective to pre-train this health foundation model. We then fine-tune the
model using various parameter-efficient fine-tuning (PEFT) methods, such as
low-rank adaptation (LoRA) and its variants, to adapt it to various downstream
disease detection tasks that rely on WMS data. In addition, COMFORT continually
stores the low-rank decomposition matrices obtained from the PEFT algorithms to
construct a library for multi-disease detection. The COMFORT library enables
scalable and memory-efficient disease detection on edge devices. Our
experimental results demonstrate that COMFORT achieves highly competitive
performance while reducing memory overhead by up to 52% relative to
conventional methods. Thus, COMFORT paves the way for personalized and
proactive solutions to efficient and effective early-stage disease detection
for consumer healthcare.

æè¦ï¼<paragraph>å¯ç©¿æ´å¼é«çææ¸¬å¨ (WMS) ééæçºãå³æç£æ¸¬ä½¿ç¨èçççè¨èï¼ç¹å¥æ¯å¨æ¶è²»èé«çä¿å¥é åï¼é²èé©æ°äºæºæ§é«çä¿å¥ãWMS èç¾ä»£æ©å¨å­¸ç¿ (ML) çæ´åï¼è®ææççæ©æç¾çåµæ¸¬æäºåææªæçè§£æ±ºæ¹æ¡ãåç®¡ Transformer å¨åç¨®é åçç²å¾æåï¼ä½ç±æ¼è³æåå¾ä¸æåé±ç§çæ®ï¼å¶å¨æºæ§é«çä¿å¥ç­ææé åçæç¨ä»æå¾æ¢ç´¢ãçºäºå½å Transformer åºç¤æ¨¡åè WMS åºç¤ç¾çåµæ¸¬ä¹éçå·®è·ï¼æåæåºäº COMFORTï¼ä¸åéå°æ¶è²»èé«çä¿å¥èè¨­è¨çåºç¤æ¨¡åæçºå¾®èª¿æ¶æ§ãCOMFORT æåºäºä¸ç¨®åµæ°çæ¹æ³ï¼å¯å¨ä¸åé¾å¤§çççè¨èè³æéä¸é è¨ç·´ Transformer åºç¤æ¨¡åï¼èéäºè³æçæ¯ééå¸å® WMS å¾å¥åº·åäººèº«ä¸æ¶éèä¾ãæåæ¡ç¨é®ç½©è³æå»ºæ¨¡ (MDM) ç®æ¨ä¾é è¨ç·´éåå¥åº·åºç¤æ¨¡åãæ¥èï¼æåä½¿ç¨åç¨®åæ¸ææççå¾®èª¿ (PEFT) æ¹æ³ï¼ä¾å¦ä½ç§©é©æ (LoRA) åå¶è®é«ï¼å¾®èª¿æ¨¡åï¼ä»¥ä½¿å¶é©æä¾è³´ WMS è³æçåç¨®ä¸æ¸¸ç¾çåµæ¸¬ä»»åãæ­¤å¤ï¼COMFORT ææçºå²å­å¾ PEFT æ¼ç®æ³åå¾çä½ç§©åè§£ç©é£ï¼ä»¥å»ºæ§ä¸åå¤ç¾çåµæ¸¬å½å¼åº«ãCOMFORT å½å¼åº«å¯å¨éç·£è£ç½®ä¸é²è¡å¯æ´åä¸è¨æ¶é«ä½¿ç¨çä½ä¸çç¾çåµæ¸¬ãæåçå¯¦é©çµæé¡¯ç¤ºï¼COMFORT éå°äºæ¥µå·ç«¶ç­åçæè½ï¼åæå°è¨æ¶é«éé·ç¸è¼æ¼å³çµ±æ¹æ³éä½äº 52%ãå æ­¤ï¼COMFORT çºæ¶è²»èé«çä¿å¥çææä¸é«ææ©æç¾çåµæ¸¬ï¼éé¢äºåäººåä¸ä¸»åçè§£æ±ºæ¹æ¡ã</paragraph>

##### **Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment**
2409.09520v1 by Xin Hu, Janet Wang, Jihun Hamm, Rie R Yotsu, Zhengming Ding

Current AI-assisted skin image diagnosis has achieved dermatologist-level
performance in classifying skin cancer, driven by rapid advancements in deep
learning architectures. However, unlike traditional vision tasks, skin images
in general present unique challenges due to the limited availability of
well-annotated datasets, complex variations in conditions, and the necessity
for detailed interpretations to ensure patient safety. Previous segmentation
methods have sought to reduce image noise and enhance diagnostic performance,
but these techniques require fine-grained, pixel-level ground truth masks for
training. In contrast, with the rise of foundation models, the Segment Anything
Model (SAM) has been introduced to facilitate promptable segmentation, enabling
the automation of the segmentation process with simple yet effective prompts.
Efforts applying SAM predominantly focus on dermatoscopy images, which present
more easily identifiable lesion boundaries than clinical photos taken with
smartphones. This limitation constrains the practicality of these approaches to
real-world applications. To overcome the challenges posed by noisy clinical
photos acquired via non-standardized protocols and to improve diagnostic
accessibility, we propose a novel Cross-Attentive Fusion framework for
interpretable skin lesion diagnosis. Our method leverages SAM to generate
visual concepts for skin diseases using prompts, integrating local visual
concepts with global image features to enhance model performance. Extensive
evaluation on two skin disease datasets demonstrates our proposed method's
effectiveness on lesion diagnosis and interpretability.

æè¦ï¼ç®åç± AI è¼å©çç®èå½±åè¨ºæ·å·²å¨ç®èçåé¡ä¸­éå°ç®èç§é«å¸«ç­ç´çè¡¨ç¾ï¼éæ­¸åæ¼æ·±åº¦å­¸ç¿æ¶æ§çå¿«éé²å±ãç¶èï¼èå³çµ±çè¦è¦ºä»»åä¸åï¼ä¸è¬ç®èå½±åç±æ¼æ¨è¨»è¯å¥½çè³æéåå¾ä¸æãçæ³è¤éå¤è®ï¼ä»¥åç¢ºä¿æ£èå®å¨æéçè©³ç´°è©®éï¼å æ­¤åç¾åºç¨ç¹çææ°ãååçåå²æ¹æ³è©¦åéä½å½±åéè¨ä¸¦æåè¨ºæ·è¡¨ç¾ï¼ä½éäºæè¡éè¦ç´°ç·»çç«ç´ ç´å°é¢å¯¦æ³é®ç½©ä¾è¨ç·´ãç¸å°å°ï¼é¨èåºç¤æ¨¡åçèèµ·ï¼å·²å°å¥ Segment Anything Model (SAM) ä»¥å©æ¼æç¤ºå¼åå²ï¼ä½¿ç¨ç°¡å®å»ææçæç¤ºèªåååå²æµç¨ãæç¨ SAM çå·¥ä½ä¸»è¦éä¸­æ¼ç®èé¡å½±åï¼å¶çç¶éçæ¯ä½¿ç¨æºæ§åææ©ææçè¨åºç§çæ´å®¹æè¾¨è­ãæ­¤éå¶æç´æéäºæ¹æ³å¨å¯¦éæç¨ä¸­çå¯¦ç¨æ§ãçºäºåæéæ¨æºåç¨åºåå¾çéè¨è¨åºç§çæé æçææ°ï¼ä¸¦æ¹åè¨ºæ·çå¯è¿æ§ï¼æåæåºä¸åæ°ç©çè·¨æ³¨æåèåæ¶æ§ï¼ç¨æ¼å¯è©®éçç®èçç¶è¨ºæ·ãæåçæ¹æ³å©ç¨ SAM ä½¿ç¨æç¤ºä¾ç¢çç®èç¾ççè¦è¦ºæ¦å¿µï¼å°å±é¨è¦è¦ºæ¦å¿µèæ´é«å½±åç¹å¾µæ´åï¼ä»¥æåæ¨¡åè¡¨ç¾ãå¨å©åç®èç¾çè³æéä¸çå»£æ³è©ä¼°é¡¯ç¤ºï¼æåæåºçæ¹æ³å¨çç¶è¨ºæ·åå¯è©®éæ§ä¸é½å·æææã

##### **Synthetic4Health: Generating Annotated Synthetic Clinical Letters**
2409.09501v1 by Libo Ren, Samuel Belkadi, Lifeng Han, Warren Del-Pinto, Goran Nenadic

Since clinical letters contain sensitive information, clinical-related
datasets can not be widely applied in model training, medical research, and
teaching. This work aims to generate reliable, various, and de-identified
synthetic clinical letters. To achieve this goal, we explored different
pre-trained language models (PLMs) for masking and generating text. After that,
we worked on Bio\_ClinicalBERT, a high-performing model, and experimented with
different masking strategies. Both qualitative and quantitative methods were
used for evaluation. Additionally, a downstream task, Named Entity Recognition
(NER), was also implemented to assess the usability of these synthetic letters.
  The results indicate that 1) encoder-only models outperform encoder-decoder
models. 2) Among encoder-only models, those trained on general corpora perform
comparably to those trained on clinical data when clinical information is
preserved. 3) Additionally, preserving clinical entities and document structure
better aligns with our objectives than simply fine-tuning the model. 4)
Furthermore, different masking strategies can impact the quality of synthetic
clinical letters. Masking stopwords has a positive impact, while masking nouns
or verbs has a negative effect. 5) For evaluation, BERTScore should be the
primary quantitative evaluation metric, with other metrics serving as
supplementary references. 6) Contextual information does not significantly
impact the models' understanding, so the synthetic clinical letters have the
potential to replace the original ones in downstream tasks.

æè¦ï¼ç±æ¼è¨åºä¿¡ä»¶åå«ææè³è¨ï¼å æ­¤èè¨åºç¸éçè³æéç¡æ³å»£æ³æç¨æ¼æ¨¡åè¨ç·´ãé«å­¸ç ç©¶åæå­¸ä¸­ãæ¬ç ç©¶æ¨å¨ç¢çå¯é ãå¤æ¨£ä¸å»è­å¥åçåæè¨åºä¿¡ä»¶ãçºäºéææ­¤ç®æ¨ï¼æåæ¢è¨äºä¸åçé è¨ç·´èªè¨æ¨¡å (PLM) ä¾é®è½åç¢çæå­ãä¹å¾ï¼æåä½¿ç¨é«æ§è½æ¨¡å Bio_ClinicalBERTï¼ä¸¦éå°ä¸åçé®è½ç­ç¥é²è¡å¯¦é©ãæåä½¿ç¨å®æ§åå®éæ¹æ³é²è¡è©ä¼°ãæ­¤å¤ï¼æåä¹å¯¦ä½äºä¸æ¸¸ä»»åï¼å³å½åå¯¦é«è­å¥ (NER) ä¾è©ä¼°éäºåæä¿¡ä»¶çå¯ç¨æ§ãçµæé¡¯ç¤ºï¼1) ç·¨ç¢¼å¨å°ç¨æ¨¡ååªæ¼ç·¨ç¢¼å¨-è§£ç¢¼å¨æ¨¡åã2) å¨ç·¨ç¢¼å¨å°ç¨æ¨¡åä¸­ï¼ç¶ä¿çè¨åºè³è¨æï¼å¨ä¸è¬èªæåº«ä¸è¨ç·´çæ¨¡åè¡¨ç¾èå¨è¨åºè³æä¸è¨ç·´çæ¨¡åç¸ç¶ã3) æ­¤å¤ï¼ä¿çè¨åºå¯¦é«åæä»¶çµæ§æ¯å®ç´å¾®èª¿æ¨¡åæ´ç¬¦åæåçç®æ¨ã4) æ­¤å¤ï¼ä¸åçé®è½ç­ç¥æå½±é¿åæè¨åºä¿¡ä»¶çåè³ªãé®è½åæ­¢è©ææ­£é¢çå½±é¿ï¼èé®è½åè©æåè©æè² é¢çå½±é¿ã5) å°æ¼è©ä¼°ï¼BERTScore æçºä¸»è¦çå®éè©ä¼°ææ¨ï¼å¶ä»ææ¨ä½çºè£ååèã6) èæ¯è³è¨ä¸æé¡¯èå½±é¿æ¨¡åççè§£ï¼å æ­¤åæè¨åºä¿¡ä»¶ææ½åå¨ä¸æ¸¸ä»»åä¸­åä»£åå§ä¿¡ä»¶ã

##### **From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging**
2409.09478v1 by Maximilian Rokuss, Balint Kovacs, Yannick Kirchhoff, Shuhan Xiao, Constantin Ulrich, Klaus H. Maier-Hein, Fabian Isensee

Automated lesion segmentation in PET/CT scans is crucial for improving
clinical workflows and advancing cancer diagnostics. However, the task is
challenging due to physiological variability, different tracers used in PET
imaging, and diverse imaging protocols across medical centers. To address this,
the autoPET series was created to challenge researchers to develop algorithms
that generalize across diverse PET/CT environments. This paper presents our
solution for the autoPET III challenge, targeting multitracer, multicenter
generalization using the nnU-Net framework with the ResEncL architecture. Key
techniques include misalignment data augmentation and multi-modal pretraining
across CT, MR, and PET datasets to provide an initial anatomical understanding.
We incorporate organ supervision as a multitask approach, enabling the model to
distinguish between physiological uptake and tracer-specific patterns, which is
particularly beneficial in cases where no lesions are present. Compared to the
default nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL
(65.31) our model significantly improved performance with a Dice score of
68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative
(FNvol: 10.35) volumes. These results underscore the effectiveness of combining
advanced network design, augmentation, pretraining, and multitask learning for
PET/CT lesion segmentation. Code is publicly available at
https://github.com/MIC-DKFZ/autopet-3-submission.

æè¦ï¼<paragraph>èªååçç¶åå²å¨ PET/CT ææä¸­å°æ¼æ¹åè¨åºå·¥ä½æµç¨åä¿é²ççè¨ºæ·è³ééè¦ãç¶èï¼ç±æ¼ççè®ç°ãPET å½±åä¸­ä½¿ç¨çä¸åè¿½è¹¤åï¼ä»¥ååé«çä¸­å¿ä¸åçå½±ååå®ï¼éé ä»»åå·æææ°æ§ãçºäºè§£æ±ºéååé¡ï¼autoPET ç³»åè³½åµç«ï¼ææ°ç ç©¶äººå¡éç¼å¨åç¨® PET/CT ç°å¢ä¸­éç¨çæ¼ç®æ³ãæ¬ææåºæåéå° autoPET III ææ°çè§£æ±ºæ¹æ¡ï¼ç®æ¨æ¯ä½¿ç¨å·æ ResEncL æ¶æ§ç nnU-Net æ¡æ¶é²è¡å¤è¿½è¹¤åãå¤ä¸­å¿æ¨å»£ãééµæè¡åæ¬é¯ä½è³ææ´ååè·¨ CTãMR å PET è³æéçå¤æ¨¡å¼é è¨ç·´ï¼ä»¥æä¾åæ­¥çè§£åçè§£ãæåå°å¨å®ç£ç£ç´å¥å¤ä»»åæ¹æ³ï¼ä½¿æ¨¡åè½å¤ ååççæååè¿½è¹¤åç¹ç°æ§æ¨¡å¼ï¼éå¨æ²æçç¶çææ³ä¸ç¹å¥æçãèéæ Dice åæ¸ 57.61 çé è¨­ nnU-Netï¼ææ´å¤§ç ResEncL (65.31) ç¸æ¯ï¼æåçæ¨¡åå¤§å¹æ¹åäºæè½ï¼Dice åæ¸çº 68.40ï¼åææ¸å°äºåé½æ§ (FPvolï¼7.82) ååé°æ§ (FNvolï¼10.35) é«ç©ãéäºçµæå¼·èª¿äºçµååé²ç¶²è·¯è¨­è¨ãæ´åãé è¨ç·´åå¤ä»»åå­¸ç¿å° PET/CT çç¶åå²çæææ§ãç¨å¼ç¢¼å·²å¬éç¼å¸æ¼ https://github.com/MIC-DKFZ/autopet-3-submissionã</paragraph>

##### **Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking**
2409.10570v1 by Cong Kong, Rui Xu, Weixi Chen, Jiawei Chen, Zhaoxia Yin

Pre-training language models followed by fine-tuning on specific tasks is
standard in NLP, but traditional models often underperform when applied to the
medical domain, leading to the development of specialized medical pre-trained
language models (Med-PLMs). These models are valuable assets but are vulnerable
to misuse and theft, requiring copyright protection. However, no existing
watermarking methods are tailored for Med-PLMs, and adapting general PLMs
watermarking techniques to the medical domain faces challenges such as task
incompatibility, loss of fidelity, and inefficiency. To address these issues,
we propose the first training-free backdoor watermarking method for Med-PLMs.
Our method uses rare special symbols as trigger words, which do not impact
downstream task performance, embedding watermarks by replacing their original
embeddings with those of specific medical terms in the Med-PLMs' word
embeddings layer. After fine-tuning the watermarked Med-PLMs on various medical
downstream tasks, the final models (FMs) respond to the trigger words in the
same way they would to the corresponding medical terms. This property can be
utilized to extract the watermark. Experiments demonstrate that our method
achieves high fidelity while effectively extracting watermarks across various
medical downstream tasks. Additionally, our method demonstrates robustness
against various attacks and significantly enhances the efficiency of watermark
embedding, reducing the embedding time from 10 hours to 10 seconds.

æè¦ï¼é è¨ç·´èªè¨æ¨¡åï¼å¾çºéå°ç¹å®ä»»åé²è¡å¾®èª¿ï¼å¨èªç¶èªè¨èçä¸­æ¯æ¨æºä½æ¥­ï¼ä½å³çµ±æ¨¡åæç¨æ¼é«çé åæï¼å¾å¾è¡¨ç¾ä¸ä½³ï¼å°è´å°éçé«çé è¨ç·´èªè¨æ¨¡å (Med-PLM) æéèçãéäºæ¨¡åæ¯å¯¶è²´çè³ç¢ï¼ä½å®¹æé­å°æ¿«ç¨åç«åï¼éè¦çæ¬ä¿è­·ãç¶èï¼ç¾æçæµ®æ°´å°æ¹æ³ä¸¦æªéå° Med-PLM éèº«æé ï¼èå°ä¸è¬ PLM æµ®æ°´å°æè¡èª¿æ´çºé«çé åæï¼åé¢è¨ä»»åä¸ç¸å®¹ãä¿çåº¦ä¸éåæçä¸å½°ç­ææ°ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºç¬¬ä¸åéå° Med-PLM çåè¨ç·´å¾éæµ®æ°´å°æ¹æ³ãæåçåæ³æ¯ä½¿ç¨ç½è¦çç¹æ®ç¬¦èä½çºè§¸ç¼å­ï¼éä¸æå½±é¿ä¸æ¸¸ä»»åçè¡¨ç¾ï¼ä¸¦ééå°å¶åå§åµå¥æ¿æçº Med-PLM å­åµå¥å±¤ä¸­ç¹å®é«çè¡èªçåµå¥ï¼ä¾åµå¥æµ®æ°´å°ãå¨éå°åç¨®é«çä¸æ¸¸ä»»åå¾®èª¿å¸¶ææµ®æ°´å°ç Med-PLM ä¹å¾ï¼æçµæ¨¡å (FM) å°è§¸ç¼å­çåææ¹å¼èå°æé«çè¡èªç¸åãæ­¤ç¹æ§å¯ç¨æ¼æåæµ®æ°´å°ãå¯¦é©è­æï¼æåçåæ³å¨åç¨®é«çä¸æ¸¸ä»»åä¸­ï¼é½è½æææåæµ®æ°´å°ï¼åæç¶­æé«ä¿çåº¦ãæ­¤å¤ï¼æåçåæ³å±ç¾åºå°åç¨®æ»æçå¼·å¥æ§ï¼ä¸¦å¤§å¹æåæµ®æ°´å°åµå¥çæçï¼å°åµå¥æéå¾ 10 å°æç¸®ç­çº 10 ç§ã

##### **Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation**
2409.09324v1 by Hui Yi Leong, Yi Fan Gao, Ji Shuai, Uktu Pamuksuz

Scientific research indicates that for every hour spent in direct patient
care, physicians spend nearly two additional hours on administrative tasks,
particularly on electronic health records (EHRs) and desk work. This excessive
administrative burden not only reduces the time available for patient care but
also contributes to physician burnout and inefficiencies in healthcare
delivery. To address these challenges, this study introduces MediGen, a
fine-tuned large language model (LLM) designed to automate the generation of
medical reports from medical dialogues. By leveraging state-of-the-art
methodologies for fine-tuning open-source pretrained models, including
LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing
clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising
results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating
its effectiveness in generating accurate and clinically relevant medical
reports. These findings suggest that MediGen has the potential to significantly
reduce the administrative workload on physicians, improving both healthcare
efficiency and physician well-being.

æè¦ï¼ç§å­¸ç ç©¶æåºï¼é«å¸«æ¯è±ä¸åå°æé²è¡ç´æ¥ççæ£ç§è­·ï¼å°±æè±è²»å°è¿å©åå°æå¨è¡æ¿äºåä¸ï¼ç¹å¥æ¯å¨é»å­çæ­· (EHR) åææ¸å·¥ä½ä¸ãéç¨®éåº¦çè¡æ¿è² æä¸åæ¸å°äºå¯è±å¨çæ£ç§è­·ä¸çæéï¼ä¹å°è´é«å¸«å¦æ åé«çä¿å¥æä¾æçä¸å½°ãçºäºæå°éäºææ°ï¼æ¬ç ç©¶å¼å¥äº MediGenï¼ä¸åç¶éå¾®èª¿çå¤§åèªè¨æ¨¡å (LLM)ï¼æ¨å¨èªååå¾é«çå°è©±ä¸­çæé«çå ±åãééå©ç¨æåé²çæ¹æ³å¾®èª¿éæºé è¨ç·´æ¨¡åï¼åæ¬ LLaMA3-8Bï¼MediGen å¨è½éåç¸½çµè¨åºäºåæ¹é¢éå°äºå¾é«çæºç¢ºåº¦ãç¶éå¾®èª¿ç LLaMA3-8B æ¨¡åå±ç¤ºäºæå¸æççµæï¼éå°äº 58% ç ROUGE åæ¸å 72% ç BERTScore-F1ï¼éè¡¨ç¤ºå®å¨çææºç¢ºä¸è¨åºä¸ç¸éçé«çå ±åæ¹é¢å¾ææãéäºç¼ç¾è¡¨æ MediGen æå¯è½å¤§å¹æ¸å°é«å¸«çè¡æ¿å·¥ä½éï¼é²èæ¹åé«çä¿å¥æçåé«å¸«çç¦ç¥ã

##### **On the limits of agency in agent-based models**
2409.10568v1 by Ayush Chopra, Shashank Kumar, Nurullah Giray-Kuru, Ramesh Raskar, Arnau Quera-Bofarull

Agent-based modeling (ABM) seeks to understand the behavior of complex
systems by simulating a collection of agents that act and interact within an
environment. Their practical utility requires capturing realistic environment
dynamics and adaptive agent behavior while efficiently simulating million-size
populations. Recent advancements in large language models (LLMs) present an
opportunity to enhance ABMs by using LLMs as agents with further potential to
capture adaptive behavior. However, the computational infeasibility of using
LLMs for large populations has hindered their widespread adoption. In this
paper, we introduce AgentTorch -- a framework that scales ABMs to millions of
agents while capturing high-resolution agent behavior using LLMs. We benchmark
the utility of LLMs as ABM agents, exploring the trade-off between simulation
scale and individual agency. Using the COVID-19 pandemic as a case study, we
demonstrate how AgentTorch can simulate 8.4 million agents representing New
York City, capturing the impact of isolation and employment behavior on health
and economic outcomes. We compare the performance of different agent
architectures based on heuristic and LLM agents in predicting disease waves and
unemployment rates. Furthermore, we showcase AgentTorch's capabilities for
retrospective, counterfactual, and prospective analyses, highlighting how
adaptive agent behavior can help overcome the limitations of historical data in
policy design. AgentTorch is an open-source project actively being used for
policy-making and scientific discovery around the world. The framework is
available here: github.com/AgentTorch/AgentTorch.

æè¦ï¼<paragraph>åºæ¼ä»£ççå»ºæ¨¡ (ABM) ééæ¨¡æ¬ä¸ç¾¤ä»£çäººï¼å¨ç°å¢ä¸­è¡çºä¸¦äºåï¼ä»¥äºè§£è¤éç³»çµ±çè¡çºãå®åçå¯¦ç¨æ§éè¦ææçå¯¦çç°å¢åæåé©ææ§ä»£çè¡çºï¼åæææçå°æ¨¡æ¬æ¸ç¾è¬è¦æ¨¡çæç¾¤ãå¤§åèªè¨æ¨¡å (LLM) çææ°é²å±æä¾äºæ©æï¼å¯ä»¥ééä½¿ç¨ LLM ä½çºä»£çäººä¾å¢å¼· ABMï¼é²èææ½åææé©ææ§è¡çºãç¶èï¼å° LLM ç¨æ¼å¤§åæç¾¤çè¨ç®ä¸å¯è¡æ§é»ç¤äºå®åçå»£æ³æ¡ç¨ãå¨æ¬æä¸­ï¼æåä»ç´¹äº AgentTorchï¼éæ¯ä¸åå° ABM æ´å±å°æ¸ç¾è¬åä»£çäººï¼åæä½¿ç¨ LLM ææé«è§£æåº¦ä»£çäººè¡çºçæ¡æ¶ãæåè©é LLM ä½çº ABM ä»£çäººçæç¨ï¼æ¢ç´¢æ¨¡æ¬è¦æ¨¡ååäººä»£çä¹éçæ¬è¡¡ãä½¿ç¨ COVID-19 å¤§æµè¡ä½çºæ¡ä¾ç ç©¶ï¼æåå±ç¤ºäº AgentTorch å¦ä½æ¨¡æ¬ä»£è¡¨ç´ç´å¸ç 840 è¬åä»£çäººï¼ææéé¢åå°±æ¥­è¡çºå°å¥åº·åç¶æ¿ææçå½±é¿ãæåæ¯è¼äºåºæ¼åç¼å¼å LLM ä»£çäººçä¸åä»£çäººæ¶æ§å¨é æ¸¬ç¾çæµªæ½®åå¤±æ¥­çæ¹é¢çè¡¨ç¾ãæ­¤å¤ï¼æåå±ç¤ºäº AgentTorch å¨åé¡§æ§ãåäºå¯¦ååç»æ§åææ¹é¢çåè½ï¼å¼·èª¿é©ææ§ä»£çè¡çºå¦ä½å¹«å©åææ¿ç­è¨­è¨ä¸­æ­·å²è³æçéå¶ãAgentTorch æ¯ç©æ¥µç¨æ¼å¨çæ¿ç­å¶å®åç§å­¸ç¼ç¾çéæºå°æ¡ãæ­¤æ¡æ¶å¯å¨éè£¡åå¾ï¼github.com/AgentTorch/AgentTorchã</paragraph>

##### **Contextual Evaluation of Large Language Models for Classifying Tropical and Infectious Diseases**
2409.09201v1 by Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Katherine Heller

While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.

æè¦ï¼åç®¡å¤§åèªè¨æ¨¡å (LLM) å¨é«çåé¡åç­æ¹é¢å·²å±ç¾æ½åï¼ä½å°æ³¨æ¼ç±å¸¶åå³æçç¹å®æ¢ç´¢çå·¥ä½å»æéãæåå»ºç«å¨éæºç±å¸¶åå³æç (TRINDs) è³æéä¸ï¼ä¸¦å°å¶æ´åä»¥ç´å¥äººå£çµ±è¨åèªç¾©è¨åºåæ¶è²»èæ´åï¼ç¢çè¶é 11000 åæç¤ºãæåè©ä¼°éäº LLM çæè½ï¼æ¯è¼éæåé«ç LLMï¼ä»¥å LLM çµæèäººé¡å°å®¶ãæåééç³»çµ±æ§å¯¦é©ï¼è­æäºèæ¯è³è¨ï¼ä¾å¦äººå£çµ±è¨ãä½ç½®ãæ§å¥ãæä½³ LLM åæçé¢¨éªå ç´ ï¼çå¥½èãæå¾ï¼æåéç¼äºä¸å TRINDs-LM ååï¼éæ¯ä¸åç ç©¶å·¥å·ï¼æä¾äºä¸åæ¢ç´¢èæ¯å¦ä½å½±é¿å¥åº· LLM è¼¸åºçéæ¨å ´ã

##### **Phikon-v2, A large and public feature extractor for biomarker prediction**
2409.09173v1 by Alexandre Filiot, Paul Jacob, Alice Mac Kain, Charlie Saillard

Gathering histopathology slides from over 100 publicly available cohorts, we
compile a diverse dataset of 460 million pathology tiles covering more than 30
cancer sites. Using this dataset, we train a large self-supervised vision
transformer using DINOv2 and publicly release one iteration of this model for
further experimentation, coined Phikon-v2. While trained on publicly available
histology slides, Phikon-v2 surpasses our previously released model (Phikon)
and performs on par with other histopathology foundation models (FM) trained on
proprietary data. Our benchmarks include eight slide-level tasks with results
reported on external validation cohorts avoiding any data contamination between
pre-training and evaluation datasets. Our downstream training procedure follows
a simple yet robust ensembling strategy yielding a +1.75 AUC increase across
tasks and models compared to one-shot retraining (p<0.001). We compare Phikon
(ViT-B) and Phikon-v2 (ViT-L) against 14 different histology feature
extractors, making our evaluation the most comprehensive to date. Our result
support evidences that DINOv2 handles joint model and data scaling better than
iBOT. Also, we show that recent scaling efforts are overall beneficial to
downstream performance in the context of biomarker prediction with GigaPath and
H-Optimus-0 (two ViT-g with 1.1B parameters each) standing out. However, the
statistical margins between the latest top-performing FMs remain mostly
non-significant; some even underperform on specific indications or tasks such
as MSI prediction - deposed by a 13x smaller model developed internally. While
latest foundation models may exhibit limitations for clinical deployment, they
nonetheless offer excellent grounds for the development of more specialized and
cost-efficient histology encoders fueling AI-guided diagnostic tools.

æè¦ï¼<paragraph>æåå¾ 100 å¤åå¬éå¯ç¨çç¾¤çµä¸­æ¶éäºçµç¹ççå­¸å¹»ççï¼ç·¨å¶äºä¸ååå« 4.6 ååççåççä¸åè³æéï¼æ¶µèäº 30 å¤åççé¨ä½ãä½¿ç¨æ­¤è³æéï¼æåä½¿ç¨ DINOv2 è¨ç·´äºä¸åå¤§åèªç£ç£è¦è¦ºè½æå¨ï¼ä¸¦å¬éç¼å¸æ­¤æ¨¡åçå¶ä¸­ä¸åçæ¬ï¼ä»¥ä¾é²ä¸æ­¥å¯¦é©ï¼ç¨±çº Phikon-v2ãéç¶å¨å¬éå¯ç¨ççµç¹åçä¸é²è¡è¨ç·´ï¼ä½ Phikon-v2 è¶è¶äºæåååç¼å¸çæ¨¡å (Phikon)ï¼ä¸¦ä¸èå¨å°æè³æä¸è¨ç·´çå¶ä»çµç¹ççå­¸åºç¤æ¨¡å (FM) æè½ç¸ç¶ãæåçåºæºåæ¬å«é å¹»ççå±¤ç´ä»»åï¼å¶çµæå¨å¤é¨é©è­ç¾¤çµä¸­å ±åï¼é¿åé è¨ç·´åè©ä¼°è³æéä¹éçä»»ä½è³ææ±¡æãæåçä¸æ¸¸è¨ç·´ç¨åºæ¡ç¨ç°¡å®ä½ç©©å¥çæ´é«ç­ç¥ï¼èä¸æ¬¡æ§éæ°è¨ç·´ç¸æ¯ï¼ä»»ååæ¨¡åç AUC å¢å  +1.75ï¼p<0.001ï¼ãæåå° Phikon (ViT-B) å Phikon-v2 (ViT-L) è 14 ç¨®ä¸åççµç¹ççå­¸ç¹å¾µèåå¨é²è¡æ¯è¼ï¼ä½¿æåçè©ä¼°æçºè¿ä»çºæ­¢æå¨é¢çè©ä¼°ãæåççµææ¯æè­æè¡¨æï¼DINOv2 æ¯ iBOT æ´è½èçè¯åæ¨¡ååè³æç¸®æ¾ãæ­¤å¤ï¼æåè¡¨æï¼å¨ä½¿ç¨ GigaPath å H-Optimus-0ï¼å©åå·æ 1.1B åæ¸ç ViT-gï¼é²è¡çç©æ¨è¨é æ¸¬çèæ¯ä¸ï¼æè¿çç¸®æ¾å·¥ä½å°ä¸æ¸¸æè½æ´é«æçãç¶èï¼ææ°æè½æä½³ç FM ä¹éççµ±è¨ééå¤§å¤ä»ç¶ä¸é¡¯èï¼æäºçè³å¨ç¹å®é©æçæä»»åï¼ä¾å¦ MSI é æ¸¬ï¼ä¸è¡¨ç¾ä¸ä½³ï¼è¢«ä¸åç±å§é¨éç¼ç 13 åè¼å°æ¨¡åæåä»£ãéç¶ææ°çåºç¤æ¨¡åå¯è½æå°è¨åºé¨ç½²é æéå¶ï¼ä½å®åä»ç¶çºéç¼æ´å°æ¥­ä¸æ´å·ææ¬æçççµç¹ççå­¸ç·¨ç¢¼å¨æä¾äºçµä½³çåºç¤ï¼ä»¥æ¨å AI å°åçè¨ºæ·å·¥å·ã</paragraph>

##### **Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation**
2409.09135v1 by Cheng Charles Ma, Kevin Hyekang Joo, Alexandria K. Vail, Sunreeta Bhattacharya, Ãlvaro FernÃ¡ndez GarcÃ­a, Kailana Baker-Matsuoka, Sheryl Mathew, Lori L. Holt, Fernando De la Torre

Over the past decade, wearable computing devices (``smart glasses'') have
undergone remarkable advancements in sensor technology, design, and processing
power, ushering in a new era of opportunity for high-density human behavior
data. Equipped with wearable cameras, these glasses offer a unique opportunity
to analyze non-verbal behavior in natural settings as individuals interact. Our
focus lies in predicting engagement in dyadic interactions by scrutinizing
verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.
Leveraging such analyses may revolutionize our understanding of human
communication, foster more effective collaboration in professional
environments, provide better mental health support through empathetic virtual
interactions, and enhance accessibility for those with communication barriers.
  In this work, we collect a dataset featuring 34 participants engaged in
casual dyadic conversations, each providing self-reported engagement ratings at
the end of each conversation. We introduce a novel fusion strategy using Large
Language Models (LLMs) to integrate multiple behavior modalities into a
``multimodal transcript'' that can be processed by an LLM for behavioral
reasoning tasks. Remarkably, this method achieves performance comparable to
established fusion techniques even in its preliminary implementation,
indicating strong potential for further research and optimization. This fusion
method is one of the first to approach ``reasoning'' about real-world human
behavior through a language model. Smart glasses provide us the ability to
unobtrusively gather high-density multimodal data on human behavior, paving the
way for new approaches to understanding and improving human communication with
the potential for important societal benefits. The features and data collected
during the studies will be made publicly available to promote further research.

æè¦ï¼<paragraph>å¨éå»åå¹´ä¸­ï¼å¯ç©¿æ´è¨ç®è£ç½®ï¼ãæºæ§ç¼é¡ãï¼å¨ææ¸¬å¨æè¡ãè¨­è¨åèçè½åæ¹é¢ç¶æ­·äºé¡¯èçé²æ­¥ï¼çºé«å¯åº¦äººé¡è¡çºè³æéåäºä¸åæ°çæ©ææä»£ãéåå¯ç©¿æ´ç¸æ©çéäºç¼é¡æä¾äºä¸åç¨ç¹çæ©æï¼å¯ä»¥å¨åäººäºåæåæèªç¶ç°å¢ä¸­çéèªè¨è¡çºãæåçéé»å¨æ¼ééä»ç´°å¯©æ¥è¨èªåéè¨èªç·ç´¢ä¾é æ¸¬éäººäºåä¸­çåèåº¦ï¼æ¨å¨åµæ¸¬åºä¸æèè¶£æå°æçè·¡è±¡ãå©ç¨æ­¤é¡åæå¯è½æå¾¹åºæ¹è®æåå°äººé¡æºéççè§£ï¼ä¿é²å°æ¥­ç°å¢ä¸­æ´ææçåä½ï¼ééåçå¿çèæ¬äºåæä¾æ´å¥½çå¿çå¥åº·æ¯æï¼ä¸¦å¢å¼·æºééç¤èçå¯åæ§ãå¨éé å·¥ä½ä¸­ï¼æåæ¶éäºä¸åè³æéï¼å¶ä¸­åå« 34 ä½åèèåèäºé¨æçéäººå°è©±ï¼æ¯ä½åèèå¨å°è©±çµææé½æä¾äºèªæå ±åçåèåº¦è©åãæåå¼å¥äºä¸ç¨®ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çæ°ç©èåç­ç¥ï¼å°å¤ç¨®è¡çºæ¨¡å¼æ´åå°ãå¤æ¨¡å¼è½éãä¸­ï¼LLM å¯ä»¥èçæ­¤è½éä»¥é²è¡è¡çºæ¨çä»»åãå¼å¾æ³¨æçæ¯ï¼å³ä½¿å¨åæ­¥å¯¦ä½ä¸­ï¼æ­¤æ¹æ³ä¹è½éå°èæ¢å®çèåæè¡ç¸ç¶çæè½ï¼é¡¯ç¤ºåºé²ä¸æ­¥ç ç©¶åæä½³åçå¼·å¤§æ½åãæ­¤èåæ¹æ³æ¯é¦æ¹ééèªè¨æ¨¡åä¾æ¢è¨éæ¼çå¯¦ä¸çäººé¡è¡çºçãæ¨çãçæ¹æ³ä¹ä¸ãæºæ§ç¼é¡è®æåè½å¤ ä¸å¼äººæ³¨ç®å°æ¶éäººé¡è¡çºçé«å¯åº¦å¤æ¨¡å¼è³æï¼çºçè§£åæ¹åäººé¡æºééªå¹³äºéè·¯ï¼ä¸¦å·æå¸¶ä¾éè¦ç¤¾ææççæ½åãç ç©¶æéæ¶éå°çç¹å¾µåè³æå°å¬éæä¾ï¼ä»¥ä¿é²é²ä¸æ­¥çç ç©¶ã</paragraph>

##### **MAISI: Medical AI for Synthetic Imaging**
2409.11169v1 by Pengfei Guo, Can Zhao, Dong Yang, Ziyue Xu, Vishwesh Nath, Yucheng Tang, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, Daguang Xu

Medical imaging analysis faces challenges such as data scarcity, high
annotation costs, and privacy concerns. This paper introduces the Medical AI
for Synthetic Imaging (MAISI), an innovative approach using the diffusion model
to generate synthetic 3D computed tomography (CT) images to address those
challenges. MAISI leverages the foundation volume compression network and the
latent diffusion model to produce high-resolution CT images (up to a landmark
volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel
spacing. By incorporating ControlNet, MAISI can process organ segmentation,
including 127 anatomical structures, as additional conditions and enables the
generation of accurately annotated synthetic images that can be used for
various downstream tasks. Our experiment results show that MAISI's capabilities
in generating realistic, anatomically accurate images for diverse regions and
conditions reveal its promising potential to mitigate challenges using
synthetic data.

æè¦ï¼é«å­¸å½±ååæé¢è¨è³æç¨å°ãæ¨è¨»ææ¬é«ãé±ç§åé¡ç­ææ°ãæ¬æä»ç´¹ç¨æ¼åæå½±åçé«å­¸ AIï¼MAISIï¼ï¼éæ¯ä¸ç¨®åµæ°çæ¹æ³ï¼ä½¿ç¨æ´æ£æ¨¡åä¾ç¢çåæ 3D é»è¦æ·å±¤ææ (CT) å½±åï¼ä»¥æå°éäºææ°ãMAISI æ¡ç¨åºç¤é«ç©å£ç¸®ç¶²è·¯åæ½å¨æ´æ£æ¨¡åï¼ä»¥ç¢çé«è§£æåº¦ CT å½±åï¼æé«å¯é 512 x 512 x 768 çå°æ¨é«ç©ç¶­åº¦ï¼ï¼å·æéæ´»çé«ç©ç¶­åº¦åé«ç´ éè·ãééæ´å ControlNetï¼MAISI å¯ä»¥èçå¨å®åå²ï¼åæ¬ 127 åè§£åçµæ§ï¼ä½çºé¡å¤çæ¢ä»¶ï¼ä¸¦è½ç¢çå¯ç²¾ç¢ºæ¨è¨»çåæå½±åï¼å¯ç¨æ¼åç¨®ä¸æ¸¸ä»»åãæåçå¯¦é©çµæé¡¯ç¤ºï¼MAISI å¨çºä¸ååååæ¢ä»¶ç¢çé¼çãè§£åå­¸ä¸ç²¾ç¢ºçå½±åæ¹é¢çè½åï¼æ­ç¤ºäºå®å¨ä½¿ç¨åæè³ææ¸è¼ææ°æ¹é¢çæ½åã

##### **SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**
2409.08936v1 by Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester

We present the SynSUM benchmark, a synthetic dataset linking unstructured
clinical notes to structured background variables. The dataset consists of
10,000 artificial patient records containing tabular variables (like symptoms,
diagnoses and underlying conditions) and related notes describing the fictional
patient encounter in the domain of respiratory diseases. The tabular portion of
the data is generated through a Bayesian network, where both the causal
structure between the variables and the conditional probabilities are proposed
by an expert based on domain knowledge. We then prompt a large language model
(GPT-4o) to generate a clinical note related to this patient encounter,
describing the patient symptoms and additional context. The SynSUM dataset is
primarily designed to facilitate research on clinical information extraction in
the presence of tabular background variables, which can be linked through
domain knowledge to concepts of interest to be extracted from the text - the
symptoms, in the case of SynSUM. Secondary uses include research on the
automation of clinical reasoning over both tabular data and text, causal effect
estimation in the presence of tabular and/or textual confounders, and
multi-modal synthetic data generation. The dataset can be downloaded from
https://github.com/prabaey/SynSUM.

æè¦ï¼æåæåº SynSUM åºæºï¼ä¸åå°éçµæ§åè¨åºè¨éé£çµå°çµæ§åèæ¯è®æ¸çåæè³æéãè©²è³æéåå« 10,000 åäººå·¥çæ­·ï¼å¶ä¸­åå«è¡¨æ ¼è®æ¸ï¼ä¾å¦ççãè¨ºæ·åæ½å¨çæ³ï¼åç¸éè¨éï¼æè¿°äºå¼å¸ç³»çµ±ç¾çé åä¸­çèæ§æ£èé­éãè³æçè¡¨æ ¼é¨åæ¯ééè²æ°ç¶²è·¯ç¢ççï¼å¶ä¸­è®æ¸ä¹éçå æçµæ§åæ¢ä»¶æ©çé½æ¯ç±å°å®¶æ ¹æé åç¥è­æåºçãç¶å¾ï¼æåæç¤ºä¸åå¤§åèªè¨æ¨¡å (GPT-4o) ç¢çèæ­¤æ£èé­éç¸éçè¨åºè¨éï¼æè¿°æ£èççåé¡å¤èæ¯ãSynSUM è³æéä¸»è¦æ¯çºäºä¿é²å¨è¡¨æ ¼èæ¯è®æ¸å­å¨çææ³ä¸é²è¡è¨åºè³è¨èåçç ç©¶ï¼éäºè®æ¸å¯ä»¥ééé åç¥è­é£çµå°å¾ææ¬ä¸­èåçç®æ¨æ¦å¿µ - å¨ SynSUM çæ¡ä¾ä¸­ï¼æ¯ççãæ¬¡è¦ç¨éåæ¬ç ç©¶è¡¨æ ¼è³æåææ¬çè¨åºæ¨çèªååãå¨è¡¨æ ¼å/æææ¬æ··æ·å å­å­å¨çææ³ä¸é²è¡å æææä¼°è¨ï¼ä»¥åå¤æ¨¡å¼åæè³æçæãæ­¤è³æéå¯å¾ https://github.com/prabaey/SynSUM ä¸è¼ã

##### **Recent Trends in Modelling the Continuous Time Series using Deep Learning: A Survey**
2409.09106v1 by Mansura Habiba, Barak A. Pearlmutter, Mehrdad Maleki

Continuous-time series is essential for different modern application areas,
e.g. healthcare, automobile, energy, finance, Internet of things (IoT) and
other related areas. Different application needs to process as well as analyse
a massive amount of data in time series structure in order to determine the
data-driven result, for example, financial trend prediction, potential
probability of the occurrence of a particular event occurrence identification,
patient health record processing and so many more. However, modeling real-time
data using a continuous-time series is challenging since the dynamical systems
behind the data could be a differential equation. Several research works have
tried to solve the challenges of modelling the continuous-time series using
different neural network models and approaches for data processing and
learning. The existing deep learning models are not free from challenges and
limitations due to diversity among different attributes, behaviour, duration of
steps, energy, and data sampling rate. This paper has described the general
problem domain of time series and reviewed the challenges of modelling the
continuous time series. We have presented a comparative analysis of recent
developments in deep learning models and their contribution to solving
different difficulties of modelling the continuous time series. We have also
identified the limitations of the existing neural network model and open
issues. The main goal of this review is to understand the recent trend of
neural network models used in a different real-world application with
continuous-time data.

æè¦ï¼é£çºæéåºåå°ä¸åçç¾ä»£æç¨é åè³ééè¦ï¼
ä¾å¦é«çä¿å¥ãæ±½è»ãè½æºãéèãç©è¯ç¶² (IoT) å
å¶ä»ç¸éé åãä¸åçæç¨éè¦èçä¸¦åæ
æéåºåçµæ§ä¸­çå¤§éæ¸æï¼ä»¥ç¢ºå®
æ¸æé©åççµæï¼ä¾å¦éèè¶¨å¢é æ¸¬ãæ½å¨
ç¹å®äºä»¶ç¼çè­å¥çç¼çæ©çã
æ£èå¥åº·è¨éèçç­ç­ãç¶èï¼ä½¿ç¨é£çºæéåºåå»ºæ¨¡å¯¦æ
æ¸æå·æææ°æ§ï¼å çºæ¸æèå¾çååç³»çµ±å¯è½æ¯å¾®åæ¹ç¨å¼ãå¹¾é ç ç©¶å·¥ä½å·²ç¶
åè©¦ä½¿ç¨ä¸åçç¥ç¶ç¶²è·¯æ¨¡ååæ¸æèçæ¹æ³ä¾è§£æ±ºå»ºæ¨¡é£çºæéåºåçææ°
åå­¸ç¿ãç¾æçæ·±åº¦å­¸ç¿æ¨¡åä¸¦éæ²æææ°å
éå¶ï¼å çºä¸åçå±¬æ§ãè¡çºãæ­¥é©æçºæéãè½éåæ¸ææ¡æ¨£çä¹éå­å¨å·®ç°ãæ¬ææè¿°äºæéåºåçä¸è¬åé¡é åï¼ä¸¦åé¡§äºå»ºæ¨¡é£çºæéåºåçææ°ãæåå°æ·±åº¦å­¸ç¿æ¨¡åçææ°ç¼å±åå¶å°è§£æ±ºå»ºæ¨¡é£çºæéåºåçä¸åé£åº¦çè²¢ç»é²è¡äºæ¯è¼åæãæåéç¢ºå®äºç¾æç¥ç¶ç¶²è·¯æ¨¡åçå±éæ§åéæ¾åé¡ãéç¯è©è«çä¸»è¦ç®çæ¯äºè§£ç¥ç¶ç¶²è·¯æ¨¡åå¨ä¸åå¯¦éæç¨ä¸­ä½¿ç¨çææ°è¶¨å¢ï¼ä¸¦ä½¿ç¨é£çºæéæ¸æã

##### **A BERT-Based Summarization approach for depression detection**
2409.08483v1 by Hossein Salahshoor Gavalan, Mohmmad Naim Rastgoo, Bahareh Nakisa

Depression is a globally prevalent mental disorder with potentially severe
repercussions if not addressed, especially in individuals with recurrent
episodes. Prior research has shown that early intervention has the potential to
mitigate or alleviate symptoms of depression. However, implementing such
interventions in a real-world setting may pose considerable challenges. A
promising strategy involves leveraging machine learning and artificial
intelligence to autonomously detect depression indicators from diverse data
sources. One of the most widely available and informative data sources is text,
which can reveal a person's mood, thoughts, and feelings. In this context,
virtual agents programmed to conduct interviews using clinically validated
questionnaires, such as those found in the DAIC-WOZ dataset, offer a robust
means for depression detection through linguistic analysis. Utilizing
BERT-based models, which are powerful and versatile yet use fewer resources
than contemporary large language models, to convert text into numerical
representations significantly enhances the precision of depression diagnosis.
These models adeptly capture complex semantic and syntactic nuances, improving
the detection accuracy of depressive symptoms. Given the inherent limitations
of these models concerning text length, our study proposes text summarization
as a preprocessing technique to diminish the length and intricacies of input
texts. Implementing this method within our uniquely developed framework for
feature extraction and classification yielded an F1-score of 0.67 on the test
set surpassing all prior benchmarks and 0.81 on the validation set exceeding
most previous results on the DAIC-WOZ dataset. Furthermore, we have devised a
depression lexicon to assess summary quality and relevance. This lexicon
constitutes a valuable asset for ongoing research in depression detection.

æè¦ï¼æé¬±çæ¯ä¸ç¨®å¨çæ®éå­å¨çå¿çç¾çï¼å¦æä¸å ä»¥è§£æ±ºï¼å¯è½æé æå´éçå¾æï¼å°¤å¶æ¯å°æå¾©ç¼æ§ç¼ä½çäººãååçç ç©¶è¡¨æï¼æ©æä»å¥æå¯è½æ¸è¼æç·©è§£æé¬±ççãç¶èï¼å¨ç¾å¯¦ç°å¢ä¸­å¯¦æ½æ­¤é¡å¹²é æªæ½å¯è½æå¸¶ä¾ç¸ç¶å¤§çææ°ãä¸åæåéçç­ç¥åæ¬å©ç¨æ©å¨å­¸ç¿åäººå·¥æºæ§ï¼å¾ä¸åçæ¸æä¾æºä¸­èªåæª¢æ¸¬æé¬±çææ¨ãæå»£æ³å¯ç¨ä¸ææè³è¨çæ¸æä¾æºä¹ä¸æ¯æå­ï¼å®å¯ä»¥æ­ç¤ºä¸åäººçæç·ãæ³æ³åæåãå¨æ­¤èçµ¡ä¸­ï¼ä½¿ç¨è¨åºé©è­åå·ï¼ä¾å¦å¨ DAIC-WOZ è³æéä¸­æ¾å°çåå·ï¼ç·¨å¯«ç¨å¼é²è¡è¨ªè«çèæ¬ä»£çäººï¼æä¾äºä¸ç¨®ééèªè¨åæé²è¡æé¬±çæª¢æ¸¬çå¼·å¤§æ¹æ³ãå©ç¨ BERT åºç¤æ¨¡åï¼åè½å¼·å¤§ä¸ç¨éå»£æ³ï¼ä½ä½¿ç¨çè³æºæ¯ç¶ä»£å¤§åèªè¨æ¨¡åå°ï¼å°æå­è½æçºæ¸å¼è¡¨ç¤ºï¼å¯é¡¯èæé«æé¬±çè¨ºæ·çæºç¢ºæ§ãéäºæ¨¡åå·§å¦å°ææè¤éçèªç¾©åå¥æ³ç´°å¾®å·®å¥ï¼æé«æé¬±çççæª¢æ¸¬æºç¢ºæ§ãéæ¼éäºæ¨¡åå¨æå­é·åº¦æ¹é¢å­å¨åºæç¼ºé·ï¼æåçç ç©¶æåºæå­æè¦ä½çºé èçæè¡ï¼ä»¥æ¸å°è¼¸å¥æå­çé·åº¦åè¤éæ§ãå¨æåç¨èªéç¼çåè½æåååé¡æ¡æ¶ä¸­å¯¦æ½æ­¤æ¹æ³ï¼å¨æ¸¬è©¦éä¸­ç¢ç 0.67 ç F1 åæ¸ï¼è¶è¶ææååçåºæºï¼å¨é©è­éä¸­ç¢ç 0.81 ç F1 åæ¸ï¼è¶é DAIC-WOZ è³æéä¸å¤§å¤æ¸ååççµæãæ­¤å¤ï¼æåè¨­è¨äºä¸åæé¬±çè©å½è¡¨ï¼ç¨æ¼è©ä¼°æè¦åè³ªåç¸éæ§ãæ­¤è©å½è¡¨æ§ææé¬±çæª¢æ¸¬æçºç ç©¶çå¯¶è²´è³ç¢ã

##### **Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**
2409.07793v1 by Fuchen Zheng, Quanjun Li, Weixuan Li, Xuhang Chen, Yihang Dong, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a critical application of semantic segmentation
in healthcare, has seen significant advancements through specialized computer
vision techniques. While deep learning-based medical image segmentation is
essential for assisting in medical diagnosis, the lack of diverse training data
causes the long-tail problem. Moreover, most previous hybrid CNN-ViT
architectures have limited ability to combine various attentions in different
layers of the Convolutional Neural Network. To address these issues, we propose
a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware
Contrastive Loss, as the overall training objective for semi-supervised
learning to mitigate the long-tail problem. Additionally, we introduce
CMAformer, a novel network that synergizes the strengths of ResUNet and
Transformer. The cross-attention block in CMAformer effectively integrates
spatial attention and channel attention for multi-scale feature fusion.
Overall, our results indicate that CMAformer, combined with the feature fusion
framework and the new consistency loss, demonstrates strong complementarity in
semi-supervised learning ensembles. We achieve state-of-the-art results on
multiple public medical image datasets. Example code are available at:
\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.

æè¦ï¼é«å­¸å½±ååå²æ¯èªæåå²å¨é«çä¿å¥é åä¸­çä¸é éè¦æç¨ï¼å·²ééå°æ¥­çé»è¦è¦è¦ºæè¡ç²å¾é¡¯èé²å±ãéç¶åºæ¼æ·±åº¦å­¸ç¿çé«å­¸å½±ååå²å°æ¼åå©é«çè¨ºæ·è³ééè¦ï¼ä½ç¼ºä¹å¤æ¨£åçè¨ç·´è³ææå°è´é·å°¾åé¡ãæ­¤å¤ï¼å¤§å¤æ¸ååçæ··åå¼ CNN-ViT æ¶æ§å¨çµåå·ç©ç¥ç¶ç¶²è·¯ä¸åå±¤ä¸­çåç¨®æ³¨æåæ¹é¢è½åæéãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®ææ ¼ææ¥å°å¶ä¸è´æ§ (LDC) æå¤±ï¼ä¸¦èéçæç¥å°æ¯æå¤±æ´åï¼ä½çºåç£ç£å¼å­¸ç¿çæ´é«è¨ç·´ç®æ¨ï¼ä»¥æ¸è¼é·å°¾åé¡ãæ­¤å¤ï¼æåä»ç´¹äº CMAformerï¼éæ¯ä¸åæ°ç©çç¶²è·¯ï¼å®ååäº ResUNet å Transformer çåªé»ãCMAformer ä¸­çäº¤åæ³¨æååå¡ææå°æ´åäºç©ºéæ³¨æååééæ³¨æåï¼ä»¥é²è¡å¤å°ºåº¦ç¹å¾µèåãç¸½çä¾èªªï¼æåççµæè¡¨æï¼CMAformer çµåç¹å¾µèåæ¶æ§åæ°çç¨ å¯æå¤±ï¼å¨åç£ç£å¼å­¸ç¿éåä¸­å±ç¾åºå¼·å¤§çäºè£æ§ãæåå¨å¤åå¬éé«å­¸å½±åè³æéä¸åå¾äºæåé²çææãç¯ä¾ç¨å¼ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}ã

##### **ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**
2409.07779v1 by Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a crucial task in computer vision, facilitates
the automated delineation of anatomical structures and pathologies, supporting
clinicians in diagnosis, treatment planning, and disease monitoring. Notably,
transformers employing shifted window-based self-attention have demonstrated
exceptional performance. However, their reliance on local window attention
limits the fusion of local and global contextual information, crucial for
segmenting microtumors and miniature organs. To address this limitation, we
propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer
architecture that effectively integrates local and global features for precise
medical image segmentation. ASSNet comprises a transformer-based U-shaped
encoder-decoder network. The encoder utilizes shifted window self-attention
across five resolutions to extract multi-scale features, which are then
propagated to the decoder through skip connections. We introduce an augmented
multi-layer perceptron within the encoder to explicitly model long-range
dependencies during feature extraction. Recognizing the constraints of
conventional symmetrical encoder-decoder designs, we propose an Adaptive
Feature Fusion (AFF) decoder to complement our encoder. This decoder
incorporates three key components: the Long Range Dependencies (LRD) block, the
Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)
block. These components synergistically facilitate the effective fusion of
multi-scale features extracted by the decoder while capturing long-range
dependencies and refining object boundaries. Comprehensive experiments on
diverse medical image segmentation tasks, including multi-organ, liver tumor,
and bladder tumor segmentation, demonstrate that ASSNet achieves
state-of-the-art results. Code and models are available at:
\url{https://github.com/lzeeorno/ASSNet}.

æè¦ï¼<paragraph>é«å­¸å½±ååå²æ¯é»è¦è¦è¦ºä¸­ä¸é éè¦çä»»åï¼æå©æ¼èªåæç¹ªè§£åçµæ§åççï¼åå©è¨åºé«å¸«é²è¡è¨ºæ·ãæ²»çè¨ç«åç¾çç£æ§ãå¼å¾æ³¨æçæ¯ï¼æ¡ç¨ä½ç§»è¦çªèªæ³¨æåæ©å¶çTransformerå±ç¾åºéå¡çæè½ãç¶èï¼å®åä¾è³´æ¼ååè¦çªæ³¨æåï¼ééå¶äºåååå¨åèçµ¡è³è¨çèåï¼èéå°æ¼åå²å¾®å°è«ç¤åå¾®åå¨å®è³ééè¦ãçºäºè§£æ±ºéåéå¶ï¼æåæåºäºèªé©æèªç¾©åå²ç¶²è·¯ (ASSNet)ï¼éæ¯ä¸åTransformeræ¶æ§ï¼å¯ä»¥æææ´ååååå¨åç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé«å­¸å½±ååå²ãASSNet åå«ä¸ååºæ¼Transformerç U åç·¨ç¢¼å¨-è§£ç¢¼å¨ç¶²è·¯ãç·¨ç¢¼å¨å©ç¨äºåè§£æåº¦çä½ç§»è¦çªèªæ³¨æåä¾èåå¤å°ºåº¦ç¹å¾µï¼ç¶å¾ééè·³èºé£ç·å°éäºç¹å¾µå³æ­å°è§£ç¢¼å¨ãæåå¨ç·¨ç¢¼å¨ä¸­å¼å¥äºæ´å¢çå¤å±¤æç¥å¨ï¼ä»¥ä¾¿å¨ç¹å¾µèåæéæç¢ºå°å»ºæ¨¡é·ç¨ä¾è³´æ§ãéæ¼å³çµ±å°ç¨±ç·¨ç¢¼å¨-è§£ç¢¼å¨è¨­è¨çéå¶ï¼æåæåºäºä¸åèªé©æç¹å¾µèå (AFF) è§£ç¢¼å¨ä¾è£åæåçç·¨ç¢¼å¨ãæ­¤è§£ç¢¼å¨åå«ä¸åééµçµæé¨åï¼é·ç¨ä¾è³´æ§ (LRD) åå¡ãå¤å°ºåº¦ç¹å¾µèå (MFF) åå¡åèªé©æèªç¾©ä¸­å¿ (ASC) åå¡ãéäºçµæé¨åç¸äºéåï¼ä¿æè§£ç¢¼å¨èåçå¤å°ºåº¦ç¹å¾µææèåï¼åæææé·ç¨ä¾è³´æ§ä¸¦å¾®èª¿ç©ä»¶éçãå¨å¤å¨å®ãèèè«ç¤åèè±è«ç¤åå²ç­åç¨®é«å­¸å½±ååå²ä»»åä¸çå¨é¢å¯¦é©è­æï¼ASSNet éå°äºæåé²çææãç¨å¼ç¢¼åæ¨¡åå¯æ¼ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/ASSNet}ã</paragraph>

##### **SoK: Security and Privacy Risks of Medical AI**
2409.07415v1 by Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang

The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.

æè¦ï¼ç§æèé«ççæ´åéåäºä¸åæ°ç´åï¼ç±äººå·¥æºæ§åæ©å¨å­¸ç¿é©åçè»é«ç³»çµ±å·²æçºé«çç¢ååæåçå¿è¦çµæé¨åãéç¶éäºé²æ­¥å°æ¹åæ£èç§è­·åé«çä¿å¥æä¾æçæå¾å¤§çå¹«å©ï¼ä½å®åä¹è®ææçé«çè³æåç³»çµ±å®æ´æ§é¢è¨æ½å¨çç¶²è·¯æ»æé¢¨éªãæ¬ææ¢è¨äºäººå·¥æºæ§/æ©å¨å­¸ç¿æç¨å¨é«çä¿å¥ä¸­å¸¶ä¾çå®å¨æ§åé±ç§å¨èãééå¾¹åºæª¢è¦åé é«çé åç¾æçç ç©¶ï¼æåç¼ç¾äºå¨äºè§£éå°é«çäººå·¥æºæ§ç³»çµ±çå°ææ§æ»ææ¹é¢æé¡¯èçå·®è·ãééæ¦è¿°é«çç°å¢çç¹å®å°ææ§å¨èæ¨¡åä¸¦æ¾åºå®¹æåæ»æçæç¨é åï¼æåçºæªä¾ç ç©¶å¥ å®åºç¤ï¼æ¢è¨äººå·¥æºæ§é©åé«çç³»çµ±çå®å¨æ§èå¾©ååãééåæä¸åçå¨èæ¨¡ååéå°ä¸åé«çé åçå°ææ§æ»æå¯è¡æ§ç ç©¶ï¼æåå°äººå·¥æºæ§é«çä¿å¥æè¡å¿«éç¼å±é åä¸­ç¶²è·¯å®å¨ç ç©¶çè¿«åéæ±æä¾äºä»¤äººä¿¡æçè¦è§£ã

##### **Federated Impression for Learning with Distributed Heterogeneous Data**
2409.07351v1 by Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.

æè¦ï¼æ¨æºçæ·±åº¦å­¸ç¿åé¡æ¹æ³å¨å¯¦éçè¨åºæç¨ä¸­å¯è½ä¸¦ä¸ç¸½æ¯å¯¦ç¨çï¼å çºå®åéè¦éä¸­æ¶éæææ¨£æ¬ãè¯é¦å­¸ç¿ (FL) æä¾äºä¸åç¯ä¾ï¼å¯ä»¥å¨ä¸è®å®¢æ¶ç«¯åäº«æ¸æçææ³ä¸å¾åå¸å¼æ¸æéå­¸ç¿ï¼éæå©æ¼æ¸è¼é±ç§åæ¸ææææ¬åé¡ãå¨ FL ä¸­ï¼ç±æ¼ä¸åé«çä¸­å¿çæ¸ææ¶éåå®åæ£èäººå£çµ±è¨è³æçå·®ç°ï¼ä¾èªä¸åé«çä¸­å¿çæ¸æä¹éå¸¸è¦çæ¸æç°è³ªæ§æå°è´æ¬¡æä½³æ¶æãééæ¬ç ç©¶ä¸­çå¯¦é©ï¼æåè¡¨ææ¸æç°è³ªæ§æå°è´å±é¨è¨ç·´æéç¼çç½é£æ§éºå¿ç¾è±¡ãæåæåº FedImpresï¼å®éééåè¡¨ç¤ºå¨çè³è¨çåæè³æä½çºè¯é¦å°è±¡ä¾æ¸è¼ç½é£æ§éºå¿ãçºæ­¤ï¼æåæçåºæ¯ä¸è¼ªéè¨æç¢ççå¨çæ¨¡åãé¨å¾ï¼æåä½¿ç¨åæè³æåæ¬å°è³æä¾å¢å¼·æ¬å°è¨ç·´çæ¦æ¬æ§ãå»£æ³çå¯¦é©è¡¨æï¼ææåºçæ¹æ³å¨ BloodMNIST å Retina æ¸æéä¸é½éå°äºæåé²çæè½ï¼éäºæ¸æéåå«æ¨ç±¤ä¸å¹³è¡¡åé åè½ç§»ï¼åé¡æºç¢ºåº¦æé«äº 20%ã

##### **MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**
2409.07314v1 by Praveen K Kanithi, ClÃ©ment Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥æç¨æ¹é¢çå¿«éç¼å±ï¼ä¿ä½¿äººåå¼ç±²é²è¡æ´é«è©ä¼°ï¼è¶è¶ç¶å¸¸å¼ç¨çåºæºï¼ä¾å¦ USMLEï¼ï¼ä»¥æ´å¥½å°åæ å¯¦éæè½ãåç®¡å¯¦éè©ä¼°æ¯å¯¦ç¨æ§çå¯¶è²´ææ¨ï¼ä½å®åéå¸¸è½å¾æ¼ LLM æ¼åçéåº¦ï¼å¨é¨ç½²å¾å¯è½æä½¿ç ç©¶çµæéæãéç¨®æéä¸çè«ç¯éè¦é²è¡å¨é¢çåæè©ä¼°ï¼ä»¥æå°ç¹å®è¨åºæç¨ç¨å¼çæ¨¡åé¸æãæåå¼é² MEDICï¼ä¸åè©ä¼° LLM è·¨è¶è¨åºè½åçäºåééµé¢åçæ¶æ§ï¼é«çæ¨çãå«çååå·®ãè³æåèªè¨çè§£ãæå¢å­¸ç¿åè¨åºå®å¨æ§ãMEDIC æ¡ç¨ä¸ç¨®æ°ç©çäº¤äºå¼æª¢æ¥æ¶æ§ï¼éå LLM å¨æ¶µèç¯ååå¹»è¦ºåµæ¸¬ç­é åçæè½ï¼èä¸éè¦åèè¼¸åºãæåä½¿ç¨ MEDIC ä¾è©ä¼° LLM å¨é«çåé¡è§£ç­ãå®å¨æ§ãæè¦ãç­è¨ç¢çåå¶ä»ä»»åä¸çè¡¨ç¾ãæåççµæé¡¯ç¤ºï¼ä¸åæ¨¡åå¤§å°ãåºæºèç¶éé«çå¾®èª¿çæ¨¡åä¹éçæè½å·®ç°ï¼ä¸¦å°éè¦ç¹å®æ¨¡ååªå¢çæç¨ç¨å¼ï¼ä¾å¦ä½å¹»è¦ºæè¼ä½çæ¨è«ææ¬ï¼çæ¨¡åé¸æç¢çå½±é¿ãMEDIC çå¤é¢åè©ä¼°æ­ç¤ºäºéäºæè½æ¬è¡¡ï¼ç¸®å°äºçè«è½åèé«çä¿å¥ç°å¢ä¸­çå¯¦éå¯¦ä½ä¹éçå·®è·ï¼ç¢ºä¿æ¾åºææå¸æçæ¨¡åï¼ä¸¦éå°ä¸åçé«çä¿å¥æç¨ç¨å¼é²è¡èª¿æ´ã

##### **Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**
2409.07186v2 by Sheng Chen, Zihao Tang, Mariano Cabezas, Xinyi Wang, Arkiev D'Souza, Michael Barnett, Fernando Calamante, Weidong Cai, Chenyu Wang

Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging
(MRI) technique sensitised to the diffusivity of water molecules, offering the
capability to inspect tissue microstructures and is the only in-vivo method to
reconstruct white matter fiber tracts non-invasively. The DWI signal can be
analysed with the diffusion tensor imaging (DTI) model to estimate the
directionality of water diffusion within voxels. Several scalar metrics,
including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity
(RD), and fractional anisotropy (FA), can be further derived from DTI to
quantitatively summarise the microstructural integrity of brain tissue. These
scalar metrics have played an important role in understanding the organisation
and health of brain tissue at a microscopic level in clinical studies. However,
reliable DTI metrics rely on DWI acquisitions with high gradient directions,
which often go beyond the commonly used clinical protocols. To enhance the
utility of clinically acquired DWI and save scanning time for robust DTI
analysis, this work proposes DirGeo-DTI, a deep learning-based method to
estimate reliable DTI metrics even from a set of DWIs acquired with the minimum
theoretical number (6) of gradient directions. DirGeo-DTI leverages directional
encoding and geometric constraints to facilitate the training process. Two
public DWI datasets were used for evaluation, demonstrating the effectiveness
of the proposed method. Extensive experimental results show that the proposed
method achieves the best performance compared to existing DTI enhancement
methods and potentially reveals further clinical insights with routine clinical
DWI scans.

æè¦ï¼æ´æ£å æ¬å½±å (DWI) æ¯ä¸ç¨®ç£æ¯é å½± (MRI) æè¡ï¼å°æ°´åå­æ´æ£ææï¼è½æª¢æ¸¬çµç¹å¾®çµæ§ï¼ä¸æ¯å¯ä¸ä¸ç¨®å¯éä¾µå¥æ§éå»ºç½è³ªçºç¶­æçé«å§æ¹æ³ãDWI è¨èå¯ç¨æ´æ£å¼µéå½±å (DTI) æ¨¡ååæï¼ä»¥ä¼°è¨é«ç´ å§æ°´åæ´æ£çæ¹åæ§ãæ¸åæ¨ééæ¸¬ï¼åæ¬è»¸åæ´æ£ç (AD)ãå¹³åæ´æ£ç (MD)ãå¾åæ´æ£ç (RD) ååæ¸ååç°æ§ (FA)ï¼å¯é²ä¸æ­¥å¾ DTI è¡çï¼ä»¥éåç¸½çµè¦çµç¹çå¾®çµæ§å®æ´æ§ãéäºæ¨ééæ¸¬å¨è¨åºç ç©¶ä¸­äºè§£è¦çµç¹å¨å¾®è§å±¤ç´ççµç¹åå¥åº·æ¹é¢ç¼æ®äºéè¦ä½ç¨ãç¶èï¼å¯é ç DTI éæ¸¬ä¾è³´æ¼å·æé«æ¢¯åº¦æ¹åç DWI æ·åï¼ééå¸¸è¶åºå¸¸ç¨çè¨åºåå®ãçºäºæåè¨åºæ·å DWI çæç¨ï¼ä¸¦ç¯çç©©å¥ DTI åæçæææéï¼æ¬ç ç©¶æåº DirGeo-DTIï¼ä¸ç¨®åºæ¼æ·±åº¦å­¸ç¿çæ¹æ³ï¼å³ä½¿å¾å·åæå°çè«æ¸é (6) åæ¢¯åº¦æ¹åç DWI çµä¹è½ä¼°è¨å¯é ç DTI éæ¸¬ãDirGeo-DTI å©ç¨æ¹åç·¨ç¢¼åå¹¾ä½ç´æä¾ä¿é²è¨ç·´éç¨ãå©åå¬éç DWI è³æéç¨æ¼è©ä¼°ï¼è­æäºææåºæ¹æ³çæææ§ãå¤§éçå¯¦é©çµæé¡¯ç¤ºï¼èç¾æç DTI å¢å¼·æ¹æ³ç¸æ¯ï¼ææåºçæ¹æ³ç²å¾äºæä½³çæè½ï¼ä¸¦æå¯è½ééä¾è¡è¨åº DWI æææ­ç¤ºé²ä¸æ­¥çè¨åºè¦è§£ã

##### **CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**
2409.07092v1 by Feiyang Jia, Zhineng Chen, Ziying Song, Lin Liu, Caiyan Jia

Super-resolution (SR) aims to enhance the quality of low-resolution images
and has been widely applied in medical imaging. We found that the design
principles of most existing methods are influenced by SR tasks based on
real-world images and do not take into account the significance of the
multi-level structure in pathological images, even if they can achieve
respectable objective metric evaluations. In this work, we delve into two
super-resolution working paradigms and propose a novel network called CWT-Net,
which leverages cross-scale image wavelet transform and Transformer
architecture. Our network consists of two branches: one dedicated to learning
super-resolution and the other to high-frequency wavelet features. To generate
high-resolution histopathology images, the Transformer module shares and fuses
features from both branches at various stages. Notably, we have designed a
specialized wavelet reconstruction module to effectively enhance the wavelet
domain features and enable the network to operate in different modes, allowing
for the introduction of additional relevant information from cross-scale
images. Our experimental results demonstrate that our model significantly
outperforms state-of-the-art methods in both performance and visualization
evaluations and can substantially boost the accuracy of image diagnostic
networks.

æè¦ï¼è¶è§£æåº¦ (SR) æ¨å¨æåä½è§£æåº¦å½±åçåè³ªï¼ä¸¦å·²å»£æ³æç¨æ¼é«å­¸å½±åãæåç¼ç¾ç¾ææ¹æ³çå¤§é¨åè¨­è¨ååé½åå°åºæ¼çå¯¦å½±åç SR ä»»åå½±é¿ï¼èä¸å³ä½¿å®åè½éå°å¯è§çå®¢è§ææ¨è©ä¼°ï¼ä¹ä¸æèæ®ççå½±åä¸­å¤å±¤ç´çµæ§çéè¦æ§ãå¨éé å·¥ä½ä¸­ï¼æåæ·±å¥æ¢è¨å©ç¨®è¶è§£æåº¦å·¥ä½ç¯ä¾ï¼ä¸¦æåºä¸ååçº CWT-Net çæ°åç¶²è·¯ï¼å®å©ç¨è·¨å°ºåº¦å½±åå°æ³¢è½æå Transformer æ¶æ§ãæåçç¶²è·¯åå«å©ååæ¯ï¼ä¸åå°éç¨æ¼å­¸ç¿è¶è§£æåº¦ï¼å¦ä¸ååç¨æ¼é«é »å°æ³¢ç¹å¾µãçºäºç¢çé«è§£æåº¦çµç¹ççå­¸å½±åï¼Transformer æ¨¡çµæå¨ä¸åéæ®µåäº«åèåä¾èªå©ååæ¯çç¹å¾µãå¼å¾æ³¨æçæ¯ï¼æåè¨­è¨äºä¸åå°éçå°æ³¢éå»ºæ¨¡çµï¼ä»¥ææå¢å¼·å°æ³¢åç¹å¾µï¼ä¸¦è®ç¶²è·¯è½å¤ å¨ä¸åæ¨¡å¼ä¸éä½ï¼åè¨±å¾è·¨å°ºåº¦å½±åä¸­å¼å¥å¶ä»ç¸éè³è¨ãæåçå¯¦é©çµæè­æï¼æåçæ¨¡åå¨æè½åè¦è¦ºåè©ä¼°æ¹é¢é½å¤§å¹åªæ¼ç¾ææè¡ï¼èä¸è½å¤§å¹æåå½±åè¨ºæ·ç¶²è·¯çæºç¢ºåº¦ã

##### **Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**
2409.07012v1 by Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi

Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals
to assess patient conditions and monitor changes over time. Generative models,
specifically diffusion-based models, have shown promise in generating realistic
synthetic X-rays. However, these models mainly focus on conditional generation
using single-time-point data, i.e., typically CXRs taken at a specific time
with their corresponding reports, limiting their clinical utility, particularly
for capturing temporal changes. To address this limitation, we propose a novel
framework, EHRXDiff, which predicts future CXR images by integrating previous
CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc.
Our framework dynamically tracks and predicts disease progression based on a
latent diffusion model, conditioned on the previous CXR image and a history of
medical events. We comprehensively evaluate the performance of our framework
across three key aspects, including clinical consistency, demographic
consistency, and visual realism. We demonstrate that our framework generates
high-quality, realistic future images that capture potential temporal changes,
suggesting its potential for further development as a clinical simulation tool.
This could offer valuable insights for patient monitoring and treatment
planning in the medical field.

æè¦ï¼è¸é¨ X åå½±åï¼CXRï¼æ¯ä¸ç¨®éè¦çè¨ºæ·å·¥å·ï¼ç¨æ¼é«é¢è©ä¼°çæ£çæ³ä¸¦ç£æ§å¶é¨èæéçè®åãçææ¨¡åï¼ç¹å¥æ¯åºæ¼æ´æ£çæ¨¡åï¼å·²å¨çæé¼ççåæ X åå½±åæ¹é¢å±ç¾åºæ½åãç¶èï¼éäºæ¨¡åä¸»è¦å°æ³¨æ¼ä½¿ç¨å®ä¸æéé»è³æé²è¡æ¢ä»¶çæï¼å³éå¸¸å¨ç¹å®æéé»ææç CXR åå¶å°æå ±åï¼ééå¶äºå¶è¨åºæç¨ï¼ç¹å¥æ¯å°æ¼æææéè®åãçºäºè§£æ±ºæ­¤éå¶ï¼æåæåºäºä¸åæ°çæ¡æ¶ EHRXDiffï¼å®ééæ´åååç CXR èå¾çºçé«çäºä»¶ï¼ä¾å¦èæ¹ãå¯¦é©å®¤æª¢æ¸¬ç­ï¼ä¾é æ¸¬æªä¾ç CXR å½±åãæåçæ¡æ¶åºæ¼æ½å¨æ´æ£æ¨¡ååæè¿½è¹¤ä¸¦é æ¸¬ç¾çé²å±ï¼æ¢ä»¶åæ±ºæ¼ååç CXR å½±ååé«çäºä»¶çæ­·å²è¨éãæåå¨é¢è©ä¼°äºæåæ¡æ¶å¨ä¸åééµæ¹é¢çæè½ï¼åæ¬è¨åºä¸è´æ§ãäººå£çµ±è¨ä¸è´æ§åè¦è¦ºé¼çåº¦ãæåè­ææåçæ¡æ¶çæäºé«åè³ªãé¼ççæªä¾å½±åï¼ææäºæ½å¨çæéè®åï¼éè¡¨æå¶é²ä¸æ­¥ç¼å±çºè¨åºæ¨¡æ¬å·¥å·çæ½åãéå¯ä»¥çºé«çé åççæ£ç£æ§åæ²»çè¦åæä¾æå¹å¼çè¦è§£ã

##### **Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**
2409.06928v1 by Jianmei Jiang, Huijin Wang, Jieyun Bai, Shun Long, Shuangping Chen, Victor M. Campello, Karim Lekadir

The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a
pivotal step in monitoring labor progression and identifying potential delivery
complications. Despite the advances in deep learning, the lack of annotated
medical images hinders the training of segmentation. Traditional
semi-supervised learning approaches primarily utilize a unified network model
based on Convolutional Neural Networks (CNNs) and apply consistency
regularization to mitigate the reliance on extensive annotated data. However,
these methods often fall short in capturing the discriminative features of
unlabeled data and in delineating the long-range dependencies inherent in the
ambiguous boundaries of PSFH within ultrasound images. To address these
limitations, we introduce a novel framework, the Dual-Student and Teacher
Combining CNN and Transformer (DSTCT), which synergistically integrates the
capabilities of CNNs and Transformers. Our framework comprises a Vision
Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN.
This dual-student setup enables mutual supervision through the generation of
both hard and soft pseudo-labels, with the consistency in their predictions
being refined by minimizing the classifier determinacy discrepancy. The teacher
model further reinforces learning within this architecture through the
imposition of consistency regularization constraints. To augment the
generalization abilities of our approach, we employ a blend of data and model
perturbation techniques. Comprehensive evaluations on the benchmark dataset of
the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT
framework outperformed ten contemporary semi-supervised segmentation methods.
Code available at https://github.com/jjm1589/DSTCT.

æè¦ï¼æ¥éª¨è¯ååèé ­ï¼PSFHï¼çåå²æ¯ç£æ¸¬ç¢ç¨é²åº¦åè­å¥æ½å¨åå¨©ä½µç¼ççééµæ­¥é©ãåç®¡æ·±åº¦å­¸ç¿åå¾é²å±ï¼ä½æ¨è¨»é«å­¸å½±åçç¼ºä¹é»ç¤äºåå²çè¨ç·´ãå³çµ±çåç£ç£å¼å­¸ç¿æ¹æ³ä¸»è¦å©ç¨åºæ¼å·ç©ç¥ç¶ç¶²è·¯ï¼CNNï¼ççµ±ä¸ç¶²è·¯æ¨¡åï¼ä¸¦æç¨ä¸è´æ§æ­£ååä¾æ¸è¼å°å¤§éæ¨è¨»æ¸æçä¾è³´ãç¶èï¼éäºæ¹æ³éå¸¸ç¡æ³æææªæ¨è¨»æ¸æçåå¥æ§ç¹å¾µï¼ä¹ç¡æ³æç¹ªè¶é³æ³¢å½±åä¸­ PSFH æ¨¡ç³éçä¸­åºæçé·ç¨ä¾è³´æ§ãçºäºè§£æ±ºéäºéå¶ï¼æåå¼å¥äºä¸åæ°çæ¡æ¶ï¼å³éå­¸çåæå¸«çµå CNN å Transformerï¼DSTCTï¼ï¼å®ååæ´åäº CNN å Transformer çåè½ãæåçæ¡æ¶åå«ä¸åè¦è¦º Transformerï¼ViTï¼ä½çºæå¸«åå©åå­¸çæ¨¡åï¼ä¸å ViT åä¸å CNNãéç¨®éå­¸çè¨­ç½®ééçæç¡¬å½æ¨ç±¤åè»å½æ¨ç±¤å¯¦ç¾ç¸äºç£ç£ï¼ä¸¦ééæå°ååé¡å¨ç¢ºå®æ§å·®ç°ä¾åªåå¶é æ¸¬çä¸è´æ§ãæå¸«æ¨¡åééæ½å ä¸è´æ§æ­£ååç´æé²ä¸æ­¥å å¼·äºæ­¤æ¶æ§ä¸­çå­¸ç¿ãçºäºå¢å¼·æåæ¹æ³çæ³åè½åï¼æåæ¡ç¨äºæ¸æåæ¨¡åæ¾åæè¡çæ··åãå¨ MICCAI 2023 ç PSFH åå²å¤§ææ°åºæºæ¸æéä¸çç¶åè©ä¼°è¡¨æï¼æåç DSTCT æ¡æ¶åªæ¼åç¨®ç¶ä»£åç£ç£å¼åå²æ¹æ³ãç¨å¼ç¢¼å¯å¨ https://github.com/jjm1589/DSTCT åå¾ã

##### **Bifurcation Identification for Ultrasound-driven Robotic Cannulation**
2409.06817v1 by Cecilia G. Morales, Dhruv Srikanth, Jack H. Good, Keith A. Dufendach, Artur Dubrawski

In trauma and critical care settings, rapid and precise intravascular access
is key to patients' survival. Our research aims at ensuring this access, even
when skilled medical personnel are not readily available. Vessel bifurcations
are anatomical landmarks that can guide the safe placement of catheters or
needles during medical procedures. Although ultrasound is advantageous in
navigating anatomical landmarks in emergency scenarios due to its portability
and safety, to our knowledge no existing algorithm can autonomously extract
vessel bifurcations using ultrasound images. This is primarily due to the
limited availability of ground truth data, in particular, data from live
subjects, needed for training and validating reliable models. Researchers often
resort to using data from anatomical phantoms or simulations. We introduce
BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a
novel algorithm that identifies vessel bifurcations and provides optimal needle
insertion sites for an autonomous robotic cannulation system. BIFURC integrates
expert knowledge with deep learning techniques to efficiently detect vessel
bifurcations within the femoral region and can be trained on a limited amount
of in-vivo data. We evaluated our algorithm using a medical phantom as well as
real-world experiments involving live pigs. In all cases, BIFURC consistently
identified bifurcation points and needle insertion locations in alignment with
those identified by expert clinicians.

æè¦ï¼å¨åµå·åéçç§è­·ç°å¢ä¸­ï¼å¿«éä¸ç²¾ç¢ºçè¡ç®¡å§éè·¯æ¯æ£èå­æ´»çééµãæåçç ç©¶æ¨å¨ç¢ºä¿éç¨®éè·¯ï¼å³ä½¿å¨çç·´çé«çäººå¡ç¡æ³ç«å³ç²å¾çææ³ä¸ãè¡ç®¡ååæ¯è§£åæ¨èªï¼å¯ä»¥æå°å¨é«çéç¨ä¸­å®å¨æ¾ç½®å°ç®¡æéé ­ãåç®¡è¶é³æ³¢ç±æ¼å¶å¯ææ§åå®å¨æ§èå¨ç·æ¥ææ³ä¸å°èªè§£åæ¨èªå·æåªå¢ï¼ä½ææåæç¥ï¼æ²æç¾ææ¼ç®æ³å¯ä»¥ä½¿ç¨è¶é³æ³¢å½±åèªåæåè¡ç®¡ååãéä¸»è¦æ¯ç±æ¼å°é¢å¯¦æ³è³æçå¯ç¨æ§æéï¼ç¹å¥æ¯ä¾èªæ´»é«åè©¦èçè³æï¼èéå°æ¼è¨ç·´åé©è­å¯é æ¨¡åæ¯å¿éçãç ç©¶äººå¡ç¶å¸¸æ±å©æ¼ä½¿ç¨è§£åæ¨¡åææ¨¡æ¬çè³æãæåå¼å¥äº BIFURCï¼å³è¶é³æ³¢é©åæ©å¨äººæç®¡çååè­å¥ï¼éæ¯ä¸ç¨®æ°ç©çæ¼ç®æ³ï¼å¯ä»¥è­å¥è¡ç®¡ååï¼ä¸¦çºèªåæ©å¨äººæç®¡ç³»çµ±æä¾æä½³éé ­æå¥ä½ç½®ãBIFURC å°å°å®¶ç¥è­èæ·±åº¦å­¸ç¿æè¡ç¸çµåï¼ä»¥æææª¢æ¸¬è¡éª¨ååå§çè¡ç®¡ååï¼ä¸¦ä¸å¯ä»¥å¨æéçé«å§è³æä¸é²è¡è¨ç·´ãæåä½¿ç¨é«ç¨æ¨¡åä»¥åæ¶åæ´»é«è±¬ççå¯¦ä¸çå¯¦é©è©ä¼°äºæåçæ¼ç®æ³ãå¨ææææ³ä¸ï¼BIFURC é½ä¸è´å°è­å¥åºååé»åéé ­æå¥ä½ç½®ï¼èå°å®¶è¨åºé«çè­å¥çä½ç½®ä¸è´ã

##### **Personalized Federated Learning Techniques: Empirical Analysis**
2409.06805v1 by Azal Ahmad Khan, Ahmad Faraz Khan, Haider Ali, Ali Anwar

Personalized Federated Learning (pFL) holds immense promise for tailoring
machine learning models to individual users while preserving data privacy.
However, achieving optimal performance in pFL often requires a careful
balancing act between memory overhead costs and model accuracy. This paper
delves into the trade-offs inherent in pFL, offering valuable insights for
selecting the right algorithms for diverse real-world scenarios. We empirically
evaluate ten prominent pFL techniques across various datasets and data splits,
uncovering significant differences in their performance. Our study reveals
interesting insights into how pFL methods that utilize personalized (local)
aggregation exhibit the fastest convergence due to their efficiency in
communication and computation. Conversely, fine-tuning methods face limitations
in handling data heterogeneity and potential adversarial attacks while
multi-objective learning methods achieve higher accuracy at the cost of
additional training and resource consumption. Our study emphasizes the critical
role of communication efficiency in scaling pFL, demonstrating how it can
significantly affect resource usage in real-world deployments.

æè¦ï¼åäººåè¯åå­¸ç¿ (pFL) å¨ç¶­è­·è³æé±ç§çåæï¼çºå®¢è£½åæ©å¨å­¸ç¿æ¨¡åçµ¦åå¥ä½¿ç¨èå¸¶ä¾æ¥µå¤§çå¸æãç¶èï¼è¦éæ pFL çæä½³æè½ï¼éå¸¸éè¦å¨è¨æ¶é«éé·ææ¬åæ¨¡åæºç¢ºåº¦ä¹éåå¾ä»ç´°çå¹³è¡¡ãæ¬ææ·±å¥æ¢è¨ pFL ä¸­åºæçæ¬è¡¡åæ¨ï¼çºå¨åç¨®å¯¦éå ´æ¯ä¸­é¸ææ­£ç¢ºçæ¼ç®æ³æä¾å¯¶è²´çè¦è§£ãæåæ ¹æåç¨®è³æéåè³æåå²ï¼å°åç¨®ååºç pFL æè¡é²è¡å¯¦è­è©ä¼°ï¼æ­é²å¶æè½çé¡¯èå·®ç°ãæåçç ç©¶æ­é²äºæè¶£çè¦è§£ï¼èªªæå©ç¨åäººå (å±é¨) èåç pFL æ¹æ³ï¼ç±æ¼å¶å¨éè¨åéç®æ¹é¢çæçï¼å±ç¾åºæå¿«çæ¶æéåº¦ãç¸åå°ï¼å¾®èª¿æ¹æ³å¨èçè³æç°è³ªæ§åæ½å¨å°ææ»ææ¹é¢é¢è¨éå¶ï¼èå¤ç®æ¨å­¸ç¿æ¹æ³åä»¥é¡å¤çè¨ç·´åè³æºæ¶èçºä»£å¹ï¼éå°æ´é«çæºç¢ºåº¦ãæåçç ç©¶å¼·èª¿äºéè¨æçå¨æ´å pFL ä¸­çééµè§è²ï¼å±ç¤ºå®å¦ä½å¨å¯¦éé¨ç½²ä¸­é¡¯èå½±é¿è³æºä½¿ç¨ã

##### **Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**
2409.06672v1 by Cristian Trout

Many experts believe that AI systems will sooner or later pose uninsurable
risks, including existential risks. This creates an extreme judgment-proof
problem: few if any parties can be held accountable ex post in the event of
such a catastrophe. This paper proposes a novel solution: a
government-provided, mandatory indemnification program for AI developers. The
program uses risk-priced indemnity fees to induce socially optimal levels of
care. Risk-estimates are determined by surveying experts, including indemnified
developers. The Bayesian Truth Serum mechanism is employed to incent honest and
effortful responses. Compared to alternatives, this approach arguably better
leverages all private information, and provides a clearer signal to indemnified
developers regarding what risks they must mitigate to lower their fees. It's
recommended that collected fees be used to help fund the safety research
developers need, employing a fund matching mechanism (Quadratic Financing) to
induce an optimal supply of this public good. Under Quadratic Financing, safety
research projects would compete for private contributions from developers,
signaling how much each is to be supplemented with public funds.

æè¦ï¼è¨±å¤å°å®¶ç¸ä¿¡ AI ç³»çµ±é²æ©æé æç¡æ³æ¿ä¿çé¢¨éªï¼åæ¬çå­é¢¨éªãéæé ææ¥µç«¯çç¡æ³è¿½ç©¶è²¬ä»»åé¡ï¼å¨ç¼çæ­¤é¡ç½é£æï¼å¹¾ä¹æ²æä»»ä½ä¸æ¹å¯ä»¥äºå¾è¢«è¿½ç©¶è²¬ä»»ãæ¬ææåºäºä¸ååµæ°çè§£æ±ºæ¹æ¡ï¼æ¿åºæä¾ç AI éç¼äººå¡å¼·å¶æ§è£åè¨ç«ãè©²è¨ç«ä½¿ç¨é¢¨éªå®å¹çè£åè²»ç¨ä¾èªä½¿éå°ç¤¾ææé©ç¨åº¦çç§è­·ãé¢¨éªä¼°è¨å¼æ¯ç±èª¿æ¥å°å®¶ï¼åæ¬ç²å¾è£åçéç¼äººå¡ï¼ä¾æ±ºå®ãè²æ°çè©±è¡æ¸æ©å¶è¢«ç¨ä¾æ¿åµèª å¯¦ä¸åªåçåæãèå¶ä»æ¹æ³ç¸æ¯ï¼éç¨®æ¹æ³å¯ä»¥èªªè½æ´å¥½å°å©ç¨ææç§äººè³è¨ï¼ä¸¦åç²å¾è£åçéç¼äººå¡æä¾æ´æç¢ºçè¨èï¼èªªæä»åå¿é æ¸è¼åªäºé¢¨éªæè½éä½è²»ç¨ãå»ºè­°å°æ¶åçè²»ç¨ç¨æ¼è³å©å®å¨ç ç©¶éç¼äººå¡æéçç ç©¶ï¼ä¸¦æ¡ç¨åºééå°æ©å¶ï¼äºæ¬¡æ¹èè³ï¼ä¾èªä½¿æä¾éç¨®å¬å±è²¡çæä½³ä¾æãå¨äºæ¬¡æ¹èè³ä¸ï¼å®å¨ç ç©¶è¨ç«å°ç«¶ç­éç¼äººå¡çç§äººææ¬¾ï¼ä¸¦è¡¨ç¤ºå¶ä¸­æå¤å°å°ç±å¬å±è³éè£åã

##### **EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**
2409.06644v2 by Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He

Early detection of eye diseases like glaucoma, macular degeneration, and
diabetic retinopathy is crucial for preventing vision loss. While artificial
intelligence (AI) foundation models hold significant promise for addressing
these challenges, existing ophthalmic foundation models primarily focus on a
single modality, whereas diagnosing eye diseases requires multiple modalities.
A critical yet often overlooked aspect is harnessing the multi-view information
across various modalities for the same patient. Additionally, due to the
long-tail nature of ophthalmic diseases, standard fully supervised or
unsupervised learning approaches often struggle. Therefore, it is essential to
integrate clinical text to capture a broader spectrum of diseases. We propose
EyeCLIP, a visual-language foundation model developed using over 2.77 million
multi-modal ophthalmology images with partial text data. To fully leverage the
large multi-modal unlabeled and labeled data, we introduced a pretraining
strategy that combines self-supervised reconstructions, multi-modal image
contrastive learning, and image-text contrastive learning to learn a shared
representation of multiple modalities. Through evaluation using 14 benchmark
datasets, EyeCLIP can be transferred to a wide range of downstream tasks
involving ocular and systemic diseases, achieving state-of-the-art performance
in disease classification, visual question answering, and cross-modal
retrieval. EyeCLIP represents a significant advancement over previous methods,
especially showcasing few-shot, even zero-shot capabilities in real-world
long-tail scenarios.

æè¦ï¼æ©æåµæ¸¬éåç¼ãé»æé¨çè®åç³å°¿çè¦ç¶²èçè®ç­ç¼ç¾å°æ¼é é²è¦ååªå¤±è³ééè¦ãåç®¡äººå·¥æºæ§ (AI) åºç¤æ¨¡åå¨æå°éäºææ°æ¹é¢æ¥µå·åæ¯ï¼ä½ç¾æçç¼ç§åºç¤æ¨¡åä¸»è¦éæ³¨æ¼å®ä¸æ¨¡å¼ï¼èè¨ºæ·ç¼ç¾éè¦å¤ç¨®æ¨¡å¼ãä¸åéè¦ä½ç¶å¸¸è¢«å¿½è¦çæ¹é¢æ¯å©ç¨åä¸æ£èä¸åæ¨¡å¼çå¤è¦åè³è¨ãæ­¤å¤ï¼ç±æ¼ç¼ç§ç¾ççé·å°¾æ§è³ªï¼æ¨æºçå¨ç£ç£æç¡ç£ç£å­¸ç¿æ¹æ³éå¸¸é£ä»¥æä»ãå æ­¤ï¼æ´åè¨åºææ¬ä»¥æ¶µèæ´å»£æ³çç¾çè­ç³»è³ééè¦ãæåæåº EyeCLIPï¼éæ¯ä¸åè¦è¦ºèªè¨åºç¤æ¨¡åï¼ä½¿ç¨è¶é 277 è¬å¼µå·æé¨åæå­è³æçå¤æ¨¡å¼ç¼ç§å½±åéç¼èæãçºäºååå©ç¨å¤§éçå¤æ¨¡å¼æªæ¨è¨åæ¨è¨è³æï¼æåå¼å¥äºä¸ç¨®é è¨ç·´ç­ç¥ï¼çµåäºèªæç£ç£éå»ºãå¤æ¨¡å¼å½±åå°æ¯å­¸ç¿åå½±åæå­å°æ¯å­¸ç¿ï¼ä»¥å­¸ç¿å¤ç¨®æ¨¡å¼çå±äº«è¡¨å¾µãééä½¿ç¨ 14 ååºæºè³æéé²è¡è©ä¼°ï¼EyeCLIP å¯ä»¥è½ç§»å°æ¶åç¼é¨åå¨èº«ç¾ççå»£æ³ä¸æ¸¸ä»»åï¼å¨ç¾çåé¡ãè¦è¦ºåé¡è§£ç­åè·¨æ¨¡å¼æª¢ç´¢ä¸­å¯¦ç¾æåé²çæè½ãEyeCLIP ä»£è¡¨äºå°ååæ¹æ³çéå¤§é²å±ï¼ç¹å¥æ¯å¨ç¾å¯¦ä¸çé·å°¾å ´æ¯ä¸­å±ç¤ºäºå°æ¨£æ¬ï¼çè³é¶æ¨£æ¬çè½åã

##### **Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**
2409.06585v1 by Zoe Hancox, Sarah R. Kingsbury, Andrew Clegg, Philip G. Conaghan, Samuel D. Relton

Background: Hip replacement procedures improve patient lives by relieving
pain and restoring mobility. Predicting hip replacement in advance could reduce
pain by enabling timely interventions, prioritising individuals for surgery or
rehabilitation, and utilising physiotherapy to potentially delay the need for
joint replacement. This study predicts hip replacement a year in advance to
enhance quality of life and health service efficiency. Methods: Adapting
previous work using Temporal Graph Convolutional Neural Network (TG-CNN)
models, we construct temporal graphs from primary care medical event codes,
sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip
replacement risk. We match hip replacement cases to controls by age, sex, and
Index of Multiple Deprivation. The model, trained on 9,187 cases and 9,187
controls, predicts hip replacement one year in advance. We validate the model
on two unseen datasets, recalibrating for class imbalance. Additionally, we
conduct an ablation study and compare against four baseline models. Results:
Our best model predicts hip replacement risk one year in advance with an AUROC
of 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),
achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after
recalibration. Conclusions: The TG-CNN model effectively predicts hip
replacement risk by identifying patterns in patient trajectories, potentially
improving understanding and management of hip-related conditions.

æè¦ï¼èæ¯ï¼é«éç¯ç½®ææè¡å¯æ¸è¼ç¼çä¸¦æ¢å¾©è¡åè½åï¼é²èæ¹åæ£èçæ´»ãé æ¸¬é«éç¯ç½®ææè¡æå©æ¼åæä»å¥ãåªåå®æåäººé²è¡æè¡æå¾©å¥ï¼ä¸¦å©ç¨ç©çæ²»çä¾å»¶ç·©éç¯ç½®ææè¡çå¿è¦æ§ï¼é²èæ¸å°ç¼çãæ¬ç ç©¶é æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡ï¼ä»¥æåçæ´»åè³ªåé«çæåæçãæ¹æ³ï¼æ¡ç¨æéåå½¢å·ç©ç¥ç¶ç¶²è·¯ (TG-CNN) æ¨¡åæ¹ç·¨ååçç ç©¶ï¼æåå¾ ResearchOne EHR 40-75 æ­²æ£èçä¸»è¦ç§è­·é«çäºä»¶ä»£ç¢¼å»ºæ§æéåå½¢ï¼ä»¥é æ¸¬é«éç¯ç½®ææè¡é¢¨éªãæåæ ¹æå¹´é½¡ãæ§å¥åå¤éåå¥ªææ¸ï¼å°é«éç¯ç½®ææè¡çä¾èå°ç§çµé²è¡éå°ãè©²æ¨¡åéå° 9,187 åçä¾å 9,187 åå°ç§çµé²è¡è¨ç·´ï¼é æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡ãæåå¨å©åæªè¦æ¸æéé©è­æ¨¡åï¼ä¸¦éæ°æ ¡æºä»¥è§£æ±ºé¡å¥ä¸å¹³è¡¡åé¡ãæ­¤å¤ï¼æåé²è¡æ¶èç ç©¶ï¼ä¸¦èåååºæºæ¨¡åé²è¡æ¯è¼ãçµæï¼æåæä½³çæ¨¡åé æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡é¢¨éªï¼AUROC çº 0.724 (95% CIï¼0.715-0.733)ï¼AUPRC çº 0.185 (95% CIï¼0.160-0.209)ï¼éæ°æ ¡æºå¾æ ¡æºæççº 1.107 (95% CIï¼1.074-1.139)ãçµè«ï¼TG-CNN æ¨¡åå¯ææé æ¸¬é«éç¯ç½®ææè¡é¢¨éªï¼æ¹æ³æ¯æ¾åºæ£èè»è·¡ä¸­çæ¨¡å¼ï¼é²èæ½å¨æ¹åå°é«éç¯ç¸éç¾ççäºè§£åç®¡çã

##### **MAGDA: Multi-agent guideline-driven diagnostic assistance**
2409.06351v1 by David Bani-Harouni, Nassir Navab, Matthias Keicher

In emergency departments, rural hospitals, or clinics in less developed
regions, clinicians often lack fast image analysis by trained radiologists,
which can have a detrimental effect on patients' healthcare. Large Language
Models (LLMs) have the potential to alleviate some pressure from these
clinicians by providing insights that can help them in their decision-making.
While these LLMs achieve high test results on medical exams showcasing their
great theoretical medical knowledge, they tend not to follow medical
guidelines. In this work, we introduce a new approach for zero-shot
guideline-driven decision support. We model a system of multiple LLM agents
augmented with a contrastive vision-language model that collaborate to reach a
patient diagnosis. After providing the agents with simple diagnostic
guidelines, they will synthesize prompts and screen the image for findings
following these guidelines. Finally, they provide understandable
chain-of-thought reasoning for their diagnosis, which is then self-refined to
consider inter-dependencies between diseases. As our method is zero-shot, it is
adaptable to settings with rare diseases, where training data is limited, but
expert-crafted disease descriptions are available. We evaluate our method on
two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing
performance improvement over existing zero-shot methods and generalizability to
rare diseases.

æè¦ï¼å¨æ¥è¨ºå®¤ãéæé«é¢ææ¬ ç¼éå°åçè¨ºæï¼è¨åºé«å¸«å¸¸å¸¸ç¼ºä¹åéè¨ç·´çæ¾å°ç§é«å¸«é²è¡å¿«éçå½±ååæï¼éå¯è½æå°çæ£çé«çä¿å¥é æä¸å©å½±é¿ãå¤§åèªè¨æ¨¡å (LLM) ææ½åæ¸è¼éäºè¨åºé«å¸«çä¸äºå£åï¼æ¹æ³æ¯æä¾è¦è§£ï¼åå©ä»åé²è¡æ±ºç­ãåç®¡éäº LLM å¨å±ç¤ºå¶è±å¯ççè«é«å­¸ç¥è­çé«å­¸èè©¦ä¸­ç²å¾äºå¾é«çæ¸¬è©¦çµæï¼ä½å®åå¾å¾ä¸éµå¾ªé«çæåãå¨éé å·¥ä½ä¸­ï¼æåä»ç´¹äºä¸ç¨®æ°çé¶æ¬¡å­¸ç¿æå°æ¹éé©åæ±ºç­æ¯æ´æ¹æ³ãæåæ¨¡æ¬äºä¸åå¤å LLM ä»£çç³»çµ±ï¼ä¸¦å¢å¼·äºä¸åå°æ¯è¦è¦ºèªè¨æ¨¡åï¼è©²æ¨¡ååä½ä»¥éæçæ£è¨ºæ·ãå¨çºä»£çæä¾ç°¡å®çè¨ºæ·æåå¾ï¼å®åå°ç¶åæç¤ºä¸¦æ ¹æéäºæåç¯©é¸å½±åä»¥æ¾åºç¼ç¾ãæå¾ï¼å®åçºå¶è¨ºæ·æä¾å¯ä»¥çè§£çæè·¯æ¨çï¼ç¶å¾èªæç²¾é²ä»¥èéç¾çä¹éçç¸äºä¾å­éä¿ãç±æ¼æåçæ¨¡åæ¯é¶æ¬¡å­¸ç¿ï¼å æ­¤å®å¯ä»¥é©æç½è¦ç¾ççè¨­å®ï¼å¨éç¨®è¨­å®ä¸­ï¼è¨ç·´è³ææéï¼ä½æå°å®¶è£½ä½çç¾çæè¿°å¯ç¨ãæåå¨å©åè¸é¨ X åçè³æéï¼CheXpert å ChestX-ray 14 Longtailï¼è©ä¼°æåçæ¨¡åï¼å±ç¤ºäºç¸è¼æ¼ç¾æçé¶æ¬¡å­¸ç¿æ¹æ³çæè½æåï¼ä»¥åå°ç½è¦ç¾ççæ¦æ¬æ§ã

##### **Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**
2409.06209v1 by Xin Zhang, Deval Mehta, Yanan Hu, Chao Zhu, David Darby, Zhen Yu, Daniel Merlo, Melissa Gresle, Anneke Van Der Walt, Helmut Butzkueven, Zongyuan Ge

Survival analysis holds a crucial role across diverse disciplines, such as
economics, engineering and healthcare. It empowers researchers to analyze both
time-invariant and time-varying data, encompassing phenomena like customer
churn, material degradation and various medical outcomes. Given the complexity
and heterogeneity of such data, recent endeavors have demonstrated successful
integration of deep learning methodologies to address limitations in
conventional statistical approaches. However, current methods typically involve
cluttered probability distribution function (PDF), have lower sensitivity in
censoring prediction, only model static datasets, or only rely on recurrent
neural networks for dynamic modelling. In this paper, we propose a novel
survival regression method capable of producing high-quality unimodal PDFs
without any prior distribution assumption, by optimizing novel
Margin-Mean-Variance loss and leveraging the flexibility of Transformer to
handle both temporal and non-temporal data, coined UniSurv. Extensive
experiments on several datasets demonstrate that UniSurv places a significantly
higher emphasis on censoring compared to other methods.

æè¦ï¼å­æ´»åæå¨ç¶æ¿ãå·¥ç¨åé«çä¿å¥ç­ä¸åå­¸ç§ä¸­æ®æ¼èè³ééè¦çè§è²ãå®è®ç ç©¶äººå¡è½å¤ åææä¸è®åæè®æ¸æï¼åå«å®¢æ¶æµå¤±ãææéè§£ååç¨®é«ççµæç­ç¾è±¡ãéæ¼æ­¤é¡æ¸æçè¤éæ§åç°è³ªæ§ï¼æè¿çåªåå·²è­ææåæ´åæ·±åº¦å­¸ç¿æ¹æ³ä»¥è§£æ±ºå³çµ±çµ±è¨æ¹æ³çéå¶ãç¶èï¼ç®åçæ¹æ³éå¸¸æ¶åéäºçæ©çåä½å½æ¸ (PDF)ï¼å¨å¯©æ¥é æ¸¬ä¸­å·æè¼ä½çæææ§ï¼åå°éææ¸æéé²è¡å»ºæ¨¡ï¼æåä¾è³´éè¿´ç¥ç¶ç¶²è·¯é²è¡åæå»ºæ¨¡ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°ç©çå­æ´»è¿´æ­¸æ¹æ³ï¼è½å¤ å¨æ²æä»»ä½åé©åä½åè¨­çææ³ä¸ç¢çé«åè³ªçå®å³° PDFï¼èç±æä½³åæ°ç©çééå¹³åå¼è®ç°æå¤±ï¼ä¸¦å©ç¨ Transformer çéæ´»æ§ä¾èçæéåéæéæ¸æï¼ç¨±çº UniSurvãå¨å¹¾åæ¸æéä¸çå»£æ³å¯¦é©è­æï¼èå¶ä»æ¹æ³ç¸æ¯ï¼UniSurv å°å¯©æ¥çéè¦ç¨åº¦é¡¯èæé«ã

##### **Can Large Language Models Unlock Novel Scientific Research Ideas?**
2409.06185v1 by Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal

"An idea is nothing more nor less than a new combination of old elements"
(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and
publicly available ChatGPT have marked a significant turning point in the
integration of Artificial Intelligence (AI) into people's everyday lives. This
study explores the capability of LLMs in generating novel research ideas based
on information from research papers. We conduct a thorough examination of 4
LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and
Physics). We found that the future research ideas generated by Claude-2 and
GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.
We also found that Claude-2 generates more diverse future research ideas than
GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the
novelty, relevancy, and feasibility of the generated future research ideas.
This investigation offers insights into the evolving role of LLMs in idea
generation, highlighting both its capability and limitations. Our work
contributes to the ongoing efforts in evaluating and utilizing language models
for generating future research ideas. We make our datasets and codes publicly
available.

æè¦ï¼ãä¸åæ³æ³ä¸éå°±æ¯èåç´ çæ°çµåèå·²ã
(Young, J.W.)ãå¤§åèªè¨æ¨¡å (LLM) åå¬éç ChatGPT å»£æ³æ¡ç¨ï¼æ¨èªèäººå·¥æºè½ (AI) æ´åå°äººåæ¥å¸¸çæ´»ä¸­çéè¦è½æé»ãæ¬ç ç©¶æ¢è¨äº LLM å¨æ ¹æç ç©¶è«æè³è¨ç¢çæ°ç ç©¶æ³æ³æ¹é¢çè½åãæåå°äºåé åï¼ä¾å¦åå­¸ãé»è¦ãç¶æ¿ãé«å­¸åç©çï¼ä¸­ç 4 å LLM é²è¡äºå¾¹åºæª¢æ¥ãæåç¼ç¾ Claude-2 å GPT-4 ç¢ççæªä¾ç ç©¶æ³æ³æ¯ GPT-3.5 å Gemini æ´ç¬¦åä½èçè§é»ãæåéç¼ç¾ï¼Claude-2 ç¢ççæªä¾ç ç©¶æ³æ³æ¯ GPT-4ãGPT-3.5 å Gemini 1.0 æ´çºå¤æ¨£åãæåé²ä¸æ­¥å°ç¢ççæªä¾ç ç©¶æ³æ³çæ°ç©æ§ãç¸éæ§åå¯è¡æ§é²è¡äºäººå·¥è©ä¼°ãæ¬èª¿æ¥æä¾äºå° LLM å¨ç¢çæ³æ³ä¸­ä¸æ·æ¼è®çè§è²çè¦è§£ï¼çªåºäºå¶è½ååéå¶ãæåçç ç©¶æå©æ¼è©ä¼°åå©ç¨èªè¨æ¨¡åä¾ç¢çæªä¾ç ç©¶æ³æ³çæçºåªåãæåå¬éæä¾æåçæ¸æéåç¨å¼ç¢¼ã

##### **Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**
2409.06173v2 by Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan

In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ä¸­çèçµ¡å­¸ç¿ (ICL) å·²æçºå·è¡èªç¶èªè¨ä»»åçä¸»æµæè¡ï¼å çºå®ä¸éè¦ä½¿ç¨åºæ¼æ¢¯åº¦çæ¨¡åä¾æ´æ°æ¨¡ååæ¸ãICL æ¿è«¾ä»¥æ¥µä½çè¨ç®ææ¬ãé©æãLLM ä»¥å¨ç«¶ç­ææåé²çå±¤ç´å·è¡ç¶åä»»åãICL å¯ä»¥ééå¨æç¤ºä¸­æç¢ºå°ç´å¥æ¨çéç¨ä¾æ´åï¼ä»¥å¾åºæçµæ¨ç±¤ï¼éé æè¡ç¨±çºæèé (CoT) æç¤ºãç¶èï¼æè¿çç ç©¶ç¼ç¾ï¼ICL ä¸»è¦ä¾è³´ä»»ååé©çæª¢ç´¢ï¼è¼å°ä¾è³´ãå­¸ç¿ãä¾å·è¡ä»»åï¼ç¹å¥æ¯å°æ¼æç·åéå¾·ç­è¤éçä¸»è§é åï¼å¶ä¸­åé©æåµåå¾é©é æ¸¬ãå¨éé å·¥ä½ä¸­ï¼æåæ¢è¨ãåç¨ãæ¨çæ¯å¦ä¹æå¨ LLM ä¸­ç¢çç¸åçè¡çºï¼å¶ä¸­ CoT çæ ¼å¼ææª¢ç´¢æ¨çåé©ï¼åç®¡æç¤ºä¸­çè­æä¸åï¼ä½éäºåé©ä»ç¶ç¸å°ä¸è®ãæåç¼ç¾ï¼ä»¤äººé©è¨çæ¯ï¼å°æ¼è¼å¤§çèªè¨æ¨¡åï¼CoT ç¢ºå¯¦è ICL é­åç¸åçå¾é©å´©æ½°ãç¨å¼ç¢¼å¯å¨ https://github.com/gchochla/cot-priors åå¾ã

##### **Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**
2409.06147v1 by Dong Han, Jihye Moon, LuÃ­s Roberto Mercado DÃ­az, Darren Chen, Devan Williams, Eric Y. Ding, Khanh-Van Tran, David D. McManus, Ki H. Chon

Most deep learning models of multiclass arrhythmia classification are tested
on fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise
ratios compared to smartwatch-derived PPG, and the best reported sensitivity
value for premature atrial/ventricular contraction (PAC/PVC) detection is only
75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF
detection, we use multi-modal data which incorporates 1D PPG, accelerometers,
and heart rate data as the inputs to a computationally efficient 1D
bi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three
arrhythmia classes. We used motion-artifact prone smartwatch PPG data from the
NIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72
subjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while
maintaining a high accuracy of 97.31% for AF detection. These results
outperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55%
for AF detection even while our model was computationally more efficient (14
times lighter and 2.7 faster).

æè¦ï¼å¤§å¤æ¸å¤é¡å¿å¾ä¸æ´åé¡çæ·±åº¦å­¸ç¿æ¨¡åé½æ¯å¨æå°åé»å®¹ç©æè¨æ³ (PPG) è³æä¸é²è¡æ¸¬è©¦ï¼èæºæ§æé¶è¡çç PPG ç¸æ¯ï¼å¶è¨èéè¨æ¯æ´é«ï¼èå°æ¼æåå¿æ¿/å¿å®¤æ¶ç¸® (PAC/PVC) åµæ¸¬æå ±åçæä½³ææåº¦å¼åçº 75%ãçºäºå¨ç¶­æé«æ¿é¡«åµæ¸¬çåææé« PAC/PVC åµæ¸¬ææåº¦ï¼æåä½¿ç¨å¤æ¨¡å¼è³æï¼å° 1D PPGãå éåº¦è¨åå¿çè³æä½çºè¨ç®æçé«ç 1D éåéæ§éè¿´å®å (1D-Bi-GRU) æ¨¡åçè¼¸å¥ï¼ä»¥åµæ¸¬ä¸é¡å¿å¾ä¸æ´ãæåä½¿ç¨äºç¾ååå®¶è¡çç ç©¶é¢è³å©ç Pulsewatch è¨åºè©¦é©ä¸­çéåå½å½±æææºæ§æé¶ PPG è³æãæåå¨ 72 ååè©¦èèº«ä¸æ¸¬è©¦çå¤æ¨¡å¼æ¨¡åï¼å°æ¼ PAC/PVC åµæ¸¬éå°äºåææªæç 83% ææåº¦ï¼åæå°æ¼æ¿é¡«åµæ¸¬ç¶­æäº 97.31% çé«æºç¢ºåº¦ãå³ä½¿æåçæ¨¡åå¨è¨ç®ä¸æ´ææçï¼è¼ 14 åï¼å¿« 2.7 åï¼ï¼éäºçµæä»æ¯æåé²çæ¨¡åå¨ PAC/PVC åµæ¸¬ä¸é«åº 20.81%ï¼å¨æ¿é¡«åµæ¸¬ä¸é«åº 2.55%ã

##### **ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**
2409.05592v1 by Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Yulan He

Predicting unknown drug-drug interactions (DDIs) is crucial for improving
medication safety. Previous efforts in DDI prediction have typically focused on
binary classification or predicting DDI categories, with the absence of
explanatory insights that could enhance trust in these predictions. In this
work, we propose to generate natural language explanations for DDI predictions,
enabling the model to reveal the underlying pharmacodynamics and
pharmacokinetics mechanisms simultaneously as making the prediction. To do
this, we have collected DDI explanations from DDInter and DrugBank and
developed various models for extensive experiments and analysis. Our models can
provide accurate explanations for unknown DDIs between known drugs. This paper
contributes new tools to the field of DDI prediction and lays a solid
foundation for further research on generating explanations for DDI predictions.

æè¦ï¼é æ¸¬æªç¥çè¥ç©äº¤äºä½ç¨ (DDI) å°æ¼æ¹åè¥ç©å®å¨è³ééè¦ãååå¨ DDI é æ¸¬æ¹é¢æåçåªåéå¸¸éä¸­æ¼äºååé¡æé æ¸¬ DDI é¡å¥ï¼èç¼ºä¹è½å¤ å¢å¼·éäºé æ¸¬çå¯ä¿¡åº¦çè§£éæ§è¦è§£ãå¨éé å·¥ä½ä¸­ï¼æåå»ºè­°çº DDI é æ¸¬ç¢çèªç¶èªè¨è§£éï¼ä½¿æ¨¡åè½å¤ åææ­ç¤ºè¥æå­¸åè¥ç©ååå­¸æ©å¶ï¼ä¸¦é²è¡é æ¸¬ãçºæ­¤ï¼æåå¾ DDInter å DrugBank æ¶éäº DDI è§£éï¼ä¸¦éç¼äºåç¨®æ¨¡åé²è¡å»£æ³çå¯¦é©ååæãæåçæ¨¡åå¯ä»¥çºå·²ç¥è¥ç©ä¹éæªç¥ç DDI æä¾æºç¢ºçè§£éãæ¬æçº DDI é æ¸¬é åè²¢ç»äºæ°çå·¥å·ï¼ä¸¦çºé²ä¸æ­¥ç ç©¶ DDI é æ¸¬çè§£éçæå¥ å®äºå å¯¦çåºç¤ã

##### **Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**
2409.05486v2 by Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri

arXiv admin comment: This version has been removed by arXiv administrators as
the submitter did not have the rights to agree to the license at the time of
submission

æè¦ï¼å¤§åèªè¨æ¨¡åçåè³ªåè½åç®åç¡æ³ééèªåååºæºè©ä¼°å®å¨è©ä¼°ãç¸åå°ï¼éè¦æ´å±èªç¶èªè¨çææç»ä¸­å³çµ±å®æ§æè¡çäººå·¥è©ä¼°ãä¸åæè¿çæä½³å¯¦ååå«ä½¿ç¨ A/B æ¸¬è©¦æ¡æ¶ï¼å®ææ·åäººé¡è©ä¼°èå°æ¼ç¹å®æ¨¡åçåå¥½ãå¨æ¬æä¸­ï¼æåæè¿°äºå°æ³¨æ¼çç©é«å­¸é åï¼å¥åº·ãçç©å­¸ãåå­¸/è¥çå­¸ï¼çäººé¡è©ä¼°å¯¦é©ï¼è©²å¯¦é©å¨ Elsevier é²è¡ãå¶ä¸­ï¼ä¸åå¤§åä½éé¾å¤§ï¼8.8B åæ¸ï¼åè§£ç¢¼å¨åºç¤è½æå¨å¨ç¸å°è¼å°ï¼135B ä»¤çï¼ä½ç¶éé«åº¦æ´çç Elsevier è³æéä¸è¨ç·´ï¼è OpenAI ç GPT-3.5-turbo å Meta çåºç¤ 7B åæ¸ Llama 2 æ¨¡åå¨å¤éæ¨æºä¸é²è¡æ¯è¼ãçµæé¡¯ç¤ºï¼å³ä½¿ IRR åæ¸éå¸¸è¼ä½ï¼ä½åå¥½ GPT-3.5-turboï¼å æ­¤åå¥½å·å°è©±è½åãéå¸¸å¤§åä¸å¨éå¸¸å¤§åè³æéä¸è¨ç·´çæ¨¡åãä½åæä¹é¡¯ç¤ºï¼å°æ¼è¼ä¸é¾å¤§çæ¨¡åï¼å¨è¼å°ä½ç¶éè¯å¥½æ´ççè¨ç·´éä¸é²è¡è¨ç·´ï¼æå¯è½å¨çç©é«å­¸é åç¢çå¯è¡çæ¿ä»£æ¹æ¡ã

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

æè¦ï¼<paragraph>å©ç¨å¤§åèªè¨æ¨¡å (LLM) å¼·å¤§çåè½ï¼é²è¡æäºçæãéè¼¯æ¨çåå¸¸è­ç¥è­æ´åï¼æ¬ç ç©¶æ·±å¥æ¢è¨å©ç¨ LLM ä¾å¢å¼·èªååæ¾å°å ±åçæ (R2Gen)ãåç®¡ LLM ææè±å¯çç¥è­ï¼ä½è¦ææè§¸ç¼éäºå¤§åæ¨¡åä¸­èç¹å®ä»»åï¼å¦ R2Genï¼ç¸éçç¥è­ï¼æ¯ä¸åéè¦çç ç©¶ææ°ãæ¬ææåºäº KARGENï¼ä¸ååºæ¼ LLM çç¥è­å¢å¼·èªååæ¾å°å ±åçææ¡æ¶ãå©ç¨åçµç LLM ä¾çæå ±åï¼è©²æ¡æ¶æ´åäºä¸åç¥è­åè­ï¼ä»¥è§£é LLM ä¸­èè¸é¨ç¾çç¸éçç¥è­ï¼ä»¥å¢å¼·çæå ±åçè¨åºæç¨ãéæ¯ééå©ç¨ç¥è­åè­ä»¥è¨­è¨çæ¹å¼æåèç¾çç¸éçç¹å¾µä¾å¯¦ç¾çãç±æ¼æ¾å°å ±ååå«æ­£å¸¸åç¾çç¸éçç¼ç¾ï¼å æ­¤æåçåå½¢å¢å¼·ç¾çç¸éç¹å¾µèååå½±åç¹å¾µæ´åï¼å¼é¡§å©åæ¹é¢ãæåæ¢ç´¢äºå©ç¨®èåæ¹æ³ï¼ä»¥èªååªåæåºåé¸ææç¸éçç¹å¾µãèåçç¹å¾µç± LLM ä½¿ç¨ï¼ä»¥çæå°ç¾çæ´ææä¸åè³ªæ´é«çå ±åãæåçåæ³å¨ MIMIC-CXR å IU-Xray è³æéä¸å±ç¤ºäºæå¸æççµæã</paragraph>

##### **Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**
2409.07493v1 by Javad Hassannataj Joloudari, Mohammad Maftoun, Bahareh Nakisa, Roohallah Alizadehsani, Meisam Yadollahzadeh-Tabari

The Complex Emotion Recognition System (CERS) deciphers complex emotional
states by examining combinations of basic emotions expressed, their
interconnections, and the dynamic variations. Through the utilization of
advanced algorithms, CERS provides profound insights into emotional dynamics,
facilitating a nuanced understanding and customized responses. The attainment
of such a level of emotional recognition in machines necessitates the knowledge
distillation and the comprehension of novel concepts akin to human cognition.
The development of AI systems for discerning complex emotions poses a
substantial challenge with significant implications for affective computing.
Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task
due to the intricacies involved in capturing subtle emotions, necessitating
specialized methods for data collection and processing. Incorporating
physiological signals such as Electrocardiogram (ECG) and Electroencephalogram
(EEG) can notably enhance CERS by furnishing valuable insights into the user's
emotional state, enhancing the quality of datasets, and fortifying system
dependability. A comprehensive literature review was conducted in this study to
assess the efficacy of machine learning, deep learning, and meta-learning
approaches in both basic and complex emotion recognition utilizing EEG, ECG
signals, and facial expression datasets. The chosen research papers offer
perspectives on potential applications, clinical implications, and results of
CERSs, with the objective of promoting their acceptance and integration into
clinical decision-making processes. This study highlights research gaps and
challenges in understanding CERSs, encouraging further investigation by
relevant studies and organizations. Lastly, the significance of meta-learning
approaches in improving CERS performance and guiding future research endeavors
is underscored.

æè¦ï¼è¤éæç·è¾¨è­ç³»çµ± (CERS) ééæª¢é©è¡¨éçåºæ¬æç·çµåãå®åçç¸äºé£çµï¼ä»¥ååæè®åä¾è§£ç¢¼è¤éçæç·çæãééä½¿ç¨é²éæ¼ç®æ³ï¼CERS æä¾äºå°æç·åæçæ·±å¥è¦è§£ï¼ä¿é²ç´°ç·»ççè§£åå®¢è£½åçåæãå¨æ©å¨ä¸­éæéç¨®ç¨åº¦çæç·è¾¨è­éè¦ç¥è­æçåçè§£é¡ä¼¼æ¼äººé¡èªç¥çæ°æ¦å¿µãç¼å±ç¨æ¼è¾¨å¥è¤éæç·çäººå·¥æºæ§ç³»çµ±å°ææéç®ä¾èªªæ¯ä¸åéå¤§çææ°ï¼ä¸¦å·æéè¦çå½±é¿ãæ­¤å¤ï¼ç±æ¼ææå¾®å¦æç·ææ¶åçè¤éæ§ï¼åå¾ CERS çå¤§éè³æéè¢«è­ææ¯ä¸é è±éçä»»åï¼å æ­¤éè¦æ¡ç¨ç¹æ®çæ¹æ³ä¾æ¶éåèçè³æãç´å¥ççè¨èï¼ä¾å¦å¿é»å (ECG) åè¦é»å (EEG)ï¼å¯ä»¥ééæä¾å°ä½¿ç¨èæç·çæçå¯¶è²´è¦è§£ãæåè³æéçåè³ªä»¥åå¼·åç³»çµ±çå¯é æ§ï¼ä¾é¡¯èå¢å¼· CERSãæ¬ç ç©¶é²è¡äºä¸é å¨é¢çæç»æ¢è¨ï¼ä»¥è©ä¼°æ©å¨å­¸ç¿ãæ·±åº¦å­¸ç¿ååå­¸ç¿æ¹æ³å¨å©ç¨è¦é»åãå¿é»åè¨èåé¢é¨è¡¨æè³æéé²è¡åºæ¬åè¤éæç·è¾¨è­æ¹é¢çæè½ãæé¸çç ç©¶è«ææä¾äºéæ¼ CERS çæ½å¨æç¨ãè¨åºå½±é¿åçµæçè§é»ï¼ç®çæ¯ä¿é²å®åè¢«æ¥åä¸¦æ´åå°è¨åºæ±ºç­å¶å®éç¨ä¸­ãæ¬ç ç©¶çªåºäºçè§£ CERS çç ç©¶å·®è·åææ°ï¼é¼åµç¸éç ç©¶åçµç¹é²ä¸æ­¥èª¿æ¥ãæå¾ï¼å¼·èª¿äºåå­¸ç¿æ¹æ³å¨æ¹å CERS æè½åæå°æªä¾ç ç©¶å·¥ä½ä¸­çéè¦æ§ã

##### **Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**
2409.05292v2 by Nirmalya Thakur

The world is currently experiencing an outbreak of mpox, which has been
declared a Public Health Emergency of International Concern by WHO. No prior
work related to social media mining has focused on the development of a dataset
of Instagram posts about the mpox outbreak. The work presented in this paper
aims to address this research gap and makes two scientific contributions to
this field. First, it presents a multilingual dataset of 60,127 Instagram posts
about mpox, published between July 23, 2022, and September 5, 2024. The
dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram
posts about mpox in 52 languages. For each of these posts, the Post ID, Post
Description, Date of publication, language, and translated version of the post
(translation to English was performed using the Google Translate API) are
presented as separate attributes in the dataset. After developing this dataset,
sentiment analysis, hate speech detection, and anxiety or stress detection were
performed. This process included classifying each post into (i) one of the
sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or
neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no
anxiety/stress detected. These results are presented as separate attributes in
the dataset. Second, this paper presents the results of performing sentiment
analysis, hate speech analysis, and anxiety or stress analysis. The variation
of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and
neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and
50.64%, respectively. In terms of hate speech detection, 95.75% of the posts
did not contain hate and the remaining 4.25% of the posts contained hate.
Finally, 72.05% of the posts did not indicate any anxiety/stress, and the
remaining 27.95% of the posts represented some form of anxiety/stress.

æè¦ï¼ä¸çç®åæ­£å¨ç»åç´çç«æï¼ä¸çå«çç»ç»å·²å®£å¸ç´çç«æä¸ºå½éå³æ³¨ççªåå¬å±å«çäºä»¶ãæ­¤åæ²¡æä¸ç¤¾äº¤åªä½ææç¸å³çç ç©¶éä¸­äºå¼åæå³ç´çç«æç Instagram å¸å­çæ°æ®éãæ¬æä»ç»çç ç©¶æ¨å¨è§£å³è¿ä¸ç ç©¶ç©ºç½ï¼å¹¶å¯¹è¯¥é¢åååºä¸¤é¡¹ç§å­¦è´¡ç®ãé¦åï¼å®æä¾äº 60,127 æ¡æå³ç´çç Instagram å¸å­çå¤è¯­è¨æ°æ®éï¼è¿äºå¸å­åå¸äº 2022 å¹´ 7 æ 23 æ¥è³ 2024 å¹´ 9 æ 5 æ¥ä¹é´ãè¯¥æ°æ®éå¯å¨ https://dx.doi.org/10.21227/7fvc-y093 å¤è·å¾ï¼å¶ä¸­åå« 52 ç§è¯­è¨çæå³ç´çç Instagram å¸å­ãå¯¹äºå¶ä¸­æ¯ç¯å¸å­ï¼å¸å­ IDãå¸å­æè¿°ãåå¸æ¥æãè¯­è¨åå¸å­çç¿»è¯çæ¬ï¼ä½¿ç¨ Google ç¿»è¯ API ç¿»è¯æè±æï¼ä½ä¸ºåç¬çå±æ§æ¾ç¤ºå¨æ°æ®éä¸­ãå¨å¼åæ­¤æ°æ®éåï¼è¿è¡äºææåæãä»æ¨è¨è®ºæ£æµä»¥åç¦èæååæ£æµãæ­¤è¿ç¨åæ¬å°æ¯ç¯å¸å­åç±»ä¸º (i) ææç±»å«ä¹ä¸ï¼å³ææ§ãæè®¶ãå¿«ä¹ãæ²ä¼¤ãæ¤æãåæ¶æä¸­ç«ï¼(ii) ä»æ¨æéä»æ¨ï¼ä»¥å (iii) æ£æµå°ç¦è/ååææªæ£æµå°ç¦è/ååãè¿äºç»æä½ä¸ºåç¬çå±æ§æ¾ç¤ºå¨æ°æ®éä¸­ãå¶æ¬¡ï¼æ¬æä»ç»äºæ§è¡ææåæãä»æ¨è¨è®ºåæåç¦èæåååæçç»æãè§å¯å°ææç±»å«çååââææ§ãæè®¶ãå¿«ä¹ãæ²ä¼¤ãæ¤æãåæ¶åä¸­ç«åå«ä¸º 27.95%ã2.57%ã8.69%ã5.94%ã2.69%ã1.53% å 50.64%ãå¨ä»æ¨è¨è®ºæ£æµæ¹é¢ï¼95.75% çå¸å­ä¸åå«ä»æ¨ï¼å¶ä½ 4.25% çå¸å­åå«ä»æ¨ãæåï¼72.05% çå¸å­æ²¡æè¡¨ç°åºä»»ä½ç¦è/ååï¼å¶ä½ 27.95% çå¸å­ä»£è¡¨æç§å½¢å¼çç¦è/ååã

##### **RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**
2409.05280v1 by Quoc-Bao Nguyen-Le, Tuan-Hy Le, Anh-Triet Do, Quoc-Huy Trinh

Cardiovascular disease is a major global health concern, contributing
significantly to global mortality. Accurately segmenting cardiac medical
imaging data is crucial for reducing fatality rates associated with these
conditions. However, current state-of-the-art (SOTA) neural networks, including
CNN-based and Transformer-based approaches, face challenges in capturing both
inter-slice connections and intra-slice details, especially in datasets
featuring intricate, long-range details along the z-axis like coronary
arteries. Existing methods also struggle with differentiating non-cardiac
components from the myocardium, resulting in segmentation inaccuracies and the
"spraying" phenomenon. To address these issues, we introduce
RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of
intricate cardiac structures. Our approach enhances global context modeling
through multiscale feature aggregation and nested skip connections in the
encoder. Transformer layers facilitate capturing intra-slice interactions,
while a rotatory attention mechanism handles inter-slice connectivity. A
channel-wise cross-attention gate integrates multiscale information and decoder
features, effectively bridging semantic gaps. Experimental results across
multiple datasets demonstrate superior performance over current methods,
achieving near-perfect annotation of coronary arteries and myocardium. Ablation
studies confirm that our rotatory attention mechanism significantly improves
segmentation accuracy by transforming embedded vectorized patches in semantic
dimensional space.

æè¦ï¼å¿è¡ç®¡ç¾çæ¯å¨çä¸»è¦çå¥åº·åé¡ï¼å°å¨çæ­»äº¡çæé¡¯èçå½±é¿ãæºç¢ºåå²å¿èé«å­¸å½±åè³æå°æ¼éä½éäºç¾çç¸éçæ­»äº¡çè³ééè¦ãç¶èï¼ç®åçåé²ç¥ç¶ç¶²è·¯ï¼åæ¬åºæ¼ CNN ååºæ¼ Transformer çæ¹æ³ï¼å¨æ·åå±¤éé£æ¥åå±¤å§ç´°ç¯æ¹é¢é¢è¨ææ°ï¼ç¹å¥æ¯å¨å·ææ²¿è z è»¸çè¤éãé·ç¨ç´°ç¯çè³æéï¼ä¾å¦å çåèãç¾ææ¹æ³ä¹é£ä»¥ååéå¿èæååå¿èï¼å°è´åå²ä¸æºç¢ºåãå´çãç¾è±¡ãçºäºè§£æ±ºéäºåé¡ï¼æåå¼å¥äº RotCAtt-TransUNet++ï¼ä¸ç¨®å°çºè¤éå¿èçµæ§çç©©å¥åå²èè¨­è¨çæ°ç©æ¶æ§ãæåçåæ³ééç·¨ç¢¼å¨ä¸­çå¤å°ºåº¦ç¹å¾µèååå·¢çè·³èºé£æ¥å¢å¼·äºå¨å±èæ¯å»ºæ¨¡ãTransformer å±¤ä¿é²æ·åå±¤å§äº¤äºä½ç¨ï¼èæè½æ³¨ææ©å¶åèçå±¤éé£æ¥ãééå¼äº¤åæ³¨æééæ´åäºå¤å°ºåº¦è³è¨åè§£ç¢¼å¨ç¹å¾µï¼ææå°å½åäºèªç¾©å·®è·ãè·¨å¤åè³æéçå¯¦é©çµæè­æäºå¶åªæ¼ç®åæ¹æ³çæè½ï¼å¯¦ç¾äºå çåèåå¿èçè¿ä¹å®ç¾çè¨»è§£ãæ¶èç ç©¶è­å¯¦ï¼æåçæè½æ³¨ææ©å¶ééè½æèªç¾©ç¶­åº¦ç©ºéä¸­çåµå¥åéåè£ä¸ï¼é¡¯èå°æé«äºåå²æºç¢ºåº¦ã

##### **Activation Function Optimization Scheme for Image Classification**
2409.04915v1 by Abdur Rahman, Lu He, Haifeng Wang

Activation function has a significant impact on the dynamics, convergence,
and performance of deep neural networks. The search for a consistent and
high-performing activation function has always been a pursuit during deep
learning model development. Existing state-of-the-art activation functions are
manually designed with human expertise except for Swish. Swish was developed
using a reinforcement learning-based search strategy. In this study, we propose
an evolutionary approach for optimizing activation functions specifically for
image classification tasks, aiming to discover functions that outperform
current state-of-the-art options. Through this optimization framework, we
obtain a series of high-performing activation functions denoted as Exponential
Error Linear Unit (EELU). The developed activation functions are evaluated for
image classification tasks from two perspectives: (1) five state-of-the-art
neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and
Compact Convolutional Transformer which cover computationally heavy to light
neural networks, and (2) eight standard datasets, including CIFAR10,
Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,
and TinyImageNet which cover from typical machine vision benchmark,
agricultural image applications to medical image applications. Finally, we
statistically investigate the generalization of the resultant activation
functions developed through the optimization scheme. With a Friedman test, we
conclude that the optimization scheme is able to generate activation functions
that outperform the existing standard ones in 92.8% cases among 28 different
cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation
function for image classification generated by the optimization scheme.

æè¦ï¼<paragraph>æ¿æ´»å½æ¸å°æ·±åº¦ç¥ç¶ç¶²è·¯çåæãæ¶æåæè½æé¡¯èçå½±é¿ãå¨æ·±åº¦å­¸ç¿æ¨¡åéç¼éç¨ä¸­ï¼ä¸ç´è´åæ¼å°æ¾ä¸è´ä¸æè½é«çæ¿æ´»å½æ¸ãç¾æçæåé²æ¿æ´»å½æ¸ï¼é¤äº Swish ä¹å¤ï¼é½æ¯ç±äººé¡å°å®¶æåè¨­è¨çãSwish æ¯ä½¿ç¨åºæ¼å¼·åå­¸ç¿çæå°ç­ç¥éç¼çãå¨æ¬ç ç©¶ä¸­ï¼æåæåºäºä¸ç¨®æ¼åæ¹æ³ï¼å°ééå°åååé¡ä»»åæä½³åæ¿æ´»å½æ¸ï¼æ¨å¨ç¼ç¾æè½åªæ¼ç¾ææåé²é¸é çå½æ¸ãéééåæä½³åæ¶æ§ï¼æåç²å¾äºä¸ç³»åæè½é«çæ¿æ´»å½æ¸ï¼è¡¨ç¤ºçºææ¸èª¤å·®ç·æ§å®å (EELU)ãå·²éå°å©åè§é»è©ä¼°å·²éç¼çæ¿æ´»å½æ¸ï¼ç¨æ¼åååé¡ä»»åï¼(1) äºç¨®æåé²çç¥ç¶ç¶²è·¯æ¶æ§ï¼ä¾å¦ ResNet50ãAlexNetãVGG16ãMobileNet å Compact Convolutional Transformerï¼æ¶µèå¾è¨ç®ééçå°è¼éçç¶²è·¯ï¼(2) å«åæ¨æºè³æéï¼åæ¬ CIFAR10ãImagenetteãMNISTãFashion MNISTãBeansãColorectal HistologyãCottonWeedID15 å TinyImageNetï¼æ¶µèå¾å¸åçæ©å¨è¦è¦ºåºæºãè¾²æ¥­å½±åæç¨å°é«å­¸å½±åæç¨ãæå¾ï¼æåçµ±è¨èª¿æ¥äºééæä½³åæ¹æ¡éç¼ççµææ¿æ´»å½æ¸çæ¦åãéé Friedman æª¢å®ï¼æåå¾åºçµè«ï¼æä½³åæ¹æ¡è½å¤ ç¢çå¨ 28 åä¸åçç ç©¶æ¡ä¾ä¸­ï¼æ 92.8% çæ¡ä¾æè½åªæ¼ç¾ææ¨æºå½æ¸ï¼ä¸¦ä¸ç¼ç¾ $-x\cdot erf(e^{-x})$ æ¯æä½³åæ¹æ¡ç¢ççæä½³å½±ååé¡æ¿æ´»å½æ¸ã</paragraph>

##### **LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**
2409.04744v1 by Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu

The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.

æè¦ï¼å¨å¼·åå­¸ç¿ï¼RLï¼çç°å¢è½ææ¨¡åä¸­ï¼åºæçä¸ç¢ºå®æ§éè¦å¨æ¢ç´¢åå©ç¨ä¹éåå¾ä»ç´°çå¹³è¡¡ï¼ä»¥æä½³åè¨ç®è³æºçä½¿ç¨ï¼ä»¥ç²¾æºä¼°è¨ä»£çé æççåµãå¨æ§å¶ç³»çµ±ä¸­åå¾å¹³è¡¡å¨çåµç¨ççææ³ä¸ç¹å¥å·æææ°æ§ãç¶èï¼ç±æ¼è¨±å¤ç°å¢é½æå»£æ³çåé©ç¥è­ï¼å æ­¤å¨éç¨®è¨­å®ä¸­å¾é ­éå§å­¸ç¿æ¯å¤é¤çãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº**L**anguage **M**odel **G**uided **T**rade-offsï¼å³**LMGT**ï¼ï¼éæ¯ä¸åæ°ç©ä¸æ¨£æ¬æçé«çæ¶æ§ï¼å®å©ç¨äºå¤§åèªè¨æ¨¡åï¼LLMï¼ä¸­åµå¥çå¨é¢åé©ç¥è­ï¼ä»¥åå®åèçéæ¨æºæ¸æå½¢å¼ï¼ä¾å¦ wiki æç¨ï¼çéæ´»æ§ãLMGT ééæ¡ç¨ç± LLM å¼å°ççåµè½ç§»ä¾çç·´å°ç®¡çæ¢ç´¢-å©ç¨æ¬è¡¡ï¼æå°ä»£ççæ¢ç´¢å·¥ä½ï¼å¾èæé«æ¨£æ¬æçãæåå·²ç¶å¾¹åºæ¸¬è©¦äº LMGT å¨åç¨® RL ä»»åä¸­çè¡¨ç¾ï¼ä¸¦å°å¶é¨ç½²å¨å·¥æ¥­ç´ RL æ¨è¦ç³»çµ±ä¸­ï¼å¨éäºç³»çµ±ä¸­ï¼å®å§çµåªæ¼åºç·æ¹æ³ãçµæè¡¨æï¼æåçæ¶æ§å¯ä»¥é¡¯èæ¸å° RL è¨ç·´éæ®µæéçæéææ¬ã

##### **NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**
2409.04723v1 by Debaditya Shome, Nasim Montazeri Ghahjaverestan, Ali Etemad

Sleep is known to be a key factor in emotional regulation and overall mental
health. In this study, we explore the integration of sleep measures from the
previous night into wearable-based mood recognition. To this end, we propose
NapTune, a novel prompt-tuning framework that utilizes sleep-related measures
as additional inputs to a frozen pre-trained wearable time-series encoder by
adding and training lightweight prompt parameters to each Transformer layer.
Through rigorous empirical evaluation, we demonstrate that the inclusion of
sleep data using NapTune not only improves mood recognition performance across
different wearable time-series namely ECG, PPG, and EDA, but also makes it more
sample-efficient. Our method demonstrates significant improvements over the
best baselines and unimodal variants. Furthermore, we analyze the impact of
adding sleep-related measures on recognizing different moods as well as the
influence of individual sleep-related measures.

æè¦ï¼ç¡ç å·²ç¥æ¯æç·èª¿ç¯åæ´é«å¿çå¥åº·ä¸­çééµå ç´ ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨å°åä¸æçç¡ç æ¸¬éæ´åå°å¯ç©¿æ´å¼æç·è¾¨è­ä¸­ãçºæ­¤ï¼æåæåºäº NapTuneï¼éæ¯ä¸åæ°ç©çæç¤ºèª¿æ´æ¡æ¶ï¼å®å©ç¨èç¡ç ç¸éçæ¸¬éä½çºåçµé è¨ç·´å¯ç©¿æ´æéåºåç·¨ç¢¼å¨çéå è¼¸å¥ï¼æ¹æ³æ¯å°è¼éç´æç¤ºåæ¸æ°å¢ä¸¦è¨ç·´å°æ¯å Transformer å±¤ãééå´è¬¹çç¶é©è©ä¼°ï¼æåè­æä½¿ç¨ NapTune ç´å¥ç¡ç æ¸æä¸åæ¹åäºä¸åå¯ç©¿æ´æéåºåï¼å³å¿é»åãåé»å®¹ç©æè¨åç®é»æ´»åï¼çæç·è¾¨è­æè½ï¼éè®å®æ´å·æ¨£æ¬æçãæåçæ¨¡åè­æäºç¸è¼æ¼æä½³åºç·åå®æ¨¡æè®ç°ï¼æé¡¯èçæ¹åãæ­¤å¤ï¼æååæäºæ°å¢èç¡ç ç¸éçæ¸¬éå°è¾¨è­ä¸åæç·çå½±é¿ï¼ä»¥ååå¥èç¡ç ç¸éçæ¸¬éçå½±é¿ã

##### **A Comprehensive Survey on Evidential Deep Learning and Its Applications**
2409.04720v1 by Junyu Gao, Mengyuan Chen, Liangyu Xiang, Changsheng Xu

Reliable uncertainty estimation has become a crucial requirement for the
industrial deployment of deep learning algorithms, particularly in high-risk
applications such as autonomous driving and medical diagnosis. However,
mainstream uncertainty estimation methods, based on deep ensembling or Bayesian
neural networks, generally impose substantial computational overhead. To
address this challenge, a novel paradigm called Evidential Deep Learning (EDL)
has emerged, providing reliable uncertainty estimation with minimal additional
computation in a single forward pass. This survey provides a comprehensive
overview of the current research on EDL, designed to offer readers a broad
introduction to the field without assuming prior knowledge. Specifically, we
first delve into the theoretical foundation of EDL, the subjective logic
theory, and discuss its distinctions from other uncertainty estimation
frameworks. We further present existing theoretical advancements in EDL from
four perspectives: reformulating the evidence collection process, improving
uncertainty estimation via OOD samples, delving into various training
strategies, and evidential regression networks. Thereafter, we elaborate on its
extensive applications across various machine learning paradigms and downstream
tasks. In the end, an outlook on future directions for better performances and
broader adoption of EDL is provided, highlighting potential research avenues.

æè¦ï¼å¯é çä¸ç¢ºå®æ§ä¼°è¨å·²æçºæ·±åº¦å­¸ç¿æ¼ç®æ³ç¢æ¥­é¨ç½²çééµéæ±ï¼ç¹å¥æ¯å¨é«é¢¨éªæç¨ä¸­ï¼ä¾å¦èªåé§é§åé«çè¨ºæ·ãç¶èï¼åºæ¼æ·±åº¦éææè²æ°ç¥ç¶ç¶²è·¯çä¸»æµä¸ç¢ºå®æ§ä¼°è¨æ¹æ³éå¸¸æé æå¤§éçè¨ç®è² æãçºäºæå°éé ææ°ï¼ä¸ç¨®ç¨±çºè­ææ·±åº¦å­¸ç¿ (EDL) çæ°ç¯ä¾æéèçï¼å®å¨å®æ¬¡ååå³éä¸­ä»¥æå°çé¡å¤éç®æä¾å¯é çä¸ç¢ºå®æ§ä¼°è¨ãéé èª¿æ¥å° EDL çç¾æç ç©¶æä¾å¨é¢çæ¦è¿°ï¼æ¨å¨çºè®èæä¾è©²é åçå»£æ³ä»ç´¹ï¼èç¡éåè¨­ååç¥è­ãå·é«ä¾èªªï¼æåé¦åæ·±å¥æ¢è¨ EDL ççè«åºç¤ï¼å³ä¸»è§éè¼¯çè«ï¼ä¸¦è¨è«å¶èå¶ä»ä¸ç¢ºå®æ§ä¼°è¨æ¶æ§çåå¥ãæåé²ä¸æ­¥å¾ååè§åº¦ä»ç´¹ EDL ä¸­ç¾æççè«é²å±ï¼éæ°å¶å®è­ææ¶ééç¨ãéé OOD æ¨£æ¬æ¹åä¸ç¢ºå®æ§ä¼°è¨ãæ·±å¥æ¢è¨åç¨®è¨ç·´ç­ç¥ä»¥åè­æåæ­¸ç¶²è·¯ãæ­¤å¾ï¼æåè©³ç´°èªªæå®å¨åç¨®æ©å¨å­¸ç¿ç¯ä¾åä¸æ¸¸ä»»åä¸­çå»£æ³æç¨ãæå¾ï¼æåæä¾äºå°æªä¾æ¹åçå±æï¼ä»¥æç²å¾æ´å¥½çæè½åæ´å»£æ³å°æ¡ç¨ EDLï¼ä¸¦éé»ä»ç´¹æ½å¨çç ç©¶éå¾ã

##### **A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**
2409.04704v1 by Cheng Wan, Chenjie Xie, Longfei Liu, Dan Wu, Ye Li

Continuous blood pressure (BP) monitoring is essential for timely diagnosis
and intervention in critical care settings. However, BP varies significantly
across individuals, this inter-patient variability motivates the development of
personalized models tailored to each patient's physiology. In this work, we
propose a personalized BP forecasting model mainly using electrocardiogram
(ECG) and photoplethysmogram (PPG) signals. This time-series model incorporates
2D representation learning to capture complex physiological relationships.
Experiments are conducted on datasets collected from three diverse scenarios
with BP measurements from 60 subjects total. Results demonstrate that the model
achieves accurate and robust BP forecasts across scenarios within the
Association for the Advancement of Medical Instrumentation (AAMI) standard
criteria. This reliable early detection of abnormal fluctuations in BP is
crucial for at-risk patients undergoing surgery or intensive care. The proposed
model provides a valuable addition for continuous BP tracking to reduce
mortality and improve prognosis.

æè¦ï¼æçºçè¡å£ (BP) ç£æ§å°æ¼éçç£è­·ç°å¢ä¸­çåæè¨ºæ·åå¹²é è³ééè¦ãç¶èï¼BP å äººèç°ï¼éç¨®æ£èéè®ç°æ§ä¿ä½¿éç¼éå°æ¯ä½æ£èçççæ³éèº«æé çåäººåæ¨¡åãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸ç¨®åäººå BP é æ¸¬æ¨¡åï¼ä¸»è¦ä½¿ç¨å¿é»å (ECG) ååé»å®¹ç©æè¨æ³ (PPG) ä¿¡èãæ­¤æéåºåæ¨¡åçµåäº 2D è¡¨å¾µå­¸ç¿ä»¥ææè¤éçççéä¿ãå¯¦é©æ¯å¨å¾ä¸ç¨®ä¸åæå¢æ¶éçè³æéä¸é²è¡ï¼ç¸½å±ä¾èª 60 ä½åè©¦èç BP æ¸¬éãçµæè¡¨æï¼è©²æ¨¡åå¨é«å­¸åå¨ä¿é²åæ (AAMI) æ¨æºæ¨æºå§å¯¦ç¾äºè·¨æå¢çæºç¢ºä¸ç©©å¥ç BP é æ¸¬ãå°æ¼æ¥åæè¡æéçç£è­·çé«é¢¨éªæ£èèè¨ï¼éç¨®å° BP ç°å¸¸æ³¢åçå¯é æ©ææª¢æ¸¬è³ééè¦ãææåºçæ¨¡åçºæçº BP è¿½è¹¤æä¾äºæå¹å¼çè£åï¼ä»¥éä½æ­»äº¡çä¸¦æ¹åé å¾ã

##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

æè¦ï¼<paragraph>ç®çï¼ä½¿ç¨ä¸åææåååå®åå¾çé«å­¸å½±åï¼å¨å½±åå¤è§ä¸å¯è½ææé¡¯èå·®ç°ãéç¨®ç¾è±¡ç¨±çºææåé ååç§»ï¼å¯è½æå°è´æ·±åº¦ç¥ç¶ç¶²è·¯çæè½ä¸éï¼èéäºç¶²è·¯æ¯éå°ç±ä¸ç¨®ææååå¾çè³æé²è¡è¨ç·´ï¼ä¸¦å¨å¦ä¸ç¨®ææåä¸é²è¡æ¸¬è©¦ãéåéè¦çå¯¦éåé¡å·²ç²å¾å»£æ³èªå¯ï¼ä½ç®åå°æªéå°ä¸åå½¢å¼åè¨ºæ·ä»»åé²è¡ç³»çµ±æ§ç ç©¶ãææåæ¹æ³ï¼å¨æ¬æä¸­ï¼æåæåºäºä¸é å»£æ³çå¯¦é©ç ç©¶ï¼è©ä¼°ææåé ååç§»å°ä¸åèªååè¨ºæ·ä»»åçå·ç©ç¥ç¶ç¶²è·¯æè½çå½±é¿ãæåå¨å¸¸è¦çæ¾å°å­¸å½¢å¼ä¸­è©ä¼°éç¨®ç¾è±¡ï¼åæ¬ X åãé»è¦æ·å±¤ææåç£æ¯é å½±ãçµæï¼æåç¼ç¾ï¼ä¾èªä¸åææåçè³æå¨ç¶²è·¯ä¸çæè½å¹¾ä¹ç¸½æ¯æ¯ä¾èªç¸åææåçè³æå·®ï¼æåéåäºä¸åè³æéæè½ä¸éçç¨åº¦ãå¼å¾æ³¨æçæ¯ï¼æåç¼ç¾éç¨®ä¸éå¨ç£æ¯é å½±ä¸­æä¸ºå´éï¼å¨ X åä¸­çºä¸­ç­ï¼å¨é»è¦æ·å±¤ææä¸­ç¸ç¶å°ï¼å¹³åèè¨ï¼æåå°å¶æ­¸å æ¼é»è¦æ·å±¤ææåå¾ç³»çµ±çæ¨æºåæ§è³ªï¼èç£æ¯é å½±æ X åä¸­ä¸å­å¨éç¨®æ§è³ªãæåéç ç©¶äºå°ä¸åæ¸éçç®æ¨é åè³ææ³¨å¥è¨ç·´éï¼ä»¥ååè¨ç·´è³æå å¥éè¨ï¼å¦ä½æå©æ¼æ³åãçµè«ï¼æåççµææä¾äºå»£æ³çå¯¦é©è­æï¼ä¸¦éåäºæ·±åº¦å­¸ç¿ä¸­ç±ææåé ååç§»é æçæè½ä¸éç¨åº¦ï¼ç®æ¨æ¯å¼å°æªä¾éå°é«å­¸å½±ååæçå¼·å¥æ·±åº¦å­¸ç¿æ¨¡åçç¼å±ã</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

æè¦ï¼çå­åææ¯çµ±è¨å­¸çä¸ååæ¯ï¼ç¨æ¼å»ºæ¨¡ç¹å®äºä»¶ç¼ççæéï¼ä¸¦å»£æ³ç¨æ¼é«å­¸ãå·¥ç¨ãéèåè¨±å¤å¶ä»é åãå¨é¸æçå­æ¨¡åæï¼éå¸¸å¨æ§è½åå¯è§£éæ§ä¹éé²è¡æ¬è¡¡ï¼å¶ä¸­æé«æ§è½æ¯ç±åºæ¼æ·±åº¦å­¸ç¿çé»çæ¨¡åå¯¦ç¾çãéå¨é«å­¸ç­é åæ¯ä¸åä¸»è¦åé¡ï¼å çºå¾æ¥­èä¸é¡æç²ç®ä¿¡ä»»é»çæ¨¡åä¾ååºéè¦çæ£èæ±ºç­ãKolmogorov-é¿è«¾å¾·ç¶²çµ¡ (KAN) æè¿è¢«æè­°ä½çºå¤å±¤æç¥å¨ (MLP) çå¯è§£éä¸æºç¢ºçæ¿ä»£æ¹æ¡ãæåå¼å¥äº CoxKANï¼éæ¯ä¸åç¨æ¼å¯è§£éãé«æ§è½çå­åæç Cox æ¯ä¾é¢¨éª Kolmogorov-Arnold ç¶²çµ¡ãæåå¨ 4 ååææ¸æéå 9 åçå¯¦é«çæ¸æéä¸è©ä¼°äºææåºç CoxKANãåæå¯¦é©è¡¨æï¼CoxKAN æºç¢ºå°æ¢å¾©äºé¢¨éªå½æ¸çå¯è§£éç¬¦èå¬å¼ï¼ä¸¦ææå°å·è¡èªåç¹å¾µé¸æãå° 9 åçå¯¦æ¸æéçè©ä¼°è¡¨æï¼CoxKAN å§çµåªæ¼ Cox æ¯ä¾é¢¨éªæ¨¡åï¼ä¸¦ä¸éå°äºåªæ¼æèèª¿æ´å¾ç MLP ç¸ç¶çæ§è½ãæ­¤å¤ï¼æåç¼ç¾ CoxKAN è­å¥äºé æ¸¬è®éä¹éçè¤éäº¤äºä½ç¨ï¼éäºäº¤äºä½ç¨ä½¿ç¨ç¾æççå­æ¹æ³æ¥µé£è­å¥ï¼ä¸¦èªåæ¾å°æ­ç¤ºéè¦çç©æ¨èªç©å°æ£èé¢¨éªçæºç¢ºå½±é¿çç¬¦èå¬å¼ã

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

æè¦ï¼å¤å¨å®ç¾çç±æ¼åæå½±é¿å¤åå¨å®ç³»çµ±ï¼å æ­¤æå¸¶ä¾éå¤§çææ°ï¼éè¦è¤éä¸å·æé©ææ§çæ²»çç­ç¥ãåç®¡ AI é©åçé«çä¿å¥æ±ºç­æ¯æ´ç³»çµ±æè¿æé²å±ï¼ä½ç¾æè§£æ±ºæ¹æ¡åéæ¼åå¥å¨å®ç³»çµ±ãå®åå¸¸å¸¸å¿½ç¥å¨å®ç³»çµ±ä¹éçè¤éä¾è³´æ§ï¼å æ­¤ç¡æ³æä¾å¯¦åä¸æç¨çæ´é«æ²»çå»ºè­°ãæåæåºä¸åæ°ç©çåå±¤å¤æºè½é«å¼·åå­¸ç¿ (HMARL) æ¶æ§ä¾è§£æ±ºéäºææ°ãæ­¤æ¶æ§çºæ¯åå¨å®ç³»çµ±ä½¿ç¨å°ç¨æºè½é«ï¼ä¸¦ééæç¢ºçæºè½é«ééè¨ç®¡éå»ºæ¨¡åæï¼è®ä¸åå¨å®ä¹éçæ²»çç­ç¥è½å¤ åèª¿ãæ­¤å¤ï¼æåå¼å¥éå±¤çæè¡¨ç¤ºæè¡ï¼å¨åç¨®å±¤ç´èªå¢åçæ£çæ³ï¼ä»¥æåæ²»çæºç¢ºæ§åç¸éæ§ãééå¨æè¡çï¼ä¸ç¨®è¤éçå¤å¨å®ç¾çï¼ç®¡çä¸­é²è¡å»£æ³çå®æ§åå®éè©ä¼°ï¼æåçåæ³å±ç¤ºäºå®å­¸ç¿æææ²»çæ¿ç­çè½åï¼å¯é¡¯èæ¹åçæ£å­æ´»çãæ­¤æ¶æ§æ¨èªèè¨åºæ±ºç­æ¯æ´ç³»çµ±çä¸å¤§é²æ­¥ï¼éåµäºå¤å¨å®æ²»çå»ºè­°çå¨é¢æ§æ¹æ³ã

##### **Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**
2409.04481v1 by Yizhen Zheng, Huan Yee Koh, Maddie Yang, Li Li, Lauren T. May, Geoffrey I. Webb, Shirui Pan, George Church

The integration of Large Language Models (LLMs) into the drug discovery and
development field marks a significant paradigm shift, offering novel
methodologies for understanding disease mechanisms, facilitating drug
discovery, and optimizing clinical trial processes. This review highlights the
expanding role of LLMs in revolutionizing various stages of the drug
development pipeline. We investigate how these advanced computational models
can uncover target-disease linkage, interpret complex biomedical data, enhance
drug molecule design, predict drug efficacy and safety profiles, and facilitate
clinical trial processes. Our paper aims to provide a comprehensive overview
for researchers and practitioners in computational biology, pharmacology, and
AI4Science by offering insights into the potential transformative impact of
LLMs on drug discovery and development.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼æ´åå°è¥ç©ç¼ç¾åéç¼é åæ¨èªèéå¤§çå¸ç¯è½ç§»ï¼æä¾äºè§£ç¾çæ©å¶ãä¿é²è¥ç©ç¼ç¾ååªåè¨åºè©¦é©æµç¨çæ°æ¹æ³ãæ¬ç¶è¿°éé»ä»ç´¹äº LLM å¨é©æ°è¥ç©éç¼ç®¡ç·ååéæ®µä¸­æ¥çéè¦çä½ç¨ãæåæ¢è¨äºéäºåé²çè¨ç®æ¨¡åå¦ä½æ­ç¤ºé¶é»ç¾çéè¯æ§ãè§£éè¤éççç©é«å­¸æ¸æãå¢å¼·è¥ç©åå­è¨­è¨ãé æ¸¬è¥ç©çæåå®å¨æ§ï¼ä»¥åä¿é²è¨åºè©¦é©æµç¨ãæåçè«ææ¨å¨çºè¨ç®çç©å­¸ãè¥çå­¸å AI4Science çç ç©¶äººå¡åå¾æ¥­èæä¾å¨é¢çæ¦è¿°ï¼æ·±å¥äºè§£ LLM å°è¥ç©ç¼ç¾åéç¼çæ½å¨è®é©æ§å½±é¿ã

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

æè¦ï¼èªåé«å­¸å½±åæè¿°çææ¨å¨ééç´æ¥å¾æ¾å°å½±åç¢çç²¾ç¢ºçè¨åºæè¿°ï¼æ¸è¼æ¾å°ç§é«å¸«çå·¥ä½è² æãç¶èï¼èä¸è¬å½±åæ¨é¡ä»»åç¸æ¯ï¼é«å­¸å½±åä¸­çç´°å¾®è¦è¦ºå·®ç°åç¹å®é åè¡èªæå¸¶ä¾éå¤§ææ°ãç¾ææ¹æ³å¸¸å¸¸å¿½ç¥æ­£å¸¸èç°å¸¸ç¼ç¾ä¹éçéè¦åå¥ï¼å°è´æ¬¡ä½³æè½ãå¨éé å·¥ä½ä¸­ï¼æåæåº FODA-PGï¼éæ¯ä¸åæ°ç©çç´°ç²åº¦å¨å®ç¾çèªé©æåå²åå½¢æ¶æ§ï¼ééé åèªé©æå­¸ç¿ä¾è§£æ±ºéäºéå¶ãFODA-PG ééå°ç¾çç¸éå±¬æ§ä¾æå¶è¨åºéè¦æ§åä½ç½®åçºä¸åçãç¹å®ç¾çãåãç¡ç¾çãé¡å¥ï¼ä¾å»ºæ§æ¾å°å­¸ç¼ç¾çç´°ç²åº¦åå½¢è¡¨ç¤ºãéç¨®èªé©æåå²ä½¿æåçæ¨¡åè½å¤ æææ­£å¸¸èçççæä¹éçç´°å¾®å·®ç°ï¼æ¸è¼è³æåå·®çå½±é¿ãééå°éç¨®ç´°ç²åº¦èªç¾©ç¥è­æ´åå°å¼·å¤§çåºæ¼è½æå¨çæ¶æ§ä¸­ï¼ä¸¦æä¾å¶æææ§çå´è¬¹æ¸å­¸è­æï¼FODA-PG è½å¤ çæç²¾ç¢ºä¸è¨åºä¸é£è²«çå ±åï¼ä¸¦å·åå¢å¼·çæ¦æ¬è½åãå¨ IU-Xray å MIMIC-CXR åºæºä¸çå»£æ³å¯¦é©è­æäºæåçæ¹æ³åªæ¼æåé²çæ¹æ³ï¼çªé¡¯äºé åé©æå¨é«å­¸å ±åçæä¸­çéè¦æ§ã

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

æè¦ï¼<paragraph>æºç¢ºéåå£é¢åªæååæå°æ¼åºç¤åæç¨ç ç©¶ä¸­çåç¨®æç¨å·æå¯¦è³ªæ§çéè¦æ§ï¼æ¶µèå¾äººé¡å¥åº·å°é£æ©è¨­è¨ååªåçé åãåç®¡å¨å¯¦é©æ¸¬éæè¡åå¾èçæ¼ç®æ³æ¹é¢åå¾äºé¡¯èé²å±ï¼ä½æéè§£æå£é¢åªæååæä»å·æè¶³å¤ çç©ºéè§£æåº¦åå¨åé©çç©ºéåä¸­ä»ç¶æ¯ä¸åé£ä»¥ææ¸çç®æ¨ãçºäºè§£æ±ºéåå·®è·ï¼æåå¼å¥äºä¸åæ·±åº¦å­¸ç¿æ¶æ§ï¼å®å¾æ¹æµå£é¢ç´ææµçå°æ¸å±¤ä¸­æåå£é¢å¹³è¡éåº¦å ´ï¼ä¸¦è¼¸åºç¸æç 2D å£é¢åªæåå ´ï¼å·æç¸åçç©ºéè§£æåº¦ååå¤§å°ãå¾ç©çè§åº¦ä¾çï¼æåçæ¡æ¶åç¶ä¸åä»£çæ¨¡åï¼æ¦æ¬äºé«è½éå¤å±¤æµçµæ§å½±é¿æ§å¶å£é¢åªæååæçåç¨®æ©å¶ãè©²ç¶²è·¯ä»¥ç£ç£æ¹å¼å¨ä¸åçµ±ä¸çæ¸æéä¸é²è¡è¨ç·´ï¼è©²æ¸æéåå«çµ±è¨ 1D æ¹æµééçç´æ¥æ¸å¼æ¨¡æ¬åç©ºéç¼å±çæ¹æµéçå±¤æµï¼æ©æ¦é·è«¾æ¸ç¯åå¾ 390 å° 1,500ãæåå±ç¤ºäºå°å¾ç²å­å½±åæ¸¬éæ¸¬éä¸­ç²å¾çå¯¦é©éåº¦å ´çé¶æ¬¡æç¨ï¼ä¸¦ä½¿ç¨å¾®æ±åªæåææ¸¬å¨å°é·è«¾æ¸æé« 2,000 çåæ­¥å£é¢åªæåæ¸¬éé©è­äºå£é¢åªæåä¼°è¨çç©çæºç¢ºæ§ãç¸½ä¹ï¼ææåºçæ¡æ¶çºå¾å®¹æç²å¾çéåº¦æ¸¬éä¸­æåç¡æ³ç²å¾çå¯¦é©å£é¢åªæåè³è¨å¥ å®äºåºç¤ï¼å¾èä¿é²äºåç¨®å¯¦é©æç¨ä¸­çé²å±ã</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

æè¦ï¼æ¬ææåºäºåéå¤æ¨¡æåæç³»ç» (MASL)ï¼
è¯¥ç³»ç»ç»åé³é¢åè§é¢æ°æ®ï¼èªå¨ä»åé¨è§é¢é¢éªéè§é¢ä¸­æåå³é®
çæ®µåææ ï¼ç¨äºä¸´åºè¯ä¼°ãMASL å°å£°é¨æ£æµä¸å³é®è¯è¯å«ç¸ç»åï¼ä»¥åæ
æ£èåå£°å¹¶ç»åè§é¢éç¹ï¼ä»¥ä¾¿æ´å¥½å°æ£æ¥å£°å¸¦è¿å¨ãè¯¥ç³»ç»åæ¬ä¸ä¸ªé¢éªè§é¢æåæ¨¡åï¼
è¯¥æ¨¡åéè¿åæè²ç¸ãé¥±ååº¦åå¼æ³¢å¨æ¥è¯å«å¸§ã
MASL è¿ä¸ºå£°å¸¦éº»ç¹æ£æµæä¾äºææçææ ï¼
éç¨ä¸¤é¶æ®µå£°é¨åå²è¿ç¨ï¼ä½¿ç¨ U-Netï¼ç¶åè¿è¡åºäºæ©æ£çç»åä»¥åå°è¯¯æ¥ãMASL ä¸ä½¿ç¨å£°é¨é¢ç§¯æ³¢å½¢ï¼èæ¯ä»å£°é¨æ©æ¨¡ä¸­ä¼°è®¡åå£°é¨è§æ³¢å½¢ (AGAW)ï¼è¯ä¼°å·¦å³å£°å¸¦ä»¥æ£æµåä¾§å£°å¸¦éº»ç¹ (UVFP)ãéè¿æ¯è¾ AGAW æ¹å·®ï¼MASL åºåå·¦å³éº»ç¹ãæ¶èç ç©¶åå¯¹å¬å±åçå®ä¸çæ°æ®éçå®éªéªè¯äº MASL çåå²æ¨¡åï¼å¹¶è¯æäºå¶æä¾å¯é ç UVFP è¯æ­ææ çè½åã

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

æè¦ï¼æ·±åº¦å­¸ç¿ç­èªååå·¥å·å¨é«å­¸å½±ååå²ä¸­ä½¿ç¨çæåï¼æ¸è¼äºæåè¼ªå»æç¹ªçç¶é ¸ãéå·²å°æåååè½ç§»å°èªåè¼ªå»çåè³ªè©ä¼° (QA)ï¼å¶ä¸­åå«åµæ¸¬é¯èª¤ä¸¦ä¿®æ­£å®åãåèªåå QA çæ½å¨è§£æ±ºæ¹æ¡æ¯ä½¿ç¨æ·±åº¦è²æ°ä¸ç¢ºå®æ§ä¾å»ºè­°æ½å¨çé¯èª¤ååï¼å¾èæ¸å°è±è²»å¨é¯èª¤åµæ¸¬ä¸çæéãååçç ç©¶å·²èª¿æ¥ä¸ç¢ºå®æ§åé¯èª¤ä¹éçå°æéä¿ï¼ç¶èï¼å°æªå°æ¹åè²æ°ä¸ç¢ºå®æ§å°åçãæç¨ãé²è¡ç ç©¶ï¼ä»¥ä½¿å¶ååºç¾å¨ä¸æºç¢ºååï¼èä¸åºç¾å¨æºç¢ºååãæåçç ç©¶ä½¿ç¨æºç¢ºåº¦å°æä¸ç¢ºå®æ§ (AvU) æå¤±ä¾è¨ç·´ FlipOut æ¨¡åï¼éæä¿ä½¿ä¸ç¢ºå®æ§ååºç¾å¨ä¸æºç¢ºååãæåå°æ­¤æ¹æ³æç¨æ¼å©åæ¾å°æ²»çé¨ä½çè³æéï¼å³é ­é ¸é¨é»è¦æ·å±¤ææåååèºæ ¸ç£å±æ¯ææãä½¿ç¨æ¥æ¶å¨æä½ç¹æ§ (ROC) åç²¾ç¢ºåº¦å¬åç (PR) æ²ç·ï¼éå°é«ç´ ä¸æºç¢ºæ§è©ä¼°ä¸ç¢ºå®æ§ç±åï¼å³é æ¸¬çµï¼ãæ¸å¼çµæé¡¯ç¤ºï¼èè²æ°åºæºç¸æ¯ï¼ææåºçæ¹æ³æåå°æå¶æºç¢ºé«ç´ çä¸ç¢ºå®æ§ï¼å°æ¼ä¸æºç¢ºé«ç´ çä¸ç¢ºå®æ§å­å¨é¡ä¼¼ææ³ãå¯å¨ https://github.com/prerakmody/bayesuncertainty-error-correspondence åå¾éç¾å¯¦é©çç¨å¼ç¢¼

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-PÃ©rez, Silvia GarcÃ­a-MÃ©ndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

æè¦ï¼<paragraph>æ ¹æå®æ¹çä¼°è¨ï¼å¨çç´æ 5000 è¬äººç½¹æ£å¤±æºçï¼ä¸éåæ¸å­æ¯å¹´å¢å  1000 è¬åæ°æ£èãå¨æ²ææ²»çæ¹æ³çææ³ä¸ï¼è¨åºé å¾åæ©æä»å¥æ¯å»¶ç·©å¶æ¡åçææææ¹æ³ãçºæ­¤ï¼äººå·¥æºæ§åè¨ç®èªè¨å­¸å¯è¢«ç¨æ¼èªç¶èªè¨åæãåäººåè©ä¼°ãç£æ§åæ²»çãç¶èï¼å³çµ±æ¹æ³éè¦æ´å¤èªç¾©ç¥è­ç®¡çåå¯è§£éæ§è½åãæ­¤å¤ï¼åç®¡éäºæ¨¡åä»£è¡¨äºä½¿ç¨æºæ§ç³»çµ±é²è¡è¨åºæ£èæºéçæåé²æ¹å¼ï¼ä½å°å¤§åèªè¨æ¨¡å (LLM) ç¨æ¼èªç¥è½åä¸éè¨ºæ·ä»ç¶å¾å°è¦ãå æ­¤ï¼æåå©ç¨èå¤©æ©å¨äººè§£æ±ºæ¹æ¡ä¸­ä½¿ç¨ææ°èªç¶èªè¨èç (NLP) æè¡ç LLMï¼ä»¥æä¾å°èªç¥è½åä¸éçæ©å¨å­¸ç¿é æ¸¬ãèªè¨æ¦å¿µç¹å¾µè¢«ç¨æ¼é©ç¶çèªç¶èªè¨åæãééå¯è§£éæ§ï¼æåæ¨å¨æ¶é¤æ¨¡åçæ½å¨åå·®ï¼ä¸¦æé«å¶å¨è¨ºæ·æ±ºç­ä¸­åå©è¨åºå·¥ä½èçæ½åãæ´è©³ç´°å°èªªï¼ææåºçç®¡éåæ¬ï¼(i) ä½¿ç¨åºæ¼ NLP çæç¤ºå·¥ç¨é²è¡è³æèåï¼(ii) ä¸²æµå¼è³æèçï¼åæ¬ç¹å¾µå·¥ç¨ãåæåé¸æï¼(iii) å³æåé¡ï¼ä»¥å (iv) å¯è§£éæ§åè¡¨æ¿ï¼ä»¥æä¾é æ¸¬çµæçå¯è¦ååèªç¶èªè¨æè¿°ãåé¡çµæå¨ææè©ä¼°ææ¨ä¸­é½è¶é 80%ï¼å¿æºéåé¡å¥çå¬åçç´çº 85%ãç¸½èè¨ä¹ï¼æåçºéé å·¥ä½è²¢ç»äºä¸åç¶æ¿å¯¦æ ãéæ´»ãéä¾µå¥æ§ãåäººåçè¨ºæ·ç³»çµ±ã</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

æè¦ï¼å¨çäººå£å¿«éèåè¶¨å¢å°è´å¤±æºççä¾å¢å ï¼åæ¬é¿è²æµ·é»çï¼çªé¡¯åºæ©æä¸æºç¢ºçè¨ºæ·æ¹æ³çè¿«åéæ±ãå³çµ±çè¨ºæ·æè¡ï¼ä¾å¦èªç¥æ¸¬é©ãç¥ç¶å½±ååçç©æ¨è¨åæï¼å¨æææ§ãå¯åæ§åææ¬æ¹é¢é¢è¨éå¤§éå¶ï¼ç¹å¥æ¯å¨æ©æéæ®µãæ¬ç ç©¶æ¢è¨æ©å¨å­¸ç¿ (ML) ä½çºä¸ç¨®è®é©æ§æ¹æ³çæ½åï¼ééå©ç¨ ML æ¨¡ååæåæ´åè¤éçå¤æ¨¡å¼æ¸æéï¼åæ¬èªç¥è©ä¼°ãç¥ç¶å½±ååéºå³ä¿¡æ¯ï¼ä¾å¢å¼·æ©æå¤±æºçæª¢æ¸¬ãå°ç¾ææç»é²è¡äºå¨é¢åé¡§ï¼ä»¥è©ä¼°åç¨® ML æ¨¡åï¼åæ¬ç£ç£å­¸ç¿ãæ·±åº¦å­¸ç¿ååé²æè¡ï¼ä¾å¦éæå­¸ç¿åTransformeræ¨¡åï¼è©ä¼°å¶æºç¢ºæ§ãå¯è§£éæ§åè¨åºæ´åçæ½åãç ç©¶çµæè¡¨æï¼åç®¡ ML æ¨¡åå¨æé«è¨ºæ·ç²¾åº¦åå¯¦ç¾æ©æå¹²é æ¹é¢é¡¯ç¤ºåºé¡¯èçå¸æï¼ä½å¶å¯æ¦åæ§ãå¯è§£éæ§åéå¾·é¨ç½²ä»ç¶å­å¨ææ°ãæ¬ç ç©¶æå¾æ¦è¿°äºæ¨å¨å¢å¼· ML æ¨¡åå¨å¤±æºçæª¢æ¸¬ä¸­çè¨åºæç¨çæªä¾æ¹åï¼å¼·èª¿è·¨å­¸ç§åä½åéå¾·å¥å¨çæ¡æ¶ï¼ä»¥æ¹åé¿è²æµ·é»çåå¶ä»å½¢å¼å¤±æºççæ©ææª¢æ¸¬åå¹²é ç­ç¥ã

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

æè¦ï¼ç®èçåå²å¨é«å­¸å½±ååæä¸­æ§æä¸é éå¤§ææ°ãç¾æè¨±å¤è§£æ±ºæ¹æ¡ï¼ä¸»è¦æ¯åºæ¼ CNNï¼é¢è¨ç¼ºä¹æ´é«èæ¯çè§£çåé¡ãæèï¼ä¸äºæ¹æ³è¨´è«¸æ¼å¤§è¦æ¨¡ Transformer æ¨¡åä¾å½åæ´é«èæ¯å·®è·ï¼ä½ç§ç²äºæ¨¡åå¤§å°åè¨ç®è¤éåº¦ãæå¾ï¼è¨±å¤åºæ¼ Transformer çæ¹æ³ä¸»è¦ä¾è³´æ¼åºæ¼ CNN çè§£ç¢¼å¨ï¼èå¿½è¦äºåºæ¼ Transformer çè§£ç¢¼æ¨¡åçåªé»ãèªè­å°éäºéå¶ï¼æåééå¼å¥ MobileUNETR ä¾è§£æ±ºå°é«æè¼éç´è§£æ±ºæ¹æ¡çéæ±ï¼å¶ç®æ¨æ¯åæè CNN å Transformer ç¸éçæè½éå¶ï¼åææå°åæ¨¡åå¤§å°ï¼çºé«æå½±ååå²éåºæå¸æçä¸æ­¥ãMobileUNETR æ 3 åä¸»è¦ç¹é»ã1) MobileUNETR åå«ä¸åè¼éç´æ··å CNN-Transformer ç·¨ç¢¼å¨ï¼ä»¥ææçæ¹å¼å¹«å©å¹³è¡¡å±é¨åæ´é«èæ¯ç¹å¾µæåï¼2) ä¸åæ°ç©çæ··åè§£ç¢¼å¨ï¼å¨è§£ç¢¼éæ®µåæå©ç¨ä¸åè§£æåº¦ä¸çä½éåæ´é«ç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé®ç½©çæï¼3) è¶è¶å¤§åèè¤éçæ¶æ§ï¼MobileUNETR ä»¥ 300 è¬ååæ¸å 1.3 GFLOP çè¨ç®è¤éåº¦å¯¦ç¾äºåè¶çæè½ï¼åå¥æ¸å°äº 10 åå 23 åçåæ¸å FLOPãå·²ç¶é²è¡äºå»£æ³çå¯¦é©ï¼ä»¥é©è­æåæåºçæ¹æ³å¨ååå¬éå¯ç¨çç®èçè®åå²è³æéï¼åæ¬ ISIC 2016ãISIC 2017ãISIC 2018 å PH2 è³æéï¼ä¸çæææ§ãç¨å¼ç¢¼å°å¬éæ¼ï¼https://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

æè¦ï¼é·æ°è¤éåå½¢æ¸¬é© (RCFT) ç­ç¹ªç«æ¸¬é©å»£æ³ç¨æ¼è©ä¼°è¦è¦ºç©ºéæè½åè¨æ¶åç­èªç¥åè½ï¼ä½¿å¶æçºæª¢æ¸¬è¼åº¦èªç¥éç¤ (MCI) çå¯¶è²´å·¥å·ãåç®¡å®åå¾æç¨ï¼ä½åºæ¼éäºæ¸¬é©çç¾æé æ¸¬æ¨¡åéå¸¸æåå°æ¨£æ¬éå°åç¼ºä¹å¤é¨é©è­ç­éå¶ï¼éææå®³å¶å¯é æ§ãæåéç¼äºä¸åå¤ä¸²æµæ·±åº¦å­¸ç¿æ¡æ¶ï¼å®æ´åäºå©åä¸åçèçä¸²æµï¼ä¸ååºæ¼å¤é ­èªæ³¨æåï¼ä½¿ç¨åå§ RCFT å½±åçç©ºéä¸²æµï¼ä»¥åä¸åæ¡ç¨ååéç¼çèªåè©åç³»çµ±çè©åä¸²æµãæåçæ¨¡åå¨éåç¾¤çµä¸­ 1,740 ååè©¦èçè³æä¸é²è¡è¨ç·´ï¼ä¸¦å¨ä¾èªéåç 222 ååè©¦èçå¤é¨é«é¢è³æéä¸é²è¡é©è­ãææåºçå¤ä¸²æµæ¨¡åå¨å¤é¨é©è­ä¸­è¡¨ç¾åºåªæ¼åºæºæ¨¡åçæè½ (AUC = 0.872ï¼æºç¢ºç = 0.781)ãç©ºéåè©åä¸²æµçæ´åä½¿æ¨¡åè½å¤ å¾åå§å½±åæ·åè¤éçè¦è¦ºç´°ç¯ï¼åæä¹è½ç´å¥çµæ§åçè©åè³æï¼éå±åå¢å¼·äºå®æª¢æ¸¬ç´°å¾®èªç¥éç¤çè½åãéç¨®ééæ¹æ³ä¸åæé«äºé æ¸¬æºç¢ºæ§ï¼ä¹å¢å äºæ¨¡åçç©©å¥æ§ï¼ä½¿å¶å¨ä¸åçè¨åºç°å¢ä¸­æ´å¯é ãæåçæ¨¡åå°è¨åºç°å¢æå¯¦éçæç¾©ï¼å®å¯ä»¥å¨å¶ä¸­ä½çºæ©æ MCI ç¯©æª¢çå·ææ¬æççå·¥å·ã

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v2 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of active fire spots detected by the AQUA\_M-T
satellite in the Amazon, Brazil. The approach employs a mixed Recurrent Neural
Network (RNN) model, combining Long Short-Term Memory (LSTM) and Gated
Recurrent Unit (GRU) architectures to predict the monthly accumulations of
daily detected active fire spots. Data analysis revealed a consistent
seasonality over time, with annual maximum and minimum values tending to repeat
at the same periods each year. The primary objective is to verify whether the
forecasts capture this inherent seasonality through machine learning
techniques. The methodology involved careful data preparation, model
configuration, and training using cross-validation with two seeds, ensuring
that the data generalizes well to both the test and validation sets for both
seeds. The results indicate that the combined LSTM and GRU model delivers
excellent forecasting performance, demonstrating its effectiveness in capturing
complex temporal patterns and modeling the observed time series. This research
significantly contributes to the application of deep learning techniques in
environmental monitoring, specifically in forecasting active fire spots. The
proposed approach highlights the potential for adaptation to other time series
forecasting challenges, opening new opportunities for research and development
in machine learning and prediction of natural phenomena.
  Keywords: Time Series Forecasting; Recurrent Neural Networks; Deep Learning.

æè¦ï¼éé ç ç©¶æåºäºä¸ç¨®ç¶åæ¹æ³ï¼ç¨æ¼å»ºæ¨¡åé æ¸¬ AQUA\_M-T è¡æå¨å·´è¥¿äºé¦¬éå°ååµæ¸¬å°çæ­·å²æ´»èºç«é»æéåºåãæ­¤æ¹æ³æ¡ç¨æ··åéè¿´ç¥ç¶ç¶²è·¯ (RNN) æ¨¡åï¼çµåé·ç­æè¨æ¶ (LSTM) åéæ§éè¿´å®å (GRU) æ¶æ§ï¼ä¾é æ¸¬æ¯æ¥åµæ¸¬å°çæ´»èºç«é»çæ¯æç´¯ç©å¼ãè³æåæé¡¯ç¤ºåºé¨èæéæ¨ç§»çä¸è´å­£ç¯æ§ï¼æ¯å¹´çæå¤§å¼åæå°å¼å¾åæ¼å¨æ¯å¹´ç¸åçææéè¤åºç¾ãä¸»è¦ç®æ¨æ¯é©è­é æ¸¬æ¯å¦ééæ©å¨å­¸ç¿æè¡ææå°éç¨®åºæçå­£ç¯æ§ãæ­¤æ¹æ³æ¶åä»ç´°çè³ææºåãæ¨¡åçµæåä½¿ç¨å©åç¨®å­çäº¤åé©è­é²è¡è¨ç·´ï¼ç¢ºä¿è³æå°æ¼å©åç¨®å­çæ¸¬è©¦åé©è­éé½è½å¾å¥½å°æ¦åãçµæè¡¨æï¼LSTM å GRU ççµåæ¨¡åæä¾äºæ¥µä½³çé æ¸¬æè½ï¼è­æå¶å¨ææè¤éçæéæ¨¡å¼åå°è§æ¸¬å°çæéåºåé²è¡å»ºæ¨¡æ¹é¢éå¸¸ææãéé ç ç©¶é¡¯èå°ä¿æäºæ·±åº¦å­¸ç¿æè¡å¨ç°å¢ç£æ¸¬ä¸­çæç¨ï¼ç¹å¥æ¯å¨é æ¸¬æ´»èºç«é»æ¹é¢ãææåºçæ¹æ³çªé¡¯äºé©æå¶ä»æéåºåé æ¸¬ææ°çæ½åï¼çºæ©å¨å­¸ç¿åèªç¶ç¾è±¡é æ¸¬æ¹é¢çç ç©¶åéç¼éåäºæ°çæ©æã
ééµå­ï¼æéåºåé æ¸¬ï¼éè¿´ç¥ç¶ç¶²è·¯ï¼æ·±åº¦å­¸ç¿ã

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

æè¦ï¼<paragraph>åºæ¼è¦è¦ºçå¤ç§å°èªç±æ¼å¶éä¾µå¥æ§ãææ¬æçåéæ´»æ§åªå¢èåå°è¶ä¾è¶å¤çéæ³¨ãç¹å¥æ¯ï¼åºæ¼è¦è¦ºçå°èªç³»çµ±çä¸åééµåç´ æ¯è¿½è¹¤æè¡å¨æ¢°ãè 2D å¨æ¢°è¿½è¹¤æ¹æ³ç¸æ¯ï¼3D å¨æ¢°è¿½è¹¤å¨è¨åºå¯¦åä¸­å·ææ´å»£æ³çå¹å¼ï¼ä½ç±æ¼ç´çå¼±ãé®æåç¼ºä¹ç¨æ¼ 3D éæºçé»è¦è¼å©è¨­è¨ (CAD) æ¨¡åï¼å æ­¤ä¹æ´å·ææ°æ§ãçºäºè§£æ±ºéäºææ°ï¼æåæåº SurgTrackï¼ä¸ç¨®é©ç¨æ¼ç¡ CAD åç©©å¥ççå¯¦ä¸çæç¨ç¨å¼çå©éæ®µ 3D å¨æ¢°è¿½è¹¤æ¹æ³ãå¨ç¬¬ä¸åéæºéæ®µï¼æåæ´åä¸åå¨æ¢°ç°½ç½²è·é¢å ´ (SDF)ï¼å°å¨æ¢°ç 3D è¡¨å¾µé²è¡å»ºæ¨¡ï¼å¯¦ç¾ç¡ CAD ç 3D éæºãå æ­¤ï¼æåå¯ä»¥ééå°è¦è¨ä¸²æµèå·²éæºç SDF æ¨¡åé²è¡å¹éï¼åå¾å¨æ¢°å¨ 3D ç©ºéä¸­çä½ç½®åæ¹åãå¨ç¬¬äºåè¿½è¹¤éæ®µï¼æåè¨­è¨ä¸åå§¿å¢åæä½³åæ¨¡çµï¼å©ç¨å§¿å¢è¨æ¶æ± çæ­·å²è¿½è¹¤çµæä¾æä½³åè¿½è¹¤çµæä¸¦æ¹åé®æçç©©å¥æ§ãæ­¤å¤ï¼æåæ¶é Instrument3D è³æéï¼ä»¥å¨é¢è©ä¼°æè¡å¨æ¢°ç 3D è¿½è¹¤ãå»£æ³çå¯¦é©é©è­äºæå SurgTrack çåªè¶æ§åå¯æ´åæ§ï¼ä»¥é¡¯èçæ¹é²åªæ¼ç¾ææè¡ãç¨å¼ç¢¼åè³æéå¯å¨ https://github.com/wenwucode/SurgTrack åå¾ã</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

æè¦ï¼ä¼°è¨çèå°çéæ¿¾ç (eGFR) æ¯è¨åºå¯¦åä¸­èèåè½çéè¦ææ¨ãéç¶å³çµ±æ¹ç¨å¼åä½¿ç¨è¨åºèå¯¦é©å®¤è³æçæ©å¨å­¸ç¿ (ML) æ¨¡åå¯ä»¥ä¼°è¨ eGFRï¼ä½æºç¢ºé æ¸¬æªä¾ eGFR æ°´å¹³ä»ç¶æ¯èèç§é«å¸«å ML ç ç©¶äººå¡çä¸å¤§ææ°ãæè¿çç ç©¶é²å±é¡¯ç¤ºï¼å¤§åèªè¨æ¨¡å (LLM) åå¤§åå¤æ¨¡ææ¨¡å (LMM) å¯ä»¥ä½çºåç¨®æç¨ç¨å¼çå¼·å¥åºç¤æ¨¡åãæ¬ç ç©¶æ¢è¨ LMM é æ¸¬æªä¾ eGFR æ°´å¹³çæ½åï¼å¶è³æéåå« 50 ä½çæ£çå¯¦é©å®¤åè¨åºæ¸å¼ãééæ´ååç¨®æç¤ºæè¡å LMM çåå¥ï¼æåçç ç©¶çµæé¡¯ç¤ºï¼éäºæ¨¡åå¨çµåç²¾ç¢ºæç¤ºå eGFR è»è·¡çè¦è¦ºåè¡¨ç¤ºæï¼å¯æä¾èç¾æ ML æ¨¡åç¸è¿çé æ¸¬æè½ãéé ç ç©¶æ´å±äºåºç¤æ¨¡åçæç¨ï¼ä¸¦çºæªä¾ç ç©¶å©ç¨éäºæ¨¡åä¾æå°è¤éçé«çé æ¸¬ææ°æä¾äºéå¾ã

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

æè¦ï¼è¶é³æ³¢å å¶æä¾éä¾µå¥æ§ãç¡è¼»å°ä¸å³æå½±åçåªé»ï¼èå»£æ³ç¨æ¼è¨åºä»å¥åè¨ºæ·ã
ç¶èï¼ç±æ¼æä½å¡éè¦å¤§éçè¨ç·´åå°æ¥­ç¥è­ï¼éå¶äºæ­¤éæ´»ç¨åºçå¯åæ§ãæ©å¨äººè¶é³æ³¢ (RUS) æä¾äºä¸åå¯è¡çè§£æ±ºæ¹æ¡ä¾è§£æ±ºæ­¤éå¶ï¼
åç®¡å¦æ­¤ï¼è¦éå°äººé¡ç­ç´ççç·´åº¦ä»ç¶å·æææ°æ§ãå­¸ç¿ç¤ºç¯ (LfD) æ¹æ³å·²å¨ RUS ä¸­é²è¡æ¢è¨ï¼å®å¾é¢ç·ç¤ºç¯çè³æéå­¸ç¿åé©ç­ç¥ï¼ä»¥ç·¨ç¢¼å°å®¶è¶é³æ³¢æª¢æ¥å¡çå¿æºæ¨¡åãç¶èï¼è¿ä»å°æªæ¢è¨å°å®¶å¨ RUS è¨ç·´æéçç©æ¥µåèï¼å³æå°ãæå°å·²ç¥å¯ä»¥æé«äººé¡è¨ç·´çæçåç¸¾æãæ¬ææåºäºä¸å RUS æå°æ¶æ§ï¼ä»¥æåå¶ç¸¾æãæ­¤æ¶æ§çµåäº DRLï¼èªæç£ç£å¯¦åï¼èééæå°æä¾çå°å®¶ç¨çåé¥ãDRL ä½¿ç¨é¢ç·ç­ç¥è»æ§åä½-è©è« (SAC) ç¶²è·¯ï¼ä¸¦æ ¹æå½±ååè³ªè©åçµ¦äºçåµãå°å®¶çæå°è¢«å»ºæ¨¡çºé¨åå¯è§å¯é¦¬å¯å¤«æ±ºç­éç¨ (POMDP)ï¼å®æ ¹æå°å®¶çä¿®æ­£ä¾æ´æ°ç­ç¥åæ¸ãå¨æ¨¡æ¬äººé«æ¨¡åä¸çé©è­ç ç©¶é¡¯ç¤ºï¼æå°å°å­¸ç¿çæé«äº $25\%$ï¼é«åè³ªå½±åæ·åæ¸éæé«äº $74.5\%$ã

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

æè¦ï¼æ³¨æåç¼ºé·éåç (ADHD) æå¨åç¨®é åé æé¡¯èçæå®³ãææ©è¨ºæ· ADHD ä¸¦æ¥åæ²»çå¯ä»¥å¤§å¹æ¹åçæ´»åè³ªååè½ãæè¿ï¼æ©å¨å­¸ç¿æ¹æ³å·²ç¶æåäº ADHD è¨ºæ·ç¨åºçæºç¢ºåº¦åæçãç¶èï¼ç¾ææ¹æ³æéçè¨­ååè¨ç·´æç´ çäººå¡ææ¬éå¸¸å¾é«ãå æ­¤ï¼æåé¦æ¬¡å°åºæ¼å½±ççå¹ç´åä½è¾¨è­ç¶²è·¯å¼å¥ ADHD è¨ºæ·ãæåä¹è¨éäºä¸åçæ­£çå¤æ¨¡å¼ ADHD è³æéï¼ä¸¦å¾å½±çæ¨¡å¼ä¸­èååºä¸ååä½é¡å¥ä»¥é²è¡ ADHD è¨ºæ·ãæ´åæµç¨çè³æå·²ç¶åå ±çµ¦ CNTW-NHS åºéæï¼å°ç±é«çé¡§å/å°æ¥­äººå£«å¯©æ¥ï¼ä¸¦å°é©æå¬éã

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

æè¦ï¼çç§ç¾çæ¯ä¸ç¨®æ®éçæ¢æ§ç¾çï¼èå¤§éçç¶æ¿è² æãåäººçè¦åå¢å çå¨èº«ç¾çé¢¨éªæéãåç®¡æ®éå»ºè­°æ¯å¤©å·çå©æ¬¡ï¼ä½ç±æ¼å¥å¿åè«é¢ç­å ç´ ï¼å°å»ºè­°çå£èèªæä¿å¥è¡çºçä¾å¾æ§ä»ç¶ä½æ¼æä½³æ°´å¹³ãçºäºè§£æ±ºéååé¡ï¼æåéç¼äº Oralyticsï¼ä¸å mHealth ä»å¥ç³»çµ±ï¼æ¨å¨è£åè¨åºé«çæä¾çé é²ä¿å¥ï¼ä»¥é é²æçç§ç¾çé¢¨éªçéç·£ååäººãOralytics çµåäºä¸åå¨ç·å¼·åå­¸ç¿æ¼ç®æ³ï¼ä»¥ç¢ºå®æä¾ä»å¥æç¤ºçæä½³æéï¼éäºæç¤ºé¼åµå£èèªæä¿å¥è¡çºãæåå·²å¨è¨»åçè¨åºè©¦é©ä¸­é¨ç½²äº Oralyticsãè©²é¨ç½²éè¦ä»ç´°çè¨­è¨ä¾ç®¡çç¾åè¨åºè©¦é©è¨­ç½®ä¸­å·é«çææ°ãå¨æ¬æä¸­ï¼æåï¼1ï¼éé»ä»ç´¹äºè§£æ±ºéäºææ°ç RL æ¼ç®æ³çééµè¨­è¨æ±ºç­ï¼ä»¥åï¼2ï¼é²è¡éæ°æ½æ¨£åæä»¥è©ä¼°æ¼ç®æ³è¨­è¨æ±ºç­ãOralytics çç¬¬äºéæ®µï¼é¨æ©å°ç§è©¦é©ï¼è¨åæ¼ 2025 å¹´æ¥å­£éå§ã

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

æè¦ï¼å¨å»çä¿å¥é¢åï¼å»å­¦å½±ååå²å¯¹äºåç¡®çç¾çè¯æ­åæææ²»çç­ç¥çå¼åè³å³éè¦ãæ©ææ£æµå¯ä»¥æå¤§å°å¸®å©æ§å¶ç¾çï¼å¹¶å¯è½é²æ­¢ç¾çè¿å±ãæºå¨å­¦ä¹ ï¼å°¤å¶æ¯æ·±åº¦å·ç§¯ç¥ç»ç½ç»ï¼å·²æä¸ºè§£å³åå²ææçä¸ç§æåéçæ¹æ³ãU-Net ç­ä¼ ç»æ¹æ³ä½¿ç¨ç¼ç åè¿è¡å±é¨è¡¨ç¤ºå»ºæ¨¡åè§£ç åæ¥æ­ç¤ºè¯­ä¹å³ç³»ãç¶èï¼è¿äºæ¨¡åéå¸¸é¾ä»¥å¤çå¨çº¹çåå½¢ç¶ä¸è¡¨ç°åºæ¾çååçå¤å°ºåº¦å¯¹è±¡ï¼å¹¶ä¸å®ä»¬ç»å¸¸æ æ³æè·è¾å¥æ°æ®ä¸­çè¿ç¨ä¾èµå³ç³»ãä¸ä¸ºåºåå°åºåé¢æµèè®¾è®¡ç Transformer å·²è¢«æåºä½ä¸ºæ¿ä»£æ¹æ¡ï¼å©ç¨å¨å±èªæ³¨æåæºå¶ãç¶èï¼ç±äºç²åº¦ç»èä¸è¶³ï¼å®ä»¬ææ¶å¯è½ç¼ºä¹ç²¾ç¡®çå®ä½ãä¸ºäºåæè¿äºéå¶ï¼æä»¬å¼å¥äº TransDAEï¼ä¸ç§æ°é¢çæ¹æ³ï¼å®éæ°ææ³äºèªæ³¨æåæºå¶ï¼ä»¥åå«æ´ä¸ªç¹å¾ç©ºé´ä¸­çç©ºé´åééå³èï¼åæ¶ä¿æè®¡ç®æçãæ­¤å¤ï¼TransDAE éè¿å°ºåº¦é´äº¤äºæ¨¡åå¢å¼ºäºè·³è·è¿æ¥è·¯å¾ï¼ä¿è¿äºç¹å¾éç¨å¹¶æé«äºå®ä½ç²¾åº¦ãå¼å¾æ³¨æçæ¯ï¼å³ä½¿ä¸ä¾èµé¢è®­ç»æéï¼TransDAE å¨ Synaps å¤å¨å®æ°æ®éä¸ä¹ä¼äºç°æçæåè¿æ¹æ³ã

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain LÃ©guillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte SÃ©roussi, Julien Grosjean, Romain Lelong, Catherine Letord, StÃ©fan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

æè¦ï¼<paragraph>èæ¯ï¼ç¨è¥å¯©æ¥æ¯ç±è¥å¸«å·è¡çä¸ç¨®çµæ§åæ£èè¨ªè«ï¼ç®çå¨æ¼åªåè¥ç©æ²»çãå¨å¯¦åä¸ï¼ç¨è¥å¯©æ¥æ¯ä¸é åé·ä¸èªç¥éæ±é«çä»»åï¼éè¦å·åç¹å®ç¥è­ãéç¶å·²æåºè¨åºå¯¦åæå¼ï¼ä½å¶æç¨å¾ç¹ç£ãæ¹æ³ï¼æåæ ¹æ STOPP/START v2 æå¼çå¯¦ä½ï¼ä¸¦ä½¿ç¨è¡¨æ ¼ãåè¡¨åè±å½¢ç¬¦èè¦è¦ºååç¾å½æ´çè¥ç©ç¥è­ï¼è¨­è¨äºä¸å¥ç¨è¥å¯©æ¥çè¨åºæ±ºç­æ¯æ´ç³»çµ± ABiMedãæåå¨é¨æ©æ¨¡æ¬è©¦é©ä¸­ï¼è® 39 ä½ç¤¾åè¥å¸«è©ä¼° ABiMedï¼æ¯ä½è¥å¸«éå°å©ä½èæ§æ£èå·è¡ç¨è¥å¯©æ¥ï¼å©æ¬¡æ²æä½¿ç¨ ABiMedï¼å©æ¬¡ä½¿ç¨ ABiMedãæåè¨éäºè¥å¸«è­å¥åºçåé¡ãå»ºè­°çä»å¥æªæ½ãåææéãæç¥å¯ç¨æ§åè©è«ãå°è¥å¸«çç¨è¥å¯©æ¥èå°å®¶è¨­è¨çéæ¨æºé²è¡æ¯è¼ãçµæï¼ä½¿ç¨ ABiMed å¾ï¼è¥å¸«å¨ç¨è¥å¯©æ¥æéç¼ç¾äºå¤ 1.6 åç¸éçè¥ç©ç¸éåé¡ï¼p=1.1e-12ï¼ï¼ä¸¦æåºæ´å¥½çä»å¥æªæ½ï¼p=9.8e-9ï¼ï¼èç¡éè±è²»æ´å¤æéï¼p=0.56ï¼ãç³»çµ±å¯ç¨æ§è©åçº 82.7ï¼è¢«è©çºãåªè¯ããå¨ä»åçè©è«ä¸­ï¼è¥å¸«è®è³ ABiMed çè¦è¦ºåé¢åï¼ä»¥åå®æ¯è¼ç®åæ²»çèå»ºè­°æ²»ççè½åãå¤å ç´ åæé¡¯ç¤ºï¼ABiMed æä¾çæ¯æ´å¨è¥å¸«çå¹´é½¡ææ§å¥æ¹é¢æ²æå·®ç°ï¼å°±è­å¥åºçåé¡ç¾åæ¯æå»ºè­°ä»å¥æªæ½çåè³ªèè¨ãçµè«ï¼ä½¿ç¨æºæ§ä¸è¦è¦ºåçè¨åºæ±ºç­æ¯æ´ç³»çµ±ï¼å¯ä»¥åå©è¥å¸«å·è¡ç¨è¥å¯©æ¥ãæåçè§é»ä¸»è¦æ¯é©è­ç³»çµ±å¨è¨åºæ¢ä»¶ä¸çæåº¦ã</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çæåå¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼é è¨ç·´éæ®µä¸­å­¸ç¿å°çæµ·éé è¨ç·´æ¸æãé è¨ç·´éç¨åè¨ç·´æ¸æçä¸éææ§å°è´è¨±å¤åºæºæ¸¬è©¦ççµæè®å¾ä¸å¯é ãå¦æä»»ä½æ¨¡åå·²å¨åºæºæ¸¬è©¦éä¸­é²è¡è¨ç·´ï¼åå¯è½æå´éé»ç¤è©²é åçç¼å±ãçºäºèªååä¸ææå°æ¸¬è©¦å¤§åèªè¨æ¨¡åçè½åï¼è¨±å¤ä¸»æµåºæºæ¡ç¨å¤é¸é¡æ ¼å¼ãç±æ¼å¤é¸é¡é¸é å§å®¹çäºæä¸å½±é¿åé¡æ¬èº«çå«ç¾©ï¼å æ­¤æåæåºäºä¸ç¨®åºæ¼æ­¤å±¬æ§çç°¡å®ä¸ææçæ°æ®æ´©æ¼æª¢æ¸¬æ¹æ³ãå·é«ä¾èªªï¼æåå°æ¸æä¸­é¸é çå§å®¹é¨æ©æåä»¥çæå°æçæ´¾çæ¸æéï¼ç¶å¾æ ¹ææ¨¡åå¨æ´¾çæ¸æéä¸çå°æ¸æ¦çåä½æª¢æ¸¬æ¸ææ´©æ¼ãå¦æå°æ¸æ¦çéä¸­å­å¨æå¤§å¼åç°å¸¸å¼ï¼åè¡¨ç¤ºæ¸æå·²æ´©æ¼ãæåçæ¹æ³è½å¤ å¨ä¸è¨ªåæ¨¡åè¨ç·´æ¸æææ¬éçé»çæ¢ä»¶ä¸å·¥ä½ï¼ææå°è­å¥æ¨¡åé è¨ç·´æ¸æä¸­åºæºæ¸¬è©¦éçæ¸ææ´©æ¼ï¼åæ¬é¸é å¯è½å·²æææç¡æå°è¢«æäºçæ­£å¸¸å ´æ¯åè¤éå ´æ¯ãééåºæ¼å©å LLM ååºæºè¨­è¨çå¯¦é©ï¼æåè­æäºæåæ¹æ³çæææ§ãæ­¤å¤ï¼æåè©ä¼°äº 31 åä¸»æµéæº LLM å¨åååºæºæ¸æéä¸çæ¸ææ´©æ¼ç¨åº¦ï¼ä¸¦å°æ¯ååºæºçæ´©æ¼ LLM é²è¡äºæåï¼æåç¼ç¾ Qwen å®¶æç LLM å·ææé«çæ¸ææ´©æ¼ç¨åº¦ã

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

æè¦ï¼æ¨å°æè½æ©å¨çå¥åº·ææ¨å°æ¼å¶ç¶­è­·è³ééè¦ãç¶èï¼éåéç¨å°æ®éæ¡ç¨çæºè½æ¹æ³ä¾èªªå·æææ°æ§ï¼å çºå®åå¯è½ææ¡ç¨æ´åè³æåä½ï¼ä¸åæå¼å¥éè¨å¹²æ¾ï¼èä¸ç¼ºä¹å¯è§£éæ§ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®åºæ¼æ´æ£çå¼±ç£ç£å¼æ¹æ³ï¼ç¨æ¼æ¨å°æè½æ©å¨çå¥åº·ææ¨ï¼å¯¦ç¾æ©ææéæª¢æ¸¬åçææ¼è®çæçºç£æ§ãéç¨®æ¹æ³ä¾è³´æ¼ä½¿ç¨å¥åº·æ¨£æ¬åä¸äºç°å¸¸å¼è¨ç·´çç¡åé¡å¨æ´æ£æ¨¡åãéåæ¨¡åæç¢çå¥åº·æ¨£æ¬ãä¸¦ä¸ééæ¯è¼å°å¥è­ä¸­åå§æ¨£æ¬åçææ¨£æ¬ä¹éçå·®ç°ï¼æåæ§å»ºäºä¸åç°å¸¸åï¼å¯ä»¥æ¸æ¥å°è­å¥æéãç¶å¾æ¨å°åºå¥åº·ææ¨ï¼å¯ä»¥è§£éæéé¡åä¸¦æ¸è¼éè¨å¹²æ¾ãå°å©åæ¡ä¾çæ¯è¼ç ç©¶è¡¨æï¼èåºæºæ¨¡åç¸æ¯ï¼ææåºçæ¹æ³æä¾äºåè¶çå¥åº·ç£æ§æææ§åé­¯æ£æ§ã

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

æè¦ï¼é»è¦è¼å©ççå­æ´»é¢¨éªé æ¸¬å¨çæ£çåææ²»çä¸­æ®æ¼èéè¦çè§è²ãéæ¯ä¸åå°é£çå¼±ç£ç£åºæ¸åæ­¸ä»»åï¼èå¤éè¨åºå ç´ æéï¼ä¾å¦ççååãåºå çµæ¸æç­ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çè¨ç·´æ¹æ³ï¼å¤æ¨¡æç©ä»¶å±¤ç´å°æ¯å­¸ç¿ï¼ç¨æ¼ççå­æ´»é¢¨éªé æ¸¬ãé¦åï¼æåæ ¹æè¨ç·´æ¨£æ¬éä¸­æ¨£æ¬ä¹éçå­æ´»é¢¨éªéä¿å»ºç«å°æ¯å­¸ç¿å°ãæ¥èï¼æåå¼å¥ç©ä»¶å±¤ç´å°æ¯å­¸ç¿æ¹æ³ä¾è¨ç·´å­æ´»é¢¨éªé æ¸¬å¨ãæåé²ä¸æ­¥å°å¶å»¶ä¼¸è³å¤æ¨¡æå ´æ¯ï¼ééæç¨è·¨æ¨¡æå°æ¯ãèéå°ççååååºå é«æ¸æçç°è³ªæ§ï¼æååå¥æ¡ç¨åºæ¼æ³¨æåçåèªæ¨æºåçç¥ç¶ç¶²è·¯ä¾å»ºæ§å¤æ¨¡æå­æ´»é¢¨éªé æ¸¬å¨ãæå¾ï¼æåæåºçæ¹æ³æè¨ç·´çå­æ´»é¢¨éªé æ¸¬å¨å¨å©åå¬éçå¤æ¨¡æççè³æéä¸ï¼å¨å­æ´»é¢¨éªé æ¸¬æ¹é¢åªæ¼æåé²çæ¹æ³ã

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana HernÃ¡ndez, Juan C. Caicedo, Gloria M. DÃ­az

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

æè¦ï¼å°æ¯å¢å¼·æ¨¡å¼åæå¨ä¹³æ¿ç£å±æ¯å½±å (MRI) ä¸­è³ééè¦ï¼å¯ç¨æ¼ååè¯æ§è«ç¤åå¯è½æ¯æ¡æ§è«ç¤ã
ç¶èï¼å°æ¯å¢å¼·å½±åçæ·åéå¸¸èæä¸æè²´ãä½çºç©çæ·åçæ¿ä»£æ¹æ¡ï¼æ¬ææåºäºä¸åå¨é¢çç®¡éï¼ç¨æ¼å¾æ©æå°æç©çææºç¢ºçé·æï¼ææï¼å°æ¯å¢å¼·ä¹³æ¿ MRIãææåºçç­ç¥èéæ¼å¨å¢å¼·ååä¸­ä¿çå°æ¯åæ¨¡å¼ï¼åæå¨æ´ååæå½±åä¸­ç¶­æè¦è¦ºå±¬æ§ãçºæ­¤ï¼æåºäºä¸ç¨®æ°ç©çæå¤±å½æ¸ï¼å©ç¨å°æ¯å (CA) å¨çµç¹ä¸­ççç©è¡çºï¼ç±æéå¼·åº¦ (TI) å¢å¼·æ²ç·çµ¦åºï¼ï¼ä»¥æä½³ååºæ¼åç´ æ³¨æåççææ¨¡åãæ­¤å¤ï¼èå³çµ±çæ­£è¦ååæ¨æºåæ¹æ³ä¸åï¼æåéç¼äºä¸ç¨®æ°çæ­£è¦åç­ç¥ï¼å¯å¨å¤åæéæ³çå½±ååºåä¸­ç¶­æå°æ¯å¢å¼·æ¨¡å¼ãéç¢ºä¿äºå½±ååèçå¾ CA æ¨¡å¼çæ®éæ§ï¼éèå³çµ±æ¹æ³ä¸åãæ­¤å¤ï¼çºäºå®¢è§è©ä¼°åæå½±åçè¨åºåè³ªï¼éå¼å¥äºå©åææ¨ä¾æ¸¬éæ·åååæå½±åçå¢å¼·ååç TI æ²ç·ä¹éçå·®ç°ãå¯¦é©çµæé¡¯ç¤ºï¼ææåºçç­ç¥ç¢ççå½±åå¨å°æ¯å¢å¼·ååä¸­çè¨ºæ·åè³ªæé¡¯åªæ¼å¶ä»å½±åï¼åæç¶­æäºæ´åå½±åçç©ºéç¹å¾µãéäºçµæè¡¨æï¼å¨è¨åºå ´æ¯ä¸­ï¼ééæ·±åº¦å­¸ç¿çæçåæææå¢å¼·å½±åå·ææ½å¨ç¨éã

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea StorÃ¥s, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, PÃ¥l Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

æè¦ï¼æåå¼é² Kvasir-VQAï¼ä¸åç± HyperKvasir å Kvasir-Instrument è³æéè¡ççå»¶ä¼¸è³æéï¼ä¸¦å å¥åé¡èè§£ç­è¨»è§£ï¼ä»¥ä¿é²å¨èè¸ (GI) è¨ºæ·ä¸­çé²éæ©å¨å­¸ç¿ä»»åãæ­¤è³æéåå« 6,500 åè¨»è§£å½±åï¼æ¶µèåç¨® GI éçæ³åæè¡å¨æ¢°ï¼ä¸¦ä¸æ¯æ´åæ¬æ¯éé¡ãé¸æé¡ãä½ç½®åæ¸å­è¨æ¸ç­å¤ç¨®é¡åçåé¡ãæ­¤è³æéé©ç¨æ¼å½±åæ¨é¡ãè¦è¦ºåç­ (VQA)ãåæé«å­¸å½±åçæå­çæãç©ä»¶åµæ¸¬ååé¡ç­æç¨ç¨å¼ãæåçå¯¦é©è­ææ­¤è³æéå¨è¨ç·´ä¸åé¸å®ä»»åçæ¨¡åä¸­å·æææï¼å±ç¤ºäºå¨é«å­¸å½±ååæåè¨ºæ·ä¸­éè¦çæç¨ãæåä¹çºæ¯åä»»åæä¾è©ä¼°ææ¨ï¼çªé¡¯æåè³æéçå¯ç¨æ§åå¤åè½æ§ãæ­¤è³æéåæ¯æ´å·¥ä»¶å¯æ¼ https://datasets.simula.no/kvasir-vqa åå¾ã

##### **EEG-Language Modeling for Pathology Detection**
2409.07480v1 by Sam Gijsen, Kerstin Ritter

Multimodal language modeling constitutes a recent breakthrough which
leverages advances in large language models to pretrain capable multimodal
models. The integration of natural language during pretraining has been shown
to significantly improve learned representations, particularly in computer
vision. However, the efficacy of multimodal language modeling in the realm of
functional brain data, specifically for advancing pathology detection, remains
unexplored. This study pioneers EEG-language models trained on clinical reports
and 15000 EEGs. We extend methods for multimodal alignment to this novel domain
and investigate which textual information in reports is useful for training
EEG-language models. Our results indicate that models learn richer
representations from being exposed to a variety of report segments, including
the patient's clinical history, description of the EEG, and the physician's
interpretation. Compared to models exposed to narrower clinical text
information, we find such models to retrieve EEGs based on clinical reports
(and vice versa) with substantially higher accuracy. Yet, this is only observed
when using a contrastive learning approach. Particularly in regimes with few
annotations, we observe that representations of EEG-language models can
significantly improve pathology detection compared to those of EEG-only models,
as demonstrated by both zero-shot classification and linear probes. In sum,
these results highlight the potential of integrating brain activity data with
clinical text, suggesting that EEG-language models represent significant
progress for clinical applications.

æè¦ï¼å¤æ¨¡æè¯­è¨å»ºæ¨¡æ¯ä¸é¡¹æè¿ççªç ´ï¼å®å©ç¨å¤§åè¯­è¨æ¨¡åçè¿æ­¥æ¥é¢è®­ç»æè½åçå¤æ¨¡ææ¨¡åãäºå®è¯æï¼å¨é¢è®­ç»æé´æ´åèªç¶è¯­è¨å¯ä»¥æ¾èæ¹åå­¦ä¹ å°çè¡¨å¾ï¼ç¹å«æ¯å¨è®¡ç®æºè§è§ä¸­ãç¶èï¼å¤æ¨¡æè¯­è¨å»ºæ¨¡å¨åè½æ§èæ°æ®é¢åä¸­çåæï¼ç¹å«æ¯å¯¹äºæ¨è¿ççæ£æµï¼ä»ç¶æªå¾å°æ¢ç´¢ãæ¬ç ç©¶å¼åäºå¨ä¸´åºæ¥åå 15000 ä¸ªèçµå¾ä¸è®­ç»çèçµå¾è¯­è¨æ¨¡åãæä»¬å°å¤æ¨¡æå¯¹é½çæ¹æ³æ©å±å°è¿ä¸ªæ°é¢åï¼å¹¶ç ç©¶æ¥åä¸­çåªäºææ¬ä¿¡æ¯å¯¹äºè®­ç»èçµå¾è¯­è¨æ¨¡åæ¯æç¨çãæä»¬çç»æè¡¨æï¼éè¿æ¥è§¦åç§æ¥åçæ®µï¼åæ¬æ£èççå²ãèçµå¾æè¿°åå»ççè§£éï¼ï¼æ¨¡åå¯ä»¥å­¦ä¹ å°æ´ä¸°å¯çè¡¨å¾ãä¸æ¥è§¦å°è¾çªçä¸´åºææ¬ä¿¡æ¯çæ¨¡åç¸æ¯ï¼æä»¬åç°æ­¤ç±»æ¨¡åå¯ä»¥æ ¹æ®ä¸´åºæ¥åï¼åä¹äº¦ç¶ï¼æ£ç´¢èçµå¾ï¼å¶åç¡®æ§å¤§å¤§æé«ãç¶èï¼åªæå¨ä½¿ç¨å¯¹æ¯å­¦ä¹ æ¹æ³æ¶æä¼è§å¯å°è¿ä¸ç¹ãç¹å«æ¯å¨æ³¨éè¾å°çæ¹æ¡ä¸­ï¼æä»¬è§å¯å°èçµå¾è¯­è¨æ¨¡åçè¡¨å¾å¯ä»¥æ¾èæ¹åççæ£æµï¼ä¸ä»èçµå¾æ¨¡åç¸æ¯ï¼é¶æ ·æ¬åç±»åçº¿æ§æ¢éé½è¯æäºè¿ä¸ç¹ãæ»ä¹ï¼è¿äºç»æçªåºäºå°èæ´»å¨æ°æ®ä¸ä¸´åºææ¬ç¸ç»åçæ½åï¼è¡¨æèçµå¾è¯­è¨æ¨¡åä»£è¡¨äºä¸´åºåºç¨çéå¤§è¿å±ã

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

æè¦ï¼é±å¼ç¥ç¶è¡¨å¾µ (INR) è¿æç±æ¼å¶ç¡éå¤§éè¨ç·´è³æéå°±è½å­¸ç¿è¨èçé£çºè¡¨å¾µçè½åï¼èæ¨åäºæ·±åº¦å­¸ç¿é åçé²å±ãåç®¡ INR æ¹æ³å·²è¢«ç ç©¶ç¨æ¼é«å­¸å½±åè¶è§£æåº¦ï¼ä½å¶å°æ¼é«å­¸å½±åä¸­å±é¨åé©çé©ææ§å°æªè¢«å»£æ³æ¢è¨ãé«å­¸å½±ååå«è±å¯çè§£åå­¸ååï¼éäºååå¯ä»¥æä¾æå¹å¼çå±é¨åé©è³è¨ï¼ä»¥å¢å¼· INR çæºç¢ºæ§åç©©å¥æ§ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸åæ°ç©çæ¶æ§ï¼ç¨±çºèªç¾©æ¢ä»¶ INR (SeCo-INR)ï¼å®ä½¿ç¨é«å­¸å½±åä¸­çå±é¨åé©ä¾èª¿æ´ INRï¼å¯¦ç¾æºç¢ºçæ¨¡åæ¬ååæå¼è½åï¼ä»¥å¯¦ç¾è¶è§£æåº¦ãæåçæ¶æ§å­¸ç¿é«å­¸å½±åçèªæåå²ç¹å¾µçé£çºè¡¨å¾µï¼ä¸¦å©ç¨å®çºå½±åçæ¯åèªæååæ¨å°æä½³ INRãæåä½¿ç¨å¤ç¨®é«å­¸å½±åæ¹å¼æ¸¬è©¦æåçæ¶æ§ï¼ä¸¦èæåé²çæ¹æ³ç¸æ¯ï¼éå°äºæ´é«çéåè©ååæ´é¼ççè¶è§£æåº¦è¼¸åºã

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

æè¦ï¼<paragraph>å¨ç®è¤çå­¦é¢åï¼äººå·¥è¯æ­ç®è¤ç¶åµçå¤ææ§éè¦ç®è¤ç§å»å¸çä¸ä¸ç¥è¯ãä»ççå°ççæ§ç¾çï¼å¯¹åç§ç®è¤ç¾ççåç¡®è¯å«è³å³éè¦ãç¶èï¼ç°æçç®è¤çå­¦äººå·¥æºè½ (AI) æ¨¡åé¢ä¸´ææï¼å°¤å¶æ¯å¨åç¡®è¯æ­ä¸åè¤è²çç¾çæ¶ï¼å¨è¾æ·±çè¤è²ä¸å­å¨ææ¾çæ§è½å·®è·ãæ­¤å¤ï¼å¬å¼å¯ç¨çæ åæ°æ®éçç¨ç¼ºæ§é»ç¢äºåå®¹æ§ AI è¯æ­å·¥å·çå¼åãä¸ºäºåºå¯¹åç¡®é¢æµä¸åè¤è²ç®è¤ç¶åµçææï¼æä»¬éç¨äºä¸ç§è¿ç§»å­¦ä¹ æ¹æ³ï¼è¯¥æ¹æ³å©ç¨äºæ¥èªåç§å¾ååçä¸°å¯å¯è½¬ç§»ç¥è¯ãæä»¬çæ¹æ³éæäºæ¥èªå¹¿æ³æ¥æºçå¤ä¸ªé¢è®­ç»æ¨¡åï¼åæ¬ä¸è¬åç¹å®çå»å­¦å¾åï¼ä»¥æé«ç®è¤ç¶åµé¢æµçç¨³å¥æ§ååå®¹æ§ãæä»¬ä½¿ç¨ Diverse Dermatology Images (DDI) æ°æ®éä¸¥æ ¼è¯ä¼°äºè¿äºæ¨¡åçæææ§ï¼è¯¥æ°æ®éç¬ç¹å°åå«äºä»£è¡¨æ§ä¸è¶³åå¸¸è§çè¤è²ï¼ä½¿å¶æä¸ºè¯ä¼°æä»¬æ¹æ³ççæ³åºåãå¨æææ¹æ³ä¸­ï¼Med-ViT ç±äºå¶ä»åç§å¾åæ¥æºä¸­å­¦å°çç»¼åç¹å¾è¡¨ç¤ºèæä¸ºè¡¨ç°æå¥½çæ¹æ³ãä¸ºäºè¿ä¸æ­¥æé«æ§è½ï¼æä»¬ä½¿ç¨ HAM10000 ç­å¶ä»ç®è¤å¾åæ°æ®éè¿è¡äºåéåºãè¿ç§éåºæ¾çæé«äºæææ¨¡åçæ¨¡åæ§è½ã</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç¶å¸¸ç¼ºä¹ç¹å®é åçç¥è­ï¼å³ä½¿ç¶éå¾®èª¿çæ¨¡åä¹å®¹æç¢çå¹»è¦ºãå æ­¤ï¼éè¦æ´å¤å¯é çæ¨¡åä¾ç´å¥å¤é¨ç¥è­ãæåæåºäºä¸åæµç¨ 4StepFocusï¼ç¹å¥æ¯é èçæ­¥é©ï¼å¯ä»¥å¤§å¹æ¹å LLM çç­æ¡ãéæ¯ééæä¾åå¼å°çå¤é¨ç¥è­å­åï¼å©ç¨æ¨¡åèªè¡æ·åéè¯æ§èçµ¡åé²è¡åºæ¬æ¨ççè½åä¾å¯¦ç¾çãæ­¤æ¹æ³ééå¨åçµæ§åç¥è­åº«ä¸­é²è¡åºæ¼ä¸åçµçæå°ï¼ä»¥ç´æ¥ä¸å¯è¿½è¹¤çæ¹å¼ç¸®å°æ½å¨æ­£ç¢ºç­æ¡çç¯åï¼ç¶å¾ååæå°æ½å¨è¡¨å¾µï¼æ ¹æéçµæ§åè³æå°éäºåé¸ç­æ¡é²è¡æåãéèç´ç²¹åºæ¼æ½å¨è¡¨å¾µçç¸éæ¹æ³ææåå¥ã4StepFocus åå«ä»¥ä¸æ­¥é©ï¼1) ç± LLM é²è¡ä¸åçµç¢çä»¥æ·åéè¯è³æï¼2) å¨éäºä¸åçµä¸­æ¿æè®æ¸ï¼ä»¥æ¡ç¨ç¥è­åè¡¨ç¸®å°ç­æ¡åé¸ç¯åï¼3) ä½¿ç¨æ¶åéè¯éçµæ§åè³æçåéç¸ä¼¼æ§æå°å°å©é¤åé¸ç­æ¡é²è¡æåºï¼4) ç± LLM éæ°å°æä½³åé¸ç­æ¡é²è¡æåï¼ä¸¦æä¾èæ¯è³æãå¨é«çãç¢åæ¨è¦åå­¸è¡è«ææå°æ¸¬è©¦éä¸­é²è¡çå¯¦é©è­æï¼éç¨®æ¹æ³ç¢ºå¯¦æ¯ä¸ç¨®å¼·å¤§çæ´åãå®ä¸åå¢å äºä¾èªè³è¨æª¢ç´¢çç¸å³å¯è¿½è¹¤èæ¯è³è¨ï¼èä¸èæåé²çæ¹æ³ç¸æ¯ï¼ä¹å¤§å¹æåäºæè½ãæ¬ææåºäºä¸åæ°ç©ä¸é®®å°æ¢ç´¢çæ¹åï¼å æ­¤æä¾äºå»£æ³çæªä¾å·¥ä½æ©æãä½¿ç¨çåå§ç¢¼å¯å¨ https://github.com/kramerlab/4StepFocus åå¾ã

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

æè¦ï¼æ¬ææåºäºä¸åç¥è­å·¥ç¨åå¤èªè¨èªç¾©æ¨çæè¡çæ¬ä½è¨­è¨ï¼ç¨æ¼å»ºç«ä¸åèªååç³»çµ±ï¼ä»¥ç¥è­åè­çå½¢å¼å¸æ¶å°åº¦æççç¹é£ªè³è¨ãéé»å¨æ¼è¨­è¨æºæ§æ¹æ³ï¼ä»¥æ¨å°æ¬ä½è¨­è¨ï¼ä¸¦å¨é¢æ·åéæ¼é£ç©ãé£è­ãé£æãç¹é£ªç¹æ§ï¼ä»¥åæéè¦ççé¤çç¥è­ï¼ä¸¦æ´å¤§è¦æ¨¡ãæåå¨éåç è¨æè«æä¸­ä»ç´¹äºæåæ­£å¨é²è¡çå·¥ä½ï¼è©³ç´°æè¿°äºæ´çå°åº¦æçç¥è­ç¸éçææ°ï¼ä¸¦æåºäºæåçé«éæ¬ä½è¨­è¨ãæåä¹æåºäºä¸ç¨®æ°çå·¥ä½æµç¨ï¼å®ä½¿ç¨ AIãLLM åèªè¨æè¡ï¼å¾å¬å±é åçé£è­é¨è½æ ¼ç¶²ç«ä¸­æ´çè³è¨ï¼ä»¥å»ºç«å°åº¦æççç¥è­åè­ãæ¬ææåºçç¥è­æ´çæ¹æ³æ¯éç¨çï¼å¯ä»¥è¤è£½å°ä»»ä½é åãè¨­è¨èæç¨ç¡éï¼å¯ç¨æ¼ AI é©åçæºæ§åæãå»ºç«åäººåæ¸ä½å¥åº·æ¨è¦ç³»çµ±ï¼ä»¥åä½¿ç¨ä½¿ç¨èè³è¨ãé£ç©çç©åå­¸ãå°çè³è¨ãè¾²æ¥­è³è¨ç­èçµ¡è³è¨ï¼ä¾è£åå°åº¦æççç¥è­åè­ã

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

æè¦ï¼è¾²æ¥­çç¢éè¦å°å¿ç®¡çè¼¸å¥ï¼ä¾å¦æ®ºèåãæ®ºè²ååé¤èåï¼ä»¥ç¢ºä¿ä½ç©æåãé«ç¢ãæå©å¯åä¸å·æåªè¯çç¨®å­åè³ªãç®åæåé²çç°éä½ç©ç®¡çä¾è³´æ¼ç²ç¥çä½ç©ç®¡çç­ç¥ï¼å¶ä¸­æ´åç°å°é½å´çäºæ§å¶çè²å®³çåå­¸ç©è³ªï¼å°è´ææ¬å¢å ååå£¤åä½ç©ç®¡çä¸ä½³ãçºäºåæéäºææ°ä¸¦åªåä½ç©çç¢ï¼æåå¨èæ¬ç°éç°å¢ä¸­å©ç¨æ©å¨å­¸ç¿å·¥å·çºè¾²æ°çæå±é¨ç®¡çè¨ç«ï¼ä»¥ç®¡ççç©å¨èä¸¦åææå¤§åå©æ½¤ãå·é«ä¾èªªï¼æåæåºäº AgGymï¼ä¸åæ¨¡çµåãä½ç©åå£åä¸å¯ç¥çæ¨¡æ¬æ¶æ§ï¼ç¨æ¼æ¨¡æ¬ç°éçç©å£åçæ´æ£ï¼ä¸¦ä¼°ç®æåæ²æåå­¸èççç¢éæå¤±ãæåä½¿ç¨çå¯¦æ¸æé²è¡é©è­ï¼é¡¯ç¤º AgGym å¯ä»¥ä½¿ç¨æéçæ¸æé²è¡èªè¨ï¼ä»¥æ¨¡æ¬åç¨®çç©å£åæ¢ä»¶ä¸çç¢éçµæãæåé²ä¸æ­¥è­æï¼æ·±åº¦å¼·åå­¸ç¿ (RL) æ¿ç­å¯ä»¥ä½¿ç¨ AgGym é²è¡è¨ç·´ï¼ä»¥è¨­è¨è¶ç²¾ç¢ºççç©å£åç·©è§£ç­ç¥ï¼ä¸¦æå¯è½ä»¥æ´å°çåå­¸ç©è³ªåæ´ä½çææ¬å¢å ç¢éæ¢å¾©ãæåæåºçæ¶æ§åç¨äºåäººåæ±ºç­æ¯æ´ï¼å¯ä»¥å°çç©å£åç®¡çå¾åºæ¼æéè¡¨åè¢«åè½è®çºæ©æä¸»ç¾©åè¦ç¯æ§ãæåéå° AgGym è»é«å¯¦ä½ä½çºç¤¾åè³æºéåºï¼ä¸¦éè«å°å®¶çºéåéæ¾åå§ç¢¼ä¸å¬éå¯ç¨çæ¨¡çµåç°å¢æ¶æ§ååºè²¢ç»ãå¯ä»¥å¨ä»¥ä¸ä½ç½®åå¾åå§ç¢¼ï¼https://github.com/SCSLabISU/AgGymã

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

æè¦ï¼è¶å»£è§è¢åè¡ç®¡é å½±ï¼UWF-FAï¼ä½¿ç¨å¯è½å·ææ½å¨å±å®³çéè¢åç´ ï¼å¯ç²¾ç¢ºè­å¥ç¼ç¾ãç¾æç ç©¶å·²éç¼åºå¾è¶å»£è§ææé·å°ç¼ç§é¡ï¼UWF-SLOï¼ç¢ç UWF-FA çæ¹æ³ï¼ä»¥æ¸å°èæ³¨å°ç¸éçä¸è¯åæãç¶èï¼éäºæ¹æ³å¨ç¢çé«åè³ªçå¾æ UWF-FA æ¹é¢ææè¼å·®ï¼ç¹å¥æ¯å¨çç¶åååç²¾ç´°ç´°ç¯æ¹é¢ãç¢çé«åè³ªå¾æ UWF-FA é¢è¨å©é ä¸»è¦ææ°ï¼éå°ç UWF-SLO åæ©æ/å¾æ UWF-FA è³æéç¨å°ï¼ä»¥åéè¦å¨çç¶é¨ä½åæ½å¨åºè¡ååé²è¡é¼ççç¢çãæ¬ç ç©¶å¼é²ä¸ç¨®æ¹è¯çæ½å¨æ´æ£æ¨¡åæ¶æ§ï¼å¾æééå°ç UWF å½±åç¢çé«åè³ªçå¾æ UWF-FAãçºäºæå°åé¢æå°çææ°ï¼æåçæ¹æ³æ¡ç¨ä¸åæ¨¡çµï¼å©ç¨è·¨æéååå·®ç°æå¤±ï¼é¼åµæ¨¡åå°æ³¨æ¼æ©æåå¾æä¹éçå·®ç°ãæ­¤å¤ï¼æåå¨æ´æ£ååéç¨ä¸­å¼é²ä¸ç¨®ä½é »å¢å¼·éè¨ç­ç¥ï¼ä»¥æ¹åé«å­¸å½±åççå¯¦æ§ãçºäºé²ä¸æ­¥å¢å¼·è®ç°èªåç·¨ç¢¼å¨æ¨¡çµçå°æè½åï¼ç¹å¥æ¯å¨è³æéæéçææ³ä¸ï¼æåå¯¦ä½ä¸åéæ§å·ç©ç·¨ç¢¼å¨ï¼å¾æ¢ä»¶å½±åä¸­èåé¡å¤è³è¨ãæåéå°è¶å»£è§å¾æè¢åè¡ç®¡é å½±ï¼LPUWF-LDMï¼çæ½å¨æ´æ£æ¨¡åææéå»ºå¾æ UWF-FA ä¸­çç²¾ç´°ç´°ç¯ï¼ä¸¦å¨ä½¿ç¨æéè³æéæï¼èå¶ä»ç¾ææ¹æ³ç¸æ¯ï¼éå°æåé²ççµæãæåçåå§ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼
https://github.com/Tinysqua/****ã

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

æè¦ï¼å¿èè½è¨ºæ¯è¨ºæ·å¿è¡ç®¡ç¾ç (CVD) çä¸é æ´åå·¥å·ï¼éå¸¸ä¾è³´æ¼è¨åºé«å¸«çä¸»è§è©®éï¼å¨ä¸è´æ§åæºç¢ºæ§æ¹é¢å­å¨éå¶ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº BUET å¤éç¾çå¿é³ (BMD-HS) è³æéï¼éæ¯ä¸åå¨é¢ä¸ç¶éç²¾å¿ç­åçå¿é³éé³è³æéãæ­¤è³æéåå«äºç¨®å¸¸è¦å¿é³ç 864 åéé³ï¼ä»£è¡¨äºå»£æ³çå¿ç£èç¾çï¼éé»å¨æ¼è¨ºæ·å°é£ççä¾ãBMD-HS è³æéççªåºç¹é»æ¯å¶åµæ°çå¤æ¨ç±¤è¨»è§£ç³»çµ±ï¼å®æ¶µèäºåç¨®ç¾çåç¨ç¹çç¾ççæãéåç³»çµ±é¡¯èå¢å¼·äºè³æéå¨éç¼èªåå¿é³åé¡åè¨ºæ·ä¸­é²éæ©å¨å­¸ç¿æ¨¡åçæç¨ãééå½åå³çµ±è½è¨ºå¯¦åèç¶ä»£è³æé©åè¨ºæ·æ¹æ³ä¹éçå·®è·ï¼BMD-HS è³æéæºåå¥½é©æ°å¿è¡ç®¡ç¾ççè¨ºæ·åç®¡çï¼çºå¿èå¥åº·ç ç©¶çé²å±æä¾å¯¶è²´çè³æºãæ­¤è³æéå¯ééä»¥ä¸é£çµå¬éåå¾ï¼https://github.com/mHealthBuet/BMD-HS-Datasetã

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

æè¦ï¼èªåè¨ºæ·æè¡å·²æ¼é²å°è½ééä½¿ç¨å®ä¸æ¨¡å¼ç¼åºå½±åæåå­¸ç¸å¹²æ·å±¤ææ (OCT) ä¾è¾¨è­å¹´é½¡ç¸éæ§é»æé¨çè® (AMD)ãçºäºåé¡ç¼ç¾ï¼ç¼åºå OCT å½±åæ¯è¨åºç°å¢ä¸­ä½¿ç¨æééµçå½±åæ¨¡å¼ãå¤§å¤æ¸åºæ¼æ·±åº¦å­¸ç¿çæè¡å»ºç«å¨å®ä¸å½±åæ¨¡å¼ä¸ï¼å®å¨ä¸å®ç¨åº¦ä¸èéäºç¼ç¾ï¼å»å¿½ç¥äºå¶ä»æ¨¡å¼ï¼èå¶ä»æ¨¡å¼åå«äºä¸åå½±åæ¨¡å¼ä¹éçè©³ç¡è³è¨ãæ¬ææåºäºä¸ç¨®æ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéåµå¥æ´åï¼ä¸¦åºæ¼ç¨æ¼åé¡çè½ç§»å­¸ç¿çæ³¨æåæ©å¶ (MCGAEc)ï¼å®è½ä½¿ç¨ä¸åçè²å½©ç©ºéå¨ä¸åçå°ºåº¦ä¸æææåä¸åçæ¨¡å¼è³è¨ãå¨éé å·¥ä½ä¸­ï¼æåé¦åä»ç´¹äºæ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéç·¨ç¢¼å¨æ¨¡åï¼å®ééå°ä¸åçç¹å¾µè²å½©ç©ºéæ´åå°å¤å°ºåº¦ä¸­ï¼ä¾ç´å¥ä¸åçç¹å¾µè¡¨å¾µå°ä¸åçµ±ä¸çæ¶æ§ä¸­ãå¾ååçç·¨ç¢¼å¨æ¨¡çµä¸­æåçç¹å¾µèæ³¨æåæ©å¶çµåï¼ä»¥æåå¨åç¹å¾µè¡¨å¾µï¼å®èååæåçç¹å¾µæ´åï¼ä¸¦è½ç§»å°é¨æ©æ£®æåé¡å¨ï¼ä»¥é²è¡ AMD åé¡ãçºäºåæææåºç MCGAEc æ¹æ³çæè½ï¼æåå©ç¨äºä¾èª Project Macula for AMD çå¬éå¤æ¨¡å¼è³æéï¼ä¸¦èç¾ææ¨¡åé²è¡æ¯è¼ã

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

æè¦ï¼èª¿æ´å¤§åé è¨ç·´åºç¤æ¨¡åï¼ä¾å¦ SAMï¼ä»¥é²è¡é«å­¸å½±ååå²ä»æ¯ä¸é éå¤§ææ°ãééµæ­¥é©æ¶åå¶å®ä¸ç³»ååå«ç¹å®è¨åºèªªæçå°éæç¤ºãéå»çå·¥ä½å¨å¾å¤§ç¨åº¦ä¸ä¾è³´æ¼æ¯åä¾é çå®ä¸æç¤ºé¡åï¼ééè¦æåè¼¸å¥çæ³çæ­£ç¢ºæç¤ºï¼æçè¼ä½ãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°å©ç¨ä¸åç²åº¦çæç¤ºï¼éäºæç¤ºä¾èªåå§å½±åï¼ä»¥æä¾æ´å»£æ³çè¨åºè¦è§£ãç¶èï¼ç±æ¼æ½å¨è¡çªï¼çµåä¸åé¡åçæç¤ºå¯è½ææ§æææ°ãçºäºè§£æ±ºéååé¡ï¼æåè¨­è¨äºä¸ç¨®ç±ç²å°ç´°çæ©å¶ï¼ç¨±çºèª²ç¨æç¤ºï¼å®éæ­¥æ´åä¸åé¡åçæç¤ºãééå°åç¨®æ¨¡å¼ä¸çä¸åå¬å±é«å­¸è³æéé²è¡å»£æ³çå¯¦é©ï¼æåè­æäºæåæåºçæ¹æ³çæææ§ï¼å®ä¸åèªååæç¤ºçæéç¨ï¼èä¸èå¶ä»åºæ¼ SAM çé«å­¸å½±ååå²æ¹æ³ç¸æ¯ï¼éç¢çäºæ´å¥½çæè½ãç¨å¼ç¢¼å¯å¨ä»¥ä¸ä½ç½®åå¾ï¼https://github.com/AnnaZzz-zxq/Curriculum-Promptingã

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

æè¦ï¼ç½è¦å©¦ç§è«ç¤ (RGT) ç±æ¼å¶ä½ç¼ççåç°è³ªæ§ï¼å°è¨åºå¸¶ä¾éå¤§ææ°ãç¼ºä¹æç¢ºçæå¼å°è´æ¬¡ä½³ç®¡çåä¸è¯é å¾ãåå­è«ç¤å§å¡æééæ ¹æçç©æ¨è¨å®¢è£½åæ²»çï¼å éåå¾ææçæ³ï¼è¶è¶ççé¡åãéè¦æåæ´ççéçµæ§åè³æé»ç¤äºçç©æ¨è¨åæå¨çæ³éå°ä¸­çææä½¿ç¨ãæ¬ç ç©¶æ¢è¨ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çº RGT çç²¾æºé«çå»ºæ§æ¸ä½éèèã
æåçæ¦å¿µé©è­æ¸ä½éèèç³»çµ±æ´åäºä¾èªæ©æ§åå·²ç¼è¡¨çæ¡ä¾ (n=21) çè¨åºåçç©æ¨è¨è³æï¼ä»¥åä¾èªæç»çè³æ (n=655 ç¯åºçç©ï¼n=404,265 åæ£è)ï¼çºè½ç§»æ§å­å®®èç¤çå¶å®å®¢è£½åæ²»çè¨ç«ï¼æ¾åºå³çµ±å®ä¸ä¾æºåæå¯è½éºæ¼çé¸é ãLLM åç¨çæ¸ä½éèèææå°æ¨¡æ¬åå¥æ£èçè»è·¡ãå¾åºæ¼å¨å®çè«ç¤å®ç¾©è½è®çºåºæ¼çç©å­¸çå®ç¾©ï¼è½å¯¦ç¾åäººåç§è­·ï¼é²èæå RGT ç®¡çä¸¦æ¹åæ£èé å¾ã

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

æè¦ï¼èªé³åæé¬±æª¢æ¸¬å°èªååæª¢æ¸¬ä¾èªªæ¯ä¸å¤§ææ°ï¼å çºå®å¨ä¸ååé«éçè¡¨ç¾ç¨ç¹ï¼ä¸è³æç¨å°ãçºäºæå°éäºææ°ï¼æåå¼å¥äº DAAMAudioCNNLSTM å DAAMAudioTransformerï¼éå©ååæ¸ææä¸å¯è§£éçæ¨¡åï¼ç¨æ¼é³è¨ç¹å¾µèååæé¬±æª¢æ¸¬ãDAAMAudioCNNLSTM æ¡ç¨åµæ°ç CNN-LSTM æ¶æ§ï¼æ­éå¤é ­å¯åº¦èªé©ææ³¨æåæ©å¶ (DAAM)ï¼åæéæ³¨æ¼ææç¾©çèªé³åæ®µãDAAMAudioTransformer å©ç¨Transformerç·¨ç¢¼å¨åä»£ CNN-LSTM æ¶æ§ï¼ä¸¦ç´å¥ç¸åç DAAM æ¨¡çµï¼ä»¥å¢å¼·æ³¨æååå¯è§£éæ§ãéäºæ¹æ³ä¸åå¢å¼·äºæª¢æ¸¬çç©©å¥æ§åå¯è§£éæ§ï¼ééå°äºæåé²çæè½ï¼DAAMAudioCNNLSTM ç F1 å·¨è§åæ¸çº 0.702ï¼DAAMAudioTransformer å¨ DAIC-WOZ è³æéä¸ç F1 å·¨è§åæ¸çº 0.72ï¼å¨è¨ç·´/é©è­æéä¸ä¾è³´æ¼è¼å©è³è¨ï¼ä¾å¦æ¯é³ä½ç½®åèªªè©±èè³è¨ï¼éèååçåæ³ä¸åãéå©åæ¨¡åå¨å©ç¨èªé³è¨èé²è¡æé¬±æª¢æ¸¬æ¹é¢å·æé¡¯èçå¯è§£éæ§åæçï¼ä»£è¡¨èæåæ´å¯é ãè¨åºä¸æç¨çè¨ºæ·å·¥å·éé²äºä¸å¤§æ­¥ï¼ä¸¦é ç¤ºèèªé³åå¿çä¿å¥çé²æ­¥ãçºäºä¿é²éåé åçé²ä¸æ­¥ç ç©¶ï¼æåå¬éäºæåçç¨å¼ç¢¼ã

