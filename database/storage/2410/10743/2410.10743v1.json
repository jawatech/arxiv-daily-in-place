{"2410.10743": {"publish_time": "2024-10-14", "title": "NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models", "paper_summary": "Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks.", "paper_summary_zh": "\u5716\u5f62\u662f\u4e00\u7a2e\u57fa\u672c\u8cc7\u6599\u7d50\u69cb\uff0c\u7528\u65bc\u8868\u793a\u73fe\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u7684\u95dc\u4fc2\u3002\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u7684\u6210\u529f\uff0c\u6574\u5408 LLM \u4ee5\u9032\u884c\u5716\u5f62\u5b78\u7fd2\u7684\u8208\u8da3\u65e5\u76ca\u6fc3\u539a\u3002\u7136\u800c\uff0c\u5c07 LLM \u61c9\u7528\u65bc\u8207\u5716\u5f62\u76f8\u95dc\u7684\u4efb\u52d9\u6703\u5e36\u4f86\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u9019\u4e9b\u6a21\u578b\u4e26\u975e\u5929\u751f\u5c31\u8a2d\u8a08\u6210\u7528\u4f86\u64f7\u53d6\u5716\u5f62\u4e2d\u5b58\u5728\u7684\u8907\u96dc\u7d50\u69cb\u8cc7\u8a0a\u3002\u73fe\u6709\u65b9\u6cd5\u900f\u904e\u5169\u7a2e\u7b56\u7565\u4f86\u61c9\u5c0d\u6b64\u6311\u6230\uff1a\u4efb\u52d9\u93c8\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u7de8\u78bc\u5716\u5f62\u7d50\u69cb\uff0c\u4ee5\u4fbf\u6e1b\u8f15 LLM \u7406\u89e3\u7a7a\u9593\u4f4d\u7f6e\u7684\u8ca0\u64d4\uff1b\u4ee5\u53ca\u5716\u5f62\u8f49\u6587\u5b57\u8f49\u63db\uff0c\u5b83\u5c07\u5716\u5f62\u7d50\u69cb\u8f49\u63db\u6210 LLM \u53ef\u4ee5\u8655\u7406\u7684\u8a9e\u610f\u6587\u5b57\u8868\u793a\u3002\u5118\u7ba1\u9019\u4e9b\u65b9\u6cd5\u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u96e3\u4ee5\u5b8c\u5168\u4fdd\u7559\u5716\u5f62\u7684\u62d3\u64b2\u8cc7\u8a0a\uff0c\u6216\u8005\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u9650\u5236\u4e86\u5b83\u5011\u7684\u5be6\u969b\u61c9\u7528\u6027\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7bc0\u9ede\u6a19\u8a18\u5668 (NT-LLM)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5b83\u900f\u904e\u9078\u64c7\u95dc\u9375\u7bc0\u9ede\u4f5c\u70ba\u9328\u9ede\uff0c\u4e26\u6839\u64da\u6bcf\u500b\u7bc0\u9ede\u8207\u9019\u4e9b\u9328\u9ede\u7684\u76f8\u5c0d\u8ddd\u96e2\u4f86\u8868\u793a\u6bcf\u500b\u7bc0\u9ede\uff0c\u5f9e\u800c\u6709\u6548\u5730\u7de8\u78bc\u5716\u5f62\u7d50\u69cb\u3002\u9019\u7a2e\u57fa\u65bc\u4f4d\u7f6e\u7684\u9328\u9ede\u7de8\u78bc\u6709\u6548\u5730\u64f7\u53d6\u4e86\u5716\u5f62\u62d3\u64b2\uff0c\u8b93 LLM \u80fd\u5920\u5c0d\u5716\u5f62\u8cc7\u6599\u9032\u884c\u589e\u5f37\u7684\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u500b\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u8abf\u6574\u7a0b\u5e8f\uff0c\u4ee5\u9032\u4e00\u6b65\u6539\u5584 LLM \u4e2d\u7684\u7d50\u69cb\u7406\u89e3\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u8b49\u8a55\u4f30\uff0cNT-LLM \u5728\u5404\u7a2e\u8207\u5716\u5f62\u76f8\u95dc\u7684\u4efb\u52d9\u4e2d\u90fd\u5c55\u793a\u51fa\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002", "author": "Yanbiao Ji et.al.", "authors": "Yanbiao Ji, Chang Liu, Xin Chen, Yue Ding, Dan Luo, Mei Li, Wenqing Lin, Hongtao Lu", "id": "2410.10743v1", "paper_url": "http://arxiv.org/abs/2410.10743v1", "repo": "null"}}