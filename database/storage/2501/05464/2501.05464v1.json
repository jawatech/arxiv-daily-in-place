{"2501.05464": {"publish_time": "2024-12-31", "title": "LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models", "paper_summary": "Accurate and efficient question-answering systems are essential for\ndelivering high-quality patient care in the medical field. While Large Language\nModels (LLMs) have made remarkable strides across various domains, they\ncontinue to face significant challenges in medical question answering,\nparticularly in understanding domain-specific terminologies and performing\ncomplex reasoning. These limitations undermine their effectiveness in critical\nmedical applications. To address these issues, we propose a novel approach\nincorporating similar case generation within a multi-agent medical\nquestion-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B\nmodel, a state-of-the-art LLM, in a multi-agent architecture to enhance\nperformance on the MedQA dataset using zero-shot learning. Our method\ncapitalizes on the model's inherent medical knowledge and reasoning\ncapabilities, eliminating the need for additional training data. Experimental\nresults show substantial performance gains over existing benchmark models, with\nimprovements of 7% in both accuracy and F1-score across various medical QA\ntasks. Furthermore, we examine the model's interpretability and reliability in\naddressing complex medical queries. This research not only offers a robust\nsolution for medical question answering but also establishes a foundation for\nbroader applications of LLMs in the medical domain.", "paper_summary_zh": "\u7cbe\u6e96\u9ad8\u6548\u7684\u554f\u984c\u89e3\u7b54\u7cfb\u7d71\u5c0d\u65bc\u63d0\u4f9b\u91ab\u7642\u9818\u57df\u7684\u9ad8\u54c1\u8cea\u75c5\u4eba\u7167\u8b77\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u500b\u9818\u57df\u90fd\u6709\u986f\u8457\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u5728\u91ab\u7642\u554f\u984c\u89e3\u7b54\u4e2d\u4ecd\u9762\u81e8\u91cd\u5927\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u7406\u89e3\u7279\u5b9a\u9818\u57df\u7684\u8853\u8a9e\u548c\u57f7\u884c\u8907\u96dc\u63a8\u7406\u65b9\u9762\u3002\u9019\u4e9b\u9650\u5236\u5f71\u97ff\u4e86\u5b83\u5011\u5728\u95dc\u9375\u91ab\u7642\u61c9\u7528\u4e2d\u7684\u6548\u80fd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u5c07\u985e\u4f3c\u6848\u4f8b\u751f\u6210\u6574\u5408\u5230\u591a\u4e3b\u9ad4\u91ab\u7642\u554f\u984c\u89e3\u7b54 (MedQA) \u7cfb\u7d71\u4e2d\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u591a\u4e3b\u9ad4\u67b6\u69cb\u4e2d\u5229\u7528\u6700\u5148\u9032\u7684 LLM Llama3.1:70B \u6a21\u578b\uff0c\u4ee5\u4f7f\u7528\u96f6\u6b21\u5b78\u7fd2\u4f86\u589e\u5f37 MedQA \u8cc7\u6599\u96c6\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e86\u8a72\u6a21\u578b\u5167\u5efa\u7684\u91ab\u7642\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\uff0c\u6d88\u9664\u4e86\u5c0d\u984d\u5916\u8a13\u7df4\u8cc7\u6599\u7684\u9700\u6c42\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u8207\u73fe\u6709\u7684\u57fa\u6e96\u6a21\u578b\u76f8\u6bd4\uff0c\u6548\u80fd\u6709\u986f\u8457\u63d0\u5347\uff0c\u5728\u5404\u7a2e\u91ab\u7642\u554f\u7b54\u4efb\u52d9\u4e2d\uff0c\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u90fd\u63d0\u5347\u4e86 7%\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u8a72\u6a21\u578b\u5728\u56de\u7b54\u8907\u96dc\u91ab\u7642\u554f\u984c\u6642\u7684\u8a6e\u91cb\u6027\u548c\u53ef\u9760\u6027\u3002\u9019\u9805\u7814\u7a76\u4e0d\u50c5\u70ba\u91ab\u7642\u554f\u984c\u89e3\u7b54\u63d0\u4f9b\u4e86\u5f37\u5065\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4e5f\u70ba LLM \u5728\u91ab\u7642\u9818\u57df\u7684\u66f4\u5ee3\u6cdb\u61c9\u7528\u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Hang Yang et.al.", "authors": "Hang Yang, Hao Chen, Hui Guo, Yineng Chen, Ching-Sheng Lin, Shu Hu, Jinrong Hu, Xi Wu, Xin Wang", "id": "2501.05464v1", "paper_url": "http://arxiv.org/abs/2501.05464v1", "repo": "null"}}