{"2503.15470": {"publish_time": "2025-03-19", "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining", "paper_summary": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.", "paper_summary_zh": "<paragraph>\u4ee5\u81ea\u6211\u70ba\u4e2d\u5fc3\u7684\u8996\u8a0a-\u8a9e\u8a00\u9810\u8a13\u7df4\u986f\u8457\u63d0\u5347\u4e86\u8996\u8a0a\u8868\u5fb5\u5b78\u7fd2\u3002\u4eba\u985e\u611f\u77e5\u4e26\u8207\u4e00\u500b\u5b8c\u6574\u7684 3D \u4e16\u754c\u4e92\u52d5\uff0c\u767c\u5c55\u51fa\u8d85\u8d8a\u57fa\u65bc\u6587\u5b57\u7406\u89e3\u7684\u7a7a\u9593\u610f\u8b58\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u5148\u524d\u7684\u5de5\u4f5c\u90fd\u662f\u5f9e 1D \u6587\u5b57\u6216 2D \u8996\u89ba\u7dda\u7d22\uff08\u4f8b\u5982\u908a\u754c\u6846\uff09\u4e2d\u5b78\u7fd2\uff0c\u9019\u4e9b\u7dda\u7d22\u672c\u8eab\u7f3a\u4e4f 3D \u7406\u89e3\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 EgoDTM\uff0c\u4e00\u500b\u4ee5\u81ea\u6211\u70ba\u4e2d\u5fc3\u3001\u6df1\u5ea6\u548c\u6587\u5b57\u611f\u77e5\u7684\u6a21\u578b\uff0c\u5b83\u901a\u904e\u5927\u898f\u6a21 3D \u611f\u77e5\u8996\u8a0a\u9810\u8a13\u7df4\u548c\u8996\u8a0a-\u6587\u5b57\u5c0d\u6bd4\u5b78\u7fd2\u9032\u884c\u806f\u5408\u8a13\u7df4\u3002EgoDTM \u6574\u5408\u4e86\u4e00\u500b\u8f15\u91cf\u7d1a\u7684 3D \u611f\u77e5\u89e3\u78bc\u5668\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5f9e\u6df1\u5ea6\u4f30\u8a08\u6a21\u578b\u751f\u6210\u7684\u507d\u6df1\u5ea6\u5716\u4e2d\u5b78\u7fd2 3D \u611f\u77e5\u80fd\u529b\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u4fc3\u9032 3D \u611f\u77e5\u8996\u8a0a\u9810\u8a13\u7df4\uff0c\u6211\u5011\u901a\u904e\u6709\u6a5f\u5730\u7d50\u5408\u5e7e\u500b\u57fa\u790e\u6a21\u578b\uff0c\u7528\u624b-\u7269\u9ad4\u8996\u89ba\u7dda\u7d22\u8c50\u5bcc\u4e86\u539f\u59cb\u7684\u7c21\u77ed\u6a19\u984c\u3002\u5927\u91cf\u5be6\u9a57\u8b49\u660e\u4e86 EgoDTM \u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u5353\u8d8a\u7684 3D \u611f\u77e5\u8996\u89ba\u7406\u89e3\u80fd\u529b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u767c\u4f48\u5728 https://github.com/xuboshen/EgoDTM\u3002</paragraph>\n", "author": "Boshen Xu et.al.", "authors": "Boshen Xu, Yuting Mei, Xinbi Liu, Sipeng Zheng, Qin Jin", "id": "2503.15470v1", "paper_url": "http://arxiv.org/abs/2503.15470v1", "repo": "null"}}