
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-31**|**Bridging Geometric States via Geometric Diffusion Bridge**|Shengjie Luo et.al.|[2410.24220v1](http://arxiv.org/abs/2410.24220v1)|null|
|**2024-10-31**|**Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use**|Jiajun Xi et.al.|[2410.24218v1](http://arxiv.org/abs/2410.24218v1)|[link](https://github.com/sled-group/teachable_rl)|
|**2024-10-31**|**Understanding Optimization in Deep Learning with Central Flows**|Jeremy M. Cohen et.al.|[2410.24206v1](http://arxiv.org/abs/2410.24206v1)|null|
|**2024-10-31**|**DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion**|Weicai Ye et.al.|[2410.24203v1](http://arxiv.org/abs/2410.24203v1)|[link](https://github.com/zju3dv/diffpano)|
|**2024-10-31**|**P-Masking: Power Law Masking Improves Multi-attribute Controlled Generation**|Mohamed Elgaar et.al.|[2410.24201v1](http://arxiv.org/abs/2410.24201v1)|null|
|**2024-10-31**|**Length-Induced Embedding Collapse in Transformer-based Models**|Yuqi Zhou et.al.|[2410.24200v1](http://arxiv.org/abs/2410.24200v1)|null|
|**2024-10-31**|**Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation**|Mohamed Elgaar et.al.|[2410.24199v1](http://arxiv.org/abs/2410.24199v1)|null|
|**2024-10-31**|**SelfCodeAlign: Self-Alignment for Code Generation**|Yuxiang Wei et.al.|[2410.24198v1](http://arxiv.org/abs/2410.24198v1)|null|
|**2024-10-31**|**Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters**|Yujin Potter et.al.|[2410.24190v1](http://arxiv.org/abs/2410.24190v1)|null|
|**2024-10-31**|**Chasing Better Deep Image Priors between Over- and Under-parameterization**|Qiming Wu et.al.|[2410.24187v1](http://arxiv.org/abs/2410.24187v1)|[link](https://github.com/vita-group/chasing-better-dips)|
|**2024-10-31**|**DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models**|Heng-Jui Chang et.al.|[2410.24177v1](http://arxiv.org/abs/2410.24177v1)|null|
|**2024-10-31**|**Constraint Back-translation Improves Complex Instruction Following of Large Language Models**|Yunjia Qi et.al.|[2410.24175v1](http://arxiv.org/abs/2410.24175v1)|null|
|**2024-10-31**|**Redefining <Creative> in Dictionary: Towards a Enhanced Semantic Understanding of Creative Generation**|Fu Feng et.al.|[2410.24160v1](http://arxiv.org/abs/2410.24160v1)|null|
|**2024-10-31**|**GPT or BERT: why not both?**|Lucas Georges Gabriel Charpentier et.al.|[2410.24159v1](http://arxiv.org/abs/2410.24159v1)|[link](https://github.com/ltgoslo/gpt-bert)|
|**2024-10-31**|**Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning**|Jinghan Zhang et.al.|[2410.24155v1](http://arxiv.org/abs/2410.24155v1)|null|
|**2024-10-31**|**Scaling Concept With Text-Guided Diffusion Models**|Chao Huang et.al.|[2410.24151v1](http://arxiv.org/abs/2410.24151v1)|null|
|**2024-10-31**|**Don't Touch My Diacritics**|Kyle Gorman et.al.|[2410.24140v1](http://arxiv.org/abs/2410.24140v1)|null|
|**2024-10-31**|**Multi-environment Topic Models**|Dominic Sobhani et.al.|[2410.24126v1](http://arxiv.org/abs/2410.24126v1)|null|
|**2024-10-31**|**Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing**|Akash Dhruv et.al.|[2410.24119v1](http://arxiv.org/abs/2410.24119v1)|[link](https://github.com/neucol/llm-conversion-performance)|
|**2024-10-31**|**AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization**|Amir Kazemi et.al.|[2410.24116v1](http://arxiv.org/abs/2410.24116v1)|null|
|**2024-10-31**|**Nearest Neighbor Normalization Improves Multimodal Retrieval**|Neil Chowdhury et.al.|[2410.24114v1](http://arxiv.org/abs/2410.24114v1)|[link](https://github.com/multimodal-interpretability/nnn)|
|**2024-10-31**|**In-Context Fine-Tuning for Time-Series Foundation Models**|Abhimanyu Das et.al.|[2410.24087v1](http://arxiv.org/abs/2410.24087v1)|null|
|**2024-10-31**|**Graph Learning for Numeric Planning**|Dillon Z. Chen et.al.|[2410.24080v1](http://arxiv.org/abs/2410.24080v1)|[link](https://github.com/DillonZChen/goose)|
|**2024-10-31**|**Dynamical similarity analysis uniquely captures how computations develop in RNNs**|Quentin Guilhot et.al.|[2410.24070v1](http://arxiv.org/abs/2410.24070v1)|[link](https://github.com/qglht/repal)|
|**2024-10-31**|**Identifying General Mechanism Shifts in Linear Causal Representations**|Tianyu Chen et.al.|[2410.24059v1](http://arxiv.org/abs/2410.24059v1)|null|
|**2024-10-31**|**Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs**|Muhammed Saeed et.al.|[2410.24049v1](http://arxiv.org/abs/2410.24049v1)|null|
|**2024-10-31**|**Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks**|Yingzhe Peng et.al.|[2410.24032v1](http://arxiv.org/abs/2410.24032v1)|null|
|**2024-10-31**|**A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems using Disparity Maps**|Ariel Larey et.al.|[2410.24031v1](http://arxiv.org/abs/2410.24031v1)|null|
|**2024-10-31**|**Joint Training for Selective Prediction**|Zhaohui Li et.al.|[2410.24029v1](http://arxiv.org/abs/2410.24029v1)|null|
|**2024-10-31**|**AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents**|Yifan Xu et.al.|[2410.24024v1](http://arxiv.org/abs/2410.24024v1)|null|
|**2024-10-31**|**Detecting text level intellectual influence with knowledge graph embeddings**|Lucian Li et.al.|[2410.24021v1](http://arxiv.org/abs/2410.24021v1)|null|
|**2024-10-31**|**Speech is More Than Words: Do Speech-to-Text Translation Systems Leverage Prosody?**|Ioannis Tsiamas et.al.|[2410.24019v1](http://arxiv.org/abs/2410.24019v1)|null|
|**2024-10-31**|**Assessing the Impact of Packing on Machine Learning-Based Malware Detection and Classification Systems**|Daniel Gibert et.al.|[2410.24017v1](http://arxiv.org/abs/2410.24017v1)|null|
|**2024-10-31**|**An Information Criterion for Controlled Disentanglement of Multimodal Data**|Chenyu Wang et.al.|[2410.23996v1](http://arxiv.org/abs/2410.23996v1)|null|
|**2024-10-31**|**Localization, balance and affinity: a stronger multifaceted collaborative salient object detector in remote sensing images**|Yakun Xie et.al.|[2410.23991v1](http://arxiv.org/abs/2410.23991v1)|null|
|**2024-10-31**|**Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical Scene Segmentation**|Yihang Zhou et.al.|[2410.23962v1](http://arxiv.org/abs/2410.23962v1)|null|
|**2024-10-31**|**Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language**|Jiayi Wang et.al.|[2410.23956v1](http://arxiv.org/abs/2410.23956v1)|null|
|**2024-10-31**|**Representative Social Choice: From Learning Theory to AI Alignment**|Tianyi Qiu et.al.|[2410.23953v1](http://arxiv.org/abs/2410.23953v1)|null|
|**2024-10-31**|**Towards Fast Algorithms for the Preference Consistency Problem Based on Hierarchical Models**|Anne-Marie George et.al.|[2410.23934v1](http://arxiv.org/abs/2410.23934v1)|null|
|**2024-10-31**|**Language Models can Self-Lengthen to Generate Long Texts**|Shanghaoran Quan et.al.|[2410.23933v1](http://arxiv.org/abs/2410.23933v1)|null|
|**2024-10-31**|**BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments**|Xinghao Wang et.al.|[2410.23918v1](http://arxiv.org/abs/2410.23918v1)|[link](https://github.com/xinghaow99/bitstack)|
|**2024-10-31**|**Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling**|Davide Celestini et.al.|[2410.23916v1](http://arxiv.org/abs/2410.23916v1)|null|
|**2024-10-31**|**Efficient Inference and Computation of Optimal Alternatives for Preference Languages Based On Lexicographic Models**|Nic Wilson et.al.|[2410.23913v1](http://arxiv.org/abs/2410.23913v1)|null|
|**2024-10-31**|**RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner**|Fu-Chieh Chang et.al.|[2410.23912v1](http://arxiv.org/abs/2410.23912v1)|null|
|**2024-10-31**|**Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource languages**|Séamus Lankford et.al.|[2410.23890v1](http://arxiv.org/abs/2410.23890v1)|null|
|**2024-10-31**|**Failure Modes of LLMs for Causal Reasoning on Narratives**|Khurram Yamin et.al.|[2410.23884v1](http://arxiv.org/abs/2410.23884v1)|[link](https://github.com/shantanu95/llm_causal_reasoning)|
|**2024-10-31**|**'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue**|Rena Gao et.al.|[2410.23883v1](http://arxiv.org/abs/2410.23883v1)|null|
|**2024-10-31**|**Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs**|Liyi Chen et.al.|[2410.23875v1](http://arxiv.org/abs/2410.23875v1)|[link](https://github.com/liyichen-cly/pog)|
|**2024-10-31**|**Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models**|Hao Yang et.al.|[2410.23861v1](http://arxiv.org/abs/2410.23861v1)|null|
|**2024-10-31**|**Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?**|Zhanke Zhou et.al.|[2410.23856v1](http://arxiv.org/abs/2410.23856v1)|[link](https://github.com/tmlr-group/noisyrationales)|
|**2024-10-31**|**RAGraph: A General Retrieval-Augmented Graph Learning Framework**|Xinke Jiang et.al.|[2410.23855v1](http://arxiv.org/abs/2410.23855v1)|null|
|**2024-10-31**|**Commonsense Knowledge Editing Based on Free-Text in LLMs**|Xiusheng Huang et.al.|[2410.23844v1](http://arxiv.org/abs/2410.23844v1)|null|
|**2024-10-31**|**Reasons and Solutions for the Decline in Model Performance after Editing**|Xiusheng Huang et.al.|[2410.23843v1](http://arxiv.org/abs/2410.23843v1)|[link](https://github.com/nlpkeg/D4S)|
|**2024-10-31**|**Counterfactual MRI Data Augmentation using Conditional Denoising Diffusion Generative Models**|Pedro Morão et.al.|[2410.23835v1](http://arxiv.org/abs/2410.23835v1)|[link](https://github.com/pedromorao/counterfactual-mri-data-augmentation)|
|**2024-10-31**|**GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages**|Amir Hossein Kargaran et.al.|[2410.23825v1](http://arxiv.org/abs/2410.23825v1)|[link](https://github.com/cisnlp/glotcc)|
|**2024-10-31**|**Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding**|Jinlong He et.al.|[2410.23822v1](http://arxiv.org/abs/2410.23822v1)|null|
|**2024-10-31**|**Disentangling Disentangled Representations: Towards Improved Latent Units via Diffusion Models**|Youngjun Jun et.al.|[2410.23820v1](http://arxiv.org/abs/2410.23820v1)|null|
|**2024-10-31**|**The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing Audio Generation Challenge**|Dake Guo et.al.|[2410.23815v1](http://arxiv.org/abs/2410.23815v1)|null|
|**2024-10-31**|**Generative AI for Accessible and Inclusive Extended Reality**|Jens Grubert et.al.|[2410.23803v1](http://arxiv.org/abs/2410.23803v1)|null|
|**2024-10-31**|**EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching**|Xinwang Chen et.al.|[2410.23788v1](http://arxiv.org/abs/2410.23788v1)|[link](https://github.com/xinwangchen/edt)|
|**2024-10-31**|**What is Wrong with Perplexity for Long-context Language Modeling?**|Lizhe Fang et.al.|[2410.23771v1](http://arxiv.org/abs/2410.23771v1)|[link](https://github.com/pku-ml/longppl)|
|**2024-10-31**|**The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams**|Yunqi Zhu et.al.|[2410.23769v1](http://arxiv.org/abs/2410.23769v1)|null|
|**2024-10-31**|**Enhancing Chess Reinforcement Learning with Graph Representation**|Tomas Rigaux et.al.|[2410.23753v1](http://arxiv.org/abs/2410.23753v1)|[link](https://github.com/akulen/alphagateau)|
|**2024-10-31**|**LSEAttention is All You Need for Time Series Forecasting**|Dizhen Liang et.al.|[2410.23749v1](http://arxiv.org/abs/2410.23749v1)|null|
|**2024-10-31**|**DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios**|Junchao Wu et.al.|[2410.23746v1](http://arxiv.org/abs/2410.23746v1)|[link](https://github.com/nlp2ct/detectrl)|
|**2024-10-31**|**Syno: Structured Synthesis for Neural Operators**|Yongqi Zhuo et.al.|[2410.23745v1](http://arxiv.org/abs/2410.23745v1)|null|
|**2024-10-31**|**What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective**|Ming Li et.al.|[2410.23743v1](http://arxiv.org/abs/2410.23743v1)|[link](https://github.com/mingliiii/layer_gradient)|
|**2024-10-31**|**GigaCheck: Detecting LLM-generated Content**|Irina Tolstykh et.al.|[2410.23728v1](http://arxiv.org/abs/2410.23728v1)|null|
|**2024-10-31**|**Towards Reliable Alignment: Uncertainty-aware RLHF**|Debangshu Banerjee et.al.|[2410.23726v1](http://arxiv.org/abs/2410.23726v1)|null|
|**2024-10-31**|**OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models**|Junda Wu et.al.|[2410.23703v1](http://arxiv.org/abs/2410.23703v1)|null|
|**2024-10-31**|**Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction**|Peizhi Tang et.al.|[2410.23692v1](http://arxiv.org/abs/2410.23692v1)|[link](https://github.com/tanghulu6/llama3-8b-mob)|
|**2024-10-31**|**Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers**|Eugene Jang et.al.|[2410.23684v1](http://arxiv.org/abs/2410.23684v1)|null|
|**2024-10-31**|**Pseudo-Conversation Injection for LLM Goal Hijacking**|Zheng Chen et.al.|[2410.23678v1](http://arxiv.org/abs/2410.23678v1)|null|
|**2024-10-31**|**Provable Benefit of Cutout and CutMix for Feature Learning**|Junsoo Oh et.al.|[2410.23672v1](http://arxiv.org/abs/2410.23672v1)|null|
|**2024-10-31**|**Kernel Looping: Eliminating Synchronization Boundaries for Peak Inference Performance**|David Koeplinger et.al.|[2410.23668v1](http://arxiv.org/abs/2410.23668v1)|null|
|**2024-10-31**|**Morphological Typology in BPE Subword Productivity and Language Modeling**|Iñigo Parra et.al.|[2410.23656v1](http://arxiv.org/abs/2410.23656v1)|null|
|**2024-10-31**|**Deep Convolutional Neural Networks on Multiclass Classification of Three-Dimensional Brain Images for Parkinson's Disease Stage Prediction**|Guan-Hua Huang et.al.|[2410.23649v1](http://arxiv.org/abs/2410.23649v1)|null|
|**2024-10-31**|**On Positional Bias of Faithfulness for Long-form Summarization**|David Wan et.al.|[2410.23609v1](http://arxiv.org/abs/2410.23609v1)|[link](https://github.com/meetdavidwan/longformfact)|
|**2024-10-31**|**Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail Knowledge in LLMs**|Shuyang Yu et.al.|[2410.23605v1](http://arxiv.org/abs/2410.23605v1)|null|
|**2024-10-31**|**Using Multimodal Deep Neural Networks to Disentangle Language from Visual Aesthetics**|Colin Conwell et.al.|[2410.23603v1](http://arxiv.org/abs/2410.23603v1)|null|
|**2024-10-31**|**How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?**|Weiguo Gao et.al.|[2410.23594v1](http://arxiv.org/abs/2410.23594v1)|null|
|**2024-10-31**|**End-to-End Ontology Learning with Large Language Models**|Andy Lo et.al.|[2410.23584v1](http://arxiv.org/abs/2410.23584v1)|[link](https://github.com/andylolu2/ollm)|
|**2024-10-31**|**BioNCERE: Non-Contrastive Enhancement For Relation Extraction In Biomedical Texts**|Farshad Noravesh et.al.|[2410.23583v1](http://arxiv.org/abs/2410.23583v1)|null|
|**2024-10-31**|**Automating Quantum Software Maintenance: Flakiness Detection and Root Cause Analysis**|Janakan Sivaloganathan et.al.|[2410.23578v1](http://arxiv.org/abs/2410.23578v1)|null|
|**2024-10-31**|**Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models**|Yiqi Yang et.al.|[2410.23558v1](http://arxiv.org/abs/2410.23558v1)|null|
|**2024-10-31**|**From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents**|Nalin Tiwary et.al.|[2410.23555v1](http://arxiv.org/abs/2410.23555v1)|null|
|**2024-10-31**|**ALISE: Accelerating Large Language Model Serving with Speculative Scheduling**|Youpeng Zhao et.al.|[2410.23537v1](http://arxiv.org/abs/2410.23537v1)|null|
|**2024-10-31**|**Simulating User Agents for Embodied Conversational-AI**|Daniel Philipov et.al.|[2410.23535v1](http://arxiv.org/abs/2410.23535v1)|null|
|**2024-10-31**|**There and Back Again: On the relation between noises, images, and their inversions in diffusion models**|Łukasz Staniszewski et.al.|[2410.23530v1](http://arxiv.org/abs/2410.23530v1)|null|
|**2024-10-31**|**Large Language Models for Patient Comments Multi-Label Classification**|Hajar Sakai et.al.|[2410.23528v1](http://arxiv.org/abs/2410.23528v1)|null|
|**2024-10-31**|**LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models**|Hieu Tran et.al.|[2410.23526v1](http://arxiv.org/abs/2410.23526v1)|null|
|**2024-10-30**|**Neural spell-checker: Beyond words with synthetic data generation**|Matej Klemen et.al.|[2410.23514v1](http://arxiv.org/abs/2410.23514v1)|[link](https://github.com/matejklemen/slonspell)|
|**2024-10-30**|**Dynamic Strategy Planning for Efficient Question Answering with Large Language Models**|Tanmay Parekh et.al.|[2410.23511v1](http://arxiv.org/abs/2410.23511v1)|null|
|**2024-10-30**|**Tiny Transformers Excel at Sentence Compression**|Peter Belcak et.al.|[2410.23510v1](http://arxiv.org/abs/2410.23510v1)|null|
|**2024-10-30**|**Efficient and Interpretable Grammatical Error Correction with Mixture of Experts**|Muhammad Reza Qorib et.al.|[2410.23507v1](http://arxiv.org/abs/2410.23507v1)|[link](https://github.com/nusnlp/moece)|
|**2024-10-30**|**Learning to Achieve Goals with Belief State Transformers**|Edward S. Hu et.al.|[2410.23506v1](http://arxiv.org/abs/2410.23506v1)|null|
|**2024-10-30**|**All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling**|Emanuele Marconato et.al.|[2410.23501v1](http://arxiv.org/abs/2410.23501v1)|null|
|**2024-10-30**|**Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm**|Sattar Vakili et.al.|[2410.23498v1](http://arxiv.org/abs/2410.23498v1)|null|
|**2024-10-30**|**Smaller Large Language Models Can Do Moral Self-Correction**|Guangliang Liu et.al.|[2410.23496v1](http://arxiv.org/abs/2410.23496v1)|null|
|**2024-10-30**|**Causality-Driven Audits of Model Robustness**|Nathan Drenkow et.al.|[2410.23494v1](http://arxiv.org/abs/2410.23494v1)|null|

#### Abstracts
##### **Bridging Geometric States via Geometric Diffusion Bridge**
2410.24220v1 by Shengjie Luo, Yixian Xu, Di He, Shuxin Zheng, Tie-Yan Liu, Liwei Wang

The accurate prediction of geometric state evolution in complex systems is
critical for advancing scientific domains such as quantum chemistry and
material modeling. Traditional experimental and computational methods face
challenges in terms of environmental constraints and computational demands,
while current deep learning approaches still fall short in terms of precision
and generality. In this work, we introduce the Geometric Diffusion Bridge
(GDB), a novel generative modeling framework that accurately bridges initial
and target geometric states. GDB leverages a probabilistic approach to evolve
geometric state distributions, employing an equivariant diffusion bridge
derived by a modified version of Doob's $h$-transform for connecting geometric
states. This tailored diffusion process is anchored by initial and target
geometric states as fixed endpoints and governed by equivariant transition
kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB
framework by using a chain of equivariant diffusion bridges, providing a more
detailed and accurate characterization of evolution dynamics. Theoretically, we
conduct a thorough examination to confirm our framework's ability to preserve
joint distributions of geometric states and capability to completely model the
underlying dynamics inducing trajectory distributions with negligible error.
Experimental evaluations across various real-world scenarios show that GDB
surpasses existing state-of-the-art approaches, opening up a new pathway for
accurately bridging geometric states and tackling crucial scientific challenges
with improved accuracy and applicability.

摘要：準確預測複雜系統中的幾何狀態演化對於推進量子化學和材料建模等科學領域至關重要。傳統的實驗和計算方法在環境約束和計算需求方面面臨挑戰，而當前的深度學習方法在精確度和普遍性方面仍然不足。在這項工作中，我們引入了幾何擴散橋 (GDB)，這是一種新穎的生成模型框架，可以準確地橋接初始幾何狀態和目標幾何狀態。GDB 採用概率方法來演化幾何狀態分佈，採用通過修改版本的 Doob $h$-變換導出的等變擴散橋來連接幾何狀態。這個量身定制的擴散過程以初始和目標幾何狀態作為固定端點，並受等變遷移核的支配。此外，軌跡數據可以使用等變擴散橋鏈在我們的 GDB 框架中無縫利用，從而提供更詳細和準確的演化動力學表徵。在理論上，我們進行了徹底的檢驗，以確認我們的框架保留幾何狀態的聯合分佈的能力，以及完全建模誘導軌跡分佈的基礎動力學的能力，且誤差可以忽略不計。在各種真實世界場景中的實驗評估表明，GDB 超越了現有的最先進方法，為準確橋接幾何狀態和以更高的準確度和適用性解決關鍵科學挑戰開闢了一條新途徑。

##### **Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use**
2410.24218v1 by Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, Joyce Chai

In real-world scenarios, it is desirable for embodied agents to have the
ability to leverage human language to gain explicit or implicit knowledge for
learning tasks. Despite recent progress, most previous approaches adopt simple
low-level instructions as language inputs, which may not reflect natural human
communication. It's not clear how to incorporate rich language use to
facilitate task learning. To address this question, this paper studies
different types of language inputs in facilitating reinforcement learning (RL)
embodied agents. More specifically, we examine how different levels of language
informativeness (i.e., feedback on past behaviors and future guidance) and
diversity (i.e., variation of language expressions) impact agent learning and
inference. Our empirical results based on four RL benchmarks demonstrate that
agents trained with diverse and informative language feedback can achieve
enhanced generalization and fast adaptation to new tasks. These findings
highlight the pivotal role of language use in teaching embodied agents new
tasks in an open world. Project website:
https://github.com/sled-group/Teachable_RL

摘要：在現實世界的情境中，具身代理擁有利用人類語言來獲取明確或含蓄知識以進行學習任務的能力是可取的。儘管有近期的進展，但大多數先前的做法採用簡單的低階指令作為語言輸入，這可能無法反映自然的人類溝通。目前尚不清楚如何整合豐富的語言使用以促進任務學習。為了探討這個問題，本文研究了不同類型的語言輸入在促進強化學習 (RL) 具身代理中的作用。更具體地說，我們探討了不同層級的語言信息量（即對過去行為和未來指導的回饋）和多樣性（即語言表達方式的變化）如何影響代理學習和推論。我們基於四個 RL 基準的實證結果證明，使用多樣且有資訊的語言回饋進行訓練的代理可以實現增強的泛化能力和快速適應新任務。這些發現強調了語言使用在開放世界中教授具身代理新任務的關鍵作用。專案網站：
https://github.com/sled-group/Teachable_RL

##### **Understanding Optimization in Deep Learning with Central Flows**
2410.24206v1 by Jeremy M. Cohen, Alex Damian, Ameet Talwalkar, Zico Kolter, Jason D. Lee

Optimization in deep learning remains poorly understood, even in the simple
setting of deterministic (i.e. full-batch) training. A key difficulty is that
much of an optimizer's behavior is implicitly determined by complex oscillatory
dynamics, referred to as the "edge of stability." The main contribution of this
paper is to show that an optimizer's implicit behavior can be explicitly
captured by a "central flow:" a differential equation which models the
time-averaged optimization trajectory. We show that these flows can empirically
predict long-term optimization trajectories of generic neural networks with a
high degree of numerical accuracy. By interpreting these flows, we reveal for
the first time 1) the precise sense in which RMSProp adapts to the local loss
landscape, and 2) an "acceleration via regularization" mechanism, wherein
adaptive optimizers implicitly navigate towards low-curvature regions in which
they can take larger steps. This mechanism is key to the efficacy of these
adaptive optimizers. Overall, we believe that central flows constitute a
promising tool for reasoning about optimization in deep learning.

摘要：深度學習中的最佳化仍然知之甚少，即使是在確定性（即全批次）訓練的簡單設定中。一個關鍵的難題是，許多最佳化器的行為都是由複雜的振盪動力學隱含決定的，稱為「穩定性邊緣」。本文的主要貢獻是展示最佳化器的隱含行為可以被「中心流」明確捕捉：一個微分方程式，它對時間平均最佳化軌跡進行建模。我們展示這些流可以憑經驗預測具有高度數值精度的通用神經網路的長期最佳化軌跡。通過解釋這些流，我們首次揭示了 1) RMSProp 適應局部損失景觀的精確意義，以及 2) 一種「通過正則化加速」機制，其中自適應最佳化器隱含地導航到低曲率區域，在這些區域中它們可以採取更大的步驟。這種機制是這些自適應最佳化器功效的關鍵。總的來說，我們相信中心流構成了一個有前途的工具，可以用於推理深度學習中的最佳化。

##### **DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion**
2410.24203v1 by Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, Guofeng Zhang

Diffusion-based methods have achieved remarkable achievements in 2D image or
3D object generation, however, the generation of 3D scenes and even
$360^{\circ}$ images remains constrained, due to the limited number of scene
datasets, the complexity of 3D scenes themselves, and the difficulty of
generating consistent multi-view images. To address these issues, we first
establish a large-scale panoramic video-text dataset containing millions of
consecutive panoramic keyframes with corresponding panoramic depths, camera
poses, and text descriptions. Then, we propose a novel text-driven panoramic
generation framework, termed DiffPano, to achieve scalable, consistent, and
diverse panoramic scene generation. Specifically, benefiting from the powerful
generative capabilities of stable diffusion, we fine-tune a single-view
text-to-panorama diffusion model with LoRA on the established panoramic
video-text dataset. We further design a spherical epipolar-aware multi-view
diffusion model to ensure the multi-view consistency of the generated panoramic
images. Extensive experiments demonstrate that DiffPano can generate scalable,
consistent, and diverse panoramic images with given unseen text descriptions
and camera poses.

摘要：基於擴散的方法在 2D 影像或 3D 物件生成方面已取得顯著成果，然而，3D 場景甚至 $360^{\circ}$ 影像的生成仍受到限制，原因在於場景資料集數量有限、3D 場景本身的複雜性，以及產生一致的多視圖影像的難度。為了解決這些問題，我們首先建立一個包含數百萬個連續全景關鍵影格的大規模全景影片文字資料集，並附有對應的全景深度、相機姿勢和文字描述。然後，我們提出一個名為 DiffPano 的新文字驅動全景生成框架，以實現可擴充、一致且多樣化的全景場景生成。具體來說，我們受益於穩定擴散的強大生成能力，在已建立的全景影片文字資料集上使用 LoRA 微調單視圖文字轉全景擴散模型。我們進一步設計了一個球面對極感知多視圖擴散模型，以確保生成的全景影像的多視圖一致性。廣泛的實驗證明，DiffPano 可以根據未見過的文字描述和相機姿勢生成可擴充、一致且多樣化的全景影像。

##### **P-Masking: Power Law Masking Improves Multi-attribute Controlled Generation**
2410.24201v1 by Mohamed Elgaar, Hadi Amiri

We introduce LingGen, a novel approach for controlled text generation that
offers precise control over a wide array of linguistic attributes, even as the
number of attributes varies. LingGen employs a dynamic P-MASKING strategy,
which samples masking rates from a power law distribution during training. This
innovative approach enables the model to develop robust representations and
adapt its attribute control capabilities across a variable number of
attributes, from a single attribute to multiple complex configurations. The
P-MASKING technique enhances LingGen's ability to manage different levels of
attribute visibility, resulting in superior performance in multi-attribute
generation tasks. Our experiments demonstrate that LingGen surpasses current
state-of-the-art models in both attribute control accuracy and text fluency,
particularly excelling in scenarios with varying attribute demands.
Additionally, our ablation studies highlight the effectiveness of P-MASKING and
the influence of different base language models on performance. These findings
demonstrate LingGen's potential for applications requiring precise and
adaptable control over multiple linguistic attributes in text generation.

摘要：我們介紹 LingGen，一種用於控制文本生成的創新方法，即使屬性數量有所不同，也能精確控制廣泛的語言屬性。LingGen 採用動態 P-MASKING 策略，在訓練期間從冪律分佈中抽取遮罩率。這種創新的方法使模型能夠開發強大的表示形式，並跨越從單一屬性到多個複雜配置的可變屬性數量調整其屬性控制能力。P-MASKING 技術增強了 LingGen 管理不同級別屬性可見性的能力，從而提高了多屬性生成任務的性能。我們的實驗表明，LingGen 在屬性控制準確性和文本流暢性方面都超越了當前最先進的模型，特別是在屬性需求不同的情況下表現出色。此外，我們的消融研究突出了 P-MASKING 的有效性以及不同基礎語言模型對性能的影響。這些發現證明了 LingGen 在需要對文本生成中的多個語言屬性進行精確且適應性控制的應用中的潛力。

##### **Length-Induced Embedding Collapse in Transformer-based Models**
2410.24200v1 by Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu

Text embeddings enable various applications, but their performance
deteriorates on longer texts. In this paper, we find that the performance
degradation is due to a phenomenon called Length Collapse, where longer text
embeddings collapse into a narrow space. This collapse results in a
distributional inconsistency between embeddings of different text lengths,
ultimately hurting the performance of downstream tasks. Theoretically, by
considering the self-attention mechanism inherently functions as a low-pass
filter, we prove that long sequences increase the attenuation rate of the
low-pass filter effect of the self-attention mechanism. With layers going
deeper, excessive low-pass filtering causes the token signals to retain only
their Direct-Current (DC) component, which means the input token feature maps
will collapse into a narrow space, especially in long texts. Based on the above
analysis, we propose to mitigate the undesirable length collapse limitation by
introducing a temperature in softmax(), which achieves a higher low-filter
attenuation rate. The tuning-free method, called TempScale, can be plugged into
multiple transformer-based embedding models. Empirically, we demonstrate that
TempScale can improve existing embedding models, especially on long text
inputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text
Embedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from
LongEmbed, which specifically focuses on long context retrieval.

摘要：文字嵌入可啟用各種應用程式，但其效能會在較長的文字中下降。在本文中，我們發現效能下降是因爲一個稱為長度崩潰的現象，其中較長的文字嵌入會崩潰成一個狹窄的空間。此崩潰導致不同文字長度的嵌入之間的分配不一致，最終損害下游任務的效能。理論上，透過考慮自我注意機制本質上作為低通濾波器運作，我們證明長序列會增加自我注意機制的低通濾波器效應的衰減率。隨著層次更深入，過度的低通濾波會導致權杖訊號僅保留其直流 (DC) 成分，這表示輸入權杖特徵圖會崩潰成一個狹窄的空間，尤其是在長文字中。根據以上的分析，我們提議透過在 softmax() 中引入溫度來減輕不希望的長度崩潰限制，這會達成更高的低濾波器衰減率。稱為 TempScale 的無調校方法可以插入多個基於Transformer的嵌入模型中。根據經驗，我們證明 TempScale 可以改善現有的嵌入模型，尤其是在長文字輸入上，在 Massive Text Embedding Benchmark (MTEB) 的 40 個資料集上帶來高達 0.53% 的效能提升，以及在 LongEmbed 的 4 個資料集上帶來 0.82% 的效能提升，LongEmbed 特別專注於長脈絡檢索。

##### **Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation**
2410.24199v1 by Mohamed Elgaar, Hadi Amiri

We present a novel approach to paraphrase generation that enables precise
control and fine-tuning of 40 linguistic attributes for English. Our model is
an encoder-decoder architecture that takes as input a source sentence and
desired linguistic attributes, and produces paraphrases of the source that
satisfy the desired attributes. To guarantee high-quality outputs at inference
time, our method is equipped with a quality control mechanism that gradually
adjusts the embedding of linguistic attributes to find the nearest and most
attainable configuration of desired attributes for paraphrase generation. We
evaluate the effectiveness of our method by comparing it to recent controllable
generation models. Experimental results demonstrate that the proposed model
outperforms baselines in generating paraphrases that satisfy desired linguistic
attributes.

摘要：我們提出了一種新的同義詞生成方法，它可以精確控制和微調英文的 40 種語言屬性。我們的模型是一種編碼器-解碼器架構，它以原始句子和所需的語言屬性作為輸入，並產生符合所需屬性的原始同義詞。為了保證在推理時輸出高品質，我們的模型配備了一個品質控制機制，它會逐步調整語言屬性的嵌入，以尋找最接近且最可實現的同義詞生成所需屬性配置。我們透過將我們的模型與最近的可控生成模型進行比較來評估其有效性。實驗結果表明，所提出的模型在生成滿足所需語言屬性的同義詞方面優於基線。

##### **SelfCodeAlign: Self-Alignment for Code Generation**
2410.24198v1 by Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang

Instruction tuning is a supervised fine-tuning approach that significantly
improves the ability of large language models (LLMs) to follow human
instructions. We propose SelfCodeAlign, the first fully transparent and
permissive pipeline for self-aligning code LLMs without extensive human
annotations or distillation. SelfCodeAlign employs the same base model for
inference throughout the data generation process. It first extracts diverse
coding concepts from high-quality seed snippets to generate new tasks. It then
samples multiple responses per task, pairs each with test cases, and validates
them in a sandbox environment. Finally, passing examples are selected for
instruction tuning. In our primary experiments, we use SelfCodeAlign with
CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.
Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on
HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.
Across all benchmarks, this finetuned model consistently outperforms the
original version trained with OctoPack, the previous state-of-the-art method
for instruction tuning without human annotations or distillation. Additionally,
we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B
to 33B, and that the base models can benefit more from alignment with their own
data distribution. We further validate each component's effectiveness in our
pipeline, showing that SelfCodeAlign outperforms both direct distillation from
GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and
Evol-Instruct. SelfCodeAlign has also led to the creation of
StarCoder2-Instruct, the first fully transparent, permissively licensed, and
self-aligned code LLM that achieves state-of-the-art coding performance.

摘要：指令調整是一種監督微調方法，它顯著提高了大型語言模型 (LLM) 遵循人類指令的能力。我們提出 SelfCodeAlign，這是第一個完全透明且允許的管道，用於自我對齊程式碼 LLM，無需大量人工註解或蒸餾。SelfCodeAlign 在整個數據生成過程中採用相同的基礎模型進行推論。它首先從高品質種子片段中提取不同的編碼概念來生成新任務。然後它對每個任務採樣多個響應，將每個響應與測試用例配對，並在沙箱環境中驗證它們。最後，選擇通過範例進行指令調整。在我們的基礎實驗中，我們使用 SelfCodeAlign 與 CodeQwen1.5-7B 生成一個包含 74k 指令響應對應組的數據集。在該數據集上的微調導致一個模型，該模型在 HumanEval+ 上實現了 67.1 的 pass@1，儘管它比 CodeLlama-70B-Instruct 小十倍。在所有基準測試中，這個微調模型始終優於使用 OctoPack 訓練的原始版本，OctoPack 是之前在沒有人工註解或蒸餾的情況下進行指令調整的最新方法。此外，我們表明 SelfCodeAlign 對各種規模的 LLM 有效，從 3B 到 33B，並且基礎模型可以更多地受益於與它們自己的數據分佈對齊。我們進一步驗證了管道中每個組件的有效性，表明 SelfCodeAlign 優於從 GPT-4o 直接蒸餾和基於 GPT-3.5 的領先蒸餾方法，例如 OSS-Instruct 和 Evol-Instruct。SelfCodeAlign 還導致了 StarCoder2-Instruct 的創建，StarCoder2-Instruct 是第一個完全透明、許可寬鬆且自我對齊的程式碼 LLM，它實現了最先進的編碼性能。

##### **Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters**
2410.24190v1 by Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song

How could LLMs influence our democracy? We investigate LLMs' political
leanings and the potential influence of LLMs on voters by conducting multiple
experiments in a U.S. presidential election context. Through a voting
simulation, we first demonstrate 18 open- and closed-weight LLMs' political
preference for a Democratic nominee over a Republican nominee. We show how this
leaning towards the Democratic nominee becomes more pronounced in
instruction-tuned models compared to their base versions by analyzing their
responses to candidate-policy related questions. We further explore the
potential impact of LLMs on voter choice by conducting an experiment with 935
U.S. registered voters. During the experiments, participants interacted with
LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results
show a shift in voter choices towards the Democratic nominee following LLM
interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs
were not asked to persuade users to support the Democratic nominee during the
discourse. This effect is larger than many previous studies on the
persuasiveness of political campaigns, which have shown minimal effects in
presidential elections. Many users also expressed a desire for further
political interaction with LLMs. Which aspects of LLM interactions drove these
shifts in voter choice requires further study. Lastly, we explore how a safety
method can make LLMs more politically neutral, while leaving some open
questions.

摘要：LLM 如何影響我們的民主？我們透過在美國總統選舉背景下進行多項實驗，調查 LLM 的政治傾向和 LLM 對選民的潛在影響。首先，我們透過投票模擬，展示 18 個開放式和封閉式權重的 LLM 對於民主黨候選人相對於共和黨候選人的政治偏好。我們透過分析他們對候選人政策相關問題的回應，展示這個傾向於民主黨候選人的傾向在經過指令調整的模型中，與其基礎版本相比變得更加明顯。我們進一步透過對 935 位美國註冊選民進行實驗，探討 LLM 對選民選擇的潛在影響。在實驗期間，參與者透過五次交流與 LLM（Claude-3、Llama-3 和 GPT-4）互動。實驗結果顯示，在 LLM 互動後，選民選擇轉向民主黨候選人，將投票差距從 0.7% 擴大到 4.6%，即使在討論期間，並未要求 LLM 說服使用者支持民主黨候選人。這個影響大於許多先前關於政治運動說服力的研究，這些研究顯示在總統選舉中影響很小。許多使用者也表達了進一步與 LLM 進行政治互動的願望。哪些方面的 LLM 互動驅動了這些選民選擇的轉變，需要進一步研究。最後，我們探討安全方法如何讓 LLM 在政治上更加中立，同時留下一些開放性的問題。

##### **Chasing Better Deep Image Priors between Over- and Under-parameterization**
2410.24187v1 by Qiming Wu, Xiaohan Chen, Yifan Jiang, Zhangyang Wang

Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep
image priors (DIP) that regularize various image inverse problems. Meanwhile,
researchers also proposed extremely compact, under-parameterized image priors
(e.g., deep decoder) that are strikingly competent for image restoration too,
despite a loss of accuracy. These two extremes push us to think whether there
exists a better solution in the middle: between over- and under-parameterized
image priors, can one identify "intermediate" parameterized image priors that
achieve better trade-offs between performance, efficiency, and even preserving
strong transferability? Drawing inspirations from the lottery ticket hypothesis
(LTH), we conjecture and study a novel "lottery image prior" (LIP) by
exploiting DNN inherent sparsity, stated as: given an over-parameterized
DNN-based image prior, it will contain a sparse subnetwork that can be trained
in isolation, to match the original DNN's performance when being applied as a
prior to various image inverse problems. Our results validate the superiority
of LIPs: we can successfully locate the LIP subnetworks from over-parameterized
DIPs at substantial sparsity ranges. Those LIP subnetworks significantly
outperform deep decoders under comparably compact model sizes (by often fully
preserving the effectiveness of their over-parameterized counterparts), and
they also possess high transferability across different images as well as
restoration task types. Besides, we also extend LIP to compressive sensing
image reconstruction, where a pre-trained GAN generator is used as the prior
(in contrast to untrained DIP or deep decoder), and confirm its validity in
this setting too. To our best knowledge, this is the first time that LTH is
demonstrated to be relevant in the context of inverse problems or image priors.

摘要：深度神经网络 (DNN) 以作为过度参数化的深度图像先验 (DIP) 而闻名，可对各种图像逆问题进行正则化。同时，研究人员还提出了极度紧凑的、参数化不足的图像先验（例如，深度解码器），尽管准确性有所下降，但对于图像修复也表现出惊人的能力。这两个极端促使我们思考是否存在一个更好的中间解决方案：在过度参数化和参数化不足的图像先验之间，能否找到“中间”参数化图像先验，在性能、效率，甚至保留强大的可迁移性之间取得更好的权衡？从彩票假设 (LTH) 中汲取灵感，我们通过利用 DNN 固有的稀疏性来推测和研究一种新颖的“彩票图像先验”(LIP)，表述如下：给定一个过度参数化的基于 DNN 的图像先验，它将包含一个稀疏子网络，该子网络可以独立训练，以匹配原始 DNN 的性能，当作为先验应用于各种图像逆问题时。我们的结果验证了 LIP 的优越性：我们可以在大量的稀疏性范围内成功地从过度参数化的 DIP 中找到 LIP 子网络。这些 LIP 子网络在相当紧凑的模型大小下明显优于深度解码器（通常完全保留其过度参数化对应模型的有效性），并且它们还对不同的图像以及修复任务类型具有很高的可迁移性。此外，我们还将 LIP 拓展到了压缩感知图像重建，其中经过预训练的 GAN 生成器被用作先验（与未经训练的 DIP 或深度解码器相反），并在此设置中确认了其有效性。据我们所知，这是 LTH 首次被证明与逆问题或图像先验相关。

##### **DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models**
2410.24177v1 by Heng-Jui Chang, Hongyu Gong, Changhan Wang, James Glass, Yu-An Chung

Spoken language models (SLMs) have gained increasing attention with
advancements in text-based, decoder-only language models. SLMs process text and
speech, enabling simultaneous speech understanding and generation. This paper
presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to
improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin
extracts speaker-invariant tokens rich in phonetic information and resilient to
input variations, enhancing zero-shot SLM tasks and speech resynthesis. We
propose a chunk-wise approach to enable streamable DC-Spin without retraining
and degradation. Comparisons of tokenization methods (self-supervised and
neural audio codecs), model scalability, and downstream task proxies show that
tokens easily modeled by an n-gram LM or aligned with phonemes offer strong
performance, providing insights for designing speech tokenizers for SLMs.

摘要：語音語言模型 (SLM) 隨著基於文字、僅解碼器語言模型的進展而受到越來越多的關注。SLM 處理文字和語音，同時實現語音理解和生成。本文提出了雙碼本說話者不變聚類 (DC-Spin)，其目標是透過連結音訊訊號和 SLM 符號來改善語音標記化。DC-Spin 萃取說話者不變符號，這些符號富含語音資訊且能抵抗輸入變化，增強零次學習 SLM 任務和語音重新合成。我們提出一個分塊方法，以在不重新訓練和退化的情況下啟用串流式 DC-Spin。符號化方法（自我監督和神經音訊編解碼器）、模型可擴充性，以及下游任務代理的比較表明，由 n-gram LM 輕鬆建模或與音素對齊的符號提供了強大的效能，為設計 SLM 的語音符號化器提供了見解。

##### **Constraint Back-translation Improves Complex Instruction Following of Large Language Models**
2410.24175v1 by Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li

Large language models (LLMs) struggle to follow instructions with complex
constraints in format, length, etc. Following the conventional
instruction-tuning practice, previous works conduct post-training on complex
instruction-response pairs generated by feeding complex instructions to
advanced LLMs. However, even advanced LLMs cannot follow complex instructions
well, thus limiting the quality of generated data. In this work, we find that
existing datasets inherently contain implicit complex constraints and propose a
novel data generation technique, constraint back-translation. Specifically, we
take the high-quality instruction-response pairs in existing datasets and only
adopt advanced LLMs to add complex constraints already met by the responses to
the instructions, which naturally reduces costs and data noise. In the
experiments, we adopt Llama3-70B-Instruct to back-translate constraints and
create a high-quality complex instruction-response dataset, named CRAB. We
present that post-training on CRAB improves multiple backbone LLMs' complex
instruction-following ability, evaluated on extensive instruction-following
benchmarks. We further find that constraint back-translation also serves as a
useful auxiliary training objective in post-training. Our code, data, and
models will be released to facilitate future research.

摘要：大型語言模型 (LLM) 難以遵循格式、長度等方面的複雜約束說明。遵循慣例的說明調整實務，先前的研究在進階 LLM 中輸入複雜的說明來產生複雜的說明回應配對，並在訓練後進行處理。然而，即使是進階 LLM 也無法遵循複雜的說明，因此限制了產生資料的品質。在這項研究中，我們發現現有的資料集本身就包含了隱含的複雜約束，並提出了一種新的資料產生技術，約束反向翻譯。具體而言，我們採用現有資料集中高品質的說明回應配對，並僅採用進階 LLM 來增加回應已滿足的複雜約束，這自然會降低成本和資料雜訊。在實驗中，我們採用 Llama3-70B-Instruct 來反向翻譯約束，並建立一個高品質的複雜說明回應資料集，稱為 CRAB。我們提出在 CRAB 上進行訓練後，可以改善多個主幹 LLM 的複雜說明遵循能力，並在廣泛的說明遵循基準上進行評估。我們進一步發現，約束反向翻譯在訓練後也可用作有用的輔助訓練目標。我們的程式碼、資料和模型將會發布，以利未來的研究。

##### **Redefining <Creative> in Dictionary: Towards a Enhanced Semantic Understanding of Creative Generation**
2410.24160v1 by Fu Feng, Yucheng Xie, Jing Wang, Xin Geng

Creativity, both in human and diffusion models, remains an inherently
abstract concept; thus, simply adding "creative" to a prompt does not yield
reliable semantic recognition by the model. In this work, we concretize the
abstract notion of "creative" through the TP2O task, which aims to merge two
unrelated concepts, and introduce CreTok, redefining "creative" as the token
$\texttt{<CreTok>}$. This redefinition offers a more concrete and universally
adaptable representation for concept blending. This redefinition occurs
continuously, involving the repeated random sampling of text pairs with
different concepts and optimizing cosine similarity between target and constant
prompts. This approach enables $\texttt{<CreTok>}$ to learn a method for
creative concept fusion. Extensive experiments demonstrate that the creative
capability enabled by $\texttt{<CreTok>}$ substantially surpasses recent SOTA
diffusion models and achieves superior creative generation. CreTok exhibits
greater flexibility and reduced time overhead, as $\texttt{<CreTok>}$ can
function as a universal token for any concept, facilitating creative generation
without retraining.

摘要：無論是人類還是擴散模型中的創造力，都仍然是一個本質上抽象的概念；因此，僅僅在提示中加入「創造力」並不會讓模型產生可靠的語義辨識。在這項工作中，我們透過 TP2O 任務具體化了「創造力」的抽象概念，其目標是合併兩個不相關的概念，並引入 CreTok，將「創造力」重新定義為符號 $\texttt{<CreTok>}$。這種重新定義提供了一個更具體且普遍適用的概念融合表示法。這種重新定義會持續進行，包括重複隨機取樣具有不同概念的文字對，並最佳化目標和常數提示之間的餘弦相似度。這種方法讓 $\texttt{<CreTok>}$ 能夠學習一種創造性概念融合方法。廣泛的實驗證明，$\texttt{<CreTok>}$ 所啟用的創造能力大幅超越了最近的 SOTA 擴散模型，並實現了優異的創造性生成。CreTok 展現出更高的彈性和更少的時間開銷，因為 $\texttt{<CreTok>}$ 可以作為任何概念的通用符號，促進創造性生成而無需重新訓練。

##### **GPT or BERT: why not both?**
2410.24159v1 by Lucas Georges Gabriel Charpentier, David Samuel

We present a simple way to merge masked language modeling with causal
language modeling. This hybrid training objective results in a model that
combines the strengths of both modeling paradigms within a single transformer
stack: GPT-BERT can be transparently used like any standard causal or masked
language model. We test the pretraining process that enables this flexible
behavior on the BabyLM Challenge 2024. The results show that the hybrid
pretraining outperforms masked-only or causal-only models. We openly release
the models, training corpora and code.

摘要：我們提出了一種簡單的方法，將遮蔽語言模型與因果語言模型合併。這種混合訓練目標會產生一個模型，結合了單一Transformer堆疊中兩種模型範例的優點：GPT-BERT 可以像任何標準因果或遮蔽語言模型一樣透明地使用。我們在 BabyLM Challenge 2024 上測試了讓這種靈活行為得以實現的預訓練過程。結果顯示，混合預訓練優於僅遮蔽或僅因果模型。我們公開發布模型、訓練語料庫和程式碼。

##### **Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning**
2410.24155v1 by Jinghan Zhang, Fengran Mo, Xiting Wang, Kunpeng Liu

Recent advances in large language models (LLMs) have demonstrated their
potential in handling complex reasoning tasks, which are usually achieved by
constructing a thought chain to guide the model to solve the problem with
multi-step thinking. However, existing methods often remain confined to
previously explored solution spaces and thus overlook the critical blind spot
within LLMs' cognitive range. To address these issues, we design the Thought
Space Explorer (TSE), a novel framework to expand and optimize thought
structures to guide LLMs to explore their blind spots of thinking. By
generating new reasoning steps and branches based on the original thought
structure with various designed strategies, TSE broadens the thought space and
alleviates the impact of blind spots for LLM reasoning. Experimental results on
multiple levels of reasoning tasks demonstrate the efficacy of TSE. We also
conduct extensive analysis to understand how structured and expansive thought
can contribute to unleashing the potential of LLM reasoning capabilities.

摘要：近來大型語言模型 (LLM) 的進展已展示它們在處理複雜推理任務的潛力，這些任務通常透過建構一個思維鏈來引導模型以多步驟思考來解決問題。然而，現有方法通常仍侷限於先前探索的解決方案空間，因此忽略了 LLM 認知範圍內的關鍵盲點。為了解決這些問題，我們設計了思維空間探索器 (TSE)，一個新穎的架構來擴展和最佳化思維結構，以引導 LLM 探索它們思考的盲點。透過根據原始思維結構產生新的推理步驟和分支，並採用各種設計策略，TSE 擴展了思維空間並減輕了 LLM 推理盲點的影響。在多個層級推理任務上的實驗結果證明了 TSE 的效能。我們也進行廣泛的分析，以了解結構化且廣泛的思維如何有助於釋放 LLM 推理能力的潛力。

##### **Scaling Concept With Text-Guided Diffusion Models**
2410.24151v1 by Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, Chenliang Xu

Text-guided diffusion models have revolutionized generative tasks by
producing high-fidelity content from text descriptions. They have also enabled
an editing paradigm where concepts can be replaced through text conditioning
(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of
replacing a concept, can we enhance or suppress the concept itself? Through an
empirical study, we identify a trend where concepts can be decomposed in
text-guided diffusion models. Leveraging this insight, we introduce
ScalingConcept, a simple yet effective method to scale decomposed concepts up
or down in real input without introducing new elements. To systematically
evaluate our approach, we present the WeakConcept-10 dataset, where concepts
are imperfect and need to be enhanced. More importantly, ScalingConcept enables
a variety of novel zero-shot applications across image and audio domains,
including tasks such as canonical pose generation and generative sound
highlighting or removal.

摘要：文本引导扩散模型通过文本描述生成高保真内容，从而彻底改变了生成任务。它们还启用了编辑范例，其中可以通过文本条件（例如，将狗替换为老虎）替换概念。在这项工作中，我们探索了一种新方法：不是替换一个概念，而是增强或抑制概念本身吗？通过实证研究，我们发现了一个趋势，即概念可以在文本引导扩散模型中分解。利用这一见解，我们引入了 ScalingConcept，这是一种简单而有效的方法，可以在不引入新元素的情况下在真实输入中放大或缩小分解的概念。为了系统地评估我们的方法，我们提出了 WeakConcept-10 数据集，其中概念不完善且需要增强。更重要的是，ScalingConcept 可以在图像和音频领域启用各种新颖的零样本应用程序，包括规范姿势生成和生成声音突出或消除等任务。

##### **Don't Touch My Diacritics**
2410.24140v1 by Kyle Gorman, Yuval Pinter

The common practice of preprocessing text before feeding it into NLP models
introduces many decision points which have unintended consequences on model
performance. In this opinion piece, we focus on the handling of diacritics in
texts originating in many languages and scripts. We demonstrate, through
several case studies, the adverse effects of inconsistent encoding of
diacritized characters and of removing diacritics altogether. We call on the
community to adopt simple but necessary steps across all models and toolkits in
order to improve handling of diacritized text and, by extension, increase
equity in multilingual NLP.

摘要：在將文本輸入 NLP 模型之前，預處理文本的常見做法會引入許多決策點，這些決策點會對模型效能造成意想不到的後果。在這篇意見文章中，我們專注於處理來自多種語言和腳本的文本中的變音符號。我們透過多項個案研究，證明了變音符號字元編碼不一致和完全移除變音符號的不利影響。我們呼籲社群在所有模型和工具組中採取簡單但必要的步驟，以改善變音符號文本的處理方式，進而擴大多語言 NLP 中的公平性。

##### **Multi-environment Topic Models**
2410.24126v1 by Dominic Sobhani, Amir Feder, David Blei

Probabilistic topic models are a powerful tool for extracting latent themes
from large text datasets. In many text datasets, we also observe per-document
covariates (e.g., source, style, political affiliation) that act as
environments that modulate a "global" (environment-agnostic) topic
representation. Accurately learning these representations is important for
prediction on new documents in unseen environments and for estimating the
causal effect of topics on real-world outcomes. To this end, we introduce the
Multi-environment Topic Model (MTM), an unsupervised probabilistic model that
separates global and environment-specific terms. Through experimentation on
various political content, from ads to tweets and speeches, we show that the
MTM produces interpretable global topics with distinct environment-specific
words. On multi-environment data, the MTM outperforms strong baselines in and
out-of-distribution. It also enables the discovery of accurate causal effects.

摘要：機率主題模型是一種強大的工具，可用於從大型文本資料集中萃取潛在主題。在許多文本資料集中，我們也會觀察到每個文件共變數（例如，來源、風格、政治聯繫），這些共變數會作為環境，調節「全球」（與環境無關）主題表徵。準確學習這些表徵對於預測未知環境中的新文件以及估計主題對現實世界結果的因果關係非常重要。為此，我們引入了多環境主題模型 (MTM)，這是一個無監督的機率模型，可將全球和特定環境的術語分開。透過在各種政治內容（從廣告到推文和演講）上進行實驗，我們證明 MTM 產生了可解釋的全球主題，並帶有不同的特定環境字詞。在多環境資料上，MTM 在分佈內和分佈外都優於強大的基準。它還能發現準確的因果關係。

##### **Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing**
2410.24119v1 by Akash Dhruv, Anshu Dubey

The emergence of foundational models and generative artificial intelligence
(GenAI) is poised to transform productivity in scientific computing, especially
in code development, refactoring, and translating from one programming language
to another. However, because the output of GenAI cannot be guaranteed to be
correct, manual intervention remains necessary. Some of this intervention can
be automated through task-specific tools, alongside additional methodologies
for correctness verification and effective prompt development. We explored the
application of GenAI in assisting with code translation, language
interoperability, and codebase inspection within a legacy Fortran codebase used
to simulate particle interactions at the Large Hadron Collider (LHC). In the
process, we developed a tool, CodeScribe, which combines prompt engineering
with user supervision to establish an efficient process for code conversion. In
this paper, we demonstrate how CodeScribe assists in converting Fortran code to
C++, generating Fortran-C APIs for integrating legacy systems with modern C++
libraries, and providing developer support for code organization and algorithm
implementation. We also address the challenges of AI-driven code translation
and highlight its benefits for enhancing productivity in scientific computing
workflows.

摘要：基礎模型和生成式人工智慧 (GenAI) 的出現，將轉變科學運算中的生產力，尤其是在程式碼開發、重構和從一種程式語言轉換到另一種程式語言。然而，由於無法保證 GenAI 的輸出正確，因此仍然需要手動介入。其中一些介入可透過特定於任務的工具自動化，並搭配額外的正確性驗證和有效提示開發方法。我們探討 GenAI 在協助程式碼轉換、語言互操作性，以及在大型強子對撞機 (LHC) 中模擬粒子交互所使用的傳統 Fortran 程式碼庫中的程式碼庫檢查方面的應用。在此過程中，我們開發了一個工具 CodeScribe，它結合提示工程和使用者監督，以建立一個用於程式碼轉換的有效率流程。在本文中，我們展示 CodeScribe 如何協助將 Fortran 程式碼轉換為 C++，產生 Fortran-C API 以將傳統系統與現代 C++ 函式庫整合，並提供開發人員支援，以進行程式碼組織和演算法實作。我們也探討 AI 驅動程式碼轉換的挑戰，並強調其在提升科學運算工作流程生產力方面的優點。

##### **AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization**
2410.24116v1 by Amir Kazemi, Qurat ul ain Fatima, Volodymyr Kindratenko, Christopher Tessum

Image labeling is a critical bottleneck in the development of computer vision
technologies, often constraining the potential of machine learning models due
to the time-intensive nature of manual annotations. This work introduces a
novel approach that leverages outpainting to address the problem of annotated
data scarcity by generating artificial contexts and annotations, significantly
reducing manual labeling efforts. We apply this technique to a particularly
acute challenge in autonomous driving, urban planning, and environmental
monitoring: the lack of diverse, eye-level vehicle images in desired classes.
Our dataset comprises AI-generated vehicle images obtained by detecting and
cropping vehicles from manually selected seed images, which are then outpainted
onto larger canvases to simulate varied real-world conditions. The outpainted
images include detailed annotations, providing high-quality ground truth data.
Advanced outpainting techniques and image quality assessments ensure visual
fidelity and contextual relevance. Augmentation with outpainted vehicles
improves overall performance metrics by up to 8\% and enhances prediction of
underrepresented classes by up to 20\%. This approach, exemplifying outpainting
as a self-annotating paradigm, presents a solution that enhances dataset
versatility across multiple domains of machine learning. The code and links to
datasets used in this study are available for further research and replication
at https://github.com/amir-kazemi/aidovecl.

摘要：影像標籤是電腦視覺技術發展中的關鍵瓶頸，由於手動標記的耗時性質，經常限制了機器學習模型的潛力。這項工作介紹了一種新穎的方法，利用 outpainting 來解決標記資料短缺的問題，透過產生人工背景和標記，大幅減少手動標籤工作。我們將此技術應用於自動駕駛、都市規劃和環境監控中一個特別嚴峻的挑戰：缺乏所需類別的多元化、與視線齊平的車輛影像。我們的資料集包含了 AI 生成的車輛影像，透過從手動選擇的種子影像中偵測和裁剪車輛而取得，然後將其 outpaint 到更大的畫布上，以模擬多變的真實世界條件。outpaint 的影像包含詳細標記，提供高品質的真實數據。進階的 outpaint 技術和影像品質評估確保了視覺保真度和背景相關性。使用 outpaint 車輛進行擴充，將整體效能指標提升了 8%，並將欠代表類別的預測值提升了 20%。這種方法以 outpaint 為自標記範例，提出了一個解決方案，可以提升機器學習中多個領域的資料集多樣性。本研究中使用的程式碼和資料集連結可於 https://github.com/amir-kazemi/aidovecl 取得，以供進一步研究和複製。

##### **Nearest Neighbor Normalization Improves Multimodal Retrieval**
2410.24114v1 by Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush

Multimodal models leverage large-scale pre-training to achieve strong but
still imperfect performance on tasks such as image captioning, visual question
answering, and cross-modal retrieval. In this paper, we present a simple and
efficient method for correcting errors in trained contrastive image-text
retrieval models with no additional training, called Nearest Neighbor
Normalization (NNN). We show an improvement on retrieval metrics in both text
retrieval and image retrieval for all of the contrastive models that we tested
(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used
(MS-COCO and Flickr30k). NNN requires a reference database, but does not
require any training on this database, and can even increase the retrieval
accuracy of a model after finetuning.

摘要：多模態模型利用大規模預訓練，在圖像標題、視覺問題解答和跨模態檢索等任務上取得了強勁但仍不完美的表現。在本文中，我們提出了一種簡單且有效的方法，用於更正訓練好的對比圖像文本檢索模型中的錯誤，無需額外訓練，稱為最近鄰正規化 (NNN)。我們展示了在所有我們測試的對比模型（CLIP、BLIP、ALBEF、SigLIP、BEiT）以及我們使用的兩個數據集（MS-COCO 和 Flickr30k）的文本檢索和圖像檢索中檢索指標的改進。NNN 需要一個參考數據庫，但不需要對這個數據庫進行任何訓練，甚至可以在微調後提高模型的檢索準確度。

##### **In-Context Fine-Tuning for Time-Series Foundation Models**
2410.24087v1 by Abhimanyu Das, Matthew Faw, Rajat Sen, Yichen Zhou

Motivated by the recent success of time-series foundation models for
zero-shot forecasting, we present a methodology for $\textit{in-context
fine-tuning}$ of a time-series foundation model. In particular, we design a
pretrained foundation model that can be prompted (at inference time) with
multiple time-series examples, in order to forecast a target time-series into
the future. Our foundation model is specifically trained to utilize examples
from multiple related time-series in its context window (in addition to the
history of the target time-series) to help it adapt to the specific
distribution of the target domain at inference time. We show that such a
foundation model that uses in-context examples at inference time can obtain
much better performance on popular forecasting benchmarks compared to
supervised deep learning methods, statistical models, as well as other
time-series foundation models. Interestingly, our in-context fine-tuning
approach even rivals the performance of a foundation model that is explicitly
fine-tuned on the target domain.

摘要：受近期时序基础模型在零次预测方面的成功所激励，我们提出了一种时序基础模型的「情境内微调」方法。具体而言，我们设计了一个预训练基础模型，可以在推理时使用多个时序示例进行提示，以便预测目标时序的未来。我们的基础模型经过专门训练，可以在其上下文窗口中利用来自多个相关时序的示例（除了目标时序的历史记录之外），以帮助它在推理时适应目标域的特定分布。我们表明，在推理时使用情境内示例的此类基础模型可以在流行的预测基准上获得比监督深度学习方法、统计模型以及其他时序基础模型更好的性能。有趣的是，我们的情境内微调方法甚至可以与在目标域上显式微调的基础模型的性能相媲美。

##### **Graph Learning for Numeric Planning**
2410.24080v1 by Dillon Z. Chen, Sylvie Thiébaux

Graph learning is naturally well suited for use in symbolic, object-centric
planning due to its ability to exploit relational structures exhibited in
planning domains and to take as input planning instances with arbitrary numbers
of objects. Numeric planning is an extension of symbolic planning in which
states may now also exhibit numeric variables. In this work, we propose
data-efficient and interpretable machine learning models for learning to solve
numeric planning tasks. This involves constructing a new graph kernel for
graphs with both continuous and categorical attributes, as well as new
optimisation methods for learning heuristic functions for numeric planning.
Experiments show that our graph kernels are vastly more efficient and
generalise better than graph neural networks for numeric planning, and also
yield competitive coverage performance compared to domain-independent numeric
planners. Code is available at https://github.com/DillonZChen/goose

摘要：圖形學習由於能夠利用規劃領域中展示的關係結構，並將具有任意數量的物件的規劃實例作為輸入，因此自然非常適合用於符號、以物件為中心規劃。數值規劃是符號規劃的延伸，其中狀態現在也可能顯示數值變數。在這項工作中，我們提出了資料有效且可解釋的機器學習模型，用於學習解決數值規劃任務。這涉及為具有連續和類別屬性的圖形構建一個新的圖形核，以及用於學習數值規劃啟發式函數的新優化方法。實驗表明，我們的圖形核比數值規劃的圖神經網路更有效率，且泛化性更好，並且與與領域無關的數值規劃器相比，也產生了具有競爭力的覆蓋效能。程式碼可在 https://github.com/DillonZChen/goose 取得

##### **Dynamical similarity analysis uniquely captures how computations develop in RNNs**
2410.24070v1 by Quentin Guilhot, Jascha Achterberg, Michał Wójcik, Rui Ponte Costa

Methods for analyzing representations in neural systems are increasingly
popular tools in neuroscience and mechanistic interpretability. Measures
comparing neural activations across conditions, architectures, and species give
scalable ways to understand information transformation within different neural
networks. However, recent findings show that some metrics respond to spurious
signals, leading to misleading results. Establishing benchmark test cases is
thus essential for identifying the most reliable metric and potential
improvements. We propose that compositional learning in recurrent neural
networks (RNNs) can provide a test case for dynamical representation alignment
metrics. Implementing this case allows us to evaluate if metrics can identify
representations that develop throughout learning and determine if
representations identified by metrics reflect the network's actual
computations. Building both attractor and RNN based test cases, we show that
the recently proposed Dynamical Similarity Analysis (DSA) is more noise robust
and reliably identifies behaviorally relevant representations compared to prior
metrics (Procrustes, CKA). We also demonstrate how such test cases can extend
beyond metric evaluation to study new architectures. Specifically, testing DSA
in modern (Mamba) state space models suggests that these models, unlike RNNs,
may not require changes in recurrent dynamics due to their expressive hidden
states. Overall, we develop test cases that showcase how DSA's enhanced ability
to detect dynamical motifs makes it highly effective for identifying ongoing
computations in RNNs and revealing how networks learn tasks.

摘要：神经系统中表示分析的方法在神经科学和机制可解释性中越来越受欢迎。比较神经激活在不同条件、架构和物种下的测量方法提供了可扩展的方法来理解不同神经网络中信息转换。然而，最近的研究结果表明，一些指标会对虚假信号做出反应，从而导致误导性结果。因此，建立基准测试用例对于识别最可靠的指标和潜在改进至关重要。我们提出，循环神经网络（RNN）中的组合学习可以为动态表示对齐指标提供一个测试用例。实现此用例允许我们评估指标是否可以识别在整个学习过程中发展的表示，并确定由指标识别的表示是否反映了网络的实际计算。通过构建吸引子和基于 RNN 的测试用例，我们表明最近提出的动态相似性分析（DSA）具有更强的噪声鲁棒性，并且与先前的指标（Procrustes、CKA）相比，可以可靠地识别与行为相关的表示。我们还演示了此类测试用例如何扩展到指标评估之外以研究新架构。具体来说，在现代（Mamba）状态空间模型中测试 DSA 表明，与 RNN 不同，这些模型可能不需要改变循环动态，因为它们具有表达性的隐藏状态。总体而言，我们开发了测试用例，展示了 DSA 增强了检测动态主题的能力如何使其非常有效地识别 RNN 中正在进行的计算，并揭示网络如何学习任务。

##### **Identifying General Mechanism Shifts in Linear Causal Representations**
2410.24059v1 by Tianyu Chen, Kevin Bello, Francesco Locatello, Bryon Aragam, Pradeep Ravikumar

We consider the linear causal representation learning setting where we
observe a linear mixing of $d$ unknown latent factors, which follow a linear
structural causal model. Recent work has shown that it is possible to recover
the latent factors as well as the underlying structural causal model over them,
up to permutation and scaling, provided that we have at least $d$ environments,
each of which corresponds to perfect interventions on a single latent node
(factor). After this powerful result, a key open problem faced by the community
has been to relax these conditions: allow for coarser than perfect single-node
interventions, and allow for fewer than $d$ of them, since the number of latent
factors $d$ could be very large. In this work, we consider precisely such a
setting, where we allow a smaller than $d$ number of environments, and also
allow for very coarse interventions that can very coarsely \textit{change the
entire causal graph over the latent factors}. On the flip side, we relax what
we wish to extract to simply the \textit{list of nodes that have shifted
between one or more environments}. We provide a surprising identifiability
result that it is indeed possible, under some very mild standard assumptions,
to identify the set of shifted nodes. Our identifiability proof moreover is a
constructive one: we explicitly provide necessary and sufficient conditions for
a node to be a shifted node, and show that we can check these conditions given
observed data. Our algorithm lends itself very naturally to the sample setting
where instead of just interventional distributions, we are provided datasets of
samples from each of these distributions. We corroborate our results on both
synthetic experiments as well as an interesting psychometric dataset. The code
can be found at https://github.com/TianyuCodings/iLCS.

摘要：<paragraph>我們考慮線性因果表示學習設置，在其中我們觀察到 $d$ 個未知潛在因子的線性混合，它們遵循線性結構因果模型。最近的研究表明，只要我們至少有 $d$ 個環境，其中每個環境對應於對單個潛在節點（因子）的完美干預，就可以恢復潛在因子以及它們之上的底層結構因果模型，直到置換和縮放。在這個強有力的結果之後，社區面臨的一個關鍵開放問題是放寬這些條件：允許比完美的單節點干預更粗糙的干預，並且允許少於 $d$ 個干預，因為潛在因子 $d$ 的數量可能非常大。在這項工作中，我們準確地考慮了這樣一個設置，在其中我們允許小於 $d$ 個環境的數量，並且還允許非常粗糙的干預，這些干預可以非常粗糙地「改變潛在因子上的整個因果圖」。另一方面，我們放寬了我們希望提取的內容，僅為「在一個或多個環境之間轉移的節點列表」。我們提供了一個令人驚訝的可識別性結果，即在一些非常溫和的標準假設下，確實可以識別出轉移節點的集合。此外，我們的可識別性證明是一個建設性的證明：我們明確地提供了節點成為轉移節點的必要條件和充分條件，並表明我們可以在給定觀察數據的情況下檢查這些條件。我們的演算法非常自然地適用於樣本設置，在這種設置中，我們不是僅提供干預分佈，而是提供來自這些分佈中每一個的樣本數據集。我們在合成實驗和一個有趣的精神測量數據集上證實了我們的結果。代碼可以在 https://github.com/TianyuCodings/iLCS 中找到。</paragraph>

##### **Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs**
2410.24049v1 by Muhammed Saeed, Elgizouli Mohamed, Mukhtar Mohamed, Shaina Raza, Shady Shehata, Muhammad Abdul-Mageed

Large language models (LLMs) are widely used but raise ethical concerns due
to embedded social biases. This study examines LLM biases against Arabs versus
Westerners across eight domains, including women's rights, terrorism, and
anti-Semitism and assesses model resistance to perpetuating these biases. To
this end, we create two datasets: one to evaluate LLM bias toward Arabs versus
Westerners and another to test model safety against prompts that exaggerate
negative traits ("jailbreaks"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA
3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases
displaying negative biases toward Arabs, with LlaMA 3.1-405B being the most
biased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being
an optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except
Claude exhibit attack success rates above 87% in three categories. We also find
Claude 3.5 Sonnet the safest, but it still displays biases in seven of eight
categories. Despite being an optimized version of GPT4, We find GPT-4o to be
more prone to biases and jailbreaks, suggesting optimization flaws. Our
findings underscore the pressing need for more robust bias mitigation
strategies and strengthened security measures in LLMs.

摘要：大型語言模型（LLM）廣泛使用，但由於嵌入的社會偏見而引起道德問題。本研究檢視 LLM 對阿拉伯人與西方人之間的偏見，涵蓋八個領域，包括婦女權利、恐怖主義和反猶太主義，並評估模型對延續這些偏見的抵抗力。為此，我們建立兩個數據集：一個用於評估 LLM 對阿拉伯人與西方人的偏見，另一個用於測試模型對誇大負面特質（「越獄」）提示的安全性。我們評估了六個 LLM——GPT-4、GPT-4o、LlaMA 3.1（8B 和 405B）、Mistral 7B 和 Claude 3.5 Sonnet。我們發現 79% 的案例對阿拉伯人表現出負面偏見，其中 LlaMA 3.1-405B 最為偏頗。我們的越獄測試顯示，儘管 GPT-4o 是經過最佳化的版本，但它是最脆弱的，其次是 LlaMA 3.1-8B 和 Mistral 7B。除了 Claude 之外，所有 LLM 在三類中的攻擊成功率都高於 87%。我們還發現 Claude 3.5 Sonnet 最安全，但它在八類中的七類中仍表現出偏見。儘管 GPT-4o 是 GPT4 的最佳化版本，但我們發現它更容易出現偏見和越獄，這表明存在最佳化缺陷。我們的研究結果強調了對 LLM 中更強大的偏見緩解策略和更嚴格的安全措施的迫切需求。

##### **Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks**
2410.24032v1 by Yingzhe Peng, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Xu Yang, Dongmei Zhang, Saravan Rajmohan, Qi Zhang

The rise of large language models (LLMs) has revolutionized user interactions
with knowledge-based systems, enabling chatbots to synthesize vast amounts of
information and assist with complex, exploratory tasks. However, LLM-based
chatbots often struggle to provide personalized support, particularly when
users start with vague queries or lack sufficient contextual information. This
paper introduces the Collaborative Assistant for Personalized Exploration
(CARE), a system designed to enhance personalization in exploratory tasks by
combining a multi-agent LLM framework with a structured user interface. CARE's
interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling
iterative query refinement and dynamic solution generation. The multi-agent
framework collaborates to identify both explicit and implicit user needs,
delivering tailored, actionable solutions. In a within-subject user study with
22 participants, CARE was consistently preferred over a baseline LLM chatbot,
with users praising its ability to reduce cognitive load, inspire creativity,
and provide more tailored solutions. Our findings highlight CARE's potential to
transform LLM-based systems from passive information retrievers to proactive
partners in personalized problem-solving and exploration.

摘要：大型語言模型 (LLM) 的興起徹底改變了使用者與基於知識的系統互動的方式，讓聊天機器人能夠綜合大量的資訊，並協助進行複雜的探索性任務。然而，基於 LLM 的聊天機器人通常難以提供個人化的支援，特別是在使用者一開始提出的查詢很模糊，或缺乏足夠的脈絡資訊時。本文介紹了個人化探索的協作助理 (CARE)，一個旨在透過結合多重代理 LLM 架構與結構化的使用者介面來增強探索性任務中個人化的系統。CARE 的介面包含聊天面板、解決方案面板和需求面板，可進行反覆的查詢精煉和動態的解決方案產生。多重代理架構協作識別明確和隱含的使用者需求，提供客製化且可行的解決方案。在一個有 22 位參與者的受試者內研究中，CARE 持續獲得比基準 LLM 聊天機器人更好的評價，使用者讚賞其減輕認知負擔、激發創造力，以及提供更客製化解決方案的能力。我們的發現突顯了 CARE 將基於 LLM 的系統從被動的資訊檢索者轉變為個人化問題解決和探索中的主動夥伴的潛力。

##### **A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems using Disparity Maps**
2410.24031v1 by Ariel Larey, Eyal Rond, Omer Achrack

Face recognition technologies are increasingly used in various applications,
yet they are vulnerable to face spoofing attacks. These spoofing attacks often
involve unique 3D structures, such as printed papers or mobile device screens.
Although stereo-depth cameras can detect such attacks effectively, their
high-cost limits their widespread adoption. Conversely, two-sensor systems
without extrinsic calibration offer a cost-effective alternative but are unable
to calculate depth using stereo techniques. In this work, we propose a method
to overcome this challenge by leveraging facial attributes to derive disparity
information and estimate relative depth for anti-spoofing purposes, using
non-calibrated systems. We introduce a multi-modal anti-spoofing model, coined
Disparity Model, that incorporates created disparity maps as a third modality
alongside the two original sensor modalities. We demonstrate the effectiveness
of the Disparity Model in countering various spoof attacks using a
comprehensive dataset collected from the Intel RealSense ID Solution F455. Our
method outperformed existing methods in the literature, achieving an Equal
Error Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False
Positive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the
errors of the best comparison method, respectively. Additionally, we introduce
a model ensemble that addresses 3D spoof attacks as well, achieving an EER of
2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a
state-of-the-art solution for the challenging task of anti-spoofing in
non-calibrated systems that lack depth information.

摘要：人臉辨識技術在各種應用中使用得愈來愈普遍，
但它們容易受到人臉欺騙攻擊。這些欺騙攻擊通常
涉及獨特的 3D 結構，例如列印紙或行動裝置螢幕。
雖然立體深度相機可以有效偵測此類攻擊，但其
高成本限制了廣泛採用。相反地，沒有外在校正的雙感測器系統
提供了一個具有成本效益的替代方案，但無法
使用立體技術計算深度。在這項工作中，我們提出了一種方法
透過利用面部屬性來克服這個挑戰，以衍生視差
資訊，並使用未校正系統估計相對深度以進行防欺騙。我們引進一個多模式防欺騙模型，稱為
視差模型，它將建立的視差圖作為第三個模式，
與兩個原始感測器模式並列。我們展示了視差模型在對抗各種欺騙攻擊中的有效性，
使用從 Intel RealSense ID Solution F455 收集的綜合資料集。我們的
方法優於文獻中的現有方法，在 1% 的誤報率 (FPR) 下，達到 1.71% 的相等錯誤率 (EER) 和 2.77% 的假陰性率 (FNR)。這些誤差分別比最佳比較方法的誤差低 2.45% 和 7.94%。此外，我們引進一個模型合集，它也處理 3D 欺騙攻擊，在 1% 的 FPR 下，達到 2.04% 的 EER 和 3.83% 的 FNR。總體而言，我們的研究為缺乏深度資訊的非校正系統中防欺騙的挑戰性任務提供了最先進的解決方案。

##### **Joint Training for Selective Prediction**
2410.24029v1 by Zhaohui Li, Rebecca J. Passonneau

Classifier models are prevalent in natural language processing (NLP), often
with high accuracy. Yet in real world settings, human-in-the-loop systems can
foster trust in model outputs and even higher performance. Selective Prediction
(SP) methods determine when to adopt a classifier's output versus defer to a
human. Previous SP approaches have addressed how to improve softmax as a
measure of model confidence, or have developed separate confidence estimators.
One previous method involves learning a deferral model based on engineered
features. We introduce a novel joint-training approach that simultaneously
optimizes learned representations used by the classifier module and a learned
deferral policy. Our results on four classification tasks demonstrate that
joint training not only leads to better SP outcomes over two strong baselines,
but also improves the performance of both modules.

摘要：分類器模型在自然語言處理 (NLP) 中很普遍，通常具有很高的準確度。然而在現實世界中，人機互動系統可以培養對模型輸出的信任，甚至可以提高效能。選擇性預測 (SP) 方法決定何時採用分類器的輸出，何時轉向人類。先前的 SP 方法解決了如何改善 softmax 作為模型信心的衡量標準，或開發了獨立的信心估計器。一種先前的做法涉及根據工程特徵學習延後模型。我們提出了一種新穎的聯合訓練方法，該方法同時最佳化分類器模組使用的學習表示，以及學習延後策略。我們在四項分類任務上的結果證明，聯合訓練不僅可以比兩個強大的基準線獲得更好的 SP 結果，還可以改善兩個模組的效能。

##### **AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents**
2410.24024v1 by Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, Yuxiao Dong

Autonomous agents have become increasingly important for interacting with the
real world. Android agents, in particular, have been recently a
frequently-mentioned interaction method. However, existing studies for training
and evaluating Android agents lack systematic research on both open-source and
closed-source models. In this work, we propose AndroidLab as a systematic
Android agent framework. It includes an operation environment with different
modalities, action space, and a reproducible benchmark. It supports both large
language models (LLMs) and multimodal models (LMMs) in the same action space.
AndroidLab benchmark includes predefined Android virtual devices and 138 tasks
across nine apps built on these devices. By using the AndroidLab environment,
we develop an Android Instruction dataset and train six open-source LLMs and
LMMs, lifting the average success rates from 4.59\% to 21.50\% for LLMs and
from 1.93\% to 13.28\% for LMMs. AndroidLab is open-sourced and publicly
available at \url{https://github.com/THUDM/Android-Lab}.

摘要：自主代理已變得越來越重要，用於與現實世界互動。特別是 Android 代理，最近已成為一種常被提及的互動方法。然而，現有的訓練和評估 Android 代理的研究，缺乏對開源和閉源模型的系統性研究。在這項工作中，我們提出 AndroidLab 作為一個系統性的 Android 代理框架。它包含一個具有不同模態、動作空間和可重現基準的運作環境。它在相同的動作空間中支援大型語言模型 (LLM) 和多模態模型 (LMM)。AndroidLab 基準包括預定義的 Android 虛擬裝置，以及建立在這些裝置上的 138 個任務，涵蓋九個應用程式。透過使用 AndroidLab 環境，我們開發了一個 Android 指令資料集，並訓練了六個開源 LLM 和 LMM，將 LLM 的平均成功率從 4.59% 提升到 21.50%，將 LMM 的平均成功率從 1.93% 提升到 13.28%。AndroidLab 是開源的，並公開於 \url{https://github.com/THUDM/Android-Lab}。

##### **Detecting text level intellectual influence with knowledge graph embeddings**
2410.24021v1 by Lucian Li, Eryclis Silva

Introduction: Tracing the spread of ideas and the presence of influence is a
question of special importance across a wide range of disciplines, ranging from
intellectual history to cultural analytics, computational social science, and
the science of science.
  Method: We collect a corpus of open source journal articles, generate
Knowledge Graph representations using the Gemini LLM, and attempt to predict
the existence of citations between sampled pairs of articles using previously
published methods and a novel Graph Neural Network based embedding model.
  Results: We demonstrate that our knowledge graph embedding method is superior
at distinguishing pairs of articles with and without citation. Once trained, it
runs efficiently and can be fine-tuned on specific corpora to suit individual
researcher needs.
  Conclusion(s): This experiment demonstrates that the relationships encoded in
a knowledge graph, especially the types of concepts brought together by
specific relations can encode information capable of revealing intellectual
influence. This suggests that further work in analyzing document level
knowledge graphs to understand latent structures could provide valuable
insights.

摘要：引言：追溯思想的传播和影响的存在是一个在广泛学科中具有特殊重要性的问题，从思想史到文化分析、计算社会科学和科学科学。
方法：我们收集了一批开源期刊文章，使用 Gemini LLM 生成了知识图谱表示，并尝试使用先前发布的方法和基于图神经网络的新型嵌入模型来预测文章样本对之间引用的存在。
结果：我们证明了我们的知识图谱嵌入方法在区分有引用和无引用的文章对方面更胜一筹。一旦经过训练，它就能高效运行，并且可以针对特定语料库进行微调以满足各个研究人员的需求。
结论：这个实验表明，知识图谱中编码的关系，特别是特定关系汇集的概念类型可以编码能够揭示智力影响的信息。这表明进一步分析文档级知识图谱以理解潜在结构的工作可以提供有价值的见解。

##### **Speech is More Than Words: Do Speech-to-Text Translation Systems Leverage Prosody?**
2410.24019v1 by Ioannis Tsiamas, Matthias Sperber, Andrew Finch, Sarthak Garg

The prosody of a spoken utterance, including features like stress, intonation
and rhythm, can significantly affect the underlying semantics, and as a
consequence can also affect its textual translation. Nevertheless, prosody is
rarely studied within the context of speech-to-text translation (S2TT) systems.
In particular, end-to-end (E2E) systems have been proposed as well-suited for
prosody-aware translation because they have direct access to the speech signal
when making translation decisions, but the understanding of whether this is
successful in practice is still limited. A main challenge is the difficulty of
evaluating prosody awareness in translation. To address this challenge, we
introduce an evaluation methodology and a focused benchmark (named ContraProST)
aimed at capturing a wide range of prosodic phenomena. Our methodology uses
large language models and controllable text-to-speech (TTS) to generate
contrastive examples. Through experiments in translating English speech into
German, Spanish, and Japanese, we find that (a) S2TT models possess some
internal representation of prosody, but the prosody signal is often not strong
enough to affect the translations, (b) E2E systems outperform cascades of
speech recognition and text translation systems, confirming their theoretical
advantage in this regard, and (c) certain cascaded systems also capture
prosodic information in the translation, but only to a lesser extent that
depends on the particulars of the transcript's surface form.

摘要：語音發話的韻律，包括重音、語調
和節奏等特徵，會顯著影響基礎語義，並因此也影響其文字翻譯。儘管如此，在語音轉文字 (S2TT) 系統的脈絡中很少研究韻律。
特別是，端對端 (E2E) 系統已被提議用於感知韻律的翻譯，因為它們在做出翻譯決策時可以直接存取語音訊號，但實務上是否成功了解仍然有限。一個主要的挑戰在於評估翻譯中韻律感知的難度。為了應對這個挑戰，我們引進一個評估方法和一個重點基準 (名為 ContraProST)，旨在捕捉廣泛的韻律現象。我們的評估方法使用大型語言模型和可控的文字轉語音 (TTS) 來產生對比範例。透過將英語語音翻譯成德語、西班牙語和日語的實驗，我們發現 (a) S2TT 模型具有一些韻律的內部表徵，但韻律訊號通常不夠強大，無法影響翻譯，(b) E2E 系統優於語音辨識和文字翻譯系統的串聯，證實了它們在這個方面的理論優勢，以及 (c) 某些串聯系統也捕捉翻譯中的韻律資訊，但程度較低，取決於轉錄表面形式的細節。

##### **Assessing the Impact of Packing on Machine Learning-Based Malware Detection and Classification Systems**
2410.24017v1 by Daniel Gibert, Nikolaos Totosis, Constantinos Patsakis, Giulio Zizzo, Quan Le

The proliferation of malware, particularly through the use of packing,
presents a significant challenge to static analysis and signature-based malware
detection techniques. The application of packing to the original executable
code renders extracting meaningful features and signatures challenging. To deal
with the increasing amount of malware in the wild, researchers and anti-malware
companies started harnessing machine learning capabilities with very promising
results. However, little is known about the effects of packing on static
machine learning-based malware detection and classification systems. This work
addresses this gap by investigating the impact of packing on the performance of
static machine learning-based models used for malware detection and
classification, with a particular focus on those using visualisation
techniques. To this end, we present a comprehensive analysis of various packing
techniques and their effects on the performance of machine learning-based
detectors and classifiers. Our findings highlight the limitations of current
static detection and classification systems and underscore the need to be
proactive to effectively counteract the evolving tactics of malware authors.

摘要：惡意軟體的激增，特別是透過使用封包，對靜態分析和基於特徵碼的惡意軟體偵測技術造成重大的挑戰。將封包應用到原始可執行程式碼會讓提取有意義的特徵和特徵碼變得具有挑戰性。為了處理野外越來越多的惡意軟體，研究人員和防惡意軟體公司開始利用機器學習功能，並取得非常有希望的成果。然而，對於封包對基於靜態機器學習的惡意軟體偵測和分類系統的影響，所知甚少。這項工作透過調查封包對用於惡意軟體偵測和分類的基於靜態機器學習模型效能的影響，來解決這個差距，特別是專注於那些使用視覺化技術的模型。為此，我們針對各種封包技術及其對基於機器學習的偵測器和分類器效能的影響，提出全面的分析。我們的研究結果突顯了目前靜態偵測和分類系統的限制，並強調需要積極主動地對抗惡意軟體作者不斷演進的策略。

##### **An Information Criterion for Controlled Disentanglement of Multimodal Data**
2410.23996v1 by Chenyu Wang, Sharut Gupta, Xinyi Zhang, Sana Tonekaboni, Stefanie Jegelka, Tommi Jaakkola, Caroline Uhler

Multimodal representation learning seeks to relate and decompose information
inherent in multiple modalities. By disentangling modality-specific information
from information that is shared across modalities, we can improve
interpretability and robustness and enable downstream tasks such as the
generation of counterfactual outcomes. Separating the two types of information
is challenging since they are often deeply entangled in many real-world
applications. We propose Disentangled Self-Supervised Learning
(DisentangledSSL), a novel self-supervised approach for learning disentangled
representations. We present a comprehensive analysis of the optimality of each
disentangled representation, particularly focusing on the scenario not covered
in prior work where the so-called Minimum Necessary Information (MNI) point is
not attainable. We demonstrate that DisentangledSSL successfully learns shared
and modality-specific features on multiple synthetic and real-world datasets
and consistently outperforms baselines on various downstream tasks, including
prediction tasks for vision-language data, as well as molecule-phenotype
retrieval tasks for biological data.

摘要：多模态表征学习旨在关联和分解内含于多模态中的信息。通过将特定于模态的信息从跨模态共享的信息中解开，我们可以提高可解释性和鲁棒性，并实现下游任务，例如反事实结果的生成。由于在许多实际应用中，这两种类型的信息通常深度纠缠在一起，因此将它们分开具有挑战性。我们提出了解耦的自监督学习 (DisentangledSSL)，这是一种用于学习解耦表征的新型自监督方法。我们对每个解耦表征的最优性进行了全面分析，特别是关注先前工作中未涵盖的所谓最小必要信息 (MNI) 点不可达到的场景。我们证明，DisentangledSSL 成功学习了多个合成和实际数据集上的共享和特定于模态的特征，并且在各种下游任务上持续优于基准，包括视觉语言数据的预测任务以及生物数据的分子表型检索任务。

##### **Localization, balance and affinity: a stronger multifaceted collaborative salient object detector in remote sensing images**
2410.23991v1 by Yakun Xie, Suning Liu, Hongyu Chen, Shaohan Cao, Huixin Zhang, Dejun Feng, Qian Wan, Jun Zhu, Qing Zhu

Despite significant advancements in salient object detection(SOD) in optical
remote sensing images(ORSI), challenges persist due to the intricate edge
structures of ORSIs and the complexity of their contextual relationships.
Current deep learning approaches encounter difficulties in accurately
identifying boundary features and lack efficiency in collaboratively modeling
the foreground and background by leveraging contextual features. To address
these challenges, we propose a stronger multifaceted collaborative salient
object detector in ORSIs, termed LBA-MCNet, which incorporates aspects of
localization, balance, and affinity. The network focuses on accurately locating
targets, balancing detailed features, and modeling image-level global context
information. Specifically, we design the Edge Feature Adaptive Balancing and
Adjusting(EFABA) module for precise edge localization, using edge features to
guide attention to boundaries and preserve spatial details. Moreover, we design
the Global Distributed Affinity Learning(GDAL) module to model global context.
It captures global context by generating an affinity map from the encoders
final layer, ensuring effective modeling of global patterns. Additionally, deep
supervision during deconvolution further enhances feature representation.
Finally, we compared with 28 state of the art approaches on three publicly
available datasets. The results clearly demonstrate the superiority of our
method.

摘要：儘管在光學遙測影像 (ORSI) 中的顯著目標偵測 (SOD) 有顯著進展，但由於 ORSI 的複雜邊緣結構及其脈絡關係的複雜性，挑戰仍然存在。當前的深度學習方法在準確識別邊界特徵方面遇到困難，並且缺乏透過利用脈絡特徵協作建模前景和背景的效率。為了應對這些挑戰，我們在 ORSI 中提出了一個更強大的多面向協作顯著目標偵測器，稱為 LBA-MCNet，它結合了定位、平衡和親和力的面向。該網路專注於準確定位目標、平衡詳細特徵，並建模影像層級的全球脈絡資訊。具體來說，我們設計了邊緣特徵自適應平衡和調整 (EFABA) 模組，用於精確的邊緣定位，使用邊緣特徵來引導對邊界的注意力並保留空間細節。此外，我們設計了全球分佈親和力學習 (GDAL) 模組來建模全球脈絡。它透過從編碼器最終層產生親和力圖來捕捉全球脈絡，確保有效地建模全球模式。此外，反捲積期間的深度監督進一步增強了特徵表徵。最後，我們在三個公開可用的資料集上與 28 種最先進的方法進行了比較。結果清楚地證明了我們方法的優越性。

##### **Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical Scene Segmentation**
2410.23962v1 by Yihang Zhou, Rebecca Towning, Zaid Awad, Stamatia Giannarou

Surgical scene segmentation is essential for enhancing surgical precision,
yet it is frequently compromised by the scarcity and imbalance of available
data. To address these challenges, semantic image synthesis methods based on
generative adversarial networks and diffusion models have been developed.
However, these models often yield non-diverse images and fail to capture small,
critical tissue classes, limiting their effectiveness. In response, we propose
the Class-Aware Semantic Diffusion Model (CASDM), a novel approach which
utilizes segmentation maps as conditions for image synthesis to tackle data
scarcity and imbalance. Novel class-aware mean squared error and class-aware
self-perceptual loss functions have been defined to prioritize critical, less
visible classes, thereby enhancing image quality and relevance. Furthermore, to
our knowledge, we are the first to generate multi-class segmentation maps using
text prompts in a novel fashion to specify their contents. These maps are then
used by CASDM to generate surgical scene images, enhancing datasets for
training and validating segmentation models. Our evaluation, which assesses
both image quality and downstream segmentation performance, demonstrates the
strong effectiveness and generalisability of CASDM in producing realistic
image-map pairs, significantly advancing surgical scene segmentation across
diverse and challenging datasets.

摘要：外科手術場景分割是提升手術精準度的關鍵，
但它經常受到可用資料的稀少性和不平衡所影響。
為了應對這些挑戰，已開發出基於生成對抗網路和擴散模型的語意影像合成方法。
然而，這些模型通常會產生多樣性不足的影像，且無法捕捉小而關鍵的組織類別，限制了它們的效能。
為了應對這個問題，我們提出了類別感知語意擴散模型 (CASDM)，這是一種新穎的方法，
它利用分割圖作為影像合成的條件，以解決資料的稀少性和不平衡問題。
已經定義了新的類別感知平均平方誤差和類別感知自我感知損失函數，
以優先考慮關鍵的、較不顯眼的類別，從而提升影像品質和相關性。
此外，據我們所知，我們是第一個使用文字提示以新穎的方式產生多類別分割圖，
以指定其內容。這些圖形接著由 CASDM 用於產生外科手術場景影像，
以增強資料集，用於訓練和驗證分割模型。
我們的評估同時評估影像品質和下游分割效能，證明了 CASDM 在產生逼真的影像圖對方面的強大效能和泛化能力，
大幅提升了在多樣且具挑戰性的資料集中的外科手術場景分割。

##### **Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language**
2410.23956v1 by Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, Pontus Stenetorp

English, as a very high-resource language, enables the pretraining of
high-quality large language models (LLMs). The same cannot be said for most
other languages, as leading LLMs still underperform for non-English languages,
likely due to a gap in the quality and diversity of the available multilingual
pretraining corpora. In this work, we find that machine-translated text from a
single high-quality source language can contribute significantly to the
pretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality
English web dataset, into French, German, and Spanish, resulting in a final
300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter
model, CuatroLLM, from scratch on this dataset. Across five non-English
reasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art
multilingual models trained using closed data, such as Llama3.2 and Gemma2,
despite using an order of magnitude less data, such as about 6% of the tokens
used for Llama3.2's training. We further demonstrate that with additional
domain-specific pretraining, amounting to less than 1% of TransWeb-Edu,
CuatroLLM surpasses the state of the art in multilingual reasoning. To promote
reproducibility, we release our corpus, models, and training pipeline under
open licenses at hf.co/britllm/CuatroLLM.

摘要：<paragraph>英語作為一種資源非常豐富的語言，能夠預訓練出高品質的大型語言模型 (LLM)。對於大多數其他語言而言，情況並非如此，因為領先的 LLM 對於非英語語言的表現仍然不佳，這可能是由於可用多語言預訓練語料庫的品質和多樣性存在差距。在這項工作中，我們發現來自單一高品質原始語言的機器翻譯文本，可以為多語言 LLM 的預訓練做出重大貢獻。我們將高品質英語網路資料集 FineWeb-Edu 翻譯成法語、德語和西班牙語，最終產生一個 300B-token 的資料集，我們稱之為 TransWeb-Edu，並從頭開始在這個資料集上預訓練一個 1.3B-參數模型 CuatroLLM。在五項非英語推理任務中，我們展示了 CuatroLLM 即使使用比 Llama3.2 少一個數量級的資料（例如 Llama3.2 訓練中使用的 token 數量的約 6%），也能與使用封閉資料訓練的最新多語言模型（例如 Llama3.2 和 Gemma2）相匹配或表現得更好。我們進一步證明，透過額外的特定領域預訓練（少於 TransWeb-Edu 的 1%），CuatroLLM 超越了多語言推理的最新技術。為了促進可複製性，我們在 hf.co/britllm/CuatroLLM 下開放授權釋出我們的語料庫、模型和訓練管道。</paragraph>

##### **Representative Social Choice: From Learning Theory to AI Alignment**
2410.23953v1 by Tianyi Qiu

Social choice theory is the study of preference aggregation across a
population, used both in mechanism design for human agents and in the
democratic alignment of language models. In this study, we propose the
representative social choice framework for the modeling of democratic
representation in collective decisions, where the number of issues and
individuals are too large for mechanisms to consider all preferences directly.
These scenarios are widespread in real-world decision-making processes, such as
jury trials, indirect elections, legislation processes, corporate governance,
and, more recently, language model alignment. In representative social choice,
the population is represented by a finite sample of individual-issue pairs
based on which social choice decisions are made. We show that many of the
deepest questions in representative social choice can be naturally formulated
as statistical learning problems, and prove the generalization properties of
social choice mechanisms using the theory of machine learning. We further
formulate axioms for representative social choice, and prove Arrow-like
impossibility theorems with new combinatorial tools of analysis. Our framework
introduces the representative approach to social choice, opening up research
directions at the intersection of social choice, learning theory, and AI
alignment.

摘要：社會選擇理論是研究人口偏好彙整，用於人類代理機制的設計和語言模型的民主對齊。在本研究中，我們提出代表性社會選擇架構，用於建模集體決策中的民主代表性，其中議題和個人數量太多，無法讓機制直接考慮所有偏好。這些場景廣泛存在於現實世界的決策過程中，例如陪審團審判、間接選舉、立法程序、公司治理，最近還有語言模型對齊。在代表性社會選擇中，人口由有限的個人議題配對樣本表示，基於此做出社會選擇決策。我們展示了代表性社會選擇中的許多最深層問題都可以自然地表述為統計學習問題，並使用機器學習理論證明了社會選擇機制的泛化性質。我們進一步為代表性社會選擇制定公理，並使用新的組合分析工具證明了類似 Arrow 的不可能定理。我們的框架引入了社會選擇的代表性方法，開啟了社會選擇、學習理論和 AI 對齊交叉領域的研究方向。

##### **Towards Fast Algorithms for the Preference Consistency Problem Based on Hierarchical Models**
2410.23934v1 by Anne-Marie George, Nic Wilson, Barry O'Sullivan

In this paper, we construct and compare algorithmic approaches to solve the
Preference Consistency Problem for preference statements based on hierarchical
models. Instances of this problem contain a set of preference statements that
are direct comparisons (strict and non-strict) between some alternatives, and a
set of evaluation functions by which all alternatives can be rated. An instance
is consistent based on hierarchical preference models, if there exists an
hierarchical model on the evaluation functions that induces an order relation
on the alternatives by which all relations given by the preference statements
are satisfied. Deciding if an instance is consistent is known to be NP-complete
for hierarchical models. We develop three approaches to solve this decision
problem. The first involves a Mixed Integer Linear Programming (MILP)
formulation, the other two are recursive algorithms that are based on
properties of the problem by which the search space can be pruned. Our
experiments on synthetic data show that the recursive algorithms are faster
than solving the MILP formulation and that the ratio between the running times
increases extremely quickly.

摘要：在本文中，我們構建和比較演算法方法，以解決基於階層模型的偏好陳述的偏好一致性問題。此問題的實例包含一組偏好陳述，這些陳述是對一些替代方案的直接比較（嚴格和非嚴格），以及一組評估函數，所有替代方案都可以通過這些函數進行評分。如果存在一個基於評估函數的階層模型，並由此誘導出替代方案的順序關係，從而滿足偏好陳述給出的所有關係，則基於階層偏好模型的實例是一致的。眾所周知，判斷一個實例是否一致對於階層模型來說是 NP 完全的。我們開發了三種方法來解決這個決策問題。第一種涉及混合整數線性規劃 (MILP) 公式，另外兩種是遞迴演算法，它們基於可以修剪搜尋空間的問題屬性。我們對合成資料進行的實驗表明，遞迴演算法比求解 MILP 公式快，並且執行時間之間的比率極快地增加。

##### **Language Models can Self-Lengthen to Generate Long Texts**
2410.23933v1 by Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin

Recent advancements in Large Language Models (LLMs) have significantly
enhanced their ability to process long contexts, yet a notable gap remains in
generating long, aligned outputs. This limitation stems from a training gap
where pre-training lacks effective instructions for long-text generation, and
post-training data primarily consists of short query-response pairs. Current
approaches, such as instruction backtranslation and behavior imitation, face
challenges including data quality, copyright issues, and constraints on
proprietary model usage. In this paper, we introduce an innovative iterative
training framework called Self-Lengthen that leverages only the intrinsic
knowledge and skills of LLMs without the need for auxiliary data or proprietary
models. The framework consists of two roles: the Generator and the Extender.
The Generator produces the initial response, which is then split and expanded
by the Extender. This process results in a new, longer response, which is used
to train both the Generator and the Extender iteratively. Through this process,
the models are progressively trained to handle increasingly longer responses.
Experiments on benchmarks and human evaluations show that Self-Lengthen
outperforms existing methods in long-text generation, when applied to top
open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at
https://github.com/QwenLM/Self-Lengthen.

摘要：大型語言模型 (LLM) 的最新進展顯著增強了它們處理長語境的能 力，然而在產生長而對齊的輸出方面仍存在顯著的差距。這種限制 來自於訓練差距，其中預訓練缺乏長文本生成的有效指令，且訓練 後的資料主要包含簡短的查詢回應配對。目前的做法，例如指令回譯 和行為模仿，面臨著資料品質、版權問題，以及對專有模型使用的 限制等挑戰。在本文中，我們介紹了一個創新的反覆訓練架構，稱 為 Self-Lengthen，它僅利用 LLM 的內在知識和技能，而不需要輔助 資料或專有模型。該架構包含兩個角色：產生器和延伸器。產生器 產生初始回應，然後由延伸器分割和擴充。這個過程產生一個新的、 更長的回應，用於反覆訓練產生器和延伸器。透過這個過程，這些 模型逐漸接受訓練以處理越來越長的回應。在基準和人類評估上的 實驗表明，當應用於頂尖的開源 LLM，例如 Qwen2 和 LLaMA3 時， Self-Lengthen 在長文本生成方面優於現有方法。我們的程式碼公開於 https://github.com/QwenLM/Self-Lengthen。

##### **BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments**
2410.23918v1 by Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu

Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.

摘要：大型語言模型 (LLM) 已經革新了許多應用程式，但由於本地裝置的記憶體限制，其部署仍然面臨挑戰。儘管規模定律增強了 LLM 的功能，但主要的瓶頸已從「功能」轉移到「可用性」，強調了有效記憶體管理的需求。傳統的壓縮方法，例如量化，通常需要預定義的壓縮率，以及針對每個設定進行獨立的壓縮程序，這使得在變動的記憶體環境中部署變得複雜。在本文中，我們介紹了 **BitStack**，這是一種新穎的免訓練權重壓縮方法，可以在記憶體使用量和模型效能之間進行兆位元組等級的權衡。透過利用權重分解，BitStack 可以動態調整模型大小，同時將執行記憶體和儲存裝置之間的傳輸降到最低。我們的做法是反覆分解權重矩陣，同時考慮每個參數的重要性，導致每個分解反覆運算中產生大約每參數 1 位元的殘差區塊。這些區塊會依序排列並堆疊在儲存中，作為基本的傳輸單位，根據目前的記憶體可用量載入不同的數量。在各種任務中進行的廣泛實驗證明，儘管提供了細緻的尺寸控制，BitStack 始終符合或超越強大的量化基準，特別是在極端的壓縮率下。據我們所知，這是第一個基於分解的方法，有效地彌合了量化等實用壓縮技術的差距。程式碼可於 https://github.com/xinghaow99/BitStack 取得。

##### **Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling**
2410.23916v1 by Davide Celestini, Daniele Gammelli, Tommaso Guffanti, Simone D'Amico, Elisa Capello, Marco Pavone

Model predictive control (MPC) has established itself as the primary
methodology for constrained control, enabling general-purpose robot autonomy in
diverse real-world scenarios. However, for most problems of interest, MPC
relies on the recursive solution of highly non-convex trajectory optimization
problems, leading to high computational complexity and strong dependency on
initialization. In this work, we present a unified framework to combine the
main strengths of optimization-based and learning-based methods for MPC. Our
approach entails embedding high-capacity, transformer-based neural network
models within the optimization process for trajectory generation, whereby the
transformer provides a near-optimal initial guess, or target plan, to a
non-convex optimization problem. Our experiments, performed in simulation and
the real world onboard a free flyer platform, demonstrate the capabilities of
our framework to improve MPC convergence and runtime. Compared to purely
optimization-based approaches, results show that our approach can improve
trajectory generation performance by up to 75%, reduce the number of solver
iterations by up to 45%, and improve overall MPC runtime by 7x without loss in
performance.

摘要：模型预测控制 (MPC) 已确立为受限控制的主要方法，可在各种现实世界场景中实现通用机器人自主性。然而，对于大多数感兴趣的问题，MPC 依赖于高度非凸轨迹优化问题的递归解，导致高计算复杂度和对初始化的强烈依赖。在这项工作中，我们提出了一个统一的框架来结合基于优化和基于学习的 MPC 方法的主要优势。我们的方法需要在用于轨迹生成优化的过程中嵌入高容量、基于变压器的神经网络模型，其中变压器为非凸优化问题提供近乎最优的初始猜测或目标计划。我们的实验在模拟和现实世界中的自由飞行器平台上进行，展示了我们框架在改善 MPC 收敛和运行时间方面的能力。与基于纯优化的方法相比，结果表明我们的方法可以将轨迹生成性能提高多达 75%，求解器迭代次数减少多达 45%，并且在不损失性能的情况下将整体 MPC 运行时间提高 7 倍。

##### **Efficient Inference and Computation of Optimal Alternatives for Preference Languages Based On Lexicographic Models**
2410.23913v1 by Nic Wilson, Anne-Marie George

We analyse preference inference, through consistency, for general preference
languages based on lexicographic models. We identify a property, which we call
strong compositionality, that applies for many natural kinds of preference
statement, and that allows a greedy algorithm for determining consistency of a
set of preference statements. We also consider different natural definitions of
optimality, and their relations to each other, for general preference languages
based on lexicographic models. Based on our framework, we show that testing
consistency, and thus inference, is polynomial for a specific preference
language LpqT, which allows strict and non-strict statements, comparisons
between outcomes and between partial tuples, both ceteris paribus and strong
statements, and their combination. Computing different kinds of optimal sets is
also shown to be polynomial; this is backed up by our experimental results.

摘要：我們透過一致性來分析偏好推論，以基於字典模型的一般偏好語言為基礎。我們辨識出一個屬性，我們稱之為強組合性，它適用於許多自然類型的偏好陳述，且允許使用貪婪演算法來判斷偏好陳述集合的一致性。我們也考慮基於字典模型的一般偏好語言的不同自然最佳化定義，以及它們彼此的關係。根據我們的架構，我們顯示測試一致性，以及推論，對於允許嚴格和非嚴格陳述、結果之間和部分元組之間的比較、ceteris paribus 和強陳述，以及它們的組合的特定偏好語言 LpqT 來說是多項式的。計算不同類型的最佳集合也被顯示為多項式的；這由我們的實驗結果支持。

##### **RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner**
2410.23912v1 by Fu-Chieh Chang, Yu-Ting Lee, Hui-Ying Shih, Pei-Yuan Wu

The reasoning abilities of large language models (LLMs) have improved with
chain-of-thought (CoT) prompting, allowing models to solve complex tasks in a
stepwise manner. However, training CoT capabilities requires detailed reasoning
data, which is often scarce. The self-taught reasoner (STaR) framework
addresses this by using reinforcement learning to automatically generate
reasoning steps, reducing reliance on human-labeled data. Although STaR and its
variants have demonstrated empirical success, a theoretical foundation
explaining these improvements is lacking. This work provides a theoretical
framework for understanding the effectiveness of reinforcement learning on CoT
reasoning and STaR. Our contributions are: (1) an analysis of policy
improvement, showing why LLM reasoning improves iteratively with STaR; (2)
conditions for convergence to an optimal reasoning policy; (3) an examination
of STaR's robustness, explaining how it can improve reasoning even when
incorporating occasional incorrect steps; and (4) criteria for the quality of
pre-trained models necessary to initiate effective reasoning improvement. This
framework aims to bridge empirical findings with theoretical insights,
advancing reinforcement learning approaches for reasoning in LLMs.

摘要：大型語言模型 (LLM) 的推理能力已透過思維鏈 (CoT) 提示而獲得改善，使模型能夠以逐步方式解決複雜任務。然而，訓練 CoT 能力需要詳細的推理資料，而這些資料通常很稀少。自學推理器 (STaR) 框架透過使用強化學習自動產生推理步驟來解決這個問題，減少對人工標記資料的依賴。儘管 STaR 及其變體已展示出經驗上的成功，但仍缺乏解釋這些改進的理論基礎。本研究提供了一個理論框架，用於了解強化學習在 CoT 推理和 STaR 上的效能。我們的貢獻包括：(1) 政策改進的分析，說明為何 LLM 推理會隨著 STaR 迭代地改進；(2) 收斂到最佳推理政策的條件；(3) 對 STaR 穩健性的檢驗，說明即使在納入偶爾的錯誤步驟時，它如何能夠改善推理；以及 (4) 啟動有效推理改進所需的預訓練模型品質準則。此框架旨在將經驗發現與理論見解聯繫起來，以推動 LLM 中推理的強化學習方法。

##### **Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource languages**
2410.23890v1 by Séamus Lankford, Andy Way

In an evolving landscape of crisis communication, the need for robust and
adaptable Machine Translation (MT) systems is more pressing than ever,
particularly for low-resource languages. This study presents a comprehensive
exploration of leveraging Large Language Models (LLMs) and Multilingual LLMs
(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique
challenges posed by crisis situations where speed, accuracy, and the ability to
handle a wide range of languages are paramount, this research outlines a novel
approach that combines the cutting-edge capabilities of LLMs with fine-tuning
techniques and community-driven corpus development strategies. At the core of
this study is the development and empirical evaluation of MT systems tailored
for two low-resource language pairs, illustrating the process from initial
model selection and fine-tuning through to deployment. Bespoke systems are
developed and modelled on the recent Covid-19 pandemic. The research highlights
the importance of community involvement in creating highly specialised,
crisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM
models. It identifies fine-tuned MLLM models as offering superior performance
compared with their LLM counterparts. A scalable and replicable model for rapid
MT system development in crisis scenarios is outlined. Our approach enhances
the field of humanitarian technology by offering a blueprint for developing
multilingual communication systems during emergencies.

摘要：在危機溝通不斷演變的環境中，對強大且適應性強的機器翻譯 (MT) 系統的需求比以往任何時候都更為迫切，特別是對於低資源語言而言。本研究對利用大型語言模型 (LLM) 和多語言 LLM (MLLM) 來增強此類場景中的 MT 能力進行了全面探討。通過專注於危機情況帶來的獨特挑戰，其中速度、準確性和處理各種語言的能力至關重要，本研究概述了一種新穎的方法，該方法結合了 LLM 的尖端能力與微調技術和社區驅動的語料庫開發策略。本研究的核心是針對兩個低資源語言對開發和實證評估 MT 系統，說明了從初始模型選擇和微調到部署的過程。根據最近的 Covid-19 大流行開發和建模了定制系統。該研究強調了社區參與在創建高度專業化、特定於危機的數據集中的重要性，並將自定義 GPT 與 NLLB 適應的 MLLM 模型進行了比較。它將微調後的 MLLM 模型確定為與其 LLM 對應模型相比提供卓越性能的模型。概述了在危機情況下快速開發 MT 系統的可擴展且可複製的模型。我們的做法通過提供在緊急情況下開發多語言通信系統的藍圖，增強了人道主義技術領域。

##### **Failure Modes of LLMs for Causal Reasoning on Narratives**
2410.23884v1 by Khurram Yamin, Shantanu Gupta, Gaurav R. Ghosal, Zachary C. Lipton, Bryan Wilder

In this work, we investigate the causal reasoning abilities of large language
models (LLMs) through the representative problem of inferring causal
relationships from narratives. We find that even state-of-the-art language
models rely on unreliable shortcuts, both in terms of the narrative
presentation and their parametric knowledge. For example, LLMs tend to
determine causal relationships based on the topological ordering of events
(i.e., earlier events cause later ones), resulting in lower performance
whenever events are not narrated in their exact causal order. Similarly, we
demonstrate that LLMs struggle with long-term causal reasoning and often fail
when the narratives are long and contain many events. Additionally, we show
LLMs appear to rely heavily on their parametric knowledge at the expense of
reasoning over the provided narrative. This degrades their abilities whenever
the narrative opposes parametric knowledge. We extensively validate these
failure modes through carefully controlled synthetic experiments, as well as
evaluations on real-world narratives. Finally, we observe that explicitly
generating a causal graph generally improves performance while naive
chain-of-thought is ineffective. Collectively, our results distill precise
failure modes of current state-of-the-art models and can pave the way for
future techniques to enhance causal reasoning in LLMs.

摘要：在這項工作中，我們透過推論敘述中的因果關係這個代表性問題，來探討大型語言模型 (LLM) 的因果推理能力。我們發現，即使是最先進的語言模型，也會依賴於不可靠的捷徑，無論是在敘述呈現或其參數知識方面。例如，LLM 傾向於根據事件的拓撲順序（即，較早的事件導致較晚的事件）來確定因果關係，當事件未按其確切的因果順序敘述時，就會導致較低的效能。同樣地，我們證明 LLM 難以進行長期因果推理，並且當敘述很長且包含許多事件時，它們通常會失敗。此外，我們表明 LLM 似乎過度依賴其參數知識，而犧牲了對所提供敘述的推理。每當敘述與參數知識相衝突時，這就會降低它們的能力。我們透過仔細控制的合成實驗以及對真實世界敘述的評估，廣泛驗證了這些失敗模式。最後，我們觀察到，明確產生因果圖通常會改善效能，而天真的思考鏈則無效。總的來說，我們的結果精確地提煉了當前最先進模型的失敗模式，並可以為未來增強 LLM 中因果推理的技術鋪路。

##### **'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue**
2410.23883v1 by Rena Gao, Xuetong Wu, Siwen Luo, Caren Han, Feng Liu

Out-of-distribution (OOD) detection in multimodal contexts is essential for
identifying deviations in combined inputs from different modalities,
particularly in applications like open-domain dialogue systems or real-life
dialogue interactions. This paper aims to improve the user experience that
involves multi-round long dialogues by efficiently detecting OOD dialogues and
images. We introduce a novel scoring framework named Dialogue Image Aligning
and Enhancing Framework (DIAEF) that integrates the visual language models with
the novel proposed scores that detect OOD in two key scenarios (1) mismatches
between the dialogue and image input pair and (2) input pairs with previously
unseen labels. Our experimental results, derived from various benchmarks,
demonstrate that integrating image and multi-round dialogue OOD detection is
more effective with previously unseen labels than using either modality
independently. In the presence of mismatched pairs, our proposed score
effectively identifies these mismatches and demonstrates strong robustness in
long dialogues. This approach enhances domain-aware, adaptive conversational
agents and establishes baselines for future studies.

摘要：在多模態環境中進行非分布式 (OOD) 偵測對於識別來自不同模態的組合輸入中的偏差至關重要，特別是在開放式對話系統或真實對話互動等應用中。本文旨在透過有效偵測 OOD 對話和影像，來改善涉及多輪長對話的使用者體驗。我們引入了一個名為對話影像比對與強化架構 (DIAEF) 的新評分架構，它將視覺語言模型與新提出的評分整合在一起，這些評分可以在兩個關鍵場景中偵測 OOD：(1) 對話和影像輸入配對之間的不匹配，以及 (2) 具有先前未見標籤的輸入配對。我們的實驗結果來自各種基準，證明整合影像和多輪對話 OOD 偵測比獨立使用任一模態更有效，特別是在先前未見的標籤中。在存在不匹配配對的情況下，我們提出的評分可以有效識別這些不匹配，並在長對話中展現強大的穩健性。這種方法增強了具有領域感知能力的自適應對話代理，並為未來的研究奠定了基準。

##### **Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs**
2410.23875v1 by Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, Hui Xiong

Large Language Models (LLMs) have shown remarkable reasoning capabilities on
complex tasks, but they still suffer from out-of-date knowledge,
hallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)
can provide explicit and editable knowledge for LLMs to alleviate these issues.
Existing paradigm of KG-augmented LLM manually predefines the breadth of
exploration space and requires flawless navigation in KGs. However, this
paradigm cannot adaptively explore reasoning paths in KGs based on the question
semantics and self-correct erroneous reasoning paths, resulting in a bottleneck
in efficiency and effect. To address these limitations, we propose a novel
self-correcting adaptive planning paradigm for KG-augmented LLM named
Plan-on-Graph (PoG), which first decomposes the question into several
sub-objectives and then repeats the process of adaptively exploring reasoning
paths, updating memory, and reflecting on the need to self-correct erroneous
reasoning paths until arriving at the answer. Specifically, three important
mechanisms of Guidance, Memory, and Reflection are designed to work together,
to guarantee the adaptive breadth of self-correcting planning for graph
reasoning. Finally, extensive experiments on three real-world datasets
demonstrate the effectiveness and efficiency of PoG.

摘要：大型語言模型 (LLM) 在複雜任務中展現出非凡的推理能力，但仍存在知識過時、幻覺和決策不透明的問題。相反地，知識圖譜 (KG) 可以提供明確且可編輯的知識，供 LLM 緩解這些問題。現有的 KG 增強 LLM 典範手動預先定義探索空間的廣度，並需要在 KG 中完美導航。然而，此典範無法根據問題語意自適應地探索 KG 中的推理路徑，並自行糾正錯誤的推理路徑，導致效率和效果的瓶頸。為了解決這些限制，我們提出了一個名為圖形計畫 (PoG) 的 KG 增強 LLM 的新穎自修正自適應規劃典範，它首先將問題分解成幾個子目標，然後重複自適應探索推理路徑、更新記憶體和反思需要自行糾正錯誤推理路徑的過程，直到得出答案。具體來說，指導、記憶和反思這三個重要機制被設計為協同運作，以保證自修正規劃在圖形推理中的自適應廣度。最後，在三個真實世界資料集上的廣泛實驗證明了 PoG 的有效性和效率。

##### **Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models**
2410.23861v1 by Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari

Large Multimodal Models (LMMs) have demonstrated the ability to interact with
humans under real-world conditions by combining Large Language Models (LLMs)
and modality encoders to align multimodal information (visual and auditory)
with text. However, such models raise new safety challenges of whether models
that are safety-aligned on text also exhibit consistent safeguards for
multimodal inputs. Despite recent safety-alignment research on vision LMMs, the
safety of audio LMMs remains under-explored. In this work, we comprehensively
red team the safety of five advanced audio LMMs under three settings: (i)
harmful questions in both audio and text formats, (ii) harmful questions in
text format accompanied by distracting non-speech audio, and (iii)
speech-specific jailbreaks. Our results under these settings demonstrate that
open-source audio LMMs suffer an average attack success rate of 69.14% on
harmful audio questions, and exhibit safety vulnerabilities when distracted
with non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro
achieve an attack success rate of 70.67% on the harmful query benchmark. We
provide insights on what could cause these reported safety-misalignments.
Warning: this paper contains offensive examples.

摘要：大型多模态模型（LMM）已展示出在现实世界条件下与人类互动，方法是结合大型语言模型（LLM）和模态编码器，将多模态信息（视觉和听觉）与文本对齐。然而，这些模型提出了新的安全挑战，即在文本上安全对齐的模型是否也对多模态输入表现出一致的保障措施。尽管最近对视觉 LMM 进行了安全对齐研究，但音频 LMM 的安全性仍未得到充分探索。在这项工作中，我们在三种设置下全面地对五种先进的音频 LMM 的安全性进行了红队测试：(i) 音频和文本格式的有害问题，(ii) 文本格式的有害问题，并伴有分散注意力的非语音音频，以及 (iii) 特定的语音越狱。我们在这些设置下的结果表明，开源音频 LMM 在有害音频问题上的平均攻击成功率为 69.14%，并且在被非语音音频噪音分散注意力时表现出安全漏洞。我们在 Gemini-1.5-Pro 上针对特定语音的越狱在有害查询基准上实现了 70.67% 的攻击成功率。我们提供了对可能导致这些报告的安全错位的原因的见解。警告：本文包含攻击性示例。

##### **Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?**
2410.23856v1 by Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, Bo Han

This paper investigates an under-explored challenge in large language models
(LLMs): chain-of-thought prompting with noisy rationales, which include
irrelevant or inaccurate reasoning thoughts within examples used for in-context
learning. We construct NoRa dataset that is tailored to evaluate the robustness
of reasoning in the presence of noisy rationales. Our findings on NoRa dataset
reveal a prevalent vulnerability to such noise among current LLMs, with
existing robust methods like self-correction and self-consistency showing
limited efficacy. Notably, compared to prompting with clean rationales, base
LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more
drastically by 2.2%-40.4% with inaccurate thoughts.
  Addressing this challenge necessitates external supervision that should be
accessible in practice. Here, we propose the method of contrastive denoising
with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning
capabilities by contrasting noisy rationales with only one clean rationale,
which can be the minimal requirement for denoising-purpose prompting. This
method follows a principle of exploration and exploitation: (1) rephrasing and
selecting rationales in the input space to achieve explicit denoising and (2)
exploring diverse reasoning paths and voting on answers in the output space.
Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy
over the base model and shows significantly stronger denoising capabilities
than baseline methods. The source code is publicly available at:
https://github.com/tmlr-group/NoisyRationales.

摘要：<paragraph>本文探討了大型語言模型 (LLM) 中一個未充分探討的挑戰：帶有雜訊論證的思考鏈提示，其中包括在用於情境中學習的範例中包含不相關或不準確的推理思考。我們構建了 NoRa 資料集，專門用於評估在存在雜訊論證的情況下推理的穩健性。我們在 NoRa 資料集上的發現揭示了當前 LLM 對這種雜訊普遍存在的脆弱性，現有的穩健方法（例如自我修正和自我一致性）顯示出有限的功效。值得注意的是，與使用乾淨論證進行提示相比，基礎 LLM 在有不相干想法的情況下準確度下降了 1.4%-19.8%，而在有錯誤想法的情況下則更大幅度地下降了 2.2%-40.4%。
解決這個挑戰需要外部監督，這在實務上應該是可行的。在此，我們提出帶有雜訊思考鏈的對比式去雜訊方法 (CD-CoT)。它通過將雜訊論證與僅一個乾淨論證進行對比來增強 LLM 的去雜訊推理能力，這可能是去雜訊目的提示的最低要求。此方法遵循探索和利用的原則：(1) 在輸入空間中重新表述和選擇論證以實現明確的去雜訊，以及 (2) 探索不同的推理路徑並對輸出空間中的答案進行投票。根據經驗，CD-CoT 在準確度上比基礎模型平均提高了 17.8%，並且顯示出比基準方法更強大的去雜訊能力。原始碼公開於：
https://github.com/tmlr-group/NoisyRationales。</paragraph>

##### **RAGraph: A General Retrieval-Augmented Graph Learning Framework**
2410.23855v1 by Xinke Jiang, Rihong Qiu, Yongxin Xu, Wentao Zhang, Yichen Zhu, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, Yasha Wang

Graph Neural Networks (GNNs) have become essential in interpreting relational
data across various domains, yet, they often struggle to generalize to unseen
graph data that differs markedly from training instances. In this paper, we
introduce a novel framework called General Retrieval-Augmented Graph Learning
(RAGraph), which brings external graph data into the general graph foundation
model to improve model generalization on unseen scenarios. On the top of our
framework is a toy graph vector library that we established, which captures key
attributes, such as features and task-specific label information. During
inference, the RAGraph adeptly retrieves similar toy graphs based on key
similarities in downstream tasks, integrating the retrieved data to enrich the
learning context via the message-passing prompting mechanism. Our extensive
experimental evaluations demonstrate that RAGraph significantly outperforms
state-of-the-art graph learning methods in multiple tasks such as node
classification, link prediction, and graph classification across both dynamic
and static datasets. Furthermore, extensive testing confirms that RAGraph
consistently maintains high performance without the need for task-specific
fine-tuning, highlighting its adaptability, robustness, and broad
applicability.

摘要：圖形神經網路 (GNN) 已成為詮釋各種領域中關聯資料的重要工具，然而，它們常常難以概括到與訓練實例顯著不同的未見圖形資料。在本文中，我們介紹一個名為通用檢索增強圖形學習 (RAGraph) 的新框架，它將外部圖形資料帶入通用圖形基礎模型，以改善模型在未見場景中的概括性。在我們框架的基礎上，我們建立了一個玩具圖形向量庫，它擷取了關鍵屬性，例如特徵和特定於任務的標籤資訊。在推理期間，RAGraph 會根據下游任務中的關鍵相似性靈巧地檢索類似的玩具圖形，並透過訊息傳遞提示機制整合檢索到的資料來豐富學習情境。我們廣泛的實驗評估證明，RAGraph 在多項任務中顯著優於最先進的圖形學習方法，例如節點分類、連結預測和動態和靜態資料集中的圖形分類。此外，廣泛的測試證實，RAGraph 持續維持高性能，而無需特定於任務的微調，突顯其適應性、穩健性和廣泛的適用性。

##### **Commonsense Knowledge Editing Based on Free-Text in LLMs**
2410.23844v1 by Xiusheng Huang, Yequan Wang, Jun Zhao, Kang Liu

Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.

摘要：知識編輯技術對於維持大型語言模型 (LLM) 的準確性和即時性至關重要。然而，此任務的設定忽略了現實世界中基於自由文本的大量常識知識，其特點是知識範圍廣泛、內容長且非實例化。先前方法（例如 MEMIT）的編輯對象是單一符號或實體，不適合自由文本形式的常識知識。為了應對上述挑戰，我們從兩個角度進行了實驗：知識定位和知識編輯。首先，我們引入了自由文本知識定位 (KLFT) 方法，揭示了常識知識在 MLP 和注意力層以及分散式分佈中分佈相關的挑戰。接下來，我們提出了一個動態感知編輯方法 (DEM)，它利用動態感知模組來定位與常識知識相應的參數位置，並使用知識編輯模組來更新知識。DEM 方法充分探索了 MLP 和注意力層的潛力，並成功編輯了基於自由文本的常識知識。實驗結果表明，DEM 可以實現出色的編輯性能。

##### **Reasons and Solutions for the Decline in Model Performance after Editing**
2410.23843v1 by Xiusheng Huang, Jiaxiang Liu, Yequan Wang, Kang Liu

Knowledge editing technology has received widespread attention for low-cost
updates of incorrect or outdated knowledge in large-scale language models.
However, recent research has found that edited models often exhibit varying
degrees of performance degradation. The reasons behind this phenomenon and
potential solutions have not yet been provided. In order to investigate the
reasons for the performance decline of the edited model and optimize the
editing method, this work explores the underlying reasons from both data and
model perspectives. Specifically, 1) from a data perspective, to clarify the
impact of data on the performance of editing models, this paper first
constructs a Multi-Question Dataset (MQD) to evaluate the impact of different
types of editing data on model performance. The performance of the editing
model is mainly affected by the diversity of editing targets and sequence
length, as determined through experiments. 2) From a model perspective, this
article explores the factors that affect the performance of editing models. The
results indicate a strong correlation between the L1-norm of the editing model
layer and the editing accuracy, and clarify that this is an important factor
leading to the bottleneck of editing performance. Finally, in order to improve
the performance of the editing model, this paper further proposes a Dump for
Sequence (D4S) method, which successfully overcomes the previous editing
bottleneck by reducing the L1-norm of the editing layer, allowing users to
perform multiple effective edits and minimizing model damage. Our code is
available at https://github.com/nlpkeg/D4S.

摘要：知識編輯技術因其低成本更新大型語言模型中不正確或過時的知識而廣受關注。然而，最近的研究發現，經過編輯的模型經常表現出不同程度的效能下降。這種現象背後的原因和潛在解決方案尚未提供。為了探究編輯模型效能下降的原因並最佳化編輯方法，本文從資料和模型的角度探討其背後原因。具體來說，1) 從資料的角度來看，為了釐清資料對編輯模型效能的影響，本文首先建構一個多問題資料集 (MQD) 來評估不同類型的編輯資料對模型效能的影響。實驗結果確定，編輯模型的效能主要受編輯目標的多樣性和序列長度影響。2) 從模型的角度來看，本文探討影響編輯模型效能的因素。結果顯示編輯模型層的 L1-norm 與編輯準確度之間有很強的相關性，並釐清這是導致編輯效能瓶頸的重要因素。最後，為了提升編輯模型的效能，本文進一步提出一個序列轉儲 (D4S) 方法，透過降低編輯層的 L1-norm 成功克服先前的編輯瓶頸，讓使用者能夠執行多個有效的編輯並將模型損害降至最低。我們的程式碼可在 https://github.com/nlpkeg/D4S 取得。

##### **Counterfactual MRI Data Augmentation using Conditional Denoising Diffusion Generative Models**
2410.23835v1 by Pedro Morão, Joao Santinha, Yasna Forghani, Nuno Loução, Pedro Gouveia, Mario A. T. Figueiredo

Deep learning (DL) models in medical imaging face challenges in
generalizability and robustness due to variations in image acquisition
parameters (IAP). In this work, we introduce a novel method using conditional
denoising diffusion generative models (cDDGMs) to generate counterfactual
magnetic resonance (MR) images that simulate different IAP without altering
patient anatomy. We demonstrate that using these counterfactual images for data
augmentation can improve segmentation accuracy, particularly in
out-of-distribution settings, enhancing the overall generalizability and
robustness of DL models across diverse imaging conditions. Our approach shows
promise in addressing domain and covariate shifts in medical imaging. The code
is publicly available at https:
//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation

摘要：深度學習 (DL) 模型在醫學影像中會因影像擷取參數 (IAP) 的變化而面臨可概括性和穩健性的挑戰。在這項工作中，我們提出了一種使用條件式去噪擴散生成模型 (cDDGMs) 的新方法，以產生反事實磁共振 (MR) 影像，模擬不同的 IAP，而不會改變患者的解剖結構。我們證明使用這些反事實影像進行資料擴充可以提高分割準確度，特別是在分佈外設定中，增強 DL 模型在不同影像條件下的整體可概括性和穩健性。我們的做法顯示了解決醫學影像中的領域和協變數轉移的前景。程式碼已公開於 https:
//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation

##### **GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages**
2410.23825v1 by Amir Hossein Kargaran, François Yvon, Hinrich Schütze

The need for large text corpora has increased with the advent of pretrained
language models and, in particular, the discovery of scaling laws for these
models. Most available corpora have sufficient data only for languages with
large dominant communities. However, there is no corpus available that (i)
covers a wide range of minority languages; (ii) is generated by an open-source
reproducible pipeline; and (iii) is rigorously cleaned from noise, making it
trustworthy to use. We present GlotCC, a clean, document-level, 2TB general
domain corpus derived from CommonCrawl, covering more than 1000 languages. We
make GlotCC and the system used to generate it - including the pipeline,
language identification model, and filters - available to the research
community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1,
Pipeline v. 3.0 https://github.com/cisnlp/GlotCC.

摘要：隨著預訓練語言模型的出現，特別是針對這些模型的規模定律的發現，對於大型文字語料庫的需求也隨之增加。大多數現有的語料庫僅對具有龐大主導社群的語言擁有足夠的資料。然而，目前並不存在一個語料庫可以 (i) 涵蓋廣泛的少數語言；(ii) 由可公開取得且可重製的管道產生；以及 (iii) 經過嚴格的雜訊清除，使其值得信賴。我們提出 GlotCC，一個乾淨的、文件層級的、2TB 通用網域語料庫，源自 CommonCrawl，涵蓋超過 1000 種語言。我們將 GlotCC 和用於產生它的系統（包括管道、語言識別模型和篩選器）提供給研究社群。語料庫 v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1，管道 v. 3.0 https://github.com/cisnlp/GlotCC。

##### **Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding**
2410.23822v1 by Jinlong He, Pengfei Li, Gang Liu, Shenjun Zhong

Multimodal Large Language Models (MLLMs) inherit the superior text
understanding capabilities of LLMs and extend these capabilities to multimodal
scenarios. These models achieve excellent results in the general domain of
multimodal tasks. However, in the medical domain, the substantial training
costs and the requirement for extensive medical data pose challenges to the
development of medical MLLMs. Furthermore, due to the free-text form of
answers, tasks such as visual grounding that need to produce output in a
prescribed form become difficult for MLLMs. So far, there have been no medical
MLLMs works in medical visual grounding area. For the medical vision grounding
task, which involves identifying locations in medical images based on short
text descriptions, we propose Parameter-efficient Fine-tuning medical
multimodal large language models for Medcial Visual Grounding (PFMVG). To
validate the performance of the model, we evaluate it on a public benchmark
dataset for medical visual grounding, where it achieves competitive results,
and significantly outperforming GPT-4v. Our code will be open sourced after
peer review.

摘要：多模态大型语言模型 (MLLM) 继承了 LLM 优越的文本理解能力，并将这些能力扩展到多模态场景。这些模型在多模态任务的通用领域中取得了出色的成果。然而，在医学领域，大量的训练成本和对广泛医学数据的需求对医学 MLLM 的发展构成了挑战。此外，由于答案的自由文本形式，需要以规定形式生成输出的任务（例如视觉基础）对于 MLLM 来说变得困难。到目前为止，还没有医学 MLLM 在医学视觉基础领域工作。对于医学视觉基础任务，它涉及根据简短的文本描述识别医学图像中的位置，我们提出了用于医学视觉基础的参数高效微调医学多模态大型语言模型 (PFMVG)。为了验证模型的性能，我们在医学视觉基础的公共基准数据集上对其进行了评估，它取得了有竞争力的结果，并且明显优于 GPT-4v。我们的代码将在同行评审后开源。

##### **Disentangling Disentangled Representations: Towards Improved Latent Units via Diffusion Models**
2410.23820v1 by Youngjun Jun, Jiwoo Park, Kyobin Choo, Tae Eun Choi, Seong Jae Hwang

Disentangled representation learning (DRL) aims to break down observed data
into core intrinsic factors for a profound understanding of the data. In
real-world scenarios, manually defining and labeling these factors are
non-trivial, making unsupervised methods attractive. Recently, there have been
limited explorations of utilizing diffusion models (DMs), which are already
mainstream in generative modeling, for unsupervised DRL. They implement their
own inductive bias to ensure that each latent unit input to the DM expresses
only one distinct factor. In this context, we design Dynamic Gaussian Anchoring
to enforce attribute-separated latent units for more interpretable DRL. This
unconventional inductive bias explicitly delineates the decision boundaries
between attributes while also promoting the independence among latent units.
Additionally, we also propose Skip Dropout technique, which easily modifies the
denoising U-Net to be more DRL-friendly, addressing its uncooperative nature
with the disentangling feature extractor. Our methods, which carefully consider
the latent unit semantics and the distinct DM structure, enhance the
practicality of DM-based disentangled representations, demonstrating
state-of-the-art disentanglement performance on both synthetic and real data,
as well as advantages in downstream tasks.

摘要：糾纏表徵學習 (DRL) 旨在將觀察到的資料分解為核心內在因素，以深入了解資料。在真實世界的情境中，手動定義和標記這些因素並非易事，這使得無監督方法具有吸引力。最近，對於利用擴散模型 (DM) 進行無監督 DRL 的探索有限，而擴散模型在生成式建模中已成為主流。它們實作自己的歸納偏誤，以確保輸入 DM 的每個潛在單元僅表示一個不同的因素。在此背景下，我們設計了動態高斯錨定，以強制屬性分離的潛在單元，以實現更具可解釋性的 DRL。這種非常規的歸納偏誤明確地描述了屬性之間的決策邊界，同時也促進了潛在單元之間的獨立性。此外，我們還提出了跳躍式中斷技術，它可以輕鬆修改去噪 U-Net 以使其更適合 DRL，解決其與解糾纏特徵提取器的非合作性質。我們的這些方法仔細考慮了潛在單元語義和不同的 DM 結構，增強了基於 DM 的糾纏表徵的實用性，在合成資料和真實資料上展示了最先進的糾纏效能，以及在下游任務中的優勢。

##### **The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing Audio Generation Challenge**
2410.23815v1 by Dake Guo, Jixun Yao, Xinfa Zhu, Kangxiang Xia, Zhao Guo, Ziyu Zhang, Yao Wang, Jie Liu, Lei Xie

This paper presents the NPU-HWC system submitted to the ISCSLP 2024
Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC). Our
system consists of two modules: a speech generator for Track 1 and a background
audio generator for Track 2. In Track 1, we employ Single-Codec to tokenize the
speech into discrete tokens and use a language-model-based approach to achieve
zero-shot speaking style cloning. The Single-Codec effectively decouples timbre
and speaking style at the token level, reducing the acoustic modeling burden on
the autoregressive language model. Additionally, we use DSPGAN to upsample 16
kHz mel-spectrograms to high-fidelity 48 kHz waveforms. In Track 2, we propose
a background audio generator based on large language models (LLMs). This system
produces scene-appropriate accompaniment descriptions, synthesizes background
audio with Tango 2, and integrates it with the speech generated by our Track 1
system. Our submission achieves the second place and the first place in Track 1
and Track 2 respectively.

摘要：本文介紹提交給 ISCSLP 2024 靈感與說服力音訊生成挑戰賽 2024 (ICAGC) 的 NPU-HWC 系統。我們的系統包含兩個模組：用於軌道 1 的語音產生器和用於軌道 2 的背景音訊產生器。在軌道 1 中，我們使用 Single-Codec 將語音代換成離散代幣，並使用基於語言模型的方法來達成零次學習的說話風格複製。Single-Codec 在代幣層級有效地將音色和說話風格解耦，減少自迴歸語言模型的聲學建模負擔。此外，我們使用 DSPGAN 將 16 kHz 的梅爾頻譜圖上採樣至高保真 48 kHz 的波形。在軌道 2 中，我們提出一個基於大型語言模型 (LLM) 的背景音訊產生器。此系統產生場景適當的伴奏描述，使用 Tango 2 合成背景音訊，並將其與軌道 1 系統產生的語音整合。我們的提交分別在軌道 1 和軌道 2 獲得第二名和第一名。

##### **Generative AI for Accessible and Inclusive Extended Reality**
2410.23803v1 by Jens Grubert, Junlong Chen, Per Ola Kristensson

Artificial Intelligence-Generated Content (AIGC) has the potential to
transform how people build and interact with virtual environments. Within this
paper, we discuss potential benefits but also challenges that AIGC has for the
creation of inclusive and accessible virtual environments. Specifically, we
touch upon the decreased need for 3D modeling expertise, benefits of
symbolic-only as well as multimodal input, 3D content editing, and 3D model
accessibility as well as foundation model-specific challenges.

摘要：人工智能生成內容 (AIGC) 有潛力轉變人們建構和互動虛擬環境的方式。在本文中，我們討論 AIGC 在建構包容且易於存取的虛擬環境中潛在的好處，但也討論了挑戰。具體來說，我們探討了對 3D 建模專業知識的降低需求、僅符號以及多模式輸入的好處、3D 內容編輯、3D 模型的可存取性以及基礎模型特定的挑戰。

##### **EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching**
2410.23788v1 by Xinwang Chen, Ning Liu, Yichen Zhu, Feifei Feng, Jian Tang

Transformer-based Diffusion Probabilistic Models (DPMs) have shown more
potential than CNN-based DPMs, yet their extensive computational requirements
hinder widespread practical applications. To reduce the computation budget of
transformer-based DPMs, this work proposes the Efficient Diffusion Transformer
(EDT) framework. The framework includes a lightweight-design diffusion model
architecture, and a training-free Attention Modulation Matrix and its
alternation arrangement in EDT inspired by human-like sketching. Additionally,
we propose a token relation-enhanced masking training strategy tailored
explicitly for EDT to augment its token relation learning capability. Our
extensive experiments demonstrate the efficacy of EDT. The EDT framework
reduces training and inference costs and surpasses existing transformer-based
diffusion models in image synthesis performance, thereby achieving a
significant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL
attained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training
phase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the
corresponding sizes of MDTv2. The source code is released at
https://github.com/xinwangChen/EDT.

摘要：<paragraph>基於 Transformer 的擴散機率模型 (DPM) 已展現出比基於 CNN 的 DPM 更大的潛力，但其龐大的運算需求卻阻礙了廣泛的實際應用。為了減少基於 Transformer 的 DPM 的運算預算，本研究提出了高效擴散 Transformer (EDT) 架構。該架構包含一個輕量級設計的擴散模型架構，以及一個無需訓練的注意力調製矩陣，以及受類人素描啟發的 EDT 中的交替排列。此外，我們提出了一種專門為 EDT 量身打造的令牌關係增強遮罩訓練策略，以增強其令牌關係學習能力。我們廣泛的實驗證明了 EDT 的功效。EDT 架構降低了訓練和推理成本，並在影像合成效能方面超越了現有的基於 Transformer 的擴散模型，從而實現了顯著的整體提升。與 MDTv2 的相應大小相比，EDT-S、EDT-B 和 EDT-XL 在訓練階段分別提高了 3.93 倍、2.84 倍和 1.92 倍的速度，在推理階段分別提高了 2.29 倍、2.29 倍和 2.22 倍。原始碼已於 https://github.com/xinwangChen/EDT 發布。</paragraph>

##### **What is Wrong with Perplexity for Long-context Language Modeling?**
2410.23771v1 by Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang

Handling long-context inputs is crucial for large language models (LLMs) in
tasks such as extended conversations, document summarization, and many-shot
in-context learning. While recent approaches have extended the context windows
of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has
proven unreliable for assessing long-context capabilities. The underlying cause
of this limitation has remained unclear. In this work, we provide a
comprehensive explanation for this issue. We find that PPL overlooks key
tokens, which are essential for long-context understanding, by averaging across
all tokens and thereby obscuring the true performance of models in long-context
scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that
focuses on key tokens by employing a long-short context contrastive method to
identify them. Our experiments demonstrate that LongPPL strongly correlates
with performance on various long-context benchmarks (e.g., Pearson correlation
of -0.96), significantly outperforming traditional PPL in predictive accuracy.
Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a
re-weighting strategy for fine-tuning that prioritizes key tokens, leading to
consistent improvements across diverse benchmarks. In summary, these
contributions offer deeper insights into the limitations of PPL and present
effective solutions for accurately evaluating and enhancing the long-context
capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.

摘要：<paragraph>處理長語境輸入對於大型語言模型 (LLM) 至關重要，其任務包括延伸對話、文件摘要和多發學習。儘管近期方法已延伸 LLM 的語境窗口，並採用困惑度 (PPL) 作為標準評量指標，但 PPL 已被證實無法可靠評量長語境能力。這種限制的根本原因仍不明確。在這項工作中，我們對這個問題提供了全面的解釋。我們發現 PPL 會忽略關鍵詞彙，而關鍵詞彙對於長語境理解至關重要，因為 PPL 會平均所有詞彙，因而模糊了模型在長語境情境中的真實效能。為了解決這個問題，我們提出 \textbf{LongPPL}，這是一個新穎的指標，透過採用長短語境對比方法來找出關鍵詞彙，進而專注於這些關鍵詞彙。我們的實驗顯示，LongPPL 與各種長語境基準的效能高度相關（例如，皮爾森相關係數為 -0.96），在預測準確度方面明顯優於傳統 PPL。此外，我們引入了 \textbf{LongCE}（長語境交叉熵）損失，這是一種重新加權策略，用於微調，並優先考慮關鍵詞彙，進而提升各種基準的表現。總而言之，這些貢獻提供了對 PPL 限制的更深入見解，並提出了有效解決方案，用於準確評估和提升 LLM 的長語境能力。程式碼可於 https://github.com/PKU-ML/LongPPL 取得。</paragraph>

##### **The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams**
2410.23769v1 by Yunqi Zhu, Wen Tang, Ying Sun, Xuebing Yang

Recent research on large language models (LLMs) has primarily focused on
their adaptation and application in specialized domains. The application of
LLMs in the medical field is mainly concentrated on tasks such as the
automation of medical report generation, summarization, diagnostic reasoning,
and question-and-answer interactions between doctors and patients. The
challenge of becoming a good teacher is more formidable than that of becoming a
good student, and this study pioneers the application of LLMs in the field of
medical education. In this work, we investigate the extent to which LLMs can
generate medical qualification exam questions and corresponding answers based
on few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic
diseases, we tasked the LLMs with generating open-ended questions and answers
based on a subset of sampled admission reports across eight widely used LLMs,
including ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and
Mistral. Furthermore, we engaged medical experts to manually evaluate these
open-ended questions and answers across multiple dimensions. The study found
that LLMs, after using few-shot prompts, can effectively mimic real-world
medical qualification exam questions, whereas there is room for improvement in
the correctness, evidence-based statements, and professionalism of the
generated answers. Moreover, LLMs also demonstrate a decent level of ability to
correct and rectify reference answers. Given the immense potential of
artificial intelligence in the medical field, the task of generating questions
and answers for medical qualification exams aimed at medical students, interns
and residents can be a significant focus of future research.

摘要：<paragraph>針對大型語言模型 (LLM) 的近期研究主要集中在它們在特定領域的適應和應用。LLM 在醫學領域的應用主要集中在自動化病歷產生、摘要、診斷推理以及醫生與病人之間問答互動等任務。成為一名好老師的挑戰比成為一名好學生更艱鉅，而本研究開創了 LLM 在醫學教育領域的應用。在這項工作中，我們探討了 LLM 在少數提示下產生醫學資格考試題目和對應答案的程度。利用一個真實世界的老年慢性疾病中文數據集，我們讓 LLM 根據八個廣泛使用的 LLM（包括 ERNIE 4、ChatGLM 4、豆包、混元、Spark 4、Qwen、Llama 3 和 Mistral）抽取的入院報告子集產生開放式問題和答案。此外，我們聘請醫學專家手動評估這些開放式問題和答案的多個面向。研究發現，LLM 在使用少數提示後，可以有效模擬真實世界的醫學資格考試題目，而產生的答案在正確性、循證陳述和專業性方面仍有改進空間。此外，LLM 也展現出相當程度更正和修正參考答案的能力。鑑於人工智能在醫學領域的巨大潛力，產生針對醫學生、實習醫生和住院醫生的醫學資格考試題目和答案的任務，可以成為未來研究的重要重點。</paragraph>

##### **Enhancing Chess Reinforcement Learning with Graph Representation**
2410.23753v1 by Tomas Rigaux, Hisashi Kashima

Mastering games is a hard task, as games can be extremely complex, and still
fundamentally different in structure from one another. While the AlphaZero
algorithm has demonstrated an impressive ability to learn the rules and
strategy of a large variety of games, ranging from Go and Chess, to Atari
games, its reliance on extensive computational resources and rigid
Convolutional Neural Network (CNN) architecture limits its adaptability and
scalability. A model trained to play on a $19\times 19$ Go board cannot be used
to play on a smaller $13\times 13$ board, despite the similarity between the
two Go variants. In this paper, we focus on Chess, and explore using a more
generic Graph-based Representation of a game state, rather than a grid-based
one, to introduce a more general architecture based on Graph Neural Networks
(GNN). We also expand the classical Graph Attention Network (GAT) layer to
incorporate edge-features, to naturally provide a generic policy output format.
Our experiments, performed on smaller networks than the initial AlphaZero
paper, show that this new architecture outperforms previous architectures with
a similar number of parameters, being able to increase playing strength an
order of magnitude faster. We also show that the model, when trained on a
smaller $5\times 5$ variant of chess, is able to be quickly fine-tuned to play
on regular $8\times 8$ chess, suggesting that this approach yields promising
generalization abilities. Our code is available at
https://github.com/akulen/AlphaGateau.

摘要：<paragraph>掌握遊戲是一項艱難的任務，因為遊戲可能極其複雜，而且彼此在結構上仍然有根本性的不同。儘管 AlphaZero 演算法已展現出令人印象深刻的能力，可以學習各種遊戲的規則和策略，從圍棋和西洋棋到雅達利遊戲，但它依賴於廣泛的計算資源和嚴格的卷積神經網路 (CNN) 架構，這限制了它的適應性和可擴充性。訓練在 $19\times 19$ 圍棋盤上進行遊戲的模型無法用於在較小的 $13\times 13$ 棋盤上進行遊戲，儘管這兩種圍棋變體之間有相似之處。在本文中，我們專注於西洋棋，並探討使用更通用的基於圖形的遊戲狀態表示，而不是基於格子的表示，以引入基於圖形神經網路 (GNN) 的更通用架構。我們還擴充了傳統的圖形注意力網路 (GAT) 層以納入邊緣特徵，以自然地提供通用的策略輸出格式。我們在比最初的 AlphaZero 論文中更小的網路中進行的實驗表明，這種新架構優於具有類似參數數量的先前架構，能夠以更快的速度提高遊戲強度。我們還表明，當模型在較小的 $5\times 5$ 西洋棋變體上進行訓練時，可以快速微調以在常規 $8\times 8$ 西洋棋上進行遊戲，這表明這種方法產生了有希望的泛化能力。我們的程式碼可在 https://github.com/akulen/AlphaGateau 取得。</paragraph>

##### **LSEAttention is All You Need for Time Series Forecasting**
2410.23749v1 by Dizhen Liang

Transformer-based architectures have achieved remarkable success in natural
language processing and computer vision. However, their performance in
multivariate long-term forecasting often lags behind simpler linear baselines.
Previous studies have identified the traditional attention mechanism as a
significant factor contributing to this limitation. To unlock the full
potential of transformers for multivariate time series forecasting, I introduce
\textbf{LSEAttention}, an approach designed to address entropy collapse and
training instability commonly observed in transformer models. I validate the
effectiveness of LSEAttention across various real-world multivariate time
series datasets, demonstrating that it not only outperforms existing time
series transformer models but also exceeds the performance of some
state-of-the-art models on specific datasets.

摘要：基於 Transformer 的架構在自然語言處理和電腦視覺方面取得顯著的成功。然而，它們在多變數長期預測中的表現通常落後於較簡單的線性基線。先前的研究已將傳統的注意力機制確定為導致此限制的一個重要因素。為了發揮 Transformer 在多變數時間序列預測方面的全部潛力，我引入了 \textbf{LSEAttention}，一種旨在解決在 Transformer 模型中普遍觀察到的熵崩潰和訓練不穩定的方法。我驗證了 LSEAttention 在各種真實世界多變數時間序列資料集中的有效性，證明它不僅優於現有的時間序列 Transformer 模型，而且在特定資料集上也超過了某些最先進模型的表現。

##### **DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios**
2410.23746v1 by Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xinyi Yang, Yulin Yuan, Lidia S. Chao

Detecting text generated by large language models (LLMs) is of great recent
interest. With zero-shot methods like DetectGPT, detection capabilities have
reached impressive levels. However, the reliability of existing detectors in
real-world applications remains underexplored. In this study, we present a new
benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection
techniques still underperformed in this task. We collected human-written
datasets from domains where LLMs are particularly prone to misuse. Using
popular LLMs, we generated data that better aligns with real-world
applications. Unlike previous studies, we employed heuristic rules to create
adversarial LLM-generated text, simulating advanced prompt usages, human
revisions like word substitutions, and writing errors. Our development of
DetectRL reveals the strengths and limitations of current SOTA detectors. More
importantly, we analyzed the potential impact of writing styles, model types,
attack methods, the text lengths, and real-world human writing factors on
different types of detectors. We believe DetectRL could serve as an effective
benchmark for assessing detectors in real-world scenarios, evolving with
advanced attack methods, thus providing more stressful evaluation to drive the
development of more efficient detectors. Data and code are publicly available
at: https://github.com/NLP2CT/DetectRL.

摘要：檢測大型語言模型 (LLM) 所產生的文字是近期備受關注的議題。運用像 DetectGPT 等零次學習方法，檢測能力已達到令人印象深刻的程度。然而，現有檢測器在真實世界應用中的可靠性仍有待探討。在本研究中，我們提出一個新的基準 DetectRL，強調即使是現今最先進 (SOTA) 的檢測技術，在這項任務中仍表現不佳。我們從 LLM 特別容易被濫用的領域收集人手撰寫的資料集。使用廣泛使用的 LLM，我們產生了更符合真實世界應用情況的資料。與先前的研究不同，我們採用啟發式規則來建立對抗性的 LLM 產生的文字，模擬進階提示使用、人類修改（如單字替換）和寫作錯誤。我們開發 DetectRL，揭露了當前 SOTA 檢測器的優點和缺點。更重要的是，我們分析了寫作風格、模型類型、攻擊方法、文字長度和真實世界人類寫作因素對不同類型檢測器的潛在影響。我們相信 DetectRL 可以作為評估真實世界場景中檢測器的有效基準，隨著進階攻擊方法的演進而演進，從而提供更嚴格的評估，以推動更有效率的檢測器開發。資料和程式碼已公開於：https://github.com/NLP2CT/DetectRL。

##### **Syno: Structured Synthesis for Neural Operators**
2410.23745v1 by Yongqi Zhuo, Zhengyuan Su, Chenggang Zhao, Mingyu Gao

The desires for better prediction accuracy and higher execution performance
in neural networks never end. Neural architecture search (NAS) and tensor
compilers are two popular techniques to optimize these two goals, but they are
both limited to composing or optimizing existing manually designed operators
rather than coming up with completely new designs. In this work, we explore the
less studied direction of neural operator synthesis, which aims to
automatically and efficiently discover novel neural operators with better
accuracy and/or speed. We develop an end-to-end framework Syno, to realize
practical neural operator synthesis. Syno makes use of a novel set of
fine-grained primitives defined on tensor dimensions, which ensure various
desired properties to ease model training, and also enable expression
canonicalization techniques to avoid redundant candidates during search. Syno
further adopts a novel guided synthesis flow to obtain valid operators matched
with the specified input/output dimension sizes, and leverages efficient
stochastic tree search algorithms to quickly explore the design space. We
demonstrate that Syno discovers better operators with an average of
$2.06\times$ speedup and less than $1\%$ accuracy loss, even on NAS-optimized
models.

摘要：神经網路中對於更佳預測準確度和更高執行效能的需求永無止盡。神經架構搜尋 (NAS) 和張量編譯器是兩種用於最佳化這兩個目標的熱門技術，但它們都僅限於組成或最佳化現有的手動設計運算子，而不是提出全新設計。在這項工作中，我們探討神經運算子合成這個較少被研究的方向，其目標在於自動且有效地找出具有更好準確度和/或速度的新穎神經運算子。我們開發了一個端對端架構 Syno，用於實現實用的神經運算子合成。Syno 使用一組定義在張量維度上的新穎細粒度基元，確保各種所需的屬性以簡化模型訓練，並啟用表達式正規化技術，以避免在搜尋期間出現重複的候選項。Syno 進一步採用新穎的引導合成流程，以取得與指定的輸入/輸出維度大小相符的有效運算子，並利用有效率的隨機樹狀搜尋演算法來快速探索設計空間。我們證明 Syno 找出更好的運算子，平均速度提升 $2.06\times$，準確度損失小於 $1\%$，即使在經過 NAS 最佳化的模型上也是如此。

##### **What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective**
2410.23743v1 by Ming Li, Yanhong Li, Tianyi Zhou

What makes a difference in the post-training of LLMs? We investigate the
training patterns of different layers in large language models (LLMs), through
the lens of gradient, when training with different responses and initial
models. We are specifically interested in how fast vs. slow thinking affects
the layer-wise gradients, given the recent popularity of training LLMs on
reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our
study, fast thinking without CoT leads to larger gradients and larger
differences of gradients across layers than slow thinking (Detailed CoT),
indicating the learning stability brought by the latter. Moreover, pre-trained
LLMs are less affected by the instability of fast thinking than
instruction-tuned LLMs. Additionally, we study whether the gradient patterns
can reflect the correctness of responses when training different LLMs using
slow vs. fast thinking paths. The results show that the gradients of slow
thinking can distinguish correct and irrelevant reasoning paths. As a
comparison, we conduct similar gradient analyses on non-reasoning knowledge
learning tasks, on which, however, trivially increasing the response length
does not lead to similar behaviors of slow thinking. Our study strengthens
fundamental understandings of LLM training and sheds novel insights on its
efficiency and stability, which pave the way towards building a generalizable
System-2 agent. Our code, data, and gradient statistics can be found in:
https://github.com/MingLiiii/Layer_Gradient.

摘要：在 LLM 的後訓練中，什麼會造成差異？我們透過梯度的視角，在訓練大型語言模型 (LLM) 時，探討不同層的訓練模式，以及在不同的回應和初始模型下訓練時，這些模式的變化。我們特別感興趣的是，在 LLM 於推理路徑（如思維鏈 (CoT) 和過程獎勵）上訓練的近期熱潮下，快速思考與慢思考如何影響逐層梯度。在我們的研究中，沒有 CoT 的快速思考會導致比慢思考（詳細的 CoT）更大的梯度和更大層間梯度差異，這表示後者帶來了學習的穩定性。此外，預先訓練的 LLM 受快速思考的不穩定性影響較小，而非指令調整的 LLM 則受影響較大。此外，我們研究梯度模式是否可以在使用慢思考與快速思考路徑訓練不同的 LLM 時，反映回應的正確性。結果顯示，慢思考的梯度可以區分正確的推理路徑和不相關的推理路徑。作為比較，我們對非推理知識學習任務進行類似的梯度分析，然而，在這些任務中，單純增加回應長度不會導致類似慢思考的行為。我們的研究強化了對 LLM 訓練的基本理解，並對其效率和穩定性提出了新的見解，為建構可概括的 System-2 代理鋪平了道路。我們的程式碼、資料和梯度統計資料可以在以下位置找到：
https://github.com/MingLiiii/Layer_Gradient。

##### **GigaCheck: Detecting LLM-generated Content**
2410.23728v1 by Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Aleksandr Gordeev, Vladimir Dokholyan, Maksim Kuprashevich

With the increasing quality and spread of LLM-based assistants, the amount of
artificially generated content is growing rapidly. In many cases and tasks,
such texts are already indistinguishable from those written by humans, and the
quality of generation tends to only increase. At the same time, detection
methods are developing more slowly, making it challenging to prevent misuse of
these technologies.
  In this work, we investigate the task of generated text detection by
proposing the GigaCheck. Our research explores two approaches: (i)
distinguishing human-written texts from LLM-generated ones, and (ii) detecting
LLM-generated intervals in Human-Machine collaborative texts. For the first
task, our approach utilizes a general-purpose LLM, leveraging its extensive
language abilities to fine-tune efficiently for the downstream task of
LLM-generated text detection, achieving high performance even with limited
data. For the second task, we propose a novel approach that combines computer
vision and natural language processing techniques. Specifically, we use a
fine-tuned general-purpose LLM in conjunction with a DETR-like detection model,
adapted from computer vision, to localize artificially generated intervals
within text.
  We evaluate the GigaCheck on five classification datasets with English texts
and three datasets designed for Human-Machine collaborative text analysis. Our
results demonstrate that GigaCheck outperforms previous methods, even in
out-of-distribution settings, establishing a strong baseline across all
datasets.

摘要：隨著基於大型語言模型 (LLM) 的助理品質提升且廣泛使用，人工產生的內容數量也快速增加。在許多情況和任務中，此類文字已與人類撰寫的文字難以區分，而產生的品質也趨於提升。同時，偵測方法的發展較慢，這使得防止這些技術被誤用變得更具挑戰性。
在此研究中，我們探討了透過提出 GigaCheck 來偵測產生的文字這項任務。我們的研究探討了兩種方法：(i) 區分人類撰寫的文字與 LLM 產生的文字，以及 (ii) 在人類與機器協作的文字中偵測 LLM 產生的區間。對於第一項任務，我們的方法利用了通用 LLM，並運用其廣泛的語言能力來針對 LLM 產生的文字偵測的下游任務進行有效微調，即使資料有限也能達到高性能。對於第二項任務，我們提出了一個結合電腦視覺和自然語言處理技術的新方法。具體來說，我們使用與 DETR 類似的偵測模型，並從電腦視覺中改編出來，來結合微調的通用 LLM，以在文字中定位人工產生的區間。
我們在五個包含英文文字的分類資料集和三個設計用於人類與機器協作文字分析的資料集上評估 GigaCheck。我們的結果證明，GigaCheck 優於先前的所有方法，即使在分布外設定中也是如此，並在所有資料集上建立了強大的基準。

##### **Towards Reliable Alignment: Uncertainty-aware RLHF**
2410.23726v1 by Debangshu Banerjee, Aditya Gopalan

Recent advances in aligning Large Language Models with human preferences have
benefited from larger reward models and better preference data. However, most
of these methodologies rely on the accuracy of the reward model. The reward
models used in Reinforcement Learning with Human Feedback (RLHF) are typically
learned from small datasets using stochastic optimization algorithms, making
them prone to high variability. We illustrate the inconsistencies between
reward models empirically on numerous open-source datasets.
  We theoretically show that the fluctuation of the reward models can be
detrimental to the alignment problem because the derived policies are more
overfitted to the reward model and, hence, are riskier if the reward model
itself is uncertain. We use concentration of measure to motivate an
uncertainty-aware, conservative algorithm for policy optimization. We show that
such policies are more risk-averse in the sense that they are more cautious of
uncertain rewards. We theoretically prove that our proposed methodology has
less risk than the vanilla method.
  We corroborate our theoretical results with experiments based on designing an
ensemble of reward models. We use this ensemble of reward models to align a
language model using our methodology and observe that our empirical findings
match our theoretical predictions.

摘要：近來，將大型語言模型與人類偏好相結合的進展受益於更大的獎勵模型和更好的偏好數據。然而，這些方法大多依賴於獎勵模型的準確性。在人類回饋強化學習 (RLHF) 中使用的獎勵模型通常是使用隨機優化演算法從小型資料集學習而得，這使得它們容易產生高度變異性。我們在許多開源資料集上根據經驗說明了獎勵模型之間的不一致性。
我們在理論上表明，獎勵模型的波動可能對校準問題有害，因為衍生的政策更過度擬合獎勵模型，因此，如果獎勵模型本身不確定，則風險更高。我們使用測度集中來激勵一種具有不確定性感知的保守政策優化演算法。我們表明，此類政策在風險規避方面更為有效，因為它們對不確定的獎勵更為謹慎。我們在理論上證明，我們提出的方法比香草方法風險更低。
我們使用基於設計獎勵模型的集合的實驗來證實我們的理論結果。我們使用這個獎勵模型的集合來使用我們的方法校準語言模型，並觀察到我們的經驗發現與我們的理論預測相符。

##### **OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models**
2410.23703v1 by Junda Wu, Xintong Li, Ruoyu Wang, Yu Xia, Yuxin Xiong, Jianing Wang, Tong Yu, Xiang Chen, Branislav Kveton, Lina Yao, Jingbo Shang, Julian McAuley

Offline evaluation of LLMs is crucial in understanding their capacities,
though current methods remain underexplored in existing research. In this work,
we focus on the offline evaluation of the chain-of-thought capabilities and
show how to optimize LLMs based on the proposed evaluation method. To enable
offline feedback with rich knowledge and reasoning paths, we use knowledge
graphs (e.g., Wikidata5m) to provide feedback on the generated chain of
thoughts. Due to the heterogeneity between LLM reasoning and KG structures,
direct interaction and feedback from KGs on LLM behavior are challenging, as
they require accurate entity linking and grounding of LLM-generated chains of
thought in the KG. To address the above challenge, we propose an offline
chain-of-thought evaluation framework, OCEAN, which models chain-of-thought
reasoning in LLMs as an MDP and evaluate the policy's alignment with KG
preference modeling. To overcome the reasoning heterogeneity and grounding
problems, we leverage on-policy KG exploration and RL to model a KG policy that
generates token-level likelihood distributions for LLM-generated
chain-of-thought reasoning paths, simulating KG reasoning preference. Then we
incorporate the knowledge-graph feedback on the validity and alignment of the
generated reasoning paths into inverse propensity scores and propose KG-IPS
estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS
estimator and provide a lower bound on its variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance chain-of-thought alignment. Our empirical study shows that
OCEAN can be efficiently optimized for generating chain-of-thought reasoning
paths with higher estimated values without affecting LLMs' general abilities in
downstream tasks or their internal knowledge.

摘要：離線評估 LLM 對於了解其容量至關重要，儘管目前的方法在現有研究中仍未得到充分探討。在這項工作中，我們專注於對思維鏈能力的離線評估，並展示如何根據提議的評估方法優化 LLM。為了使用豐富的知識和推理路徑啟用離線回饋，我們使用知識圖譜（例如 Wikidata5m）對生成的思維鏈提供回饋。由於 LLM 推理和 KG 結構之間的異質性，KG 對 LLM 行為的直接互動和回饋具有挑戰性，因為它們需要準確的實體連結和 LLM 生成的思維鏈在 KG 中的基礎。為了應對上述挑戰，我們提出了一個離線思維鏈評估框架 OCEAN，它將 LLM 中的思維鏈推理建模為 MDP，並評估策略與 KG 偏好建模的一致性。為了克服推理異質性和基礎問題，我們利用策略內 KG 探索和 RL 來建模 KG 策略，該策略為 LLM 生成的思維鏈推理路徑生成令牌級別的可能性分布，模擬 KG 推理偏好。然後，我們將知識圖譜對生成推理路徑的有效性和一致性的回饋納入逆傾向得分並提出 KG-IPS 估計器。在理論上，我們證明了所提出的 KG-IPS 估計器的無偏性，並提供了其變異數的下限。透過策略外評估的價值函數，我們可以直接啟用策略外最佳化，以進一步增強思維鏈一致性。我們的實證研究表明，OCEAN 可以針對生成思維鏈推理路徑進行有效最佳化，並提高估計值，而不會影響 LLM 在下游任務或其內部知識中的一般能力。

##### **Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction**
2410.23692v1 by Peizhi Tang, Chuang Yang, Tong Xing, Xiaohang Xu, Renhe Jiang, Kaoru Sezaki

Human mobility prediction plays a critical role in applications such as
disaster response, urban planning, and epidemic forecasting. Traditional
methods often rely on designing crafted, domain-specific models, and typically
focus on short-term predictions, which struggle to generalize across diverse
urban environments. In this study, we introduce Llama-3-8B-Mob, a large
language model fine-tuned with instruction tuning, for long-term citywide
mobility prediction -- in a Q&A manner. We validate our approach using
large-scale human mobility data from four metropolitan areas in Japan, focusing
on predicting individual trajectories over the next 15 days. The results
demonstrate that Llama-3-8B-Mob excels in modeling long-term human mobility --
surpassing the state-of-the-art on multiple prediction metrics. It also
displays strong zero-shot generalization capabilities -- effectively
generalizing to other cities even when fine-tuned only on limited samples from
a single city. Source codes are available at
https://github.com/TANGHULU6/Llama3-8B-Mob.

摘要：人類流動預測在災難應變、都市規劃和疫情預測等應用中扮演著至關重要的角色。傳統方法通常依賴於設計精心製作的特定領域模型，並且通常專注於短期預測，難以概括到不同的城市環境中。在這項研究中，我們引入了 Llama-3-8B-Mob，這是一個使用指令調整進行微調的大型語言模型，用於長期全市流動預測——以問答的方式。我們使用來自日本四個大都市地區的大規模人類流動數據驗證了我們的做法，重點預測未來 15 天的個人軌跡。結果表明，Llama-3-8B-Mob 在建模長期人類流動方面表現出色——在多項預測指標上超越了最先進的技術。它還展現了強大的零次學習概括能力——即使僅根據單一城市中的有限樣本進行微調，也能有效地概括到其他城市。原始碼可在 https://github.com/TANGHULU6/Llama3-8B-Mob 取得。

##### **Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers**
2410.23684v1 by Eugene Jang, Kimin Lee, Jin-Woo Chung, Keuntae Park, Seungwon Shin

Tokenization is a crucial step that bridges human-readable text with
model-readable discrete tokens. However, recent studies have revealed that
tokenizers can be exploited to elicit unwanted model behaviors. In this work,
we investigate incomplete tokens, i.e., undecodable tokens with stray bytes
resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize
that such tokens are heavily reliant on their adjacent tokens and are fragile
when paired with unfamiliar tokens. To demonstrate this vulnerability, we
introduce improbable bigrams: out-of-distribution combinations of incomplete
tokens designed to exploit their dependency. Our experiments show that
improbable bigrams are significantly prone to hallucinatory behaviors.
Surprisingly, alternative tokenizations of the same phrases result in
drastically lower rates of hallucination (93% reduction in Llama3.1). We
caution against the potential vulnerabilities introduced by byte-level BPE
tokenizers, which may impede the development of trustworthy language models.

摘要：符號化是將人類可讀文字與模型可讀離散符號之間的橋樑，這是一個至關重要的步驟。然而，最近的研究表明，符號化器可被利用來引發模型的非預期行為。在這項工作中，我們研究了不完整的符號，也就是說，由於位元組層級位元組對編碼 (BPE) 符號化而產生的具有雜散位元組的不可解碼符號。我們假設此類符號極度依賴其相鄰符號，並且在與不熟悉的符號配對時很脆弱。為了證明此漏洞，我們引入了不可能的雙字組：不完整的符號的離散分佈組合，旨在利用其依賴性。我們的實驗表明，不可能的雙字組極易出現幻覺行為。令人驚訝的是，相同短語的替代符號化導致幻覺發生的機率大幅降低 (Llama3.1 降低 93%)。我們警告，位元組層級 BPE 符號化器引入的潛在漏洞可能會阻礙可信賴語言模型的發展。

##### **Pseudo-Conversation Injection for LLM Goal Hijacking**
2410.23678v1 by Zheng Chen, Buhui Yao

Goal hijacking is a type of adversarial attack on Large Language Models
(LLMs) where the objective is to manipulate the model into producing a
specific, predetermined output, regardless of the user's original input. In
goal hijacking, an attacker typically appends a carefully crafted malicious
suffix to the user's prompt, which coerces the model into ignoring the user's
original input and generating the target response. In this paper, we introduce
a novel goal hijacking attack method called Pseudo-Conversation Injection,
which leverages the weaknesses of LLMs in role identification within
conversation contexts. Specifically, we construct the suffix by fabricating
responses from the LLM to the user's initial prompt, followed by a prompt for a
malicious new task. This leads the model to perceive the initial prompt and
fabricated response as a completed conversation, thereby executing the new,
falsified prompt. Following this approach, we propose three Pseudo-Conversation
construction strategies: Targeted Pseudo-Conversation, Universal
Pseudo-Conversation, and Robust Pseudo-Conversation. These strategies are
designed to achieve effective goal hijacking across various scenarios. Our
experiments, conducted on two mainstream LLM platforms including ChatGPT and
Qwen, demonstrate that our proposed method significantly outperforms existing
approaches in terms of attack effectiveness.

摘要：目標劫持是一種針對大型語言模型 (LLM) 的對抗性攻擊類型，其目標是操縱模型產生特定、預先確定的輸出，而不管使用者的原始輸入為何。在目標劫持中，攻擊者通常會將精心製作的惡意字尾附加到使用者的提示中，這會強迫模型忽略使用者的原始輸入並產生目標回應。在本文中，我們介紹了一種稱為偽對話注入的新型目標劫持攻擊方法，它利用了 LLM 在對話語境中角色識別方面的弱點。具體來說，我們通過製造 LLM 對使用者初始提示的回應來構建字尾，然後提示一個惡意新任務。這會導致模型將初始提示和虛構的回應視為已完成的對話，從而執行新的、虛假的提示。遵循這種方法，我們提出了三種偽對話建構策略：目標偽對話、通用偽對話和強健偽對話。這些策略旨在在各種場景中實現有效的目標劫持。我們的實驗在包括 ChatGPT 和 Qwen 在內的兩個主流 LLM 平台上進行，證明我們提出的方法在攻擊有效性方面顯著優於現有方法。

##### **Provable Benefit of Cutout and CutMix for Feature Learning**
2410.23672v1 by Junsoo Oh, Chulhee Yun

Patch-level data augmentation techniques such as Cutout and CutMix have
demonstrated significant efficacy in enhancing the performance of vision tasks.
However, a comprehensive theoretical understanding of these methods remains
elusive. In this paper, we study two-layer neural networks trained using three
distinct methods: vanilla training without augmentation, Cutout training, and
CutMix training. Our analysis focuses on a feature-noise data model, which
consists of several label-dependent features of varying rarity and
label-independent noises of differing strengths. Our theorems demonstrate that
Cutout training can learn low-frequency features that vanilla training cannot,
while CutMix training can learn even rarer features that Cutout cannot capture.
From this, we establish that CutMix yields the highest test accuracy among the
three. Our novel analysis reveals that CutMix training makes the network learn
all features and noise vectors "evenly" regardless of the rarity and strength,
which provides an interesting insight into understanding patch-level
augmentation.

摘要：修补层级资料扩增技术，例如 Cutout 和 CutMix，已证实能显著提升视觉任务的效能。
然而，对于这些方法的全面理论理解仍然难以捉摸。在本文中，我们研究使用三种不同方法训练的两层神经网络：没有扩增的普通训练、Cutout 训练和 CutMix 训练。我们的分析重点在于特征杂讯资料模型，它包含了若干个稀有度不同的标签相关特征和强度不同的标签无关杂讯。我们的定理证明，Cutout 训练可以学习普通训练无法学习的低频特征，而 CutMix 训练甚至可以学习 Cutout 无法捕捉到的更稀有的特征。由此，我们确定 CutMix 在这三种方法中产生了最高的测试准确率。我们新颖的分析揭示，CutMix 训练使网络学习所有特征和杂讯向量“均匀”，而不管稀有度和强度如何，这为理解修补层级扩增提供了有趣的见解。

##### **Kernel Looping: Eliminating Synchronization Boundaries for Peak Inference Performance**
2410.23668v1 by David Koeplinger, Darshan Gandhi, Pushkar Nandkar, Nathan Sheeley, Matheen Musaddiq, Leon Zhang, Reid Goodbar, Matthew Shaffer, Han Wang, Angela Wang, Mingran Wang, Raghu Prabhakar

Token generation speed is critical to power the next wave of AI inference
applications. GPUs significantly underperform during token generation due to
synchronization overheads at kernel boundaries, utilizing only 21% of their
peak memory bandwidth. While recent dataflow architectures mitigate these
overheads by enabling aggressive fusion of decoder layers into a single kernel,
they too leave performance on the table due to synchronization penalties at
layer boundaries.
  This paper presents kernel looping, a specialized global optimization
technique which exploits an optimization opportunity brought by combining the
unique layer-level fusion possible in modern dataflow architectures with the
repeated layer structure found in language models. Kernel looping eliminates
synchronization costs between consecutive calls to the same kernel by
transforming these calls into a single call to a modified kernel containing a
pipelined outer loop. We evaluate kernel looping on the SambaNova SN40L
Reconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.
Experiments demonstrate that kernel looping speeds up the decode phase of a
wide array of powerful open-source models by up to 2.2$\times$ on SN40L. Kernel
looping allows scaling of decode performance over multiple SN40L sockets,
achieving speedups of up to 2.5$\times$. Finally, kernel looping enables SN40L
to achieve over 90% of peak performance on 8 and 16 sockets and achieve a
speedup of up to 3.7$\times$ over DGX H100. Kernel looping, as well as the
models evaluated in this paper, are deployed in production in a commercial AI
inference cloud.

摘要：權杖產生速度對於推動下波 AI 推論應用至關重要。GPU 在權杖產生期間會因為核心邊界的同步開銷而大幅度表現不佳，僅使用其峰值記憶體頻寬的 21%。儘管最近的資料流程架構透過讓解碼器層積極融合成單一核心來減輕這些開銷，但它們也會因為層邊界的同步懲罰而讓效能停滯不前。
本文提出核心迴圈，這是一種專門的全球最佳化技術，它利用現代資料流程架構中可能出現的獨特層級融合以及語言模型中發現的重複層級結構，來利用最佳化機會。核心迴圈透過將這些呼叫轉換成對包含管線化外層迴圈的修改核心之單一呼叫，消除了對同一個核心連續呼叫之間的同步成本。我們在 SambaNova SN40L 可重新配置資料流程單元 (RDU) 上評估核心迴圈，這是一種用於 AI 的商用資料流程加速器。實驗證明，核心迴圈在 SN40L 上將各種強大開放原始碼模型的解碼階段加速了 2.2 倍。核心迴圈允許在多個 SN40L 插槽上擴展解碼效能，加速速度最高可達 2.5 倍。最後，核心迴圈讓 SN40L 在 8 個和 16 個插槽上達到超過 90% 的峰值效能，並在 DGX H100 上加速最高達 3.7 倍。核心迴圈以及本文評估的模型都已部署在商用 AI 推論雲端的生產環境中。

##### **Morphological Typology in BPE Subword Productivity and Language Modeling**
2410.23656v1 by Iñigo Parra

This study investigates the impact of morphological typology on tokenization
and language modeling performance. We focus on languages with synthetic and
analytical morphological structures and examine their productivity when
tokenized using the byte-pair encoding (BPE) algorithm. We compare the
performance of models trained with similar amounts of data in different
languages. Our experiments reveal that languages with synthetic features
exhibit greater subword regularity and productivity with BPE tokenization and
achieve better results in language modeling tasks. We also observe that the
typological continuum from linguistic theory is reflected in several
experiments. These findings suggest a correlation between morphological
typology and BPE tokenization efficiency.

摘要：本研究探討形態類型對分詞化和語言模型效能的影響。我們專注於具有綜合和分析形態結構的語言，並在使用位元組對編碼 (BPE) 演算法進行分詞化時檢驗其生產力。我們比較以不同語言中相似資料量訓練的模型效能。我們的實驗顯示，具有綜合特徵的語言在 BPE 分詞化中展現出較高的次字詞規則性和生產力，並在語言模型任務中獲得較佳的結果。我們也觀察到，語言學理論中的類型學連續體反映在多項實驗中。這些發現表明形態類型和 BPE 分詞化效率之間存在相關性。

##### **Deep Convolutional Neural Networks on Multiclass Classification of Three-Dimensional Brain Images for Parkinson's Disease Stage Prediction**
2410.23649v1 by Guan-Hua Huang, Wan-Chen Lai, Tai-Been Chen, Chien-Chin Hsu, Huei-Yung Chen, Yi-Chen Wu, Li-Ren Yeh

Parkinson's disease (PD), a degenerative disorder of the central nervous
system, is commonly diagnosed using functional medical imaging techniques such
as single-photon emission computed tomography (SPECT). In this study, we
utilized two SPECT data sets (n = 634 and n = 202) from different hospitals to
develop a model capable of accurately predicting PD stages, a multiclass
classification task. We used the entire three-dimensional (3D) brain images as
input and experimented with various model architectures. Initially, we treated
the 3D images as sequences of two-dimensional (2D) slices and fed them
sequentially into 2D convolutional neural network (CNN) models pretrained on
ImageNet, averaging the outputs to obtain the final predicted stage. We also
applied 3D CNN models pretrained on Kinetics-400. Additionally, we incorporated
an attention mechanism to account for the varying importance of different
slices in the prediction process. To further enhance model efficacy and
robustness, we simultaneously trained the two data sets using weight sharing, a
technique known as cotraining. Our results demonstrated that 2D models
pretrained on ImageNet outperformed 3D models pretrained on Kinetics-400, and
models utilizing the attention mechanism outperformed both 2D and 3D models.
The cotraining technique proved effective in improving model performance when
the cotraining data sets were sufficiently large.

摘要：帕金森氏症 (PD) 是一種中樞神經系統退化性疾病，通常使用功能性醫學影像技術，例如單光子發射斷層掃描 (SPECT) 來診斷。在這項研究中，我們利用來自不同醫院的兩個 SPECT 資料集 (n = 634 和 n = 202) 來開發一個模型，能夠準確預測 PD 分期，這是一個多類別分類任務。我們使用整個三維 (3D) 大腦影像作為輸入，並嘗試使用各種模型架構。最初，我們將 3D 影像視為二維 (2D) 切片的序列，並將它們依序輸入到預先在 ImageNet 上訓練過的 2D 卷積神經網路 (CNN) 模型中，取平均輸出值來取得最終預測的期別。我們也應用預先在 Kinetics-400 上訓練過的 3D CNN 模型。此外，我們納入一個注意力機制，以考量不同切片在預測過程中的重要性差異。為了進一步增強模型的效能和穩健性，我們使用權重共享同時訓練兩個資料集，這是一種稱為共同訓練的技術。我們的結果顯示，預先在 ImageNet 上訓練過的 2D 模型優於預先在 Kinetics-400 上訓練過的 3D 模型，而使用注意力機制的模型則優於 2D 和 3D 模型。當共同訓練的資料集夠大的時候，共同訓練技術已被證明能有效改善模型效能。

##### **On Positional Bias of Faithfulness for Long-form Summarization**
2410.23609v1 by David Wan, Jesse Vig, Mohit Bansal, Shafiq Joty

Large Language Models (LLMs) often exhibit positional bias in long-context
settings, under-attending to information in the middle of inputs. We
investigate the presence of this bias in long-form summarization, its impact on
faithfulness, and various techniques to mitigate this bias. To consistently
evaluate faithfulness, we first compile a benchmark of eight human-annotated
long-form summarization datasets and perform a meta-evaluation of faithfulness
metrics. We show that LLM-based faithfulness metrics, though effective with
full-context inputs, remain sensitive to document order, indicating positional
bias. Analyzing LLM-generated summaries across six datasets, we find a
"U-shaped" trend in faithfulness, where LLMs faithfully summarize the beginning
and end of documents but neglect middle content. Perturbing document order
similarly reveals models are less faithful when important documents are placed
in the middle of the input. We find that this behavior is partly due to
shifting focus with context length: as context increases, summaries become less
faithful, but beyond a certain length, faithfulness improves as the model
focuses on the end. Finally, we experiment with different generation techniques
to reduce positional bias and find that prompting techniques effectively direct
model attention to specific positions, whereas more sophisticated approaches
offer limited improvements. Our data and code are available in
https://github.com/meetdavidwan/longformfact.

摘要：大型語言模型 (LLM) 在長語境設定中經常表現出位置偏差，對輸入中段的資訊關注不足。我們探討這種偏差在長篇摘要中的存在、它對忠實度的影響，以及減輕這種偏差的各種技術。為了持續評估忠實度，我們首先編制了一個基準，包含八個由人工標註的長篇摘要資料集，並對忠實度指標進行元評估。我們表明，基於 LLM 的忠實度指標雖然對全語境輸入有效，但仍然對文件順序敏感，這表示存在位置偏差。在六個資料集分析 LLM 生成的摘要時，我們發現忠實度呈「U 型」趨勢，其中 LLM 忠實地摘要文件的開頭和結尾，但忽略中間內容。以類似的方式擾動文件順序，揭示了當重要文件放在輸入的中間時，模型的忠實度較低。我們發現這種行為部分歸因於隨著語境長度而轉移焦點：隨著語境的增加，摘要的忠實度降低，但超過一定長度後，忠實度會隨著模型專注於結尾而提高。最後，我們嘗試使用不同的生成技術來減少位置偏差，發現提示技術有效地將模型注意力引導至特定位置，而更複雜的方法則提供有限的改進。我們的資料和程式碼可在 https://github.com/meetdavidwan/longformfact 中取得。

##### **Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail Knowledge in LLMs**
2410.23605v1 by Shuyang Yu, Runxue Bao, Parminder Bhatia, Taha Kass-Hout, Jiayu Zhou, Cao Xiao

Large language models (LLMs) can learn vast amounts of knowledge from diverse
domains during pre-training. However, long-tail knowledge from specialized
domains is often scarce and underrepresented, rarely appearing in the models'
memorization. Prior work has shown that in-context learning (ICL) with
retriever augmentation can help LLMs better capture long-tail knowledge,
reducing their reliance on pre-trained data. Despite these advances, we observe
that LLM predictions for long-tail questions remain uncertain to variations in
retrieved samples. To take advantage of the uncertainty in ICL for guiding LLM
predictions toward correct answers on long-tail samples, we propose a
reinforcement learning-based dynamic uncertainty ranking method for ICL that
accounts for the varying impact of each retrieved sample on LLM predictions.
Our approach prioritizes more informative and stable samples while demoting
misleading ones, updating rankings based on the feedback from the LLM w.r.t.
each retrieved sample. To enhance training efficiency and reduce query costs,
we introduce a learnable dynamic ranking threshold, adjusted when the model
encounters negative prediction shifts. Experimental results on various
question-answering datasets from different domains show that our method
outperforms the best baseline by $2.76\%$, with a notable $5.96\%$ boost in
accuracy on long-tail questions that elude zero-shot inference.

摘要：大型語言模型 (LLM) 能在預訓練期間從多元領域學習大量的知識。然而，來自特定領域的長尾知識通常稀少且代表性不足，很少出現在模型的記憶中。先前的研究表明，結合檢索增強的脈絡中學習 (ICL) 能幫助 LLM 更佳擷取長尾知識，減少它們對預訓練資料的依賴。儘管有這些進展，我們觀察到 LLM 對長尾問題的預測仍不確定檢索到的樣本變化。為了利用 ICL 中的不確定性來引導 LLM 預測長尾樣本的正確答案，我們提出了一種基於強化學習的動態不確定性排序方法，用於 ICL，該方法考慮了每個檢索到的樣本對 LLM 預測的影響。我們的做法優先考慮更多資訊且穩定的樣本，同時降級誤導性的樣本，根據 LLM 對每個檢索到的樣本的回饋更新排名。為了提高訓練效率並降低查詢成本，我們引入了一個可學習的動態排名閾值，在模型遇到負面預測轉移時進行調整。在來自不同領域的各種問答資料集上的實驗結果表明，我們的模型比最佳基準高出 $2.76\%$，在迴避零次推論的長尾問題上，準確度顯著提升了 $5.96\%$。

##### **Using Multimodal Deep Neural Networks to Disentangle Language from Visual Aesthetics**
2410.23603v1 by Colin Conwell, Christopher Hamblin, Chelsea Boccagno, David Mayo, Jesse Cummings, Leyla Isik, Andrei Barbu

When we experience a visual stimulus as beautiful, how much of that
experience derives from perceptual computations we cannot describe versus
conceptual knowledge we can readily translate into natural language?
Disentangling perception from language in visually-evoked affective and
aesthetic experiences through behavioral paradigms or neuroimaging is often
empirically intractable. Here, we circumnavigate this challenge by using linear
decoding over the learned representations of unimodal vision, unimodal
language, and multimodal (language-aligned) deep neural network (DNN) models to
predict human beauty ratings of naturalistic images. We show that unimodal
vision models (e.g. SimCLR) account for the vast majority of explainable
variance in these ratings. Language-aligned vision models (e.g. SLIP) yield
small gains relative to unimodal vision. Unimodal language models (e.g. GPT2)
conditioned on visual embeddings to generate captions (via CLIPCap) yield no
further gains. Caption embeddings alone yield less accurate predictions than
image and caption embeddings combined (concatenated). Taken together, these
results suggest that whatever words we may eventually find to describe our
experience of beauty, the ineffable computations of feedforward perception may
provide sufficient foundation for that experience.

摘要：當我們體驗到視覺刺激時，其中有多少是源自於我們無法描述的知覺計算，相較於我們可以輕易翻譯成自然語言的概念知識？透過行為範例或神經影像學，要將視覺誘發的情感和美學體驗中的知覺與語言區分開來，通常在經驗上難以捉摸。在此，我們透過對單模態視覺、單模態語言和多模態（語言對齊）深度神經網路（DNN）模型的學習表徵進行線性解碼，來預測人類對自然影像的美感評分，從而規避了這個挑戰。我們表明，單模態視覺模型（例如 SimCLR）解釋了這些評分中絕大部分的可解釋變異。與單模態視覺相比，語言對齊的視覺模型（例如 SLIP）產生了較小的增益。以視覺嵌入為條件產生字幕的單模態語言模型（例如 GPT2）（透過 CLIPCap）沒有產生進一步的增益。單獨的字幕嵌入產生的預測準確度低於影像和字幕嵌入的組合（串接）。綜合起來，這些結果表明，無論我們最終找到什麼詞來描述我們的審美體驗，前饋感知的難以言喻的計算可能為這種體驗提供了充分的基礎。

##### **How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?**
2410.23594v1 by Weiguo Gao, Ming Li

Real-world data is often assumed to lie within a low-dimensional structure
embedded in high-dimensional space. In practical settings, we observe only a
finite set of samples, forming what we refer to as the sample data subspace. It
serves an essential approximation supporting tasks such as dimensionality
reduction and generation. A major challenge lies in whether generative models
can reliably synthesize samples that stay within this subspace rather than
drifting away from the underlying structure. In this work, we provide
theoretical insights into this challenge by leveraging Flow Matching models,
which transform a simple prior into a complex target distribution via a learned
velocity field. By treating the real data distribution as discrete, we derive
analytical expressions for the optimal velocity field under a Gaussian prior,
showing that generated samples memorize real data points and represent the
sample data subspace exactly. To generalize to suboptimal scenarios, we
introduce the Orthogonal Subspace Decomposition Network (OSDNet), which
systematically decomposes the velocity field into subspace and off-subspace
components. Our analysis shows that the off-subspace component decays, while
the subspace component generalizes within the sample data subspace, ensuring
generated samples preserve both proximity and diversity.

摘要：現實世界中的資料通常假設存在於嵌入在高維空間中的低維結構中。在實際設定中，我們僅觀察到有限的樣本集合，形成我們稱之為樣本資料子空間。它提供了一個必要的近似值，支援降維和生成等任務。一個主要的挑戰在於生成模型是否能可靠地合成停留在這個子空間內的樣本，而不是偏離基礎結構。在這項工作中，我們透過利用流匹配模型提供對這個挑戰的理論見解，它透過學習到的速度場將簡單的先驗轉換為複雜的目標分佈。透過將真實資料分佈視為離散，我們推導出在高斯先驗下最佳速度場的分析表達式，顯示生成的樣本會記住真實資料點，並精確地表示樣本資料子空間。為了推廣到次佳情境，我們引入了正交子空間分解網路 (OSDNet)，它系統性地將速度場分解為子空間和非子空間組成部分。我們的分析顯示，非子空間組成部分會衰減，而子空間組成部分會在樣本資料子空間內推廣，確保生成的樣本同時保留接近性和多樣性。

##### **End-to-End Ontology Learning with Large Language Models**
2410.23584v1 by Andy Lo, Albert Q. Jiang, Wenda Li, Mateja Jamnik

Ontologies are useful for automatic machine processing of domain knowledge as
they represent it in a structured format. Yet, constructing ontologies requires
substantial manual effort. To automate part of this process, large language
models (LLMs) have been applied to solve various subtasks of ontology learning.
However, this partial ontology learning does not capture the interactions
between subtasks. We address this gap by introducing OLLM, a general and
scalable method for building the taxonomic backbone of an ontology from
scratch. Rather than focusing on subtasks, like individual relations between
entities, we model entire subcomponents of the target ontology by finetuning an
LLM with a custom regulariser that reduces overfitting on high-frequency
concepts. We introduce a novel suite of metrics for evaluating the quality of
the generated ontology by measuring its semantic and structural similarity to
the ground truth. In contrast to standard metrics, our metrics use deep
learning techniques to define more robust distance measures between graphs.
Both our quantitative and qualitative results on Wikipedia show that OLLM
outperforms subtask composition methods, producing more semantically accurate
ontologies while maintaining structural integrity. We further demonstrate that
our model can be effectively adapted to new domains, like arXiv, needing only a
small number of training examples. Our source code and datasets are available
at https://github.com/andylolu2/ollm.

摘要：本体对于领域知识的自动机器处理很有用，因为它们以结构化格式表示知识。然而，构建本体需要大量的手动工作。为了自动化这个过程的一部分，大型语言模型（LLM）已被应用于解决本体学习的各种子任务。然而，这种部分本体学习并没有捕捉到子任务之间的交互。我们通过引入 OLLM 来解决这一差距，这是一种从头开始构建本体分类骨架的通用且可扩展的方法。我们没有专注于子任务，例如实体之间的个别关系，而是通过使用自定义正则化器微调 LLM 来对目标本体的整个子组件进行建模，该正则化器减少了对高频概念的过度拟合。我们引入了一套新的指标来评估生成本体的质量，方法是测量它与地面真实值的语义和结构相似性。与标准指标相反，我们的指标使用深度学习技术来定义图之间的更稳健的距离度量。我们在维基百科上的定量和定性结果表明，OLLM 优于子任务组合方法，在保持结构完整性的同时生成语义上更准确的本体。我们进一步证明，我们的模型可以有效地适应新的领域，如 arXiv，只需要少量的训练样本。我们的源代码和数据集可在 https://github.com/andylolu2/ollm 获得。

##### **BioNCERE: Non-Contrastive Enhancement For Relation Extraction In Biomedical Texts**
2410.23583v1 by Farshad Noravesh

State-of-the-art models for relation extraction (RE) in the biomedical domain
consider finetuning BioBERT using classification, but they may suffer from the
anisotropy problem. Contrastive learning methods can reduce this anisotropy
phenomena, and also help to avoid class collapse in any classification problem.
In the present paper, a new training method called biological non-contrastive
relation extraction (BioNCERE) is introduced for relation extraction without
using any named entity labels for training to reduce annotation costs. BioNCERE
uses transfer learning and non-contrastive learning to avoid full or
dimensional collapse as well as bypass overfitting. It resolves RE in three
stages by leveraging transfer learning two times. By freezing the weights
learned in previous stages in the proposed pipeline and by leveraging
non-contrastive learning in the second stage, the model predicts relations
without any knowledge of named entities. Experiments have been done on SemMedDB
that are almost similar to State-of-the-art performance on RE without using the
information of named entities.

摘要：生物医学领域的關係萃取 (RE) 最先進模型考慮使用分類微調 BioBERT，但它們可能會遭受各向異性問題的困擾。對比學習方法可以減少此各向異性現象，也有助於避免任何分類問題中的類別崩潰。在本文中，提出了一種稱為生物非對比關係萃取 (BioNCERE) 的新訓練方法，用於關係萃取，而無需使用任何命名實體標籤進行訓練，以降低註解成本。BioNCERE 使用遷移學習和非對比學習來避免完全或維度崩潰，並繞過過度擬合。它通過兩次利用遷移學習來解決 RE 的三個階段。通過凍結在提議管線中先前階段中學習的權重，並在第二階段利用非對比學習，該模型在沒有任何命名實體知識的情況下預測關係。已在 SemMedDB 上進行了實驗，其與不使用命名實體資訊的 RE 最先進效能幾乎相似。

##### **Automating Quantum Software Maintenance: Flakiness Detection and Root Cause Analysis**
2410.23578v1 by Janakan Sivaloganathan, Ainaz Jamshidi, Andriy Miranskyy, Lei Zhang

Flaky tests, which pass or fail inconsistently without code changes, are a
major challenge in software engineering in general and in quantum software
engineering in particular due to their complexity and probabilistic nature,
leading to hidden issues and wasted developer effort.
  We aim to create an automated framework to detect flaky tests in quantum
software and an extended dataset of quantum flaky tests, overcoming the
limitations of manual methods.
  Building on prior manual analysis of 14 quantum software repositories, we
expanded the dataset and automated flaky test detection using transformers and
cosine similarity. We conducted experiments with Large Language Models (LLMs)
from the OpenAI GPT and Meta LLaMA families to assess their ability to detect
and classify flaky tests from code and issue descriptions.
  Embedding transformers proved effective: we identified 25 new flaky tests,
expanding the dataset by 54%. Top LLMs achieved an F1-score of 0.8871 for
flakiness detection but only 0.5839 for root cause identification.
  We introduced an automated flaky test detection framework using machine
learning, showing promising results but highlighting the need for improved root
cause detection and classification in large quantum codebases. Future work will
focus on improving detection techniques and developing automatic flaky test
fixes.

摘要：不穩定的測試，在沒有代碼變更的情況下通過或失敗，是軟體工程中的一大挑戰，特別是在量子軟體工程中，由於其複雜性和機率性質，導致隱藏的問題和浪費開發人員的精力。
我們旨在為量子軟體中的不穩定測試創建一個自動化框架和一個擴展的不穩定量子測試資料集，克服手動方法的限制。
建立在先前對 14 個量子軟體儲存庫的手動分析之上，我們擴展了資料集，並使用Transformer和餘弦相似度自動化了不穩定測試的偵測。我們使用 OpenAI GPT 和 Meta LLaMA 家族的大語言模型 (LLM) 進行了實驗，以評估它們從代碼和問題描述中偵測和分類不穩定測試的能力。
嵌入Transformer被證明是有效的：我們識別出 25 個新的不穩定測試，將資料集擴展了 54%。頂級 LLM 在不穩定性偵測中達到了 0.8871 的 F1 分數，但在根本原因識別中僅達到了 0.5839。
我們使用機器學習引入了一個自動化不穩定測試偵測框架，展示了有希望的結果，但強調了在大型量子程式碼庫中改進根本原因偵測和分類的必要性。未來的研究將重點放在改進偵測技術和開發自動化不穩定測試修復上。

##### **Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models**
2410.23558v1 by Yiqi Yang, Hongye Fu

In this report, we propose a novel black-box jailbreak attacking framework
that incorporates various LLM-as-Attacker methods to deliver transferable and
powerful jailbreak attacks. Our method is designed based on three key
observations from existing jailbreaking studies and practices. First, we
consider an ensemble approach should be more effective in exposing the
vulnerabilities of an aligned LLM compared to individual attacks. Second,
different malicious instructions inherently vary in their jailbreaking
difficulty, necessitating differentiated treatment to ensure more efficient
attacks. Finally, the semantic coherence of a malicious instruction is crucial
for triggering the defenses of an aligned LLM; therefore, it must be carefully
disrupted to manipulate its embedding representation, thereby increasing the
jailbreak success rate. We validated our approach by participating in the
Competition for LLM and Agent Safety 2024, where our team achieved top
performance in the Jailbreaking Attack Track.

摘要：在報告中，我們提出了一個新穎的黑盒子越獄攻擊框架，它結合了各種 LLM 作為攻擊者的方法，以提供可轉移且強大的越獄攻擊。我們的攻擊方法是根據現有越獄研究和實務中的三個關鍵觀察而設計的。首先，我們認為一個整體的方法應該比個別攻擊更能有效地揭露一個對齊的 LLM 的漏洞。其次，不同的惡意指令在越獄難度上固有地有所不同，需要區別對待以確保更有效的攻擊。最後，一個惡意指令的語義一致性對於觸發一個對齊的 LLM 的防禦至關重要；因此，必須小心地破壞它，以操縱它的嵌入表示，從而增加越獄成功率。我們透過參與 2024 年 LLM 和 Agent 安全競賽來驗證我們的攻擊方法，我們的團隊在越獄攻擊軌道中取得了最佳表現。

##### **From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents**
2410.23555v1 by Nalin Tiwary, Vardhan Dongre, Sanil Arun Chawla, Ashwin Lamani, Dilek Hakkani-Tür

Recent advancements in Large Language Model (LLM)-based frameworks have
extended their capabilities to complex real-world applications, such as
interactive web navigation. These systems, driven by user commands, navigate
web browsers to complete tasks through multi-turn dialogues, offering both
innovative opportunities and significant challenges. Despite the introduction
of benchmarks for conversational web navigation, a detailed understanding of
the key contextual components that influence the performance of these agents
remains elusive. This study aims to fill this gap by analyzing the various
contextual elements crucial to the functioning of web navigation agents. We
investigate the optimization of context management, focusing on the influence
of interaction history and web page representation. Our work highlights
improved agent performance across out-of-distribution scenarios, including
unseen websites, categories, and geographic locations through effective context
management. These findings provide insights into the design and optimization of
LLM-based agents, enabling more accurate and effective web navigation in
real-world applications.

摘要：大型語言模型 (LLM) 框架的最新進展已將其功能擴展到複雜的現實世界應用程式，例如互動式網頁瀏覽。這些系統由使用者指令驅動，透過多輪對話瀏覽網頁瀏覽器以完成任務，既提供了創新的機會，也帶來了重大的挑戰。儘管引入了對話式網頁瀏覽的基準，但對於影響這些代理程式效能的主要脈絡組成的詳細了解仍然難以捉摸。本研究旨在透過分析對網頁瀏覽代理程式運作至關重要的各種脈絡元素來填補這一空白。我們探討了情境管理的最佳化，重點關注互動歷程和網頁呈現的影響。我們的研究重點說明了在各種分佈外情境中，包括未見過的網站、類別和地理位置，透過有效的情境管理改善代理程式效能。這些發現提供了對 LLM 基礎代理程式的設計和最佳化的見解，讓現實世界應用程式中的網頁瀏覽更準確、更有效。

##### **ALISE: Accelerating Large Language Model Serving with Speculative Scheduling**
2410.23537v1 by Youpeng Zhao, Jun Wang

Large Language Models (LLMs) represent a revolutionary advancement in the
contemporary landscape of artificial general intelligence (AGI). As exemplified
by ChatGPT, LLM-based applications necessitate minimal response latency and
maximal throughput for inference serving. However, due to the unpredictability
of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed
by current LLM serving systems suffers from head-of-line (HoL) blocking issues
and long job response times.
  In this paper, we propose a new efficient LLM inference serving framework,
named ALISE. The key design paradigm of ALISE is to leverage a novel
speculative scheduler by estimating the execution time for each job and
exploiting such prior knowledge to assign appropriate job priority orders, thus
minimizing potential queuing delays for heterogeneous workloads. Furthermore,
to mitigate the memory overhead of the intermediate key-value (KV) cache, we
employ a priority-based adaptive memory management protocol and
quantization-based compression techniques. Evaluations demonstrate that in
comparison to the state-of-the-art solution vLLM, ALISE improves the throughput
of inference serving by up to 1.8x and 2.1x under the same latency constraint
on the Alpaca and ShareGPT datasets, respectively.

摘要：大型語言模型 (LLM) 代表了當代人工通用智能 (AGI) 領域的一場革命性進展。以 ChatGPT 為例，基於 LLM 的應用需要最小的回應延遲和最大的推論服務傳輸量。然而，由於 LLM 執行不可預測，當前 LLM 服務系統採用的先到先服務 (FCFS) 排程政策會出現隊頭 (HoL) 阻塞問題和長的作業回應時間。
在本文中，我們提出一個新的高效 LLM 推論服務架構，名為 ALISE。ALISE 的主要設計範例是利用一個新的推測排程器，透過估計每個作業的執行時間，並利用此類先驗知識來指定適當的作業優先順序，從而最大程度地減少異質工作負載的潛在排隊延遲。此外，為了減輕中間鍵值 (KV) 快取的記憶體開銷，我們採用基於優先順序的自適應記憶體管理協定和基於量化的壓縮技術。評估表明，與最先進的解決方案 vLLM 相比，ALISE 在 Alpaca 和 ShareGPT 資料集上分別在相同的延遲限制下將推論服務的傳輸量提升了 1.8 倍和 2.1 倍。

##### **Simulating User Agents for Embodied Conversational-AI**
2410.23535v1 by Daniel Philipov, Vardhan Dongre, Gokhan Tur, Dilek Hakkani-Tür

Embodied agents designed to assist users with tasks must engage in natural
language interactions, interpret instructions, execute actions, and communicate
effectively to resolve issues. However, collecting large-scale, diverse
datasets of situated human-robot dialogues to train and evaluate such agents is
expensive, labor-intensive, and time-consuming. To address this challenge, we
propose building a large language model (LLM)-based user agent that can
simulate user behavior during interactions with an embodied agent in a virtual
environment. Given a user goal (e.g., make breakfast), at each time step, the
user agent may observe" the robot actions or speak" to either intervene with
the robot or answer questions. Such a user agent assists in improving the
scalability and efficiency of embodied dialogues dataset generation and is
critical for enhancing and evaluating the robot's interaction and task
completion ability, as well as for research in reinforcement learning using AI
feedback. We evaluate our user agent's ability to generate human-like behaviors
by comparing its simulated dialogues with the TEACh dataset. We perform three
experiments: zero-shot prompting to predict dialogue acts, few-shot prompting,
and fine-tuning on the TEACh training subset. Results show the LLM-based user
agent achieves an F-measure of 42% with zero-shot prompting and 43.4% with
few-shot prompting in mimicking human speaking behavior. Through fine-tuning,
performance in deciding when to speak remained stable, while deciding what to
say improved from 51.1% to 62.5%. These findings showcase the feasibility of
the proposed approach for assessing and enhancing the effectiveness of robot
task completion through natural language communication.

摘要：<paragraph>旨在協助使用者執行任務的具身代理必須參與自然語言互動、解讀指示、執行動作，並有效溝通以解決問題。然而，收集大量、多元的具體人類機器人對話資料集來訓練和評估此類代理既昂貴又費時費力。為了應對這項挑戰，我們提議建立一個大型語言模型 (LLM) 基於使用者代理，它可以在虛擬環境中模擬使用者與具身代理互動時的行為。給定一個使用者目標（例如，準備早餐），在每個時間步驟中，使用者代理可能會觀察機器人的動作或「說話」來干預機器人或回答問題。這樣的使用者代理有助於提高具身對話資料集生成的擴充性和效率，對於增強和評估機器人的互動和任務完成能力以及使用 AI 回饋進行強化學習的研究至關重要。我們通過將其模擬對話與 TEACh 資料集進行比較來評估我們的使用者代理生成類似人類行為的能力。我們進行了三個實驗：零次學習提示以預測對話行為、少次學習提示以及在 TEACh 訓練子集上進行微調。結果表明，基於 LLM 的使用者代理在零次學習提示中實現了 42% 的 F 值，在少次學習提示中實現了 43.4%，模擬了人類的說話行為。通過微調，決定何時說話的性能保持穩定，而決定說什麼的性能從 51.1% 提高到 62.5%。這些發現展示了所提出的方法在通過自然語言溝通評估和提高機器人任務完成的有效性的可行性。</paragraph>

##### **There and Back Again: On the relation between noises, images, and their inversions in diffusion models**
2410.23530v1 by Łukasz Staniszewski, Łukasz Kuciński, Kamil Deja

Denoising Diffusion Probabilistic Models (DDPMs) achieve state-of-the-art
performance in synthesizing new images from random noise, but they lack
meaningful latent space that encodes data into features. Recent DDPM-based
editing techniques try to mitigate this issue by inverting images back to their
approximated staring noise. In this work, we study the relation between the
initial Gaussian noise, the samples generated from it, and their corresponding
latent encodings obtained through the inversion procedure. First, we interpret
their spatial distance relations to show the inaccuracy of the DDIM inversion
technique by localizing latent representations manifold between the initial
noise and generated samples. Then, we demonstrate the peculiar relation between
initial Gaussian noise and its corresponding generations during diffusion
training, showing that the high-level features of generated images stabilize
rapidly, keeping the spatial distance relationship between noises and
generations consistent throughout the training.

摘要：去噪擴散概率模型 (DDPM) 在從隨機雜訊中合成新影像方面取得最先進的效能，但它們缺乏將資料編碼成特徵的有意義潛在空間。最近基於 DDPM 的編輯技術嘗試透過將影像反轉回其近似起始雜訊來減輕這個問題。在這項工作中，我們研究了初始高斯雜訊、從中產生的樣本，以及透過反轉程序取得的對應潛在編碼之間的關係。首先，我們詮釋它們的空間距離關係，以顯示 DDIM 反轉技術的不準確性，方法是在初始雜訊和產生的樣本之間定位潛在表示流形。然後，我們展示了在擴散訓練期間初始高斯雜訊及其對應生成之間的特殊關係，顯示生成影像的高階特徵會快速穩定，使雜訊和生成之間的空間距離關係在整個訓練過程中保持一致。

##### **Large Language Models for Patient Comments Multi-Label Classification**
2410.23528v1 by Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin

Patient experience and care quality are crucial for a hospital's
sustainability and reputation. The analysis of patient feedback offers valuable
insight into patient satisfaction and outcomes. However, the unstructured
nature of these comments poses challenges for traditional machine learning
methods following a supervised learning paradigm. This is due to the
unavailability of labeled data and the nuances these texts encompass. This
research explores leveraging Large Language Models (LLMs) in conducting
Multi-label Text Classification (MLTC) of inpatient comments shared after a
stay in the hospital. GPT-4o-Turbo was leveraged to conduct the classification.
However, given the sensitive nature of patients' comments, a security layer is
introduced before feeding the data to the LLM through a Protected Health
Information (PHI) detection framework, which ensures patients'
de-identification. Additionally, using the prompt engineering framework,
zero-shot learning, in-context learning, and chain-of-thought prompting were
experimented with. Results demonstrate that GPT-4o-Turbo, whether following a
zero-shot or few-shot setting, outperforms traditional methods and Pre-trained
Language Models (PLMs) and achieves the highest overall performance with an
F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the
few-shot learning results. Subsequently, the results' association with other
patient experience structured variables (e.g., rating) was conducted. The study
enhances MLTC through the application of LLMs, offering healthcare
practitioners an efficient method to gain deeper insights into patient feedback
and deliver prompt, appropriate responses.

摘要：病患體驗和照護品質對於醫院的永續經營和聲譽至關重要。分析病患回饋意見能提供寶貴的見解，了解病患滿意度和治療結果。然而，這些評論的非結構化特性對遵循監督式學習典範的傳統機器學習方法構成挑戰。這是因為缺乏標籤資料，而且這些文字包含許多細微差別。本研究探討利用大型語言模型 (LLM) 進行住院病患在出院後分享的評論的多標籤文字分類 (MLTC)。利用 GPT-4o-Turbo 進行分類。然而，鑑於病患評論的敏感性質，在透過受保護健康資訊 (PHI) 偵測架構將資料提供給 LLM 之前，會加入一層安全防護，以確保病患的去識別化。此外，還實驗了提示工程架構、零次學習、情境中學習和思考鏈提示。結果顯示，無論是遵循零次或少次設定，GPT-4o-Turbo 都優於傳統方法和預訓練語言模型 (PLM)，並以 76.12% 的 F1 分數和 73.61% 的加權 F1 分數達到最高的整體效能，緊接在後的則是少次學習結果。隨後，進行了結果與其他病患體驗結構化變數（例如評分）的關聯性分析。本研究透過應用 LLM 來強化 MLTC，為醫療從業人員提供一種有效的方法，以深入了解病患回饋意見並提供迅速且適當的回應。

##### **LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models**
2410.23526v1 by Hieu Tran, Junda Wang, Yujan Ting, Weijing Huang, Terrence Chen

Large language models (LLMs) have shown remarkable capabilities in various
natural language processing tasks, yet they often struggle with maintaining
factual accuracy, particularly in knowledge-intensive domains like healthcare.
This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,
a novel approach designed to enhance the factual reliability of LLMs, with a
focus on medical question answering (QA). LEAF utilizes a dual strategy to
enhance the factual accuracy of responses from models such as Llama 3 70B
Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,
improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking
results to guide the retrieval process without updating model parameters. The
second strategy, Learning from Fact-Checks via Self-Training, involves
supervised fine-tuning (SFT) on fact-checked responses or applying Simple
Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both
updating LLM parameters from supervision. These findings suggest that
integrating fact-checked responses whether through RAG enhancement or
self-training enhances the reliability and factual correctness of LLM outputs,
offering a promising solution for applications where information accuracy is
crucial.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務中展現出卓越的能力，然而它們在維持事實準確性方面常常面臨困難，特別是在像醫療保健這樣的知識密集領域。本研究引入了 LEAF：透過事實查核增強的學習與評估，這是一種新穎的方法，旨在提升 LLM 的事實可靠性，並專注於醫療問題解答 (QA)。LEAF 利用雙重策略來提升 LLM 回應的事實準確性，例如 Llama 3 70B Instruct 和 Llama 3 8B Instruct。第一種策略 Fact-Check-Then-RAG，透過整合事實查核結果來改進檢索增強生成 (RAG)，以引導檢索程序，而不會更新模型參數。第二種策略透過自我訓練學習事實查核，涉及針對經過事實查核的回應進行監督微調 (SFT)，或將簡單偏好最佳化 (SimPO) 應用於事實查核作為排名機制，這兩種方法都會從監督中更新 LLM 參數。這些發現表明，無論是透過 RAG 增強或自我訓練，整合經過事實查核的回應，都能提升 LLM 輸出的可靠性和事實正確性，為資訊準確性至關重要的應用程式提供了一個有前景的解決方案。

##### **Neural spell-checker: Beyond words with synthetic data generation**
2410.23514v1 by Matej Klemen, Martin Božič, Špela Arhar Holdt, Marko Robnik-Šikonja

Spell-checkers are valuable tools that enhance communication by identifying
misspelled words in written texts. Recent improvements in deep learning, and in
particular in large language models, have opened new opportunities to improve
traditional spell-checkers with new functionalities that not only assess
spelling correctness but also the suitability of a word for a given context. In
our work, we present and compare two new spell-checkers and evaluate them on
synthetic, learner, and more general-domain Slovene datasets. The first
spell-checker is a traditional, fast, word-based approach, based on a
morphological lexicon with a significantly larger word list compared to
existing spell-checkers. The second approach uses a language model trained on a
large corpus with synthetically inserted errors. We present the training data
construction strategies, which turn out to be a crucial component of neural
spell-checkers. Further, the proposed neural model significantly outperforms
all existing spell-checkers for Slovene in both precision and recall.

摘要：拼字檢查器是透過辨識書面文字中拼錯的字詞，來強化溝通的寶貴工具。深度學習最近的進步，特別是在大型語言模型中，為傳統拼字檢查器開啟了新的機會，有了新的功能，不只評估拼字的正確性，也評估一個字詞是否適合特定脈絡。在我們的研究中，我們提出並比較了兩個新的拼字檢查器，並在合成的、學習者的，以及更通用的斯洛維尼亞語資料集上對它們進行評估。第一個拼字檢查器是一種傳統的、快速的、基於字詞的方法，它基於一個形態詞彙表，與現有的拼字檢查器相比，它的字詞清單顯著增加。第二種方法使用在一個大型語料庫上訓練的語言模型，並在語料庫中加入合成的錯誤。我們提出了訓練資料建構策略，結果證明這是神經拼字檢查器的一個關鍵組成部分。此外，所提出的神經模型在精確度和召回率方面都顯著優於所有現有的斯洛維尼亞語拼字檢查器。

##### **Dynamic Strategy Planning for Efficient Question Answering with Large Language Models**
2410.23511v1 by Tanmay Parekh, Pradyot Prakash, Alexander Radovic, Akshay Shekher, Denis Savenkov

Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),
planning (e.g., SelfAsk), and retrieval augmented generation strategies to
improve the performance of Large Language Models (LLMs) on various tasks, such
as question answering. However, using a single fixed strategy to answer
different kinds of questions is suboptimal in performance and inefficient in
terms of generated output tokens and performed retrievals. In our work, we
propose a novel technique DyPlan, to induce a dynamic strategy selection
process in LLMs, to improve performance and reduce costs in question-answering.
DyPlan incorporates an initial decision step to select the most suitable
strategy conditioned on the input question and guides the LLM's response
generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal
verification and correction process to further enrich the generated answer.
Experiments on three prominent multi-hop question answering (MHQA) datasets
reveal how DyPlan can improve model performance by 7-13% while reducing the
cost by 11-32% relative to the best baseline model.

摘要：研究已表明推理（例如，思考链）、规划（例如，自问）和检索增强生成策略在提高大型语言模型 (LLM) 在各种任务（例如问答）上的性能方面的有效性。然而，使用单一的固定策略来回答不同类型的问题在性能上是次优的，并且在生成输出标记和执行检索方面效率低下。在我们的工作中，我们提出了一种新技术 DyPlan，以在 LLM 中诱导动态策略选择过程，以提高性能并降低问答中的成本。DyPlan 结合了一个初始决策步骤，以根据输入问题选择最合适的策略，并相应地指导 LLM 的响应生成。我们将 DyPlan 扩展到 DyPlan-verify，添加了一个内部验证和校正过程，以进一步丰富生成的答案。在三个突出的多跳问答 (MHQA) 数据集上的实验揭示了 DyPlan 如何将模型性能提高 7-13%，同时相对于最佳基线模型将成本降低 11-32%。

##### **Tiny Transformers Excel at Sentence Compression**
2410.23510v1 by Peter Belcak, Roger Wattenhofer

It is staggering that words of the English language, which are on average
represented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served
to large language models. We show that there is room for more information in
every token embedding. We demonstrate that 1--3-layer transformers are capable
of encoding and subsequently decoding standard English sentences into as little
as a single 3-kilobyte token. Our work implies that even small networks can
learn to construct valid English sentences and suggests the possibility of
optimising large language models by moving from sub-word token embeddings
towards larger fragments of text.

摘要：令人驚訝的是，平均用 5 到 6 個 ASCII 位元組表示的英文單字，在大語言模型中提供時需要多達 24 千位元組。我們展示了每個標記嵌入中還有更多資訊的空間。我們證明了 1 到 3 層的轉換器能夠將標準英文句子編碼並隨後解碼成僅 3 千位元組的單一標記。我們的研究意味著即使是小網路也能學會建構有效的英文句子，並建議透過從子字詞標記嵌入轉移到更大的文字片段來最佳化大型語言模型的可能性。

##### **Efficient and Interpretable Grammatical Error Correction with Mixture of Experts**
2410.23507v1 by Muhammad Reza Qorib, Alham Fikri Aji, Hwee Tou Ng

Error type information has been widely used to improve the performance of
grammatical error correction (GEC) models, whether for generating corrections,
re-ranking them, or combining GEC models. Combining GEC models that have
complementary strengths in correcting different error types is very effective
in producing better corrections. However, system combination incurs a high
computational cost due to the need to run inference on the base systems before
running the combination method itself. Therefore, it would be more efficient to
have a single model with multiple sub-networks that specialize in correcting
different error types. In this paper, we propose a mixture-of-experts model,
MoECE, for grammatical error correction. Our model successfully achieves the
performance of T5-XL with three times fewer effective parameters. Additionally,
our model produces interpretable corrections by also identifying the error type
during inference.

摘要：錯誤類型資訊已被廣泛用於改善文法錯誤修正 (GEC) 模型的效能，無論是產生修正、重新排序，還是結合 GEC 模型。結合在修正不同錯誤類型上具有互補優勢的 GEC 模型，在產生更好的修正上非常有效。然而，系統結合會產生高運算成本，這是因為在執行結合方法本身之前，需要對基礎系統執行推論。因此，擁有單一模型，其中包含多個專門修正不同錯誤類型的子網路，將會更有效率。在本文中，我們提出了一個專家混合模型，MoECE，用於文法錯誤修正。我們的模型成功達到了 T5-XL 的效能，但有效參數卻少了三倍。此外，我們的模型透過在推論期間識別錯誤類型，產生可解釋的修正。

##### **Learning to Achieve Goals with Belief State Transformers**
2410.23506v1 by Edward S. Hu, Kwangjun Ahn, Qinghua Liu, Haoran Xu, Manan Tomar, Ada Langford, Dinesh Jayaraman, Alex Lamb, John Langford

We introduce the "Belief State Transformer", a next-token predictor that
takes both a prefix and suffix as inputs, with a novel objective of predicting
both the next token for the prefix and the previous token for the suffix. The
Belief State Transformer effectively learns to solve challenging problems that
conventional forward-only transformers struggle with, in a domain-independent
fashion. Key to this success is learning a compact belief state that captures
all relevant information necessary for accurate predictions. Empirical
ablations show that each component of the model is essential in difficult
scenarios where standard Transformers fall short. For the task of story writing
with known prefixes and suffixes, our approach outperforms the
Fill-in-the-Middle method for reaching known goals and demonstrates improved
performance even when the goals are unknown. Altogether, the Belief State
Transformer enables more efficient goal-conditioned decoding, better test-time
inference, and high-quality text representations on small scale problems.

摘要：<paragraph>我們介紹「信念狀態Transformer」，一種以詞首和詞尾作為輸入的下一詞元預測器，並以預測詞首的下一詞元和詞尾的前一詞元為新目標。信念狀態Transformer有效地學會解決傳統僅前向Transformer難以應付的挑戰性問題，而且不受特定領域限制。成功的關鍵在於學習一個精簡的信念狀態，它擷取了所有精準預測所需的相关資訊。實證消融研究顯示，模型的每個組成部分在標準Transformer不足以應付的困難場景中都至關重要。對於擁有已知詞首和詞尾的故事撰寫任務，我們的做法在達成已知目標方面優於填入中間法，即使目標未知時也展現出進步的效能。總而言之，信念狀態Transformer能進行更有效率的目標條件解碼、更好的測試時間推論，以及在小規模問題中提供高品質的文字表徵。</paragraph>

##### **All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling**
2410.23501v1 by Emanuele Marconato, Sébastien Lachapelle, Sebastian Weichwald, Luigi Gresele

We analyze identifiability as a possible explanation for the ubiquity of
linear properties across language models, such as the vector difference between
the representations of "easy" and "easiest" being parallel to that between
"lucky" and "luckiest". For this, we ask whether finding a linear property in
one model implies that any model that induces the same distribution has that
property, too. To answer that, we first prove an identifiability result to
characterize distribution-equivalent next-token predictors, lifting a diversity
requirement of previous results. Second, based on a refinement of relational
linearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how
many notions of linearity are amenable to our analysis. Finally, we show that
under suitable conditions, these linear properties either hold in all or none
distribution-equivalent next-token predictors.

摘要：我們分析可識別性，作為語言模型中普遍存在線性特性的可能解釋，例如「容易」和「最容易」的表示之間的向量差與「幸運」和「最幸運」之間的向量差平行。為此，我們詢問在一個模型中找到一個線性特徵是否意味著任何誘導出相同分布的模型也具有該特徵。為了回答這個問題，我們首先證明一個可識別性結果，以描述等價於分布的下一代預測器，並解除先前結果的多樣性要求。其次，根據關係線性的改進 [Paccanaro and Hinton, 2001; Hernandez et al., 2024]，我們展示了多少線性概念適用於我們的分析。最後，我們表明在適當的條件下，這些線性特徵在所有或沒有等價於分布的下一代預測器中成立。

##### **Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm**
2410.23498v1 by Sattar Vakili, Julia Olkhovskaya

Reinforcement learning utilizing kernel ridge regression to predict the
expected value function represents a powerful method with great
representational capacity. This setting is a highly versatile framework
amenable to analytical results. We consider kernel-based function approximation
for RL in the infinite horizon average reward setting, also referred to as the
undiscounted setting. We propose an optimistic algorithm, similar to
acquisition function based algorithms in the special case of bandits. We
establish novel no-regret performance guarantees for our algorithm, under
kernel-based modelling assumptions. Additionally, we derive a novel confidence
interval for the kernel-based prediction of the expected value function,
applicable across various RL problems.

摘要：利用核岭回归预测期望值函数的强化学习是一种功能强大的方法，具有巨大的表示能力。这种设置是一个高度通用的框架，适用于分析结果。我们考虑了在无限时间平均奖励设置（也称为无折扣设置）中基于核的函数逼近，用于 RL。我们提出了一种乐观算法，类似于在赌博机的特殊情况下基于获取函数的算法。在基于核的建模假设下，我们为我们的算法建立了新的无遗憾性能保证。此外，我们导出了一个新的置信区间，用于基于核的期望值函数预测，适用于各种 RL 问题。

##### **Smaller Large Language Models Can Do Moral Self-Correction**
2410.23496v1 by Guangliang Liu, Zhiyu Xue, Rongrong Wang, Kristen Marie Johnson

Self-correction is one of the most amazing emerging capabilities of Large
Language Models (LLMs), enabling LLMs to self-modify an inappropriate output
given a natural language feedback which describes the problems of that output.
Moral self-correction is a post-hoc approach correcting unethical generations
without requiring a gradient update, making it both computationally lightweight
and capable of preserving the language modeling ability. Previous works have
shown that LLMs can self-debias, and it has been reported that small models,
i.e., those with less than 22B parameters, are not capable of moral
self-correction. However, there is no direct proof as to why such smaller
models fall short of moral self-correction, though previous research
hypothesizes that larger models are skilled in following instructions and
understanding abstract social norms. In this paper, we empirically validate
this hypothesis in the context of social stereotyping, through meticulous
prompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs
with proper safety alignment fine-tuning can achieve very good moral
self-correction performance, highlighting the significant effects of safety
alignment; and (ii) small LLMs are indeed weaker than larger-scale models in
terms of comprehending social norms and self-explanation through CoT, but all
scales of LLMs show bad self-correction performance given unethical
instructions.

摘要：自我修正是大语言模型 (LLM) 最令人惊叹的新兴功能之一，它使 LLM 能够根据描述该输出问题的自然语言反馈来自我修改不当的输出。道德自我修正是一种事后方法，无需梯度更新即可修正不道德的生成，使其既计算量轻巧，又能保留语言建模能力。先前的研究表明，LLM 可以自我去偏，据报道，小模型（即参数少于 22B 的模型）无法进行道德自我修正。然而，对于为什么这些较小的模型无法进行道德自我修正，目前还没有直接的证据，尽管先前的研究假设较大的模型善于遵循指示并理解抽象的社会规范。在本文中，我们通过细致的提示，在社会刻板印象的背景下对这一假设进行了实证验证。我们的实验结果表明：(i) 出人意料的是，具有适当安全对齐微调的 3.8B LLM 可以实现非常好的道德自我修正性能，突出了安全对齐的显着影响；(ii) 在理解社会规范和通过 CoT 进行自我解释方面，小型 LLM 确实比大规模模型弱，但所有规模的 LLM 在给定不道德指令时都表现出较差的自我修正性能。

##### **Causality-Driven Audits of Model Robustness**
2410.23494v1 by Nathan Drenkow, Chris Ribaudo, Mathias Unberath

Robustness audits of deep neural networks (DNN) provide a means to uncover
model sensitivities to the challenging real-world imaging conditions that
significantly degrade DNN performance in-the-wild. Such conditions are often
the result of the compounding of multiple factors inherent to the environment,
sensor, or processing pipeline and may lead to complex image distortions that
are not easily categorized. When robustness audits are limited to a set of
pre-determined imaging effects or distortions, the results cannot be (easily)
transferred to real-world conditions where image corruptions may be more
complex or nuanced. To address this challenge, we present a new alternative
robustness auditing method that uses causal inference to measure DNN
sensitivities to the factors of the imaging process that cause complex
distortions. Our approach uses causal models to explicitly encode assumptions
about the domain-relevant factors and their interactions. Then, through
extensive experiments on natural and rendered images across multiple vision
tasks, we show that our approach reliably estimates causal effects of each
factor on DNN performance using observational domain data. These causal effects
directly tie DNN sensitivities to observable properties of the imaging pipeline
in the domain of interest towards reducing the risk of unexpected DNN failures
when deployed in that domain.

摘要：深度神经网络 (DNN) 的稳健性审核提供了一种方法来揭示模型对具有挑战性的现实世界成像条件的敏感性，这些条件会显著降低 DNN 在实际环境中的性能。此类条件通常是环境、传感器或处理管道中固有的多个因素共同作用的结果，并且可能导致难以分类的复杂图像失真。当稳健性审核仅限于一组预先确定的成像效果或失真时，结果无法（轻松）转移到图像损坏可能更复杂或细微差别更大的实际条件。为了应对这一挑战，我们提出了一种新的替代稳健性审核方法，该方法使用因果推理来衡量 DNN 对导致复杂失真的成像过程因素的敏感性。我们的方法使用因果模型来明确编码有关领域相关因素及其相互作用的假设。然后，通过对跨多个视觉任务的自然和渲染图像进行广泛的实验，我们表明我们的方法使用观测域数据可靠地估计了每个因素对 DNN 性能的因果效应。这些因果效应将 DNN 敏感性直接与感兴趣域中成像管道的可观察属性联系起来，以降低在该域中部署时意外 DNN 故障的风险。

