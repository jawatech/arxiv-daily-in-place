{"2411.02142": {"publish_time": "2024-11-04", "title": "Training Compute-Optimal Protein Language Models", "paper_summary": "We explore optimally training protein language models, an area of significant\ninterest in biological research where guidance on best practices is limited.\nMost models are trained with extensive compute resources until performance\ngains plateau, focusing primarily on increasing model sizes rather than\noptimizing the efficient compute frontier that balances performance and compute\nbudgets. Our investigation is grounded in a massive dataset consisting of 939\nmillion protein sequences. We trained over 300 models ranging from 3.5 million\nto 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate\nthe relations between model sizes, training token numbers, and objectives.\nFirst, we observed the effect of diminishing returns for the Causal Language\nModel (CLM) and that of overfitting for the Masked Language Model~(MLM) when\nrepeating the commonly used Uniref database. To address this, we included\nmetagenomic protein sequences in the training set to increase the diversity and\navoid the plateau or overfitting effects. Second, we obtained the scaling laws\nof CLM and MLM on Transformer, tailored to the specific characteristics of\nprotein sequence data. Third, we observe a transfer scaling phenomenon from CLM\nto MLM, further demonstrating the effectiveness of transfer through scaling\nbehaviors based on estimated Effectively Transferred Tokens. Finally, to\nvalidate our scaling laws, we compare the large-scale versions of ESM-2 and\nPROGEN2 on downstream tasks, encompassing evaluations of protein generation as\nwell as structure- and function-related tasks, all within less or equivalent\npre-training compute budgets.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a2\u8a0e\u6700\u4f73\u8a13\u7df4\u86cb\u767d\u8cea\u8a9e\u8a00\u6a21\u578b\uff0c\u9019\u662f\u751f\u7269\u7814\u7a76\u4e2d\u4e00\u500b\u91cd\u8981\u7684\u9818\u57df\uff0c\u4f46\u6700\u4f73\u5be6\u52d9\u7684\u6307\u5c0e\u65b9\u91dd\u6709\u9650\u3002\n\u5927\u591a\u6578\u6a21\u578b\u90fd\u4f7f\u7528\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\u9032\u884c\u8a13\u7df4\uff0c\u76f4\u5230\u6548\u80fd\u589e\u76ca\u9054\u5230\u5e73\u7a69\u671f\uff0c\u4e3b\u8981\u8457\u91cd\u65bc\u589e\u52a0\u6a21\u578b\u898f\u6a21\uff0c\u800c\u4e0d\u662f\u6700\u4f73\u5316\u5e73\u8861\u6548\u80fd\u8207\u904b\u7b97\u9810\u7b97\u7684\u6709\u6548\u904b\u7b97\u524d\u7de3\u3002\u6211\u5011\u7684\u8abf\u67e5\u662f\u57fa\u65bc\u4e00\u500b\u5305\u542b 9.39 \u5104\u500b\u86cb\u767d\u8cea\u5e8f\u5217\u7684\u9f90\u5927\u8cc7\u6599\u96c6\u3002\u6211\u5011\u8a13\u7df4\u4e86 300 \u591a\u500b\u6a21\u578b\uff0c\u7bc4\u570d\u5f9e 350 \u842c\u5230 107 \u5104\u500b\u53c3\u6578\uff0c\u4f7f\u7528 50 \u5104\u5230 2000 \u5104\u500b\u7368\u7279\u7b26\u865f\uff0c\u4f86\u63a2\u8a0e\u6a21\u578b\u898f\u6a21\u3001\u8a13\u7df4\u7b26\u865f\u6578\u91cf\u548c\u76ee\u6a19\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\n\u9996\u5148\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u56e0\u679c\u8a9e\u8a00\u6a21\u578b (CLM) \u7684\u5831\u916c\u905e\u6e1b\u6548\u61c9\uff0c\u4ee5\u53ca\u91cd\u8907\u4f7f\u7528\u5e38\u898b\u7684 Uniref \u8cc7\u6599\u5eab\u6642\uff0c\u906e\u853d\u8a9e\u8a00\u6a21\u578b (MLM) \u7684\u904e\u5ea6\u64ec\u5408\u6548\u61c9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5728\u8a13\u7df4\u96c6\u4e2d\u52a0\u5165\u4e86\u5b8f\u57fa\u56e0\u7d44\u86cb\u767d\u8cea\u5e8f\u5217\uff0c\u4ee5\u589e\u52a0\u591a\u6a23\u6027\u4e26\u907f\u514d\u5e73\u7a69\u671f\u6216\u904e\u5ea6\u64ec\u5408\u7684\u6548\u61c9\u3002\u5176\u6b21\uff0c\u6211\u5011\u7372\u5f97\u4e86\u91dd\u5c0d\u86cb\u767d\u8cea\u5e8f\u5217\u8cc7\u6599\u7684\u7279\u5b9a\u7279\u5fb5\u8abf\u6574\u7684 Transformer \u4e0a\u7684 CLM \u548c MLM \u7684\u7e2e\u653e\u5b9a\u5f8b\u3002\u7b2c\u4e09\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u5f9e CLM \u5230 MLM \u7684\u50b3\u8f38\u7e2e\u653e\u73fe\u8c61\uff0c\u9032\u4e00\u6b65\u8b49\u660e\u4e86\u57fa\u65bc\u4f30\u8a08\u7684\u6709\u6548\u50b3\u8f38\u7b26\u865f\u7684\u7e2e\u653e\u884c\u70ba\u7684\u50b3\u8f38\u6548\u80fd\u3002\u6700\u5f8c\uff0c\u70ba\u4e86\u9a57\u8b49\u6211\u5011\u7684\u7e2e\u653e\u5b9a\u5f8b\uff0c\u6211\u5011\u6bd4\u8f03\u4e86 ESM-2 \u548c PROGEN2 \u7684\u5927\u578b\u7248\u672c\u5728\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\uff0c\u5305\u62ec\u86cb\u767d\u8cea\u751f\u6210\u7684\u8a55\u4f30\u4ee5\u53ca\u8207\u7d50\u69cb\u548c\u529f\u80fd\u76f8\u95dc\u7684\u4efb\u52d9\uff0c\u6240\u6709\u9019\u4e9b\u90fd\u5728\u8f03\u5c11\u6216\u76f8\u7576\u7684\u9810\u8a13\u7df4\u904b\u7b97\u9810\u7b97\u5167\u3002</paragraph>", "author": "Xingyi Cheng et.al.", "authors": "Xingyi Cheng, Bo Chen, Pan Li, Jing Gong, Jie Tang, Le Song", "id": "2411.02142v1", "paper_url": "http://arxiv.org/abs/2411.02142v1", "repo": "https://github.com/cxysteven/scalingproteinlm"}}