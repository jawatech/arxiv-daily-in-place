{"2502.04194": {"publish_time": "2025-02-06", "title": "The Best Instruction-Tuning Data are Those That Fit", "paper_summary": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.", "paper_summary_zh": "\u9ad8\u54c1\u8cea\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u8cc7\u6599\u5c0d\u65bc\u5f15\u767c\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5f37\u5927\u529f\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u901a\u5e38\uff0c\u6307\u4ee4\u6703\u8207\u5f9e\u5176\u4ed6 LLM \u63a1\u6a23\u7684\u5404\u7a2e\u56de\u61c9\u914d\u5c0d\uff0c\u9019\u4e9b\u56de\u61c9\u901a\u5e38\u8d85\u51fa\u8981\u5fae\u8abf\u7684\u76ee\u6a19\u6a21\u578b\u7684\u5206\u5e03\u3002\u5f9e\u898f\u6a21\u4f86\u770b\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u5831\u916c\u905e\u6e1b\uff0c\u751a\u81f3\u640d\u5bb3\u6a21\u578b\u7684\u6548\u80fd\u548c\u7a69\u5065\u6027\u3002\u6211\u5011\u63d0\u51fa **GRAPE**\uff0c\u4e00\u500b\u65b0\u7684 SFT \u6846\u67b6\uff0c\u5b83\u8003\u616e\u4e86\u76ee\u6a19\u6a21\u578b\u7684\u7368\u7279\u7279\u5fb5\u3002\u5c0d\u65bc\u6bcf\u500b\u6307\u4ee4\uff0c\u5b83\u6703\u5f9e\u5404\u7a2e LLM \u6536\u96c6\u56de\u61c9\uff0c\u4e26\u9078\u64c7\u76ee\u6a19\u6a21\u578b\u6e2c\u91cf\u6a5f\u7387\u6700\u9ad8\u7684\u90a3\u500b\uff0c\u8868\u793a\u5b83\u8207\u76ee\u6a19\u6a21\u578b\u7684\u9810\u8a13\u7df4\u5206\u5e03\u6700\u63a5\u8fd1\uff1b\u7136\u5f8c\u9032\u884c\u6a19\u6e96\u7684 SFT \u8a13\u7df4\u3002\n\u6211\u5011\u9996\u5148\u4f7f\u7528\u53d7\u63a7\u5be6\u9a57\u8a55\u4f30 GRAPE\uff0c\u5728\u8a72\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5f9e\u591a\u500b\u6a21\u578b\u4e2d\u70ba UltraInteract \u4e2d\u7684\u6bcf\u500b\u554f\u984c\u63a1\u6a23\u5404\u7a2e\u89e3\uff0c\u4e26\u5728 GRAPE \u9078\u64c7\u7684\u8cc7\u6599\u4e0a\u5fae\u8abf\u5e38\u7528\u7684 LLM\uff0c\u4f8b\u5982 LLaMA3.1-8B\u3001Mistral-7B \u548c Qwen2.5-7B\u3002GRAPE \u660e\u986f\u512a\u65bc\u5f37\u5927\u7684\u57fa\u6e96\uff0c\u5305\u62ec\u5f9e\u6700\u5f37\u7684\u6a21\u578b\u4e2d\u8403\u53d6\uff0c\u7d55\u5c0d\u589e\u76ca\u9ad8\u9054 13.8%\uff0c\u5728\u57fa\u6e96\u4e0a\u5e73\u5747\u8a08\u7b97\uff0c\u4e26\u5728\u8cc7\u6599\u591a 3 \u500d\u7684\u60c5\u6cc1\u4e0b\u9032\u884c\u8a13\u7df4\uff0c\u6548\u80fd\u6700\u9ad8\u63d0\u5347 17.3%\u3002GRAPE \u7684\u5f37\u5927\u6548\u80fd\u63a8\u5ee3\u5230\u5be6\u969b\u8a2d\u5b9a\u3002\u6211\u5011\u5be6\u9a57\u4e86\u7528\u65bc Tulu3 \u548c Olmo-2 \u7684\u8a13\u7df4\u5f8c\u8cc7\u6599\u3002GRAPE \u512a\u65bc\u5f37\u5927\u7684\u57fa\u6e96\uff0c\u9019\u4e9b\u57fa\u6e96\u5728\u8cc7\u6599\u591a 4.5 \u500d\u7684\u60c5\u6cc1\u4e0b\u8a13\u7df4\uff0c\u5e73\u5747\u6548\u80fd\u9ad8\u51fa 6.1%\uff0c\u800c\u6700\u5148\u9032\u7684\u8cc7\u6599\u9078\u64c7\u65b9\u6cd5\u9ad8\u51fa 3%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528 1/3 \u7684\u8cc7\u6599\u548c\u4e00\u534a\u7684\u6642\u4ee3\uff0cGRAPE \u4f7f LLaMA3.1-8B \u8d85\u8d8a\u4e86 Tulu3-SFT \u7684\u6548\u80fd\uff0c\u9ad8\u51fa 3.5%\u3002", "author": "Dylan Zhang et.al.", "authors": "Dylan Zhang, Qirun Dai, Hao Peng", "id": "2502.04194v1", "paper_url": "http://arxiv.org/abs/2502.04194v1", "repo": "null"}}