{"2405.12528": {"publish_time": "2024-05-21", "title": "SirLLM: Streaming Infinite Retentive LLM", "paper_summary": "As Large Language Models (LLMs) become increasingly prevalent in various\ndomains, their ability to process inputs of any length and maintain a degree of\nmemory becomes essential. However, the one-off input of overly long texts is\nlimited, as studies have shown that when input lengths exceed the LLMs'\npre-trained text length, there is a dramatic decline in text generation\ncapabilities. Moreover, simply extending the length of pre-training texts is\nimpractical due to the difficulty in obtaining long text data and the\nsubstantial memory consumption costs this would entail for LLMs. Recent efforts\nhave employed streaming inputs to alleviate the pressure of excessively long\ntext inputs, but this approach can significantly impair the model's long-term\nmemory capabilities.\n  Motivated by this challenge, we introduce Streaming Infinite Retentive LLM\n(SirLLM), which allows LLMs to maintain longer memory during infinite-length\ndialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy\nmetric and a memory decay mechanism to filter key phrases, endowing LLMs with\nboth long-lasting and flexible memory. We designed three distinct tasks and\nconstructed three datasets to measure the effectiveness of SirLLM from various\nangles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our\nexperimental results robustly demonstrate that SirLLM can achieve stable and\nsignificant improvements across different LLMs and tasks, compellingly proving\nits effectiveness. When having a coversation, \"A sir could forget himself,\" but\nSirLLM never does! Our code is publicly available at\nhttps://github.com/Zoeyyao27/SirLLM", "paper_summary_zh": "<paragraph>\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u9818\u57df\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5b83\u5011\u8655\u7406\u4efb\u4f55\u9577\u5ea6\u7684\u8f38\u5165\u4e26\u4fdd\u6301\u4e00\u5b9a\u7a0b\u5ea6\u8a18\u61b6\u7684\u80fd\u529b\u8b8a\u5f97\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u904e\u5ea6\u5197\u9577\u7684\u6587\u672c\u7684\u4e00\u6b21\u6027\u8f38\u5165\u662f\u6709\u9650\u7684\uff0c\u56e0\u70ba\u7814\u7a76\u8868\u660e\uff0c\u7576\u8f38\u5165\u9577\u5ea6\u8d85\u904e LLM \u7684\u9810\u8a13\u7df4\u6587\u672c\u9577\u5ea6\u6642\uff0c\u6587\u672c\u751f\u6210\u80fd\u529b\u6703\u6025\u5287\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u50c5\u50c5\u5ef6\u9577\u9810\u8a13\u7df4\u6587\u672c\u7684\u9577\u5ea6\u662f\u4e0d\u5207\u5be6\u969b\u7684\uff0c\u56e0\u70ba\u96e3\u4ee5\u7372\u53d6\u9577\u6587\u672c\u6578\u64da\uff0c\u4e26\u4e14\u9019\u5c07\u7d66 LLM \u5e36\u4f86\u5de8\u5927\u7684\u5167\u5b58\u6d88\u8017\u6210\u672c\u3002\u6700\u8fd1\u7684\u52aa\u529b\u63a1\u7528\u4e32\u6d41\u8f38\u5165\u4f86\u7de9\u89e3\u904e\u9577\u6587\u672c\u8f38\u5165\u7684\u58d3\u529b\uff0c\u4f46\u9019\u7a2e\u65b9\u6cd5\u6703\u986f\u8457\u640d\u5bb3\u6a21\u578b\u7684\u9577\u671f\u8a18\u61b6\u80fd\u529b\u3002\n\u53d7\u6b64\u6311\u6230\u7684\u6fc0\u52f5\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e32\u6d41\u7121\u9650\u4fdd\u7559 LLM (SirLLM)\uff0c\u5b83\u5141\u8a31 LLM \u5728\u7121\u9650\u9577\u7684\u5c0d\u8a71\u4e2d\u4fdd\u6301\u66f4\u9577\u7684\u8a18\u61b6\uff0c\u800c\u7121\u9700\u9032\u884c\u5fae\u8abf\u3002SirLLM \u5229\u7528 Token Entropy \u5ea6\u91cf\u548c\u8a18\u61b6\u8870\u6e1b\u6a5f\u5236\u4f86\u904e\u6ffe\u95dc\u9375\u77ed\u8a9e\uff0c\u8ce6\u4e88 LLM \u6301\u4e45\u4e14\u9748\u6d3b\u7684\u8a18\u61b6\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e09\u500b\u4e0d\u540c\u7684\u4efb\u52d9\uff0c\u4e26\u69cb\u5efa\u4e86\u4e09\u500b\u6578\u64da\u96c6\uff0c\u5f9e\u4e0d\u540c\u7684\u89d2\u5ea6\u8861\u91cf SirLLM \u7684\u6709\u6548\u6027\uff1a(1) DailyDialog\uff1b(2) Grocery Shopping\uff1b(3) Rock-Paper-Scissors\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u6709\u529b\u5730\u8b49\u660e\uff0cSirLLM \u53ef\u4ee5\u8de8\u4e0d\u540c\u7684 LLM \u548c\u4efb\u52d9\u5be6\u73fe\u7a69\u5b9a\u4e14\u986f\u8457\u7684\u6539\u9032\uff0c\u6709\u529b\u5730\u8b49\u660e\u4e86\u5b83\u7684\u6709\u6548\u6027\u3002\u5728\u9032\u884c\u5c0d\u8a71\u6642\uff0c\u201c\u4e00\u4f4d\u5148\u751f\u53ef\u80fd\u6703\u5fd8\u8a18\u81ea\u5df1\u201d\uff0c\u4f46 SirLLM \u6c38\u9060\u4e0d\u6703\uff01\u6211\u5011\u7684\u4ee3\u78bc\u53ef\u5728 https://github.com/Zoeyyao27/SirLLM \u516c\u958b\u7372\u5f97</paragraph>", "author": "Yao Yao et.al.", "authors": "Yao Yao, Zuchao Li, Hai Zhao", "id": "2405.12528v1", "paper_url": "http://arxiv.org/abs/2405.12528v1", "repo": "https://github.com/zoeyyao27/sirllm"}}