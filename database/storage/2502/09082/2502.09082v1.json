{"2502.09082": {"publish_time": "2025-02-13", "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles", "paper_summary": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.", "paper_summary_zh": "\u89d2\u8272\u626e\u6f14\u8a9e\u8a00\u4ee3\u7406\uff08RPLA\uff09\u5df2\u6210\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6709\u524d\u9014\u7684\u61c9\u7528\u3002\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u771f\u5be6\u89d2\u8272\u8cc7\u6599\u96c6\u548c\u4f7f\u7528\u6b64\u985e\u8cc7\u6599\u7684\u7d30\u7dfb\u8a55\u4f30\u65b9\u6cd5\uff0c\u6a21\u64ec\u65e2\u6709\u89d2\u8272\u5c0d RPLA \u4f86\u8aaa\u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 CoSER\uff0c\u9019\u662f\u4e00\u500b\u9ad8\u54c1\u8cea\u8cc7\u6599\u96c6\u3001\u958b\u653e\u6a21\u578b\u548c\u8a55\u4f30\u5354\u8b70\u7684\u96c6\u5408\uff0c\u7528\u65bc\u6709\u6548\u5730\u626e\u6f14\u65e2\u6709\u89d2\u8272\u7684 RPLA\u3002CoSER \u8cc7\u6599\u96c6\u6db5\u84cb\u4e86\u4f86\u81ea 771 \u672c\u8457\u540d\u66f8\u7c4d\u7684 17,966 \u500b\u89d2\u8272\u3002\u5b83\u63d0\u4f9b\u4e86\u5177\u6709\u771f\u5be6\u4e16\u754c\u8907\u96dc\u6027\u7684\u771f\u5be6\u5c0d\u8a71\uff0c\u4ee5\u53ca\u5c0d\u8a71\u8a2d\u5b9a\u3001\u89d2\u8272\u9ad4\u9a57\u548c\u5167\u5fc3\u60f3\u6cd5\u7b49\u591a\u7a2e\u8cc7\u6599\u985e\u578b\u3002\u501f\u9451\u8868\u6f14\u65b9\u6cd5\uff0c\u6211\u5011\u5f15\u5165\u4e86\u65e2\u5b9a\u60c5\u5883\u8868\u6f14\uff0c\u7528\u65bc\u8a13\u7df4\u548c\u8a55\u4f30\u89d2\u8272\u626e\u6f14 LLM\uff0c\u5176\u4e2d LLM \u5728\u66f8\u7c4d\u5834\u666f\u4e2d\u4f9d\u6b21\u626e\u6f14\u591a\u500b\u89d2\u8272\u3002\u4f7f\u7528\u6211\u5011\u7684\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u958b\u767c\u4e86 CoSER 8B \u548c CoSER 70B\uff0c\u5373\u5efa\u7acb\u5728 LLaMA-3.1 \u6a21\u578b\u4e0a\u7684\u5148\u9032\u958b\u653e\u89d2\u8272\u626e\u6f14 LLM\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\u4e86 CoSER \u8cc7\u6599\u96c6\u5c0d\u65bc RPLA \u8a13\u7df4\u3001\u8a55\u4f30\u548c\u6aa2\u7d22\u7684\u50f9\u503c\u3002\u6b64\u5916\uff0cCoSER 70B \u5728\u6211\u5011\u7684\u8a55\u4f30\u548c\u4e09\u500b\u73fe\u6709\u57fa\u6e96\u4e0a\u5c55\u73fe\u4e86\u8d85\u8d8a\u6216\u5339\u914d GPT-4o \u7684\u6700\u5148\u9032\u6548\u80fd\uff0c\u5373\u5206\u5225\u5728 InCharacter \u548c LifeChoice \u57fa\u6e96\u4e0a\u9054\u5230\u4e86 75.80% \u548c 93.47% \u7684\u6e96\u78ba\u7387\u3002", "author": "Xintao Wang et.al.", "authors": "Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Wei Wang, Yanghua Xiao, Shuchang Zhou", "id": "2502.09082v1", "paper_url": "http://arxiv.org/abs/2502.09082v1", "repo": "null"}}