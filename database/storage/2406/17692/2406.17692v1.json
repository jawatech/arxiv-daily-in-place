{"2406.17692": {"publish_time": "2024-06-25", "title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment", "paper_summary": "The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f38\u51fa\u5206\u4f48\u6703\u56e0\u70ba\u5c0d\u9f4a\u7a0b\u5e8f\u800c\u7522\u751f\u8b8a\u52d5\u3002\u6211\u5011\u5206\u6790 LLM \u56de\u61c9\u5728\u5c0d\u9f4a\u5f8c\u5206\u4f48\u8f49\u79fb\u7684\u5169\u500b\u9762\u5411\u3002\u9996\u5148\uff0c\u6211\u5011\u91cd\u65b0\u6aa2\u8996\u5148\u524d\u5831\u544a\u7684\u56de\u61c9\u591a\u6a23\u6027\u5728\u5c0d\u9f4a\u5f8c\u4e0b\u964d\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u56de\u61c9\u591a\u6a23\u6027\u660e\u986f\u4e0b\u964d\uff0c\u9019\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53ef\u6b78\u56e0\u65bc\u54c1\u8cea\u63a7\u7ba1\u548c\u8cc7\u8a0a\u5f59\u6574\u3002\u5c0d\u9f4a\u6703\u6291\u5236\u4e0d\u76f8\u95dc\u4e14\u7121\u76ca\u7684\u5167\u5bb9\uff0c\u540c\u6642\u5c07\u8f38\u51fa\u5206\u4f48\u8f49\u79fb\u81f3\u8f03\u9577\u7684\u56de\u61c9\uff0c\u6db5\u84cb\u4e86 LLM \u57fa\u790e\u6a21\u578b\u4e2d\u591a\u500b\u56de\u61c9\u7684\u8cc7\u8a0a\uff0c\u672c\u8cea\u4e0a\u662f\u5728\u55ae\u4e00\u56de\u61c9\u4e2d\u5448\u73fe\u591a\u6a23\u5316\u7684\u8cc7\u8a0a\u3002\u7531\u65bc\u5e7e\u4e4e\u6c92\u6709\u8b49\u64da\u986f\u793a\u5c0d\u9f4a\u6703\u6291\u5236\u6709\u7528\u7684\u8cc7\u8a0a\uff0c\u56e0\u6b64\u81ea\u7136\u6703\u554f\u76f8\u53cd\u7684\u554f\u984c\uff1a\u5c0d\u9f4a\u7684\u6a21\u578b\u662f\u5426\u6703\u6d6e\u73fe\u57fa\u790e\u6a21\u578b\u7121\u6cd5\u5fa9\u539f\u7684\u8cc7\u8a0a\uff1f\u6211\u5011\u7684\u7b2c\u4e8c\u6b21\u8abf\u67e5\u986f\u793a\u4e26\u975e\u5982\u6b64\uff0c\u800c\u4e14\u5c0d\u9f4a\u6a21\u578b\u7684\u884c\u70ba\u53ef\u4ee5\u5f9e\u57fa\u790e\u6a21\u578b\u4e2d\u5fa9\u539f\uff0c\u800c\u7121\u9700\u5fae\u8abf\u3002\u7d50\u5408\u60c5\u5883\u4e2d\u7684\u7bc4\u4f8b\u548c\u95dc\u65bc\u56de\u61c9\u5167\u5bb9\u7684\u8f03\u4f4e\u89e3\u6790\u5ea6\u8a9e\u610f\u63d0\u793a\uff0c\u53ef\u4ee5\u5f15\u767c\u57fa\u790e LLM \u7684\u56de\u61c9\uff0c\u9019\u4e9b\u56de\u61c9\u8207\u5c0d\u9f4a\u5fae\u8abf\u7684 LLM \u56de\u61c9\u4e00\u6a23\u985e\u4f3c\uff0c\u800c\u5c0d\u9f4a\u5fae\u8abf\u7684 LLM \u56de\u61c9\u5f7c\u6b64\u4e4b\u9593\u4e5f\u4e00\u6a23\u985e\u4f3c\u3002\u7d9c\u5408\u4f86\u770b\uff0c\u9019\u4e9b\u7d50\u679c\u986f\u793a\u76ee\u524d\u7684\u5c0d\u9f4a\u6280\u8853\u6355\u6349\u4e86\u985e\u52a9\u7406\u57fa\u790e LLM \u884c\u70ba\u7684\u6709\u7528\u5b50\u96c6\uff0c\u4f46\u4e26\u672a\u5ef6\u4f38\u8a72\u5b50\u96c6\uff0c\u9032\u4e00\u6b65\u8b49\u5be6\u4e86\u8868\u9762\u5c0d\u9f4a\u5047\u8aaa\u3002\u5b83\u5011\u4e5f\u986f\u793a\u60c5\u5883\u4e2d\u5c0d\u9f4a\u53ef\u4ee5\u6210\u70ba\u4e00\u7a2e\u4ee4\u4eba\u9a5a\u8a1d\u7684\u7b56\u7565\uff0c\u7528\u65bc\u6a21\u4eff\u5c0d\u9f4a\u7684 LLM\uff0c\u800c\u7121\u9700\u5fae\u8abf\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u65bc https://github.com/thomlake/investigating-alignment \u53d6\u5f97\u3002", "author": "Thom Lake et.al.", "authors": "Thom Lake, Eunsol Choi, Greg Durrett", "id": "2406.17692v1", "paper_url": "http://arxiv.org/abs/2406.17692v1", "repo": "https://github.com/thomlake/investigating-alignment"}}