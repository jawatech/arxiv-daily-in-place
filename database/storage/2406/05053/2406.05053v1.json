{"2406.05053": {"publish_time": "2024-06-07", "title": "Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation", "paper_summary": "Generative AI and large language models hold great promise in enhancing\nprogramming education by generating individualized feedback and hints for\nlearners. Recent works have primarily focused on improving the quality of\ngenerated feedback to achieve human tutors' quality. While quality is an\nimportant performance criterion, it is not the only criterion to optimize for\nreal-world educational deployments. In this paper, we benchmark language models\nfor programming feedback generation across several performance criteria,\nincluding quality, cost, time, and data privacy. The key idea is to leverage\nrecent advances in the new paradigm of in-browser inference that allow running\nthese models directly in the browser, thereby providing direct benefits across\ncost and data privacy. To boost the feedback quality of small models compatible\nwith in-browser inference engines, we develop a fine-tuning pipeline based on\nGPT-4 generated synthetic data. We showcase the efficacy of fine-tuned\nLlama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser\ninference engine on three different Python programming datasets. We will\nrelease the full implementation along with a web app and datasets to facilitate\nfurther research on in-browser language models.", "paper_summary_zh": "\u751f\u6210\u5f0f AI \u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u8fc7\u4e3a\u5b66\u4e60\u8005\u751f\u6210\u4e2a\u6027\u5316\u53cd\u9988\u548c\u63d0\u793a\uff0c\u4ece\u800c\u589e\u5f3a\u7f16\u7a0b\u6559\u80b2\u65b9\u9762\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u63d0\u9ad8\u751f\u6210\u53cd\u9988\u7684\u8d28\u91cf\uff0c\u4ee5\u8fbe\u5230\u4eba\u7c7b\u5bfc\u5e08\u7684\u8d28\u91cf\u3002\u867d\u7136\u8d28\u91cf\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6027\u80fd\u6807\u51c6\uff0c\u4f46\u5b83\u5e76\u4e0d\u662f\u73b0\u5b9e\u4e16\u754c\u6559\u80b2\u90e8\u7f72\u4e2d\u552f\u4e00\u9700\u8981\u4f18\u5316\u7684\u6807\u51c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6839\u636e\u51e0\u4e2a\u6027\u80fd\u6807\u51c6\u5bf9\u7528\u4e8e\u7f16\u7a0b\u53cd\u9988\u751f\u6210\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u8d28\u91cf\u3001\u6210\u672c\u3001\u65f6\u95f4\u548c\u6570\u636e\u9690\u79c1\u3002\u5173\u952e\u601d\u60f3\u662f\u5229\u7528\u6d4f\u89c8\u5668\u5185\u63a8\u7406\u65b0\u8303\u5f0f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u8be5\u8fdb\u5c55\u5141\u8bb8\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\u8fd9\u4e9b\u6a21\u578b\uff0c\u4ece\u800c\u5728\u6210\u672c\u548c\u6570\u636e\u9690\u79c1\u65b9\u9762\u63d0\u4f9b\u76f4\u63a5\u7684\u597d\u5904\u3002\u4e3a\u4e86\u63d0\u9ad8\u4e0e\u6d4f\u89c8\u5668\u5185\u63a8\u7406\u5f15\u64ce\u517c\u5bb9\u7684\u5c0f\u6a21\u578b\u7684\u53cd\u9988\u8d28\u91cf\uff0c\u6211\u4eec\u57fa\u4e8e GPT-4 \u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5f00\u53d1\u4e86\u4e00\u4e2a\u5fae\u8c03\u7ba1\u9053\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4f7f\u7528 WebLLM \u7684\u6d4f\u89c8\u5668\u5185\u63a8\u7406\u5f15\u64ce\u5728\u4e09\u4e2a\u4e0d\u540c\u7684 Python \u7f16\u7a0b\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u8fc7\u7684 Llama3-8B \u548c Phi3-3.8B 4 \u4f4d\u91cf\u5316\u6a21\u578b\u7684\u529f\u6548\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u5b8c\u6574\u7684\u5b9e\u73b0\u4ee5\u53ca\u4e00\u4e2a web \u5e94\u7528\u7a0b\u5e8f\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u5bf9\u6d4f\u89c8\u5668\u5185\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "author": "Nachiket Kotalwar et.al.", "authors": "Nachiket Kotalwar, Alkis Gotovos, Adish Singla", "id": "2406.05053v1", "paper_url": "http://arxiv.org/abs/2406.05053v1", "repo": "null"}}