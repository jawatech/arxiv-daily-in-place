{"2408.02193": {"publish_time": "2024-08-05", "title": "CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs", "paper_summary": "Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u7684\u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u6975\u5927\u7684\u6f5b\u529b\uff0c\u7136\u800c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u537b\u843d\u5f8c\u65bc\u9589\u6e90\u6a21\u578b\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u7a2e\u6548\u80fd\u5dee\u8ddd\uff0c\u73fe\u6709\u7684\u65b9\u6cd5\u6703\u7522\u751f\u5927\u91cf\u7684\u5408\u6210\u8cc7\u6599\u9032\u884c\u5fae\u8abf\uff0c\u5c0e\u81f4\u8a13\u7df4\u6548\u7387\u4e0d\u5f70\u3002\u7531\u65bc\u9700\u8981\u66f4\u6709\u6548\u7387\u7684\u8a13\u7df4\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Code Adaptive Compute-efficient Tuning (CodeACT) \u67b6\u69cb\u3002CodeACT \u5f15\u5165\u4e86\u300c\u8907\u96dc\u5ea6\u548c\u591a\u6a23\u6027\u611f\u77e5\u53d6\u6a23\u300d(CDAS) \u65b9\u6cd5\uff0c\u6839\u64da\u8907\u96dc\u5ea6\u548c\u591a\u6a23\u6027\u4f86\u6311\u9078\u9ad8\u54c1\u8cea\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u4ee5\u53ca\u300c\u52d5\u614b\u5c01\u5305\u586b\u5145\u300d\u7b56\u7565\uff0c\u900f\u904e\u5728\u8a13\u7df4\u671f\u9593\u5c07\u586b\u5145\u7b26\u865f\u6e1b\u5230\u6700\u5c11\uff0c\u4ee5\u964d\u4f4e\u904b\u7b97\u8cc7\u6e90\u4f7f\u7528\u91cf\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u5728\u50c5\u4f7f\u7528 40% \u7684 EVOL-Instruct \u8cc7\u6599\u9032\u884c\u5fae\u8abf\u5f8c\uff0cCodeACT-DeepSeek-Coder-6.7B \u5728 HumanEval \u4e0a\u7684\u6548\u80fd\u63d0\u5347\u4e86 8.6%\uff0c\u8a13\u7df4\u6642\u9593\u6e1b\u5c11\u4e86 78%\uff0c\u800c\u4e14 GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 27%\u3002\u9019\u4e9b\u767c\u73fe\u51f8\u986f\u4e86 CodeACT \u63d0\u5347\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u6548\u80fd\u548c\u6548\u7387\u7684\u80fd\u529b\u3002\u900f\u904e\u6700\u4f73\u5316\u8cc7\u6599\u9078\u64c7\u548c\u8a13\u7df4\u6d41\u7a0b\uff0cCodeACT \u63d0\u4f9b\u4e86\u4e00\u7a2e\u5168\u9762\u6027\u7684\u65b9\u6cd5\u4f86\u63d0\u5347\u958b\u653e\u539f\u59cb\u78bc LLM \u7684\u529f\u80fd\uff0c\u540c\u6642\u5927\u5e45\u964d\u4f4e\u904b\u7b97\u9700\u6c42\uff0c\u89e3\u6c7a\u8cc7\u6599\u54c1\u8cea\u548c\u8a13\u7df4\u6548\u7387\u7684\u96d9\u91cd\u6311\u6230\uff0c\u4e26\u70ba\u66f4\u7701\u8cc7\u6e90\u4e14\u6548\u80fd\u66f4\u9ad8\u7684\u6a21\u578b\u92ea\u8def\u3002", "author": "Weijie Lv et.al.", "authors": "Weijie Lv, Xuan Xia, Sheng-Jun Huang", "id": "2408.02193v1", "paper_url": "http://arxiv.org/abs/2408.02193v1", "repo": "null"}}