
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-10**|**LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts**|Anh-Quan Cao et.al.|[2410.08211v1](http://arxiv.org/abs/2410.08211v1)|null|
|**2024-10-10**|**PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection**|Botao Ren et.al.|[2410.08210v1](http://arxiv.org/abs/2410.08210v1)|null|
|**2024-10-10**|**Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision**|Shengcao Cao et.al.|[2410.08209v1](http://arxiv.org/abs/2410.08209v1)|null|
|**2024-10-10**|**SPA: 3D Spatial-Awareness Enables Effective Embodied Representation**|Haoyi Zhu et.al.|[2410.08208v1](http://arxiv.org/abs/2410.08208v1)|null|
|**2024-10-10**|**Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training**|Gen Luo et.al.|[2410.08202v1](http://arxiv.org/abs/2410.08202v1)|null|
|**2024-10-10**|**From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions**|Changle Qu et.al.|[2410.08197v1](http://arxiv.org/abs/2410.08197v1)|null|
|**2024-10-10**|**MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code**|Zimu Lu et.al.|[2410.08196v1](http://arxiv.org/abs/2410.08196v1)|[link](https://github.com/mathllm/mathcoder2)|
|**2024-10-10**|**GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment**|Yuancheng Xu et.al.|[2410.08193v1](http://arxiv.org/abs/2410.08193v1)|null|
|**2024-10-10**|**DifFRelight: Diffusion-Based Facial Performance Relighting**|Mingming He et.al.|[2410.08188v1](http://arxiv.org/abs/2410.08188v1)|null|
|**2024-10-10**|**MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models**|Wenbo Hu et.al.|[2410.08182v1](http://arxiv.org/abs/2410.08182v1)|null|
|**2024-10-10**|**Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models**|Qingni Wang et.al.|[2410.08174v1](http://arxiv.org/abs/2410.08174v1)|null|
|**2024-10-10**|**On the Evaluation of Generative Robotic Simulations**|Feng Chen et.al.|[2410.08172v1](http://arxiv.org/abs/2410.08172v1)|null|
|**2024-10-10**|**Agent S: An Open Agentic Framework that Uses Computers Like a Human**|Saaket Agashe et.al.|[2410.08164v1](http://arxiv.org/abs/2410.08164v1)|[link](https://github.com/simular-ai/agent-s)|
|**2024-10-10**|**The Effect of Surprisal on Reading Times in Information Seeking and Repeated Reading**|Keren Gruteke Klein et.al.|[2410.08162v1](http://arxiv.org/abs/2410.08162v1)|null|
|**2024-10-10**|**Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning**|Amrith Setlur et.al.|[2410.08146v1](http://arxiv.org/abs/2410.08146v1)|null|
|**2024-10-10**|**Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**|Xiaoyuan Liu et.al.|[2410.08145v1](http://arxiv.org/abs/2410.08145v1)|null|
|**2024-10-10**|**DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory**|Yutong Wang et.al.|[2410.08143v1](http://arxiv.org/abs/2410.08143v1)|[link](https://github.com/yutongwang1216/docmtagent)|
|**2024-10-10**|**Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction**|Jarrid Rector-Brooks et.al.|[2410.08134v1](http://arxiv.org/abs/2410.08134v1)|null|
|**2024-10-10**|**Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks**|Mathis Pink et.al.|[2410.08133v1](http://arxiv.org/abs/2410.08133v1)|null|
|**2024-10-10**|**Think Beyond Size: Dynamic Prompting for More Effective Reasoning**|Kamesh R et.al.|[2410.08130v1](http://arxiv.org/abs/2410.08130v1)|null|
|**2024-10-10**|**Mars: Situated Inductive Reasoning in an Open-World Environment**|Xiaojuan Tang et.al.|[2410.08126v1](http://arxiv.org/abs/2410.08126v1)|null|
|**2024-10-10**|**Heterogeneous Graph Auto-Encoder for CreditCard Fraud Detection**|Moirangthem Tiken Singh et.al.|[2410.08121v1](http://arxiv.org/abs/2410.08121v1)|null|
|**2024-10-10**|**Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System**|Weize Chen et.al.|[2410.08115v1](http://arxiv.org/abs/2410.08115v1)|null|
|**2024-10-10**|**Robust AI-Generated Text Detection by Restricted Embeddings**|Kristian Kuznetsov et.al.|[2410.08113v1](http://arxiv.org/abs/2410.08113v1)|null|
|**2024-10-10**|**Active Fourier Auditor for Estimating Distributional Properties of ML Models**|Ayoub Ajarra et.al.|[2410.08111v1](http://arxiv.org/abs/2410.08111v1)|null|
|**2024-10-10**|**A Closer Look at Machine Unlearning for Large Language Models**|Xiaojian Yuan et.al.|[2410.08109v1](http://arxiv.org/abs/2410.08109v1)|[link](https://github.com/sail-sg/closer-look-llm-unlearning)|
|**2024-10-10**|**What Makes Large Language Models Reason in (Multi-Turn) Code Generation?**|Kunhao Zheng et.al.|[2410.08105v1](http://arxiv.org/abs/2410.08105v1)|null|
|**2024-10-10**|**Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining**|Tianyi Bai et.al.|[2410.08102v1](http://arxiv.org/abs/2410.08102v1)|null|
|**2024-10-10**|**A Generative AI Technique for Synthesizing a Digital Twin for U.S. Residential Solar Adoption and Generation**|Aparna Kishore et.al.|[2410.08098v1](http://arxiv.org/abs/2410.08098v1)|null|
|**2024-10-10**|**Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering**|Yuan Sui et.al.|[2410.08085v1](http://arxiv.org/abs/2410.08085v1)|null|
|**2024-10-10**|**Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning**|Shuhe Wang et.al.|[2410.08081v1](http://arxiv.org/abs/2410.08081v1)|[link](https://github.com/shuhewang1998/packing-analysis)|
|**2024-10-10**|**Unlearning-based Neural Interpretations**|Ching Lam Choi et.al.|[2410.08069v1](http://arxiv.org/abs/2410.08069v1)|null|
|**2024-10-10**|**Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models**|Wenting Tan et.al.|[2410.08068v1](http://arxiv.org/abs/2410.08068v1)|[link](https://github.com/sallytan13/teaching-inspired-prompting)|
|**2024-10-10**|**Reward-Augmented Data Enhances Direct Preference Alignment of LLMs**|Shenao Zhang et.al.|[2410.08067v1](http://arxiv.org/abs/2410.08067v1)|[link](https://github.com/shenao-zhang/reward-augmented-preference)|
|**2024-10-10**|**Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions**|Inderjeet Nair et.al.|[2410.08058v1](http://arxiv.org/abs/2410.08058v1)|null|
|**2024-10-10**|**A Target-Aware Analysis of Data Augmentation for Hate Speech Detection**|Camilla Casula et.al.|[2410.08053v1](http://arxiv.org/abs/2410.08053v1)|null|
|**2024-10-10**|**VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based Verifiers**|Jianing Qi et.al.|[2410.08048v1](http://arxiv.org/abs/2410.08048v1)|null|
|**2024-10-10**|**Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations**|Yiyuan Zhang et.al.|[2410.08049v1](http://arxiv.org/abs/2410.08049v1)|[link](https://github.com/ailab-cvc/unireplknet)|
|**2024-10-10**|**Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning**|Hyun Ryu et.al.|[2410.08047v1](http://arxiv.org/abs/2410.08047v1)|null|
|**2024-10-10**|**The Rise of AI-Generated Content in Wikipedia**|Creston Brooks et.al.|[2410.08044v1](http://arxiv.org/abs/2410.08044v1)|null|
|**2024-10-10**|**Strategic Classification With Externalities**|Yiling Chen et.al.|[2410.08032v1](http://arxiv.org/abs/2410.08032v1)|null|
|**2024-10-10**|**Private Language Models via Truncated Laplacian Mechanism**|Tianhao Huang et.al.|[2410.08027v1](http://arxiv.org/abs/2410.08027v1)|null|
|**2024-10-10**|**The Computational Complexity of Circuit Discovery for Inner Interpretability**|Federico Adolfi et.al.|[2410.08025v1](http://arxiv.org/abs/2410.08025v1)|null|
|**2024-10-10**|**Pretraining Graph Transformers with Atom-in-a-Molecule Quantum Properties for Improved ADMET Modeling**|Alessio Fallani et.al.|[2410.08024v1](http://arxiv.org/abs/2410.08024v1)|null|
|**2024-10-10**|**GrabDAE: An Innovative Framework for Unsupervised Domain Adaptation Utilizing Grab-Mask and Denoise Auto-Encoder**|Junzhou Chen et.al.|[2410.08023v1](http://arxiv.org/abs/2410.08023v1)|null|
|**2024-10-10**|**Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs**|Jonas Hübotter et.al.|[2410.08020v1](http://arxiv.org/abs/2410.08020v1)|null|
|**2024-10-10**|**LLM Cascade with Multi-Objective Optimal Consideration**|Kai Zhang et.al.|[2410.08014v1](http://arxiv.org/abs/2410.08014v1)|null|
|**2024-10-10**|**Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation**|Qingwen Bu et.al.|[2410.08001v1](http://arxiv.org/abs/2410.08001v1)|null|
|**2024-10-10**|**Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets**|Tommaso Giorgi et.al.|[2410.07991v1](http://arxiv.org/abs/2410.07991v1)|null|
|**2024-10-10**|**Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models**|Bofei Gao et.al.|[2410.07985v1](http://arxiv.org/abs/2410.07985v1)|null|
|**2024-10-10**|**MolMix: A Simple Yet Effective Baseline for Multimodal Molecular Representation Learning**|Andrei Manolache et.al.|[2410.07981v1](http://arxiv.org/abs/2410.07981v1)|null|
|**2024-10-10**|**Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling**|Yuanqi Du et.al.|[2410.07974v1](http://arxiv.org/abs/2410.07974v1)|null|
|**2024-10-10**|**Towards Assurance of LLM Adversarial Robustness using Ontology-Driven Argumentation**|Tomas Bueno Momcilovic et.al.|[2410.07962v1](http://arxiv.org/abs/2410.07962v1)|null|
|**2024-10-10**|**COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act**|Philipp Guldimann et.al.|[2410.07959v1](http://arxiv.org/abs/2410.07959v1)|null|
|**2024-10-10**|**Disease Entity Recognition and Normalization is Improved with Large Language Model Derived Synthetic Normalized Mentions**|Kuleen Sasse et.al.|[2410.07951v1](http://arxiv.org/abs/2410.07951v1)|null|
|**2024-10-10**|**The Function-Representation Unification Framework**|Alfredo Ibias et.al.|[2410.07928v1](http://arxiv.org/abs/2410.07928v1)|null|
|**2024-10-10**|**InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions**|Xiang Zhuang et.al.|[2410.07919v1](http://arxiv.org/abs/2410.07919v1)|null|
|**2024-10-10**|**ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation**|Léo Machado et.al.|[2410.07908v1](http://arxiv.org/abs/2410.07908v1)|null|
|**2024-10-10**|**Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines**|Junyu Lai et.al.|[2410.07896v1](http://arxiv.org/abs/2410.07896v1)|null|
|**2024-10-10**|**Unsupervised Data Validation Methods for Efficient Model Training**|Yurii Paniv et.al.|[2410.07880v1](http://arxiv.org/abs/2410.07880v1)|null|
|**2024-10-10**|**Benchmarking Agentic Workflow Generation**|Shuofei Qiao et.al.|[2410.07869v1](http://arxiv.org/abs/2410.07869v1)|null|
|**2024-10-10**|**System-2 Reasoning via Generality and Adaptation**|Sejin Kim et.al.|[2410.07866v1](http://arxiv.org/abs/2410.07866v1)|null|
|**2024-10-10**|**RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation**|Songming Liu et.al.|[2410.07864v1](http://arxiv.org/abs/2410.07864v1)|null|
|**2024-10-10**|**From Logits to Hierarchies: Hierarchical Clustering made Simple**|Emanuele Palumbo et.al.|[2410.07858v1](http://arxiv.org/abs/2410.07858v1)|null|
|**2024-10-10**|**Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency**|Tim Knappe et.al.|[2410.07839v1](http://arxiv.org/abs/2410.07839v1)|null|
|**2024-10-10**|**MinorityPrompt: Text to Minority Image Generation via Prompt Optimization**|Soobin Um et.al.|[2410.07838v1](http://arxiv.org/abs/2410.07838v1)|null|
|**2024-10-10**|**Masked Generative Priors Improve World Models Sequence Modelling Capabilities**|Cristian Meo et.al.|[2410.07836v1](http://arxiv.org/abs/2410.07836v1)|null|
|**2024-10-10**|**NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models**|William Tan et.al.|[2410.07830v1](http://arxiv.org/abs/2410.07830v1)|null|
|**2024-10-10**|**Why do objects have many names? A study on word informativeness in language use and lexical systems**|Eleonora Gualdoni et.al.|[2410.07827v1](http://arxiv.org/abs/2410.07827v1)|null|
|**2024-10-10**|**Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses**|Pranav Senthilkumar et.al.|[2410.07826v1](http://arxiv.org/abs/2410.07826v1)|null|
|**2024-10-10**|**Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models**|Zhipeng Chen et.al.|[2410.07825v1](http://arxiv.org/abs/2410.07825v1)|null|
|**2024-10-10**|**Mitigating Gender Bias in Code Large Language Models via Model Editing**|Zhanyue Qin et.al.|[2410.07820v1](http://arxiv.org/abs/2410.07820v1)|null|
|**2024-10-10**|**Uncovering Overfitting in Large Language Model Editing**|Mengqi Zhang et.al.|[2410.07819v1](http://arxiv.org/abs/2410.07819v1)|null|
|**2024-10-10**|**Temporal-Difference Variational Continual Learning**|Luckeciano C. Melo et.al.|[2410.07812v1](http://arxiv.org/abs/2410.07812v1)|null|
|**2024-10-10**|**Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?**|Gürkan Soykan et.al.|[2410.07809v1](http://arxiv.org/abs/2410.07809v1)|[link](https://github.com/gglab-ku/ling-informed-mit)|
|**2024-10-10**|**Rewriting Conversational Utterances with Instructed Large Language Models**|Elnara Galimzhanova et.al.|[2410.07797v1](http://arxiv.org/abs/2410.07797v1)|null|
|**2024-10-10**|**Do Current Language Models Support Code Intelligence for R Programming Language?**|ZiXiao Zhao et.al.|[2410.07793v1](http://arxiv.org/abs/2410.07793v1)|null|
|**2024-10-10**|**Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation**|Sweta Agrawal et.al.|[2410.07779v1](http://arxiv.org/abs/2410.07779v1)|null|
|**2024-10-10**|**Full-Rank No More: Low-Rank Weight Training for Modern Speech Recognition Models**|Adriana Fernandez-Lopez et.al.|[2410.07771v1](http://arxiv.org/abs/2410.07771v1)|null|
|**2024-10-10**|**Dialectical Behavior Therapy Approach to LLM Prompting**|Oxana Vitman et.al.|[2410.07768v1](http://arxiv.org/abs/2410.07768v1)|null|
|**2024-10-10**|**GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps**|Muhammad Umair Nasir et.al.|[2410.07765v1](http://arxiv.org/abs/2410.07765v1)|[link](https://github.com/umair-nasir14/game-traversal-benchmark)|
|**2024-10-10**|**HARIVO: Harnessing Text-to-Image Models for Video Generation**|Mingi Kwon et.al.|[2410.07763v1](http://arxiv.org/abs/2410.07763v1)|null|
|**2024-10-10**|**$\textit{Jump Your Steps}$: Optimizing Sampling Schedule of Discrete Diffusion Models**|Yong-Hyun Park et.al.|[2410.07761v1](http://arxiv.org/abs/2410.07761v1)|null|
|**2024-10-10**|**Learning Low-Level Causal Relations using a Simulated Robotic Arm**|Miroslav Cibula et.al.|[2410.07751v1](http://arxiv.org/abs/2410.07751v1)|null|
|**2024-10-10**|**StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs**|Yuanqing Yu et.al.|[2410.07745v1](http://arxiv.org/abs/2410.07745v1)|[link](https://github.com/yuyq18/steptool)|
|**2024-10-10**|**SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture**|Jiayi Han et.al.|[2410.07739v1](http://arxiv.org/abs/2410.07739v1)|null|
|**2024-10-10**|**Enhancing Federated Domain Adaptation with Multi-Domain Prototype-Based Federated Fine-Tuning**|Jingyuan Zhang et.al.|[2410.07738v1](http://arxiv.org/abs/2410.07738v1)|null|
|**2024-10-10**|**On the Generalization Properties of Deep Learning for Aircraft Fuel Flow Estimation Models**|Gabriel Jarry et.al.|[2410.07717v1](http://arxiv.org/abs/2410.07717v1)|null|
|**2024-10-10**|**Learning Tree Pattern Transformations**|Daniel Neider et.al.|[2410.07708v1](http://arxiv.org/abs/2410.07708v1)|null|
|**2024-10-10**|**AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories**|Yifan Song et.al.|[2410.07706v1](http://arxiv.org/abs/2410.07706v1)|null|
|**2024-10-10**|**Multi-Facet Counterfactual Learning for Content Quality Evaluation**|Jiasheng Zheng et.al.|[2410.07693v1](http://arxiv.org/abs/2410.07693v1)|null|
|**2024-10-10**|**Smart Audit System Empowered by LLM**|Xu Yao et.al.|[2410.07677v1](http://arxiv.org/abs/2410.07677v1)|null|
|**2024-10-10**|**Adversarial Robustness Overestimation and Instability in TRADES**|Jonathan Weiping Li et.al.|[2410.07675v1](http://arxiv.org/abs/2410.07675v1)|null|
|**2024-10-10**|**Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference**|Jianxing Yu et.al.|[2410.07673v1](http://arxiv.org/abs/2410.07673v1)|null|
|**2024-10-10**|**MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization**|Yougang Lyu et.al.|[2410.07672v1](http://arxiv.org/abs/2410.07672v1)|null|
|**2024-10-10**|**DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for Interpretable Job Recommendation**|Xiaoshan Yu et.al.|[2410.07671v1](http://arxiv.org/abs/2410.07671v1)|null|
|**2024-10-10**|**StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models**|Minchan Kwon et.al.|[2410.07652v1](http://arxiv.org/abs/2410.07652v1)|null|
|**2024-10-10**|**Almost Minimax Optimal Best Arm Identification in Piecewise Stationary Linear Bandits**|Yunlong Hou et.al.|[2410.07638v1](http://arxiv.org/abs/2410.07638v1)|null|
|**2024-10-10**|**Automatic Curriculum Expert Iteration for Reliable LLM Reasoning**|Zirui Zhao et.al.|[2410.07627v1](http://arxiv.org/abs/2410.07627v1)|null|
|**2024-10-10**|**Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation**|Kaiyuan Liu et.al.|[2410.07618v1](http://arxiv.org/abs/2410.07618v1)|null|

#### Abstracts
##### **LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts**
2410.08211v1 by Anh-Quan Cao, Maximilian Jaritz, Matthieu Guillaumin, Raoul de Charette, Loris Bazzani

Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are
renowned for their versatility, as they can be applied to diverse applications
in a zero-shot setup. However, when these models are used in specific domains,
their performance often falls short due to domain gaps or the
under-representation of these domains in the training data. While fine-tuning
VLP models on custom datasets with human-annotated labels can address this
issue, annotating even a small-scale dataset (e.g., 100k samples) can be an
expensive endeavor, often requiring expert annotators if the task is complex.
To address these challenges, we propose LatteCLIP, an unsupervised method for
fine-tuning CLIP models on classification with known class names in custom
domains, without relying on human annotations. Our method leverages Large
Multimodal Models (LMMs) to generate expressive textual descriptions for both
individual images and groups of images. These provide additional contextual
information to guide the fine-tuning process in the custom domains. Since
LMM-generated descriptions are prone to hallucination or missing details, we
introduce a novel strategy to distill only the useful information and stabilize
the training. Specifically, we learn rich per-class prototype representations
from noisy generated texts and dual pseudo-labels. Our experiments on 10
domain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot
methods by an average improvement of +4.74 points in top-1 accuracy and other
state-of-the-art unsupervised methods by +3.45 points.

摘要：大型視覺語言預訓練 (VLP) 模型 (例如 CLIP) 以其多功能性而聞名，因為它們可以用於各種應用程式中，而無需進行任何設定。然而，當這些模型用於特定領域時，它們的效能通常會因領域差距或訓練資料中這些領域的代表性不足而下降。雖然在具有人工標記標籤的客製化資料集上微調 VLP 模型可以解決這個問題，但即使是標記小規模資料集 (例如 100k 個樣本) 也可能是一項昂貴的工作，如果任務很複雜，通常需要專家標記員。為了應對這些挑戰，我們提出了 LatteCLIP，這是一種無監督方法，用於在客製化領域中對 CLIP 模型進行微調，以對已知的類別名稱進行分類，而不需要依賴人工標記。我們的模型利用大型多模態模型 (LMM) 為個別影像和影像群組產生具表現力的文字描述。這些描述提供了額外的脈絡資訊，以指導客製化領域中的微調過程。由於 LMM 生成的描述容易出現幻覺或遺漏細節，因此我們引入了一種新策略，僅提取有用的資訊並穩定訓練。具體來說，我們從雜訊產生的文字和雙重偽標籤中學習豐富的每個類別原型表示。我們在 10 個特定領域的資料集上進行的實驗表明，LatteCLIP 在前 1 名準確率上比預先訓練的零次學習方法平均提高了 +4.74 個百分點，比其他最先進的無監督方法提高了 +3.45 個百分點。

##### **PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection**
2410.08210v1 by Botao Ren, Xue Yang, Yi Yu, Junwei Luo, Zhidong Deng

Single point supervised oriented object detection has gained attention and
made initial progress within the community. Diverse from those approaches
relying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBB
has shown promise due to its prior-free feature. In this paper, we propose
PointOBB-v2, a simpler, faster, and stronger method to generate pseudo rotated
boxes from points without relying on any other prior. Specifically, we first
generate a Class Probability Map (CPM) by training the network with non-uniform
positive and negative sampling. We show that the CPM is able to learn the
approximate object regions and their contours. Then, Principal Component
Analysis (PCA) is applied to accurately estimate the orientation and the
boundary of objects. By further incorporating a separation mechanism, we
resolve the confusion caused by the overlapping on the CPM, enabling its
operation in high-density scenarios. Extensive comparisons demonstrate that our
method achieves a training speed 15.58x faster and an accuracy improvement of
11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets compared to the
previous state-of-the-art, PointOBB. This significantly advances the cutting
edge of single point supervised oriented detection in the modular track.

摘要：單點監督導向的目標偵測已獲得關注，並在社群中取得初步進展。不同於依賴單次樣本或強大預訓練模型（例如 SAM）的那些方法，PointOBB 由於其無先驗特徵而顯示出前景。在本文中，我們提出 PointOBB-v2，這是一種更簡單、更快速且更強大的方法，用於在不依賴任何其他先驗的情況下從點生成偽旋轉框。具體來說，我們首先透過使用非均勻正負取樣來訓練網路，來生成類別機率圖 (CPM)。我們表明 CPM 能夠學習近似的物件區域及其輪廓。然後，應用主成分分析 (PCA) 來準確估計物體的方向和邊界。透過進一步納入分離機制，我們解決了 CPM 上重疊所造成的混淆，使其能夠在高密度場景中操作。廣泛的比較表明，與先前的技術 PointOBB 相比，我們的模型在 DOTA-v1.0/v1.5/v2.0 資料集上實現了快 15.58 倍的訓練速度，以及 11.60%/25.15%/21.19% 的準確度提升。這顯著提升了模組化軌道中單點監督導向偵測的尖端技術。

##### **Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision**
2410.08209v1 by Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang

Current large multimodal models (LMMs) face challenges in grounding, which
requires the model to relate language components to visual entities. Contrary
to the common practice that fine-tunes LMMs with additional grounding
supervision, we find that the grounding ability can in fact emerge in LMMs
trained without explicit grounding supervision. To reveal this emerging
grounding, we introduce an "attend-and-segment" method which leverages
attention maps from standard LMMs to perform pixel-level segmentation.
Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM
utilizing a diffusion-based visual encoder, as opposed to the standard CLIP
visual encoder, and trained with the same weak supervision. Without being
constrained by the biases and limited scale of grounding-specific supervision
data, our approach is more generalizable and scalable. We achieve competitive
performance on both grounding-specific and general visual question answering
benchmarks, compared with grounding LMMs and generalist LMMs, respectively.
Notably, we achieve a 44.2 grounding mask recall on grounded conversation
generation without any grounding supervision, outperforming the extensively
supervised model GLaMM. Project page: https://groundLMM.github.io.

摘要：目前的大型多模態模型 (LMM) 面臨著基礎問題，這需要模型將語言組成部分與視覺實體關聯起來。與使用額外的基礎監督微調 LMM 的常見做法相反，我們發現基礎能力實際上可以在沒有明確基礎監督的情況下訓練的 LMM 中出現。為了揭示這種新興的基礎，我們引入了一個「關注並分割」方法，該方法利用標準 LMM 的注意力圖來執行像素級分割。此外，為了增強基礎能力，我們提出了 DIFFLMM，這是一種 LMM，它使用基於擴散的視覺編碼器，而不是標準 CLIP 視覺編碼器，並使用相同的弱監督進行訓練。在不受基礎特定監督數據的偏差和有限規模的約束下，我們的做法更具通用性和可擴展性。與基礎 LMM 和通才 LMM 相比，我們在基礎特定和一般視覺問題解答基準上實現了競爭力。值得注意的是，我們在沒有任何基礎監督的情況下，對基礎對話生成實現了 44.2 的基礎遮罩召回率，優於廣泛監督的模型 GLaMM。專案頁面：https://groundLMM.github.io。

##### **SPA: 3D Spatial-Awareness Enables Effective Embodied Representation**
2410.08208v1 by Haoyi Zhu, Honghui Yang, Yating Wang, Jiange Yang, Limin Wang, Tong He

In this paper, we introduce SPA, a novel representation learning framework
that emphasizes the importance of 3D spatial awareness in embodied AI. Our
approach leverages differentiable neural rendering on multi-view images to
endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding.
We present the most comprehensive evaluation of embodied representation
learning to date, covering 268 tasks across 8 simulators with diverse policies
in both single-task and language-conditioned multi-task scenarios. The results
are compelling: SPA consistently outperforms more than 10 state-of-the-art
representation methods, including those specifically designed for embodied AI,
vision-centric tasks, and multi-modal applications, while using less training
data. Furthermore, we conduct a series of real-world experiments to confirm its
effectiveness in practical scenarios. These results highlight the critical role
of 3D spatial awareness for embodied representation learning. Our strongest
model takes more than 6000 GPU hours to train and we are committed to
open-sourcing all code and model weights to foster future research in embodied
representation learning. Project Page: https://haoyizhu.github.io/spa/.

摘要：<paragraph>在本文中，我們介紹 SPA，一種新穎的表徵學習框架，強調了 3D 空間感知在具身 AI 中的重要性。我們的做法利用多視圖影像上的可微分神經渲染，賦予 vanilla 視覺 Transformer (ViT) 內在的空間理解。我們展示了迄今為止最全面的具身表徵學習評估，涵蓋了 8 個模擬器中的 268 個任務，在單一任務和語言條件的多任務場景中採用不同的策略。結果令人信服：SPA 持續優於 10 種以上的最新表徵方法，包括專門為具身 AI、以視覺為中心的任務和多模態應用程式設計的方法，同時使用較少的訓練資料。此外，我們進行了一系列真實世界的實驗，以確認其在實際場景中的有效性。這些結果突出了 3D 空間感知對於具身表徵學習的關鍵作用。我們最強大的模型需要超過 6000 個 GPU 小時才能訓練，我們致力於開放原始碼和所有模型權重，以促進具身表徵學習的未來研究。專案頁面：https://haoyizhu.github.io/spa/。</paragraph>

##### **Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training**
2410.08202v1 by Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, Xizhou Zhu

The rapid advancement of Large Language Models (LLMs) has led to an influx of
efforts to extend their capabilities to multimodal tasks. Among them, growing
attention has been focused on monolithic Multimodal Large Language Models
(MLLMs) that integrate visual encoding and language decoding into a single LLM.
Despite the structural simplicity and deployment-friendliness, training a
monolithic MLLM with promising performance still remains challenging. In
particular, the popular approaches adopt continuous pre-training to extend a
pre-trained LLM to a monolithic MLLM, which suffers from catastrophic
forgetting and leads to performance degeneration. In this paper, we aim to
overcome this limitation from the perspective of delta tuning. Specifically,
our core idea is to embed visual parameters into a pre-trained LLM, thereby
incrementally learning visual knowledge from massive data via delta tuning,
i.e., freezing the LLM when optimizing the visual parameters. Based on this
principle, we present Mono-InternVL, a novel monolithic MLLM that seamlessly
integrates a set of visual experts via a multimodal mixture-of-experts
structure. Moreover, we propose an innovative pre-training strategy to maximize
the visual capability of Mono-InternVL, namely Endogenous Visual Pre-training
(EViP). In particular, EViP is designed as a progressive learning process for
visual experts, which aims to fully exploit the visual knowledge from noisy
data to high-quality data. To validate our approach, we conduct extensive
experiments on 16 benchmarks. Experimental results not only validate the
superior performance of Mono-InternVL compared to the state-of-the-art MLLM on
6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but
also confirm its better deployment efficiency, with first token latency reduced
by up to 67%.

摘要：大型語言模型 (LLM) 的快速進展已導致大量努力將其能力擴展到多模態任務。其中，越來越多的關注集中在單體多模態大型語言模型 (MLLM) 上，它將視覺編碼和語言解碼整合到一個 LLM 中。儘管結構簡單且易於部署，但訓練一個具有良好性能的單體 MLLM 仍然具有挑戰性。具體而言，流行的方法採用連續預訓練將預訓練的 LLM 擴展到單體 MLLM，這會導致災難性遺忘並導致性能下降。在本文中，我們旨在從增量調整的角度克服這一限制。具體來說，我們的核心思想是將視覺參數嵌入到預訓練的 LLM 中，從而通過增量調整從海量數據中增量學習視覺知識，即在優化視覺參數時凍結 LLM。基於這一原理，我們提出了 Mono-InternVL，這是一個新穎的單體 MLLM，它通過多模態專家混合結構無縫整合了一組視覺專家。此外，我們提出了一種創新的預訓練策略，以最大化 Mono-InternVL 的視覺能力，即內生視覺預訓練 (EViP)。具體來說，EViP 被設計為視覺專家的漸進學習過程，旨在充分利用從噪聲數據到高質量數據的視覺知識。為了驗證我們的做法，我們在 16 個基準上進行了廣泛的實驗。實驗結果不僅驗證了 Mono-InternVL 與最先進的 MLLM 在 6 個多模態基準上的卓越性能，例如，在 OCRBench 上比 InternVL-1.5 高 113 分，而且還確認了其更好的部署效率，首個令牌延遲降低了高達 67%。

##### **From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions**
2410.08197v1 by Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Tool learning enables Large Language Models (LLMs) to interact with external
environments by invoking tools, serving as an effective strategy to mitigate
the limitations inherent in their pre-training data. In this process, tool
documentation plays a crucial role by providing usage instructions for LLMs,
thereby facilitating effective tool utilization. This paper concentrates on the
critical challenge of bridging the comprehension gap between LLMs and external
tools due to the inadequacies and inaccuracies inherent in existing
human-centric tool documentation. We propose a novel framework, DRAFT, aimed at
Dynamically Refining tool documentation through the Analysis of Feedback and
Trails emanating from LLMs' interactions with external tools. This methodology
pivots on an innovative trial-and-error approach, consisting of three distinct
learning phases: experience gathering, learning from experience, and
documentation rewriting, to iteratively enhance the tool documentation. This
process is further optimized by implementing a diversity-promoting exploration
strategy to ensure explorative diversity and a tool-adaptive termination
mechanism to prevent overfitting while enhancing efficiency. Extensive
experiments on multiple datasets demonstrate that DRAFT's iterative,
feedback-based refinement significantly ameliorates documentation quality,
fostering a deeper comprehension and more effective utilization of tools by
LLMs. Notably, our analysis reveals that the tool documentation refined via our
approach demonstrates robust cross-model generalization capabilities.

摘要：工具學習讓大型語言模型 (LLM) 能夠透過呼叫工具與外部環境互動，作為一種有效的策略來減輕其預訓練資料中固有的限制。在此過程中，工具文件扮演了關鍵的角色，為 LLM 提供使用說明，從而促進有效的工具利用。本文專注於由於現有人類為中心的工具文件中的不足和不準確性，在 LLM 和外部工具之間彌合理解差距的重大挑戰。我們提出了一個新的框架 DRAFT，旨在透過分析 LLM 與外部工具互動產生的回饋和軌跡，動態精煉工具文件。此方法基於創新的試錯法，包含三個不同的學習階段：經驗收集、從經驗中學習和文件重寫，以反覆增強工具文件。此流程進一步透過實施促進多樣性的探索策略來最佳化，以確保探索的多樣性，並透過工具適應終止機制來防止過度擬合，同時提高效率。在多個資料集上進行的廣泛實驗表明，DRAFT 的迭代式、基於回饋的精煉顯著改善了文件品質，促進 LLM 對工具的更深入理解和更有效的利用。值得注意的是，我們的分析表明，透過我們的方法精煉的工具文件展示了強大的跨模型泛化能力。

##### **MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code**
2410.08196v1 by Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li

Code has been shown to be effective in enhancing the mathematical reasoning
abilities of large language models due to its precision and accuracy. Previous
works involving continued mathematical pretraining often include code that
utilizes math-related packages, which are primarily designed for fields such as
engineering, machine learning, signal processing, or module testing, rather
than being directly focused on mathematical reasoning. In this paper, we
introduce a novel method for generating mathematical code accompanied with
corresponding reasoning steps for continued pretraining. Our approach begins
with the construction of a high-quality mathematical continued pretraining
dataset by incorporating math-related web data, code using mathematical
packages, math textbooks, and synthetic data. Next, we construct reasoning
steps by extracting LaTeX expressions, the conditions needed for the
expressions, and the results of the expressions from the previously collected
dataset. Based on this extracted information, we generate corresponding code to
accurately capture the mathematical reasoning process. Appending the generated
code to each reasoning step results in data consisting of paired natural
language reasoning steps and their corresponding code. Combining this data with
the original dataset results in a 19.2B-token high-performing mathematical
pretraining corpus, which we name MathCode-Pile. Training several popular base
models with this corpus significantly improves their mathematical abilities,
leading to the creation of the MathCoder2 family of models. All of our data
processing and training code is open-sourced, ensuring full transparency and
easy reproducibility of the entire data collection and training pipeline. The
code is released at https://github.com/mathllm/MathCoder2 .

摘要：<paragraph>程式碼已被證實能有效提升大型語言模型的數學推理能力，因為它精準且準確。先前涉及持續數學預訓練的研究，通常包含使用與數學相關套件的程式碼，這些套件主要設計給工程、機器學習、訊號處理或模組測試等領域，而不是直接專注於數學推理。在本文中，我們介紹一種新的方法，用於產生數學程式碼，並附上對應的推理步驟，以持續進行預訓練。我們的做法從建立一個高品質的數學持續預訓練資料集開始，方法是納入與數學相關的網路資料、使用數學套件的程式碼、數學教科書和合成資料。接下來，我們透過從先前收集的資料集中萃取 LaTeX 表達式、表達式所需的條件，以及表達式的結果，來建構推理步驟。根據這些萃取的資訊，我們產生對應的程式碼，以準確捕捉數學推理過程。將產生的程式碼附加到每個推理步驟，就會產生由成對的自然語言推理步驟及其對應程式碼組成的資料。將這些資料與原始資料集結合，就會產生一個 19.2B 個符號的高效能數學預訓練語料庫，我們將其命名為 MathCode-Pile。使用這個語料庫訓練幾個熱門的基本模型，大幅提升了它們的數學能力，進而創造出 MathCoder2 模型家族。我們所有的資料處理和訓練程式碼都是開源的，確保整個資料收集和訓練管線的完全透明度和容易重現性。程式碼已於 https://github.com/mathllm/MathCoder2 發布。</paragraph>

##### **GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment**
2410.08193v1 by Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh

Large Language Models (LLMs) exhibit impressive capabilities but require
careful alignment with human preferences. Traditional training-time methods
finetune LLMs using human preference datasets but incur significant training
costs and require repeated training to handle diverse user preferences.
Test-time alignment methods address this by using reward models (RMs) to guide
frozen LLMs without retraining. However, existing test-time approaches rely on
trajectory-level RMs which are designed to evaluate complete responses, making
them unsuitable for autoregressive text generation that requires computing
next-token rewards from partial responses. To address this, we introduce
GenARM, a test-time alignment approach that leverages the Autoregressive Reward
Model--a novel reward parametrization designed to predict next-token rewards
for efficient and effective autoregressive generation. Theoretically, we
demonstrate that this parametrization can provably guide frozen LLMs toward any
distribution achievable by traditional RMs within the KL-regularized
reinforcement learning framework. Experimental results show that GenARM
significantly outperforms prior test-time alignment baselines and matches the
performance of training-time methods. Additionally, GenARM enables efficient
weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high
costs of training larger models. Furthermore, GenARM supports multi-objective
alignment, allowing real-time trade-offs between preference dimensions and
catering to diverse user preferences without retraining.

摘要：大型語言模型 (LLM) 展示了令人印象深刻的能力，但需要仔細配合人類的偏好。傳統的訓練時間方法使用人類偏好資料集微調 LLM，但會產生大量的訓練成本，並且需要反覆訓練才能處理不同的使用者偏好。測試時間對齊方法透過使用獎勵模型 (RM) 來指導凍結的 LLM 而無需重新訓練來解決這個問題。然而，現有的測試時間方法依賴於軌跡級別的 RM，這些 RM 被設計用於評估完整的回應，這使得它們不適合需要從部分回應中計算下一個代幣獎勵的自迴歸文本生成。為了解決這個問題，我們引入了 GenARM，一種測試時間對齊方法，它利用自迴歸獎勵模型——一種新穎的獎勵參數化，旨在預測下一個代幣獎勵，以實現高效且有效的自迴歸生成。在理論上，我們證明了這個參數化可以證明引導凍結的 LLM 朝著任何在 KL 正則化強化學習框架內由傳統 RM 可實現的分布。實驗結果表明，GenARM 明顯優於先前的測試時間對齊基準，並且與訓練時間方法的性能相匹配。此外，GenARM 能夠實現高效的弱到強引導，在不增加訓練大型模型的高成本的情況下，將較大的 LLM 與較小的 RM 對齊。此外，GenARM 支援多目標對齊，允許在偏好維度之間進行實時權衡，並在不重新訓練的情況下迎合不同的使用者偏好。

##### **DifFRelight: Diffusion-Based Facial Performance Relighting**
2410.08188v1 by Mingming He, Pascal Clausen, Ahmet Levent Taşel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, Ning Yu, Paul Debevec

We present a novel framework for free-viewpoint facial performance relighting
using diffusion-based image-to-image translation. Leveraging a subject-specific
dataset containing diverse facial expressions captured under various lighting
conditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, we
train a diffusion model for precise lighting control, enabling high-fidelity
relit facial images from flat-lit inputs. Our framework includes
spatially-aligned conditioning of flat-lit captures and random noise, along
with integrated lighting information for global control, utilizing prior
knowledge from the pre-trained Stable Diffusion model. This model is then
applied to dynamic facial performances captured in a consistent flat-lit
environment and reconstructed for novel-view synthesis using a scalable dynamic
3D Gaussian Splatting method to maintain quality and consistency in the relit
results. In addition, we introduce unified lighting control by integrating a
novel area lighting representation with directional lighting, allowing for
joint adjustments in light size and direction. We also enable high dynamic
range imaging (HDRI) composition using multiple directional lights to produce
dynamic sequences under complex lighting conditions. Our evaluations
demonstrate the models efficiency in achieving precise lighting control and
generalizing across various facial expressions while preserving detailed
features such as skintexture andhair. The model accurately reproduces complex
lighting effects like eye reflections, subsurface scattering, self-shadowing,
and translucency, advancing photorealism within our framework.

摘要：<paragraph>我們提出一個新穎的架構，用於自由視點人臉性能重新打光，使用基於擴散的影像轉影像轉換。利用包含在各種光照條件下擷取的各種面部表情的主題特定資料集，包括平面光和一次一光（OLAT）場景，我們訓練一個擴散模型以進行精確的光照控制，從平面光輸入中啟用高保真重新點亮的面部影像。我們的架構包括平面光擷取和隨機雜訊的空間對齊條件，以及用於全局控制的整合光照資訊，利用預先訓練的 Stable Diffusion 模型的先驗知識。然後將此模型應用於在一致的平面光環境中擷取的動態面部表演，並使用可擴充的動態 3D 高斯噴繪方法重建以進行新視圖合成，以維持重新點亮結果的品質和一致性。此外，我們透過整合一個新穎區域光照表示與方向光照來引入統一光照控制，允許對光照大小和方向進行聯合調整。我們還啟用使用多個方向光的高動態範圍成像（HDRI）合成，以在複雜的光照條件下產生動態序列。我們的評估證明了該模型在實現精確光照控制和概括各種面部表情方面的效率，同時保留了皮膚紋理和頭髮等詳細特徵。該模型準確地重現了複雜的光照效果，例如眼睛反射、次表面散射、自遮蔽和半透明，推動了我們架構中的寫實主義。</paragraph>

##### **MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models**
2410.08182v1 by Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, Nanyun Peng

Existing multimodal retrieval benchmarks primarily focus on evaluating
whether models can retrieve and utilize external textual knowledge for question
answering. However, there are scenarios where retrieving visual information is
either more beneficial or easier to access than textual data. In this paper, we
introduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in
which we systematically identify and categorize scenarios where visually
augmented knowledge is better than textual knowledge, for instance, more images
from varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353
human-annotated multiple-choice questions across 9 distinct scenarios. With
MRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large
vision-language models (LVLMs). Our results show that all LVLMs exhibit greater
improvements when augmented with images compared to textual knowledge,
confirming that MRAG-Bench is vision-centric. Additionally, we conduct
extensive analysis with MRAG-Bench, which offers valuable insights into
retrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces
challenges in effectively leveraging retrieved knowledge, achieving only a
5.82% improvement with ground-truth information, in contrast to a 33.16%
improvement observed in human participants. These findings highlight the
importance of MRAG-Bench in encouraging the community to enhance LVLMs' ability
to utilize retrieved visual knowledge more effectively.

摘要：現有的多模態檢索基準主要集中在評估模型是否能檢索和利用外部文本知識來回答問題。然而，在某些情況下，檢索視覺資訊比檢索文字資料更有利或更容易。在本文中，我們引進了一個多模態檢索增強生成基準 MRAG-Bench，其中我們系統性地找出並分類出視覺增強知識優於文字知識的情況，例如更多來自不同視角的影像。MRAG-Bench 包含 16,130 張影像和 1,353 個由人類註解的多重選擇題，涵蓋 9 種不同的情況。透過 MRAG-Bench，我們對 10 個開源和 4 個專有的大型視覺語言模型 (LVLMs) 進行評估。我們的結果顯示，所有 LVLMs 在加入影像後都展現出比加入文字知識時更大的進步，證實 MRAG-Bench 以視覺為中心。此外，我們使用 MRAG-Bench 進行廣泛的分析，這提供了有價值的見解，讓我們深入了解檢索增強 LVLMs。值得注意的是，表現最佳的模型 GPT-4o 在有效利用檢索到的知識方面面臨挑戰，僅獲得 5.82% 的真實資訊進步，與人類參與者觀察到的 33.16% 進步形成對比。這些發現突顯了 MRAG-Bench 的重要性，它鼓勵社群提升 LVLMs 更有效地利用檢索到的視覺知識的能力。

##### **Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models**
2410.08174v1 by Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng

Multimodal Large Language Models (MLLMs) exhibit promising advancements
across various tasks, yet they still encounter significant trustworthiness
issues. Prior studies apply Split Conformal Prediction (SCP) in language
modeling to construct prediction sets with statistical guarantees. However,
these methods typically rely on internal model logits or are restricted to
multiple-choice settings, which hampers their generalizability and adaptability
in dynamic, open-ended environments. In this paper, we introduce TRON, a
two-step framework for risk control and assessment, applicable to any MLLM that
supports sampling in both open-ended and closed-ended scenarios. TRON comprises
two main components: (1) a novel conformal score to sample response sets of
minimum size, and (2) a nonconformity score to identify high-quality responses
based on self-consistency theory, controlling the error rates by two specific
risk levels. Furthermore, we investigate semantic redundancy in prediction sets
within open-ended contexts for the first time, leading to a promising
evaluation metric for MLLMs based on average set size. Our comprehensive
experiments across four Video Question-Answering (VideoQA) datasets utilizing
eight MLLMs show that TRON achieves desired error rates bounded by two
user-specified risk levels. Additionally, deduplicated prediction sets maintain
adaptiveness while being more efficient and stable for risk assessment under
different risk levels.

摘要：多模态大型语言模型 (MLLM) 在各种任务中表现出有希望的进步，但它们仍然遇到重大的可信度问题。先前的研究在语言建模中应用分裂保形预测 (SCP) 来构建具有统计保证的预测集。然而，这些方法通常依赖于内部模型 logit 或仅限于多项选择设置，这阻碍了它们在动态、开放式环境中的泛化性和适应性。在本文中，我们介绍了 TRON，这是一个适用于任何支持在开放式和封闭式场景中进行采样的 MLLM 的风险控制和评估的两步框架。TRON 包含两个主要组件：(1) 一个新颖的保形分数，用于对最小大小的响应集进行采样，以及 (2) 一个不符合分数，用于根据自洽性理论识别高质量的响应，通过两个特定的风险级别控制错误率。此外，我们首次研究了开放式语境中预测集中的语义冗余，从而为基于平均集大小的 MLLM 提供了一个有希望的评估指标。我们在利用八个 MLLM 的四个视频问答 (VideoQA) 数据集上进行的全面实验表明，TRON 实现了由两个用户指定的风险级别限定的期望错误率。此外，重复数据删除的预测集保持适应性，同时在不同风险级别下对风险评估更有效、更稳定。

##### **On the Evaluation of Generative Robotic Simulations**
2410.08172v1 by Feng Chen, Botian Xu, Pu Hua, Peiqi Duan, Yanchao Yang, Yi Ma, Huazhe Xu

Due to the difficulty of acquiring extensive real-world data, robot
simulation has become crucial for parallel training and sim-to-real transfer,
highlighting the importance of scalable simulated robotic tasks. Foundation
models have demonstrated impressive capacities in autonomously generating
feasible robotic tasks. However, this new paradigm underscores the challenge of
adequately evaluating these autonomously generated tasks. To address this, we
propose a comprehensive evaluation framework tailored to generative
simulations. Our framework segments evaluation into three core aspects:
quality, diversity, and generalization. For single-task quality, we evaluate
the realism of the generated task and the completeness of the generated
trajectories using large language models and vision-language models. In terms
of diversity, we measure both task and data diversity through text similarity
of task descriptions and world model loss trained on collected task
trajectories. For task-level generalization, we assess the zero-shot
generalization ability on unseen tasks of a policy trained with multiple
generated tasks. Experiments conducted on three representative task generation
pipelines demonstrate that the results from our framework are highly consistent
with human evaluations, confirming the feasibility and validity of our
approach. The findings reveal that while metrics of quality and diversity can
be achieved through certain methods, no single approach excels across all
metrics, suggesting a need for greater focus on balancing these different
metrics. Additionally, our analysis further highlights the common challenge of
low generalization capability faced by current works. Our anonymous website:
https://sites.google.com/view/evaltasks.

摘要：<paragraph>由於難以取得廣泛的真實世界資料，機器人模擬已成為平行訓練和模擬到真實轉移的關鍵，強調可擴充模擬機器人任務的重要性。基礎模型已展示出自主產生可行機器人任務的驚人能力。然而，這個新範例強調了充分評估這些自主產生任務的挑戰。為了解決這個問題，我們提出一個量身打造的綜合評估架構，適用於生成模擬。我們的架構將評估分為三個核心面向：品質、多樣性和概化。對於單一任務品質，我們使用大型語言模型和視覺語言模型評估生成任務的真實性和生成軌跡的完整性。在多樣性方面，我們透過任務描述的文字相似性和在收集的任務軌跡上訓練的世界模型損失，來衡量任務和資料的多樣性。對於任務層級的概化，我們評估在使用多個生成任務訓練的策略中，對未見任務的零次學習概化能力。在三個代表性任務生成管線上進行的實驗證明，我們架構的結果與人類評估高度一致，證實我們方法的可行性和有效性。研究結果顯示，雖然品質和多樣性的指標可透過特定方法達成，但沒有單一方法在所有指標上都表現出色，這表示需要更專注於平衡這些不同的指標。此外，我們的分析進一步強調了當前工作面臨的低概化能力的共同挑戰。我們的匿名網站：https://sites.google.com/view/evaltasks。</paragraph>

##### **Agent S: An Open Agentic Framework that Uses Computers Like a Human**
2410.08164v1 by Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, Xin Eric Wang

We present Agent S, an open agentic framework that enables autonomous
interaction with computers through a Graphical User Interface (GUI), aimed at
transforming human-computer interaction by automating complex, multi-step
tasks. Agent S aims to address three key challenges in automating computer
tasks: acquiring domain-specific knowledge, planning over long task horizons,
and handling dynamic, non-uniform interfaces. To this end, Agent S introduces
experience-augmented hierarchical planning, which learns from external
knowledge search and internal experience retrieval at multiple levels,
facilitating efficient task planning and subtask execution. In addition, it
employs an Agent-Computer Interface (ACI) to better elicit the reasoning and
control capabilities of GUI agents based on Multimodal Large Language Models
(MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the
baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves
a new state-of-the-art. Comprehensive analysis highlights the effectiveness of
individual components and provides insights for future improvements.
Furthermore, Agent S demonstrates broad generalizability to different operating
systems on a newly-released WindowsAgentArena benchmark. Code available at
https://github.com/simular-ai/Agent-S.

摘要：<paragraph>我們提出 Agent S，一個開放的代理框架，可透過圖形使用者介面 (GUI) 與電腦進行自主互動，旨在透過自動化複雜的多步驟任務來轉變人機互動。Agent S 旨在解決自動化電腦任務中的三個關鍵挑戰：擷取特定領域知識、規劃長任務時程，以及處理動態、非統一的介面。為此，Agent S 導入了經驗增強階層式規劃，它會從多個層級的外部知識搜尋和內部經驗擷取中學習，促進有效率的任務規劃和子任務執行。此外，它採用代理電腦介面 (ACI)，以根據多模態大型語言模型 (MLLM) 更佳地引出 GUI 代理的推理和控制能力。在 OSWorld 基準上的評估顯示，Agent S 在成功率上比基準高出 9.37%（相對提升 83.6%），並創下新的技術水準。全面的分析突顯了個別元件的有效性，並為未來的改進提供見解。此外，Agent S 在新發布的 WindowsAgentArena 基準上展示了對不同作業系統的廣泛通用性。程式碼可於 https://github.com/simular-ai/Agent-S 取得。</paragraph>

##### **The Effect of Surprisal on Reading Times in Information Seeking and Repeated Reading**
2410.08162v1 by Keren Gruteke Klein, Yoav Meiri, Omer Shubi, Yevgeni Berzak

The effect of surprisal on processing difficulty has been a central topic of
investigation in psycholinguistics. Here, we use eyetracking data to examine
three language processing regimes that are common in daily life but have not
been addressed with respect to this question: information seeking, repeated
processing, and the combination of the two. Using standard regime-agnostic
surprisal estimates we find that the prediction of surprisal theory regarding
the presence of a linear effect of surprisal on processing times, extends to
these regimes. However, when using surprisal estimates from regime-specific
contexts that match the contexts and tasks given to humans, we find that in
information seeking, such estimates do not improve the predictive power of
processing times compared to standard surprisals. Further, regime-specific
contexts yield near zero surprisal estimates with no predictive power for
processing times in repeated reading. These findings point to misalignments of
task and memory representations between humans and current language models, and
question the extent to which such models can be used for estimating cognitively
relevant quantities. We further discuss theoretical challenges posed by these
results.

摘要：驚訝對處理難度之影響一直是心理語言學調查的中心議題。在此，我們使用眼動追蹤資料來檢視日常生活中常見的三種語言處理模式，但尚未針對此問題加以探討：資訊尋求、重複處理，以及兩者的結合。使用標準的與模式無關的驚訝估計，我們發現驚訝理論對處理時間中驚訝線性效應存在的預測，延伸至這些模式。然而，當使用來自與人類所給予的脈絡和任務相符的模式特定脈絡的驚訝估計時，我們發現，在資訊尋求中，此類估計並未改善處理時間的預測能力，與標準驚訝相比。此外，模式特定脈絡產生接近於零的驚訝估計，在重複閱讀中對處理時間沒有預測能力。這些發現指出人類與當前語言模型之間的任務和記憶表徵的不一致，並質疑此類模型可用於估計認知相關數量。我們進一步討論這些結果提出的理論挑戰。

##### **Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning**
2410.08146v1 by Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar

A promising approach for improving reasoning in large language models is to
use process reward models (PRMs). PRMs provide feedback at each step of a
multi-step reasoning trace, potentially improving credit assignment over
outcome reward models (ORMs) that only provide feedback at the final step.
However, collecting dense, per-step human labels is not scalable, and training
PRMs from automatically-labeled data has thus far led to limited gains. To
improve a base policy by running search against a PRM or using it as dense
rewards for reinforcement learning (RL), we ask: "How should we design process
rewards?". Our key insight is that, to be effective, the process reward for a
step should measure progress: a change in the likelihood of producing a correct
response in the future, before and after taking the step, corresponding to the
notion of step-level advantages in RL. Crucially, this progress should be
measured under a prover policy distinct from the base policy. We theoretically
characterize the set of good provers and our results show that optimizing
process rewards from such provers improves exploration during test-time search
and online RL. In fact, our characterization shows that weak prover policies
can substantially improve a stronger base policy, which we also observe
empirically. We validate our claims by training process advantage verifiers
(PAVs) to predict progress under such provers, and show that compared to ORMs,
test-time search against PAVs is $>8\%$ more accurate, and $1.5-5\times$ more
compute-efficient. Online RL with dense rewards from PAVs enables one of the
first results with $5-6\times$ gain in sample efficiency, and $>6\%$ gain in
accuracy, over ORMs.

摘要：一種改善大型語言模型中推理能力的 promising 方法是使用過程獎勵模型 (PRM)。PRM 在多步驟推理追蹤的每一步提供回饋，潛在改善了對結果獎勵模型 (ORM) 的信用分配，而後者僅在最後一步提供回饋。
然而，收集密集的、按步驟進行的人類標籤並非可擴充的，而從自動標籤資料訓練 PRM 至今僅獲得有限的進展。為了透過針對 PRM 執行搜尋或將其用作強化學習 (RL) 的密集獎勵來改善基礎政策，我們提出疑問：「我們應如何設計過程獎勵？」我們的關鍵見解在於，過程獎勵若要有效，對於某個步驟而言，應衡量進度：在採取步驟之前和之後，產生正確回應的可能性發生變化，這對應於 RL 中步驟層級優勢的概念。至關重要的是，應在與基礎政策不同的證明者政策下衡量此進度。我們在理論上描述了良好的證明者組合，我們的結果顯示，最佳化來自此類證明者的過程獎勵會改善測試時間搜尋和線上 RL 中的探索。事實上，我們的描述顯示，弱證明者政策可以大幅改善較強的基礎政策，這也是我們在經驗上觀察到的。我們透過訓練過程優勢驗證器 (PAV) 來驗證我們的說法，以預測在這些證明者下的進度，並顯示與 ORM 相比，針對 PAV 的測試時間搜尋準確度提高了 $>8\%$，計算效率提高了 $1.5-5\times$。透過 PAV 的密集獎勵進行線上 RL，獲得了樣本效率提高 $5-6\times$、準確度提高 $>6\%$ 的首批結果，優於 ORM。

##### **Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**
2410.08145v1 by Xiaoyuan Liu, Wenxuan Wang, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Pinjia He, Zhaopeng Tu

This paper explores the problem of commonsense-level vision-knowledge
conflict in Multimodal Large Language Models (MLLMs), where visual information
contradicts model's internal commonsense knowledge (see Figure 1). To study
this issue, we introduce an automated pipeline, augmented with
human-in-the-loop quality control, to establish a benchmark aimed at simulating
and assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted
a diagnostic benchmark comprising 374 original images and 1,122 high-quality
question-answer (QA) pairs. This benchmark covers two types of conflict target
and three question difficulty levels, providing a thorough assessment tool.
Through this benchmark, we evaluate the conflict-resolution capabilities of
nine representative MLLMs across various model families and find a noticeable
over-reliance on textual queries. Drawing on these findings, we propose a novel
prompting strategy, "Focus-on-Vision" (FoV), which markedly enhances MLLMs'
ability to favor visual data over conflicting textual knowledge. Our detailed
analysis and the newly proposed strategy significantly advance the
understanding and mitigating of vision-knowledge conflicts in MLLMs. The data
and code are made publicly available.

摘要：本文探討多模態大型語言模型 (MLLM) 中常識層級視覺知識衝突的問題，其中視覺資訊與模型的內部常識知識相矛盾（見圖 1）。為了研究這個問題，我們引進了一個自動化管道，並結合人工迴圈品質控管，以建立一個基準，用於模擬和評估 MLLM 中的衝突。利用此管道，我們製作了一個診斷基準，包含 374 張原始圖片和 1,122 個高品質的問答 (QA) 配對。此基準涵蓋兩種衝突目標和三個問題難度等級，提供了一個全面的評估工具。透過此基準，我們評估了九個代表性 MLLM 在不同模型系列中的衝突解決能力，並發現明顯過度依賴文字查詢。根據這些發現，我們提出了一種新穎的提示策略「專注於視覺」(FoV)，它顯著增強了 MLLM 優先視覺資料而非衝突文字知識的能力。我們詳細的分析和新提出的策略大幅推進了對 MLLM 中視覺知識衝突的理解和緩解。資料和程式碼已公開提供。

##### **DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory**
2410.08143v1 by Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, Min Zhang

Large language models (LLMs) have achieved reasonable quality improvements in
machine translation (MT). However, most current research on MT-LLMs still faces
significant challenges in maintaining translation consistency and accuracy when
processing entire documents. In this paper, we introduce DelTA, a
Document-levEL Translation Agent designed to overcome these limitations. DelTA
features a multi-level memory structure that stores information across various
granularities and spans, including Proper Noun Records, Bilingual Summary,
Long-Term Memory, and Short-Term Memory, which are continuously retrieved and
updated by auxiliary LLM-based components. Experimental results indicate that
DelTA significantly outperforms strong baselines in terms of translation
consistency and quality across four open/closed-source LLMs and two
representative document translation datasets, achieving an increase in
consistency scores by up to 4.58 percentage points and in COMET scores by up to
3.16 points on average. DelTA employs a sentence-by-sentence translation
strategy, ensuring no sentence omissions and offering a memory-efficient
solution compared to the mainstream method. Furthermore, DelTA improves pronoun
translation accuracy, and the summary component of the agent also shows promise
as a tool for query-based summarization tasks. We release our code and data at
https://github.com/YutongWang1216/DocMTAgent.

摘要：大型語言模型 (LLM) 在機器翻譯 (MT) 中獲得了合理的品質提升。然而，目前大多數關於 MT-LLM 的研究在處理完整文件時，在維持翻譯的一致性和準確性方面仍面臨重大挑戰。在本文中，我們介紹了 DelTA，這是一個文件級別翻譯代理，旨在克服這些限制。DelTA 具有多層級記憶體結構，可在各種粒度和範圍內儲存資訊，包括專有名詞記錄、雙語摘要、長期記憶體和短期記憶體，這些記憶體會由輔助的基於 LLM 的組件持續擷取和更新。實驗結果表明，在翻譯一致性和品質方面，DelTA 明顯優於強大的基準，涵蓋四個開放/閉源 LLM 和兩個具代表性的文件翻譯資料集，在一致性評分方面提高了多達 4.58 個百分點，在 COMET 評分方面平均提高了多達 3.16 分。DelTA 採用逐句翻譯策略，確保不遺漏句子，並提供與主流方法相比記憶體效率高的解決方案。此外，DelTA 改善了代名詞翻譯的準確性，而代理的摘要組件也顯示出作為基於查詢的摘要任務工具的潛力。我們在 https://github.com/YutongWang1216/DocMTAgent 釋出我們的程式碼和資料。

##### **Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction**
2410.08134v1 by Jarrid Rector-Brooks, Mohsin Hasan, Zhangzhi Peng, Zachary Quinn, Chenghao Liu, Sarthak Mittal, Nouha Dziri, Michael Bronstein, Yoshua Bengio, Pranam Chatterjee, Alexander Tong, Avishek Joey Bose

Generative modeling of discrete data underlies important applications
spanning text-based agents like ChatGPT to the design of the very building
blocks of life in protein sequences. However, application domains need to exert
control over the generated data by steering the generative process - typically
via RLHF - to satisfy a specified property, reward, or affinity metric. In this
paper, we study the problem of steering Masked Diffusion Models (MDMs), a
recent class of discrete diffusion models that offer a compelling alternative
to traditional autoregressive models. We introduce Discrete Denoising Posterior
Prediction (DDPP), a novel framework that casts the task of steering
pre-trained MDMs as a problem of probabilistic inference by learning to sample
from a target Bayesian posterior. Our DDPP framework leads to a family of three
novel objectives that are all simulation-free, and thus scalable while applying
to general non-differentiable reward functions. Empirically, we instantiate
DDPP by steering MDMs to perform class-conditional pixel-level image modeling,
RLHF-based alignment of MDMs using text-based rewards, and finetuning protein
language models to generate more diverse secondary structures and shorter
proteins. We substantiate our designs via wet-lab validation, where we observe
transient expression of reward-optimized protein sequences.

摘要：生成式离散数据建模是重要应用的基础，涵盖了基于文本的代理（如 ChatGPT）到蛋白质序列中生命基本组成部分的设计。然而，应用领域需要通过控制生成过程（通常通过 RLHF）来控制生成的数据，以满足指定的属性、奖励或亲和度指标。在本文中，我们研究了控制掩码扩散模型 (MDM) 的问题，MDM 是离散扩散模型的最新类别，为传统的自回归模型提供了引人注目的替代方案。我们引入了离散去噪后验预测 (DDPP)，这是一种新颖的框架，它将控制预训练 MDM 的任务转化为概率推理问题，方法是学习从目标贝叶斯后验中采样。我们的 DDPP 框架产生了一系列三个新颖的目标，它们都是无模拟的，因此在应用于一般不可微分奖励函数时具有可扩展性。凭经验，我们通过控制 MDM 来执行基于类的像素级图像建模、使用基于文本奖励的 MDM 的 RLHF 对齐，以及微调蛋白质语言模型来生成更多样化的二级结构和更短的蛋白质，从而实例化 DDPP。我们通过湿实验室验证来证实我们的设计，在该验证中，我们观察到了奖励优化蛋白质序列的瞬时表达。

##### **Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks**
2410.08133v1 by Mathis Pink, Vy A. Vo, Qinyuan Wu, Jianing Mu, Javier S. Turek, Uri Hasson, Kenneth A. Norman, Sebastian Michelmann, Alexander Huth, Mariya Toneva

Current LLM benchmarks focus on evaluating models' memory of facts and
semantic relations, primarily assessing semantic aspects of long-term memory.
However, in humans, long-term memory also includes episodic memory, which links
memories to their contexts, such as the time and place they occurred. The
ability to contextualize memories is crucial for many cognitive tasks and
everyday functions. This form of memory has not been evaluated in LLMs with
existing benchmarks. To address the gap in evaluating memory in LLMs, we
introduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to
study episodic memory in cognitive psychology. SORT requires LLMs to recall the
correct order of text segments, and provides a general framework that is both
easily extendable and does not require any additional annotations. We present
an initial evaluation dataset, Book-SORT, comprising 36k pairs of segments
extracted from 9 books recently added to the public domain. Based on a human
experiment with 155 participants, we show that humans can recall sequence order
based on long-term memory of a book. We find that models can perform the task
with high accuracy when relevant text is given in-context during the SORT
evaluation. However, when presented with the book text only during training,
LLMs' performance on SORT falls short. By allowing to evaluate more aspects of
memory, we believe that SORT will aid in the emerging development of
memory-augmented models.

摘要：現有的 LLM 基準側重於評估模型對事實和語義關係的記憶，主要評估長期記憶的語義方面。然而，在人類中，長期記憶也包括情節記憶，它將記憶與其背景聯繫起來，例如它們發生的時間和地點。將記憶情境化的能力對於許多認知任務和日常功能至關重要。這種形式的記憶尚未在現有基準中使用 LLM 進行評估。為了解決評估 LLM 中記憶的差距，我們引入了序列順序召回任務 (SORT)，我們根據用於研究認知心理學中情節記憶的任務對其進行了改編。SORT 要求 LLM 召回文本片段的正確順序，並提供了一個既容易擴展又不需要任何額外註釋的通用框架。我們提出了初始評估數據集 Book-SORT，其中包含從最近添加到公共領域的 9 本書中提取的 36k 對片段。根據對 155 名參與者的實驗，我們表明人類可以根據一本書的長期記憶來回憶序列順序。我們發現，當在 SORT 評估期間在上下文中給出相關文本時，模型可以非常準確地執行任務。然而，當在訓練期間僅提供書籍文本時，LLM 在 SORT 上的表現會下降。通過允許評估記憶的更多方面，我們相信 SORT 將有助於記憶增強模型的新興發展。

##### **Think Beyond Size: Dynamic Prompting for More Effective Reasoning**
2410.08130v1 by Kamesh R

This paper presents Dynamic Prompting, a novel framework aimed at improving
the reasoning capabilities of Large Language Models (LLMs). In contrast to
conventional static prompting methods, Dynamic Prompting enables the adaptive
modification of prompt sequences and step counts based on real-time task
complexity and model performance. This dynamic adaptation facilitates more
efficient problem-solving, particularly in smaller models, by reducing
hallucinations and repetitive cycles. Our empirical evaluations demonstrate
that Dynamic Prompting allows smaller LLMs to perform competitively with much
larger models, thereby challenging the conventional emphasis on model size as
the primary determinant of reasoning efficacy.

摘要：本文提出動態提示，這是一個新穎的架構，旨在改善大型語言模型 (LLM) 的推理能力。與傳統的靜態提示方法相比，動態提示能夠根據實時任務複雜度和模型效能，自適應地修改提示序列和步驟數。這種動態適應有助於更有效地解決問題，特別是在較小的模型中，因為它能減少幻覺和重複的循環。我們的實證評估表明，動態提示讓較小的 LLM 能與較大的模型競爭，從而挑戰了傳統上強調模型大小作為推理效能主要決定因素的觀點。

##### **Mars: Situated Inductive Reasoning in an Open-World Environment**
2410.08126v1 by Xiaojuan Tang, Jiaqi Li, Yitao Liang, Song-chun Zhu, Muhan Zhang, Zilong Zheng

Large Language Models (LLMs) trained on massive corpora have shown remarkable
success in knowledge-intensive tasks. Yet, most of them rely on pre-stored
knowledge. Inducing new general knowledge from a specific environment and
performing reasoning with the acquired knowledge -- \textit{situated inductive
reasoning}, is crucial and challenging for machine intelligence. In this paper,
we design Mars, an interactive environment devised for situated inductive
reasoning. It introduces counter-commonsense game mechanisms by modifying
terrain, survival setting and task dependency while adhering to certain
principles. In Mars, agents need to actively interact with their surroundings,
derive useful rules and perform decision-making tasks in specific contexts. We
conduct experiments on various RL-based and LLM-based methods, finding that
they all struggle on this challenging situated inductive reasoning benchmark.
Furthermore, we explore \textit{Induction from Reflection}, where we instruct
agents to perform inductive reasoning from history trajectory. The superior
performance underscores the importance of inductive reasoning in Mars. Through
Mars, we aim to galvanize advancements in situated inductive reasoning and set
the stage for developing the next generation of AI systems that can reason in
an adaptive and context-sensitive way.

摘要：<paragraph>在龐大語料庫上訓練的大型語言模型 (LLM) 已在知識密集型任務中展現出驚人的成果。然而，它們大多依賴於預先儲存的知識。從特定環境中推演出新的常識，並利用獲得的知識進行推理——「情境歸納推理」，對於機器智慧而言至關重要且具有挑戰性。在本文中，我們設計了火星，一個專為情境歸納推理而設計的互動式環境。它透過修改地形、生存設定和任務依賴關係，引入了反常識的遊戲機制，同時遵守某些原則。在火星中，代理人需要積極與周圍環境互動，推導出有用的規則，並在特定情境中執行決策任務。我們對各種基於 RL 和基於 LLM 的方法進行實驗，發現它們在這個具有挑戰性的情境歸納推理基準上都面臨困難。此外，我們探討了「從反思中歸納」，其中我們指示代理人從歷史軌跡中執行歸納推理。優異的表現強調了歸納推理在火星中的重要性。透過火星，我們旨在激勵情境歸納推理的進展，並為開發下一代能夠以適應性和情境敏感的方式推理的人工智慧系統奠定基礎。</paragraph>

##### **Heterogeneous Graph Auto-Encoder for CreditCard Fraud Detection**
2410.08121v1 by Moirangthem Tiken Singh, Rabinder Kumar Prasad, Gurumayum Robert Michael, N K Kaphungkui, N. Hemarjit Singh

The digital revolution has significantly impacted financial transactions,
leading to a notable increase in credit card usage. However, this convenience
comes with a trade-off: a substantial rise in fraudulent activities.
Traditional machine learning methods for fraud detection often struggle to
capture the inherent interconnectedness within financial data. This paper
proposes a novel approach for credit card fraud detection that leverages Graph
Neural Networks (GNNs) with attention mechanisms applied to heterogeneous graph
representations of financial data. Unlike homogeneous graphs, heterogeneous
graphs capture intricate relationships between various entities in the
financial ecosystem, such as cardholders, merchants, and transactions,
providing a richer and more comprehensive data representation for fraud
analysis. To address the inherent class imbalance in fraud data, where genuine
transactions significantly outnumber fraudulent ones, the proposed approach
integrates an autoencoder. This autoencoder, trained on genuine transactions,
learns a latent representation and flags deviations during reconstruction as
potential fraud. This research investigates two key questions: (1) How
effectively can a GNN with an attention mechanism detect and prevent credit
card fraud when applied to a heterogeneous graph? (2) How does the efficacy of
the autoencoder with attention approach compare to traditional methods? The
results are promising, demonstrating that the proposed model outperforms
benchmark algorithms such as Graph Sage and FI-GRL, achieving a superior AUC-PR
of 0.89 and an F1-score of 0.81. This research significantly advances fraud
detection systems and the overall security of financial transactions by
leveraging GNNs with attention mechanisms and addressing class imbalance
through an autoencoder.

摘要：<paragraph>數位革命對金融交易產生重大影響，導致信用卡使用率大幅增加。然而，這種便利性也帶來了一個取捨：詐騙活動大幅增加。傳統的機器學習方法常難以捕捉金融資料中固有的相互關聯性。本文提出了一種新的信用卡詐騙偵測方法，它利用圖神經網路 (GNN) 和注意機制，應用於金融資料的異質圖表表示。與同質圖表不同，異質圖表捕捉了金融生態系統中各種實體之間的複雜關係，例如持卡人、商家和交易，為詐騙分析提供更豐富、更全面的資料表示。為了解決詐騙資料中固有的類別不平衡問題，其中真實交易遠多於詐騙交易，所提出的方法整合了一個自動編碼器。這個自動編碼器在真實交易中訓練，學習潛在表示，並在重建過程中標記偏差為潛在詐騙。本研究探討了兩個關鍵問題：(1) 當應用於異質圖表時，具有注意力機制的 GNN 在偵測和預防信用卡詐騙方面有多有效？(2) 具有注意力方法的自動編碼器的效能與傳統方法相比如何？結果令人滿意，證明所提出的模型優於基準演算法，例如 Graph Sage 和 FI-GRL，達到 0.89 的優異 AUC-PR 和 0.81 的 F1 分數。本研究透過利用具有注意力機制的 GNN，並透過自動編碼器解決類別不平衡問題，大幅提升了詐騙偵測系統和金融交易的整體安全性。</paragraph>

##### **Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System**
2410.08115v1 by Weize Chen, Jiarui Yuan, Chen Qian, Cheng Yang, Zhiyuan Liu, Maosong Sun

Large Language Model (LLM) based multi-agent systems (MAS) show remarkable
potential in collaborative problem-solving, yet they still face critical
challenges: low communication efficiency, poor scalability, and a lack of
effective parameter-updating optimization methods. We present Optima, a novel
framework that addresses these issues by significantly enhancing both
communication efficiency and task effectiveness in LLM-based MAS through LLM
training. Optima employs an iterative generate, rank, select, and train
paradigm with a reward function balancing task performance, token efficiency,
and communication readability. We explore various RL algorithms, including
Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid
approaches, providing insights into their effectiveness-efficiency trade-offs.
We integrate Monte Carlo Tree Search-inspired techniques for DPO data
generation, treating conversation turns as tree nodes to explore diverse
interaction paths. Evaluated on common multi-agent tasks, including
information-asymmetric question answering and complex reasoning, Optima shows
consistent and substantial improvements over single-agent baselines and vanilla
MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than
10\% tokens on tasks requiring heavy information exchange. Moreover, Optima's
efficiency gains open new possibilities for leveraging inference-compute more
effectively, leading to improved inference-time scaling laws. By addressing
fundamental challenges in LLM-based MAS, Optima shows the potential towards
scalable, efficient, and effective MAS
(https://chenweize1998.github.io/optima-project-page).

摘要：<paragraph>大型語言模型 (LLM) 為基礎的多重代理系統 (MAS) 在協作問題解決方面展現了非凡的潛力，但仍面臨關鍵挑戰：低溝通效率、可擴充性差，以及缺乏有效的參數更新最佳化方法。我們提出 Optima，一個新穎的架構，透過 LLM 訓練大幅提升 LLM 為基礎的 MAS 中的溝通效率和任務效能，來解決這些問題。Optima 採用反覆生成、排名、選擇和訓練的範例，其獎勵函數平衡了任務表現、符號效率和溝通可讀性。我們探討了各種 RL 演算法，包括監督微調、直接偏好最佳化，以及其混合方法，進一步了解其效能效率的權衡取捨。我們整合了蒙地卡羅樹狀搜尋啟發的技術，用於 DPO 資料產生，將對話回合視為樹狀節點，以探索多樣化的互動路徑。在常見的多重代理任務中進行評估，包括資訊不對稱問答和複雜推理，Optima 在單一代理基準和基於 Llama 3 8B 的香草 MAS 上展現了一致且顯著的進步，在需要大量資訊交換的任務中，以不到 10% 的符號獲得了高達 2.8 倍的效能提升。此外，Optima 的效率提升為更有效地運用推理運算開啟了新的可能性，進而改善推理時間的縮放定律。透過解決 LLM 為基礎的 MAS 中的基本挑戰，Optima 展現了邁向可擴充、高效和有效的 MAS 的潛力
(https://chenweize1998.github.io/optima-project-page)。</paragraph>

##### **Robust AI-Generated Text Detection by Restricted Embeddings**
2410.08113v1 by Kristian Kuznetsov, Eduard Tulchinskii, Laida Kushnareva, German Magai, Serguei Barannikov, Sergey Nikolenko, Irina Piontkovskaya

Growing amount and quality of AI-generated texts makes detecting such content
more difficult. In most real-world scenarios, the domain (style and topic) of
generated data and the generator model are not known in advance. In this work,
we focus on the robustness of classifier-based detectors of AI-generated text,
namely their ability to transfer to unseen generators or semantic domains. We
investigate the geometry of the embedding space of Transformer-based text
encoders and show that clearing out harmful linear subspaces helps to train a
robust classifier, ignoring domain-specific spurious features. We investigate
several subspace decomposition and feature selection strategies and achieve
significant improvements over state of the art methods in cross-domain and
cross-generator transfer. Our best approaches for head-wise and
coordinate-based subspace removal increase the mean out-of-distribution (OOD)
classification score by up to 9% and 14% in particular setups for RoBERTa and
BERT embeddings respectively. We release our code and data:
https://github.com/SilverSolver/RobustATD

摘要：人工智能生成文本的数量和质量不断提高，使得检测此类内容变得更加困难。在大多数实际场景中，生成数据的域（样式和主题）和生成器模型事先并不知道。在这项工作中，我们专注于基于分类器的 AI 生成的文本检测器的鲁棒性，即它们转移到未见的生成器或语义域的能力。我们研究了基于 Transformer 的文本编码器的嵌入空间的几何形状，并表明清除有害的线性子空间有助于训练鲁棒分类器，忽略特定于域的虚假特征。我们研究了几个子空间分解和特征选择策略，并在跨域和跨生成器转移中实现了对现有技术的重大改进。我们用于逐头和基于坐标的子空间移除的最佳方法将平均分布外 (OOD) 分类分数分别提高了 RoBERTa 和 BERT 嵌入的特定设置中的 9% 和 14%。我们发布我们的代码和数据：https://github.com/SilverSolver/RobustATD

##### **Active Fourier Auditor for Estimating Distributional Properties of ML Models**
2410.08111v1 by Ayoub Ajarra, Bishwamittra Ghosh, Debabrota Basu

With the pervasive deployment of Machine Learning (ML) models in real-world
applications, verifying and auditing properties of ML models have become a
central concern. In this work, we focus on three properties: robustness,
individual fairness, and group fairness. We discuss two approaches for auditing
ML model properties: estimation with and without reconstruction of the target
model under audit. Though the first approach is studied in the literature, the
second approach remains unexplored. For this purpose, we develop a new
framework that quantifies different properties in terms of the Fourier
coefficients of the ML model under audit but does not parametrically
reconstruct it. We propose the Active Fourier Auditor (AFA), which queries
sample points according to the Fourier coefficients of the ML model, and
further estimates the properties. We derive high probability error bounds on
AFA's estimates, along with the worst-case lower bounds on the sample
complexity to audit them. Numerically we demonstrate on multiple datasets and
models that AFA is more accurate and sample-efficient to estimate the
properties of interest than the baselines.

摘要：隨著機器學習 (ML) 模型在實際應用中廣泛部署，驗證和審核 ML 模型的屬性已成為一項核心關注事項。在這項工作中，我們專注於三個屬性：穩健性、個別公平性和群體公平性。我們討論了審核 ML 模型屬性的兩種方法：在有和沒有重建審核目標模型的情況下進行估計。儘管第一種方法在文獻中得到了研究，但第二種方法仍未得到探索。為此，我們開發了一個新的框架，該框架根據審核中的 ML 模型的傅立葉係數對不同的屬性進行量化，但不會對其進行參數化重建。我們提出了主動傅立葉審核器 (AFA)，它根據 ML 模型的傅立葉係數查詢樣本點，並進一步估計屬性。我們推導出 AFA 估計的高機率誤差界限，以及審核它們的樣本複雜度的最壞情況下界。我們在多個數據集和模型上數值證明，AFA 比基準線更準確且更具樣本效率，可以估計感興趣的屬性。

##### **A Closer Look at Machine Unlearning for Large Language Models**
2410.08109v1 by Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin

Large language models (LLMs) may memorize sensitive or copyrighted content,
raising privacy and legal concerns. Due to the high cost of retraining from
scratch, researchers attempt to employ machine unlearning to remove specific
content from LLMs while preserving the overall performance. In this paper, we
discuss several issues in machine unlearning for LLMs and provide our insights
on possible approaches. To address the issue of inadequate evaluation of model
outputs after unlearning, we introduce three additional metrics to evaluate
token diversity, sentence semantics, and factual correctness. We then
categorize unlearning methods into untargeted and targeted, and discuss their
issues respectively. Specifically, the behavior that untargeted unlearning
attempts to approximate is unpredictable and may involve hallucinations, and
existing regularization is insufficient for targeted unlearning. To alleviate
these issues, we propose using the objective of maximizing entropy (ME) for
untargeted unlearning and incorporate answer preservation (AP) loss as
regularization for targeted unlearning. Experimental results across three
scenarios, i.e., fictitious unlearning, continual unlearning, and real-world
unlearning, demonstrate the effectiveness of our approaches. The code is
available at https://github.com/sail-sg/closer-look-LLM-unlearning.

摘要：大型語言模型 (LLM) 可能會記憶敏感或受版權保護的內容，引發隱私和法律問題。由於從頭開始重新訓練的成本很高，研究人員嘗試採用機器遺忘來移除 LLM 中的特定內容，同時保留整體效能。在本文中，我們討論了 LLM 機器遺忘的幾個問題，並提供我們對可能方法的見解。為了解決遺忘後模型輸出的評估不足問題，我們引入了三個額外的指標來評估代幣多樣性、句子語義和事實正確性。然後，我們將遺忘方法分類為非目標和目標，並分別討論它們的問題。具體來說，非目標遺忘嘗試近似的行為是不可預測的，可能涉及幻覺，而現有的正則化不足以進行目標遺忘。為了緩解這些問題，我們建議使用最大化熵 (ME) 的目標進行非目標遺忘，並將答案保留 (AP) 損失納入目標遺忘的正則化。在虛構遺忘、持續遺忘和真實世界遺忘這三種場景中的實驗結果證明了我們方法的有效性。程式碼可在 https://github.com/sail-sg/closer-look-LLM-unlearning 取得。

##### **What Makes Large Language Models Reason in (Multi-Turn) Code Generation?**
2410.08105v1 by Kunhao Zheng, Juliette Decugis, Jonas Gehring, Taco Cohen, Benjamin Negrevergne, Gabriel Synnaeve

Prompting techniques such as chain-of-thought have established themselves as
a popular vehicle for improving the outputs of large language models (LLMs).
For code generation, however, their exact mechanics and efficacy are
under-explored. We thus investigate the effects of a wide range of prompting
strategies with a focus on automatic re-prompting over multiple turns and
computational requirements. After systematically decomposing reasoning,
instruction, and execution feedback prompts, we conduct an extensive grid
search on the competitive programming benchmarks CodeContests and TACO for
multiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).
Our study reveals strategies that consistently improve performance across all
models with small and large sampling budgets. We then show how finetuning with
such an optimal configuration allows models to internalize the induced
reasoning process and obtain improvements in performance and scalability for
multi-turn code generation.

摘要：提示技術，例如思想鏈，已確立為改善大型語言模型 (LLM) 輸出的熱門載體。
然而，對於程式碼產生，它們確切的機制和效力尚未得到充分探討。因此，我們研究了各種提示策略的影響，重點關注多輪自動重新提示和運算需求。在系統分解推理、指令和執行回饋提示後，我們對競爭性程式設計基準 CodeContests 和 TACO 進行了廣泛的網格搜尋，以涵蓋多個 LLM 家族和規模（Llama 3.0 和 3.1、8B、70B、405B 和 GPT-4o）。
我們的研究揭示了在所有模型中持續改善效能的策略，這些模型具有小和大抽樣預算。然後，我們展示了如何使用此類最佳配置進行微調，使模型能夠內化誘導推理過程，並獲得多輪程式碼產生的效能和可擴充性改善。

##### **Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining**
2410.08102v1 by Tianyi Bai, Ling Yang, Zhen Hao Wong, Jiahui Peng, Xinlin Zhuang, Chi Zhang, Lijun Wu, Qiu Jiantao, Wentao Zhang, Binhang Yuan, Conghui He

Efficient data selection is crucial to accelerate the pretraining of large
language models (LLMs). While various methods have been proposed to enhance
data efficiency, limited research has addressed the inherent conflicts between
these approaches to achieve optimal data selection for LLM pretraining. To
tackle this problem, we propose a novel multi-agent collaborative data
selection mechanism. In this framework, each data selection method serves as an
independent agent, and an agent console is designed to dynamically integrate
the information from all agents throughout the LLM training process. We conduct
extensive empirical studies to evaluate our multi-agent framework. The
experimental results demonstrate that our approach significantly improves data
efficiency, accelerates convergence in LLM training, and achieves an average
performance gain of 10.5% across multiple language model benchmarks compared to
the state-of-the-art methods.

摘要：有效率的資料選取對於加速大型語言模型 (LLM) 的預訓練至關重要。雖然已經提出了各種方法來提升資料效率，但針對這些方法之間的固有衝突以達成 LLM 預訓練的最佳資料選取，相關研究卻很有限。為了解決這個問題，我們提出了一種新穎的多重代理人協作資料選取機制。在這個架構中，每個資料選取方法都作為一個獨立的代理人，而一個代理人控制台則被設計成在 LLM 訓練過程中動態整合所有代理人的資訊。我們進行了廣泛的實證研究來評估我們的多重代理人架構。實驗結果證明，我們的方法顯著提升了資料效率，加速了 LLM 訓練中的收斂速度，並且在多個語言模型基準測試中，與最先進的方法相比，達到了平均 10.5% 的效能提升。

##### **A Generative AI Technique for Synthesizing a Digital Twin for U.S. Residential Solar Adoption and Generation**
2410.08098v1 by Aparna Kishore, Swapna Thorve, Madhav Marathe

Residential rooftop solar adoption is considered crucial for reducing carbon
emissions. The lack of photovoltaic (PV) data at a finer resolution (e.g.,
household, hourly levels) poses a significant roadblock to informed
decision-making. We discuss a novel methodology to generate a highly granular,
residential-scale realistic dataset for rooftop solar adoption across the
contiguous United States. The data-driven methodology consists of: (i)
integrated machine learning models to identify PV adopters, (ii) methods to
augment the data using explainable AI techniques to glean insights about key
features and their interactions, and (iii) methods to generate household-level
hourly solar energy output using an analytical model. The resulting synthetic
datasets are validated using real-world data and can serve as a digital twin
for modeling downstream tasks. Finally, a policy-based case study utilizing the
digital twin for Virginia demonstrated increased rooftop solar adoption with
the 30\% Federal Solar Investment Tax Credit, especially in
Low-to-Moderate-Income communities.

摘要：住宅屋頂太陽能採用被認為是減少碳排放的關鍵。缺乏更細緻解析度（例如家庭、每小時等級）的光電（PV）數據，對明智決策制定構成重大障礙。我們討論一種新方法，以產生一個高度細緻、住宅規模的實際數據集，用於美國本土的屋頂太陽能採用。資料驅動的方法包括：(i) 整合機器學習模型以識別光電採用者，(ii) 使用可解釋 AI 技術擴充數據的方法，以收集關於關鍵特徵及其交互作用的見解，以及 (iii) 使用分析模型產生家庭等級每小時太陽能輸出的方法。產生的合成數據集使用真實世界數據驗證，並且可以用作建模下游任務的數位雙胞胎。最後，利用數位雙胞胎針對維吉尼亞州進行的基於政策的案例研究，證明屋頂太陽能採用率隨著 30% 聯邦太陽能投資稅收抵免而增加，尤其是在中低收入社區。

##### **Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering**
2410.08085v1 by Yuan Sui, Bryan Hooi

Recent works integrating Knowledge Graphs (KGs) have led to promising
improvements in enhancing reasoning accuracy of Large Language Models (LLMs).
However, current benchmarks mainly focus on closed tasks, leaving a gap in the
assessment of more complex, real-world scenarios. This gap has also obscured
the evaluation of KGs' potential to mitigate the problem of hallucination in
LLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically
designed to assess LLMs enhanced with KGs under open-ended, real-world question
answering scenarios. OKGQA is designed to closely reflect the complexities of
practical applications using questions from different types, and incorporates
specific metrics to measure both the reduction in hallucinations and the
enhancement in reasoning capabilities. To consider the scenario in which KGs
may have varying levels of mistakes, we further propose another experiment
setting OKGQA-P to assess model performance when the semantics and structure of
KGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore
whether KGs can make LLMs more trustworthy in an open-ended setting, and (2)
conduct a comparative analysis to shed light on methods and future directions
for leveraging KGs to reduce LLMs' hallucination. We believe that this study
can facilitate a more complete performance comparison and encourage continuous
improvement in integrating KGs with LLMs.

摘要：近期的知识图谱 (KG) 整合研究，已提升大型语言模型 (LLM) 推理准确度的表现。
然而，现有的基准测试主要着重于封闭式任务，在评估更复杂、更实际的场景时存在缺口。此缺口也模糊了知识图谱在减轻 LLM 幻觉问题上的潜力评估。为了填补此缺口，我们引入了 OKGQA，这是一个专门设计用来评估在开放式、实际问答场景中，增强了知识图谱的 LLM 的新基准测试。OKGQA 旨在紧密反映实际应用中的复杂性，使用不同类型的题目，并纳入特定指标来衡量幻觉的减少和推理能力的增强。为了考虑知识图谱可能存在不同程度错误的场景，我们进一步提出了另一个实验设置 OKGQA-P，以评估当知识图谱的语义和结构被故意扰动和污染时的模型性能。OKGQA 旨在 (1) 探索知识图谱是否能使 LLM 在开放式设置中更值得信赖，以及 (2) 进行比较分析，以阐明利用知识图谱来减少 LLM 幻觉的方法和未来方向。我们相信这项研究可以促进更完整的性能比较，并鼓励持续改进知识图谱与 LLM 的整合。

##### **Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning**
2410.08081v1 by Shuhe Wang, Guoyin Wang, Jiwei Li, Eduard Hovy, Chen Guo

Packing, initially utilized in the pre-training phase, is an optimization
technique designed to maximize hardware resource efficiency by combining
different training sequences to fit the model's maximum input length. Although
it has demonstrated effectiveness during pre-training, there remains a lack of
comprehensive analysis for the supervised fine-tuning (SFT) stage on the
following points: (1) whether packing can effectively enhance training
efficiency while maintaining performance, (2) the suitable size of the model
and dataset for fine-tuning with the packing method, and (3) whether packing
unrelated or related training samples might cause the model to either
excessively disregard or over-rely on the context.
  In this paper, we perform extensive comparisons between SFT methods using
padding and packing, covering SFT datasets ranging from 69K to 1.2M and models
from 8B to 70B. This provides the first comprehensive analysis of the
advantages and limitations of packing versus padding, as well as practical
considerations for implementing packing in various training scenarios. Our
analysis covers various benchmarks, including knowledge, reasoning, and coding,
as well as GPT-based evaluations, time efficiency, and other fine-tuning
parameters. We also open-source our code for fine-tuning and evaluation and
provide checkpoints fine-tuned on datasets of different sizes, aiming to
advance future research on packing methods. Code is available at:
https://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.

摘要：<paragraph>在預訓練階段最初使用的封裝是一種最佳化技術，旨在透過結合不同的訓練序列以符合模型的最大輸入長度來最大化硬體資源效率。儘管它已在預訓練過程中證明其有效性，但針對監督式微調 (SFT) 階段仍缺乏以下方面的全面分析：(1) 封裝是否能有效提升訓練效率，同時維持效能，(2) 適用於封裝方法微調的模型和資料集的適當大小，以及 (3) 封裝無關或相關的訓練樣本是否可能導致模型過度忽略或過度依賴上下文。
在本論文中，我們對使用填充和封裝的 SFT 方法進行廣泛比較，涵蓋從 69K 到 1.2M 的 SFT 資料集和從 8B 到 70B 的模型。這提供了封裝相對於填充的優缺點，以及在各種訓練場景中實作封裝的實務考量的首次全面分析。我們的分析涵蓋各種基準，包括知識、推理和編碼，以及基於 GPT 的評估、時間效率和其他微調參數。我們也開放原始碼進行微調和評估，並提供針對不同大小資料集微調的檢查點，目標是推進封裝方法的未來研究。程式碼可於此處取得：
https://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file。</paragraph>

##### **Unlearning-based Neural Interpretations**
2410.08069v1 by Ching Lam Choi, Alexandre Duplessis, Serge Belongie

Gradient-based interpretations often require an anchor point of comparison to
avoid saturation in computing feature importance. We show that current
baselines defined using static functions--constant mapping, averaging or
blurring--inject harmful colour, texture or frequency assumptions that deviate
from model behaviour. This leads to accumulation of irregular gradients,
resulting in attribution maps that are biased, fragile and manipulable.
Departing from the static approach, we propose UNI to compute an (un)learnable,
debiased and adaptive baseline by perturbing the input towards an unlearning
direction of steepest ascent. Our method discovers reliable baselines and
succeeds in erasing salient features, which in turn locally smooths the
high-curvature decision boundaries. Our analyses point to unlearning as a
promising avenue for generating faithful, efficient and robust interpretations.

摘要：基於梯度的詮釋通常需要一個比較錨點來避免在計算特徵重要性時飽和。我們顯示目前使用靜態函數定義的基準線（恆定映射、平均或模糊）會注入有害的顏色、紋理或頻率假設，這些假設會偏離模型行為。這會導致不規則梯度的累積，進而產生有偏差、脆弱且可操縱的歸因圖。我們跳脫靜態方法，提出 UNI 來計算一個（不可）學習、去偏且自適應的基準線，方法是朝向最陡上升的取消學習方向擾動輸入。我們的模型發現可靠的基準線，並成功消除顯著特徵，進而局部平滑高曲率決策邊界。我們的分析指出取消學習是一個有希望的途徑，可以產生忠實、有效且強健的詮釋。

##### **Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models**
2410.08068v1 by Wenting Tan, Dongxiao Chen, Jieting Xue, Zihao Wang, Taijie Chen

Large Language Models (LLMs) exhibit impressive performance across various
domains but still struggle with arithmetic reasoning tasks. Recent work shows
the effectiveness of prompt design methods in enhancing reasoning capabilities.
However, these approaches overlook crucial requirements for prior knowledge of
specific concepts, theorems, and tricks to tackle most arithmetic reasoning
problems successfully. To address this issue, we propose a novel and effective
Teaching-Inspired Integrated Framework, which emulates the instructional
process of a teacher guiding students. This method equips LLMs with essential
concepts, relevant theorems, and similar problems with analogous solution
approaches, facilitating the enhancement of reasoning abilities. Additionally,
we introduce two new Chinese datasets, MathMC and MathToF, both with detailed
explanations and answers. Experiments are conducted on nine benchmarks which
demonstrates that our approach improves the reasoning accuracy of LLMs. With
GPT-4 and our framework, we achieve new state-of-the-art performance on four
math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%
(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are
available at https://github.com/SallyTan13/Teaching-Inspired-Prompting.

摘要：大型語言模型 (LLM) 在各種領域展現出令人印象深刻的表現，但在算術推理任務中仍有困難。最近的研究顯示，提示設計方法在增強推理能力方面很有效。然而，這些方法忽略了對特定概念、定理和技巧的先備知識，這些知識對於成功解決大多數算術推理問題至關重要。為了解決這個問題，我們提出了一個新穎且有效的教學靈感整合框架，模擬老師指導學生的教學過程。此方法為 LLM 提供了必要的概念、相關定理和類似問題，以及類似的解決方法，有助於增強推理能力。此外，我們還介紹了兩個新的中文資料集 MathMC 和 MathToF，它們都提供了詳細的解釋和答案。在九個基準上進行了實驗，結果表明我們的做法提高了 LLM 的推理準確度。透過 GPT-4 和我們的框架，我們在四個數學基準 (AddSub、SVAMP、Math23K 和 AQuA) 上達到了新的最先進性能，準確度分別為 98.2% (+3.3%)、93.9% (+0.2%)、94.3% (+7.2%) 和 81.1% (+1.2%)。我們的資料和程式碼可在 https://github.com/SallyTan13/Teaching-Inspired-Prompting 取得。

##### **Reward-Augmented Data Enhances Direct Preference Alignment of LLMs**
2410.08067v1 by Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang

Preference alignment in Large Language Models (LLMs) has significantly
improved their ability to adhere to human instructions and intentions. However,
existing direct alignment algorithms primarily focus on relative preferences
and often overlook the qualitative aspects of responses. Striving to maximize
the implicit reward gap between the chosen and the slightly inferior rejected
responses can cause overfitting and unnecessary unlearning of the high-quality
rejected responses. The unawareness of the reward scores also drives the LLM to
indiscriminately favor the low-quality chosen responses and fail to generalize
to responses with the highest rewards, which are sparse in data. To overcome
these shortcomings, our study introduces reward-conditioned LLM policies that
discern and learn from the entire spectrum of response quality within the
dataset, helping extrapolate to more optimal regions. We propose an effective
yet simple data relabeling method that conditions the preference pairs on
quality scores to construct a reward-augmented dataset. This dataset is easily
integrated with existing direct alignment algorithms and is applicable to any
preference dataset. The experimental results across instruction-following
benchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that
our approach consistently boosts the performance of DPO by a considerable
margin across diverse models. Additionally, our method improves the average
accuracy on various academic benchmarks. When applying our method to on-policy
data, the resulting DPO model achieves SOTA results on AlpacaEval. Through
ablation studies, we demonstrate that our method not only maximizes the utility
of preference data but also mitigates the issue of unlearning, demonstrating
its broad effectiveness beyond mere dataset expansion. Our code is available at
https://github.com/shenao-zhang/reward-augmented-preference.

摘要：大型語言模型 (LLM) 中的偏好對齊顯著提升了它們遵守人類指令和意圖的能力。然而，現有的直接對齊演算法主要關注相對偏好，且常常忽略回應的質化面向。努力最大化所選回應和略遜一籌的被拒絕回應之間的隱含獎勵差距，可能導致過度擬合和不必要地遺忘高品質的被拒絕回應。對獎勵分數的無知也驅使 LLM 毫無區別地偏好低品質的所選回應，且無法概括到獎勵最高的回應，而這些回應在資料中是稀疏的。為了克服這些缺點，我們的研究引入了獎勵條件的 LLM 政策，它能辨別並從資料集內回應品質的整個範圍中學習，有助於外推到更佳的區域。我們提出一個有效且簡單的資料重新標籤方法，它以品質分數為條件的偏好對來建構一個獎勵增強的資料集。這個資料集很容易與現有的直接對齊演算法整合，且適用於任何偏好資料集。在包括 AlpacaEval、MT-Bench 和 Arena-Hard-Auto 在內的指令遵循基準中的實驗結果顯示，我們的做法一致地以相當大的幅度提升了不同模型的 DPO 效能。此外，我們的做法也提升了各種學術基準的平均準確度。當將我們的做法應用於策略資料時，產生的 DPO 模型在 AlpacaEval 上達到了 SOTA 結果。透過消融研究，我們證明了我們的做法不僅最大化了偏好資料的效用，也減輕了遺忘的問題，證明了它超越單純的資料集擴充的廣泛效力。我們的程式碼可以在 https://github.com/shenao-zhang/reward-augmented-preference 取得。

##### **Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions**
2410.08058v1 by Inderjeet Nair, Jiaye Tan, Xiaotian Su, Anne Gere, Xu Wang, Lu Wang

Providing feedback is widely recognized as crucial for refining students'
writing skills. Recent advances in language models (LMs) have made it possible
to automatically generate feedback that is actionable and well-aligned with
human-specified attributes. However, it remains unclear whether the feedback
generated by these models is truly effective in enhancing the quality of
student revisions. Moreover, prompting LMs with a precise set of instructions
to generate feedback is nontrivial due to the lack of consensus regarding the
specific attributes that can lead to improved revising performance. To address
these challenges, we propose PROF that PROduces Feedback via learning from LM
simulated student revisions. PROF aims to iteratively optimize the feedback
generator by directly maximizing the effectiveness of students' overall
revising performance as simulated by LMs. Focusing on an economic essay
assignment, we empirically test the efficacy of PROF and observe that our
approach not only surpasses a variety of baseline methods in effectiveness of
improving students' writing but also demonstrates enhanced pedagogical values,
even though it was not explicitly trained for this aspect.

摘要：提供回饋被廣泛認為對於精進學生的寫作技巧至關重要。語言模型 (LM) 的最新進展讓自動產生回饋成為可能，而這些回饋具有可操作性，且與人類指定的屬性高度一致。然而，由這些模型產生的回饋是否真的能有效提升學生修改的品質，仍不清楚。此外，由於對於可能導致修改表現提升的特定屬性缺乏共識，因此提示 LM 產生回饋時，使用一組精確的說明並非易事。為了應對這些挑戰，我們提出 PROF，它透過學習 LM 模擬的學生修改來產生回饋。PROF 的目標是透過直接最大化學生整體修改表現的有效性（由 LM 模擬），來反覆最佳化回饋產生器。我們著重於經濟論文作業，實證測試 PROF 的效能，並觀察到我們的做法不僅在提升學生寫作的有效性方面超越各種基準方法，即使在這個面向並未經過明確訓練，也展現出增強的教學價值。

##### **A Target-Aware Analysis of Data Augmentation for Hate Speech Detection**
2410.08053v1 by Camilla Casula, Sara Tonelli

Hate speech is one of the main threats posed by the widespread use of social
networks, despite efforts to limit it. Although attention has been devoted to
this issue, the lack of datasets and case studies centered around scarcely
represented phenomena, such as ableism or ageism, can lead to hate speech
detection systems that do not perform well on underrepresented identity groups.
Given the unpreceded capabilities of LLMs in producing high-quality data, we
investigate the possibility of augmenting existing data with generative
language models, reducing target imbalance. We experiment with augmenting 1,000
posts from the Measuring Hate Speech corpus, an English dataset annotated with
target identity information, adding around 30,000 synthetic examples using both
simple data augmentation methods and different types of generative models,
comparing autoregressive and sequence-to-sequence approaches. We find
traditional DA methods to often be preferable to generative models, but the
combination of the two tends to lead to the best results. Indeed, for some hate
categories such as origin, religion, and disability, hate speech classification
using augmented data for training improves by more than 10% F1 over the no
augmentation baseline. This work contributes to the development of systems for
hate speech detection that are not only better performing but also fairer and
more inclusive towards targets that have been neglected so far.

摘要：仇恨言論是社交網路廣泛使用所帶來的主要威脅之一，儘管已採取措施加以限制。儘管已對此問題給予關注，但缺乏以鮮少被代表的現象（例如歧視殘疾或歧視年齡）為中心的資料集和案例研究，可能導致仇恨言論偵測系統無法對代表性不足的身分群體發揮良好效用。考量到 LLM 在產生高品質資料方面的無與倫比能力，我們探討了使用生成式語言模型擴充現有資料的可能性，以減少目標失衡。我們嘗試擴充「衡量仇恨言論」語料庫中的 1,000 篇貼文，這是一個標註有目標身分資訊的英文資料集，使用簡單的資料擴充方法和不同類型的生成式模型新增約 30,000 個合成範例，比較自迴歸和序列到序列的方法。我們發現傳統的 DA 方法通常優於生成式模型，但兩者的結合往往會帶來最佳結果。事實上，對於某些仇恨類別，例如出身、宗教和殘疾，使用擴充資料進行訓練的仇恨言論分類在 F1 上比沒有擴充的基準線改進了 10% 以上。這項工作有助於開發仇恨言論偵測系統，不僅效能更好，而且對迄今被忽視的目標更公平、更具包容性。

##### **VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based Verifiers**
2410.08048v1 by Jianing Qi, Hao Tang, Zhigang Zhu

Recent advancements in test time compute, particularly through the use of
verifier models, have significantly enhanced the reasoning capabilities of
Large Language Models (LLMs). This generator-verifier approach closely
resembles the actor-critic framework in reinforcement learning (RL). However,
current verifier models in LLMs often rely on supervised fine-tuning without
temporal difference learning such as Q-learning. This paper introduces
VerifierQ, a novel approach that integrates Offline Q-learning into LLM
verifier models. We address three key challenges in applying Q-learning to
LLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)
managing large action spaces, and (3) mitigating overestimation bias. VerifierQ
introduces a modified Bellman update for bounded Q-values, incorporates
Implicit Q-learning (IQL) for efficient action space management, and integrates
a novel Conservative Q-learning (CQL) formulation for balanced Q-value
estimation. Our method enables parallel Q-value computation and improving
training efficiency. While recent work has explored RL techniques like MCTS for
generators, VerifierQ is among the first to investigate the verifier (critic)
aspect in LLMs through Q-learning. This integration of RL principles into
verifier models complements existing advancements in generator techniques,
potentially enabling more robust and adaptive reasoning in LLMs. Experimental
results on mathematical reasoning tasks demonstrate VerifierQ's superior
performance compared to traditional supervised fine-tuning approaches, with
improvements in efficiency, accuracy and robustness. By enhancing the synergy
between generation and evaluation capabilities, VerifierQ contributes to the
ongoing evolution of AI systems in addressing complex cognitive tasks across
various domains.

摘要：<paragraph>最近在測試時間計算方面的進展，特別是透過驗證模型的使用，大幅提升了大型語言模型 (LLM) 的推理能力。這種生成器驗證方法與強化學習 (RL) 中的行動者-評論者架構非常相似。然而，LLM 中目前的驗證模型通常依賴於監督微調，而沒有時間差學習，例如 Q 學習。本文介紹了 VerifierQ，這是一種新方法，將離線 Q 學習整合到 LLM 驗證模型中。我們解決了將 Q 學習應用於 LLM 的三個主要挑戰：(1) 處理話語級馬可夫決策過程 (MDP)，(2) 管理大型動作空間，以及 (3) 減輕高估偏差。VerifierQ 針對有界 Q 值引入了修改後的 Bellman 更新，結合了隱式 Q 學習 (IQL) 以進行有效的動作空間管理，並整合了一種新的保守 Q 學習 (CQL) 公式，以進行平衡的 Q 值估計。我們的模型支援並行 Q 值計算，並提高訓練效率。雖然最近的研究探索了 MCTS 等 RL 技術以用於生成器，但 VerifierQ 是第一個透過 Q 學習探討 LLM 中驗證者（評論者）方面的研究。這種將 RL 原則整合到驗證模型中的做法，補充了生成器技術中現有的進展，潛在地讓 LLM 能夠進行更強健且適應性更強的推理。數學推理任務的實驗結果證明了 VerifierQ 優於傳統監督微調方法，在效率、準確性和強健性方面都有所提升。透過加強生成與評估能力之間的協同作用，VerifierQ 有助於 AI 系統持續演進，以解決各種領域中的複雜認知任務。</paragraph>

##### **Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations**
2410.08049v1 by Yiyuan Zhang, Xiaohan Ding, Xiangyu Yue

This paper proposes the paradigm of large convolutional kernels in designing
modern Convolutional Neural Networks (ConvNets). We establish that employing a
few large kernels, instead of stacking multiple smaller ones, can be a superior
design strategy. Our work introduces a set of architecture design guidelines
for large-kernel ConvNets that optimize their efficiency and performance. We
propose the UniRepLKNet architecture, which offers systematical architecture
design principles specifically crafted for large-kernel ConvNets, emphasizing
their unique ability to capture extensive spatial information without deep
layer stacking. This results in a model that not only surpasses its
predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a
COCO box AP of 56.4% but also demonstrates impressive scalability and
performance on various modalities such as time-series forecasting, audio, point
cloud, and video recognition. These results indicate the universal modeling
abilities of large-kernel ConvNets with faster inference speed compared with
vision transformers. Our findings reveal that large-kernel ConvNets possess
larger effective receptive fields and a higher shape bias, moving away from the
texture bias typical of smaller-kernel CNNs. All codes and models are publicly
available at https://github.com/AILab-CVC/UniRepLKNet promoting further
research and development in the community.

摘要：本文提出在設計現代卷積神經網路 (ConvNets) 時使用大型卷積核的範例。我們確立使用少數大型核，而不是堆疊多個較小的核，可能是一種優越的設計策略。我們的研究引入了一組大型核 ConvNets 的架構設計準則，以最佳化其效率和效能。我們提出 UniRepLKNet 架構，它提供了專門為大型核 ConvNets 設計的系統化架構設計原則，強調它們在不進行深度層堆疊的情況下擷取廣泛空間資訊的獨特能力。這產生了一個模型，不僅以 88.0% 的 ImageNet 準確度、55.6% 的 ADE20K mIoU 和 56.4% 的 COCO 盒子 AP 超越其前身，還展示了令人印象深刻的可擴充性和效能在各種模式上的表現，例如時間序列預測、音訊、點雲和影片辨識。這些結果表明，與視覺轉換器相比，大型核 ConvNets 具有通用的建模能力，且推理速度更快。我們的研究結果表明，大型核 ConvNets 具有更大的有效感受野和更高的形狀偏差，遠離了較小核 CNN 的典型紋理偏差。所有程式碼和模型都公開在 https://github.com/AILab-CVC/UniRepLKNet，以促進社群進一步的研究和開發。

##### **Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning**
2410.08047v1 by Hyun Ryu, Gyeongman Kim, Hyemin S. Lee, Eunho Yang

Complex logical reasoning tasks require a long sequence of reasoning, which a
large language model (LLM) with chain-of-thought prompting still falls short.
To alleviate this issue, neurosymbolic approaches incorporate a symbolic
solver. Specifically, an LLM only translates a natural language problem into a
satisfiability (SAT) problem that consists of first-order logic formulas, and a
sound symbolic solver returns a mathematically correct solution. However, we
discover that LLMs have difficulties to capture complex logical semantics
hidden in the natural language during translation. To resolve this limitation,
we propose a Compositional First-Order Logic Translation. An LLM first parses a
natural language sentence into newly defined logical dependency structures that
consist of an atomic subsentence and its dependents, then sequentially
translate the parsed subsentences. Since multiple logical dependency structures
and sequential translations are possible for a single sentence, we also
introduce two Verification algorithms to ensure more reliable results. We
utilize an SAT solver to rigorously compare semantics of generated first-order
logic formulas and select the most probable one. We evaluate the proposed
method, dubbed CLOVER, on seven logical reasoning benchmarks and show that it
outperforms the previous neurosymbolic approaches and achieves new
state-of-the-art results.

摘要：複雜的邏輯推理任務需要一連串的推理，而具有思考鏈提示的大語言模型 (LLM) 仍無法達成。
為了緩解這個問題，神經符號方法整合了一個符號求解器。具體來說，LLM 只會將自然語言問題轉換為由一階邏輯公式組成的可滿足性 (SAT) 問題，而健全的符號求解器會回傳一個數學上正確的解。然而，我們發現 LLM 難以在轉換過程中擷取隱藏在自然語言中的複雜邏輯語義。為了解決這個限制，我們提出了一個組合一階邏輯轉換。LLM 會先將一個自然語言句子解析成新定義的邏輯依賴結構，其中包含一個原子子句及其依賴項，然後依序轉換解析後的子句。由於單一句子有多個可能的邏輯依賴結構和順序轉換，我們也引入了兩個驗證演算法來確保結果更可靠。我們利用 SAT 求解器嚴格比較產生的邏輯公式的語義，並選出最可能的公式。我們在七個邏輯推理基準上評估所提出的方法，稱為 CLOVER，並顯示它優於先前的神經符號方法，並取得新的最先進結果。

##### **The Rise of AI-Generated Content in Wikipedia**
2410.08044v1 by Creston Brooks, Samuel Eggert, Denis Peskoff

The rise of AI-generated content in popular information sources raises
significant concerns about accountability, accuracy, and bias amplification.
Beyond directly impacting consumers, the widespread presence of this content
poses questions for the long-term viability of training language models on vast
internet sweeps. We use GPTZero, a proprietary AI detector, and Binoculars, an
open-source alternative, to establish lower bounds on the presence of
AI-generated content in recently created Wikipedia pages. Both detectors reveal
a marked increase in AI-generated content in recent pages compared to those
from before the release of GPT-3.5. With thresholds calibrated to achieve a 1%
false positive rate on pre-GPT-3.5 articles, detectors flag over 5% of newly
created English Wikipedia articles as AI-generated, with lower percentages for
German, French, and Italian articles. Flagged Wikipedia articles are typically
of lower quality and are often self-promotional or partial towards a specific
viewpoint on controversial topics.

摘要：人工智能生成內容在熱門資訊來源中興起，引發了對於問責制、準確性和偏見擴散的重大疑慮。
除了直接影響消費者之外，此類內容的廣泛存在也對在廣泛網際網路掃描中訓練語言模型的長期可行性提出了質疑。我們使用專有的 AI 偵測器 GPTZero 和開放原始碼替代方案 Binoculars，來建立最近建立的維基百科頁面中 AI 生成的內容存在的下限。與 GPT-3.5 發布之前的頁面相比，兩個偵測器都顯示最近的頁面中 AI 生成的內容顯著增加。設定閾值以在 GPT-3.5 之前的文章中達到 1% 的誤判率，偵測器將超過 5% 的新建立的英文維基百科文章標記為 AI 生成的，德文、法文和義大利文文章的百分比較低。標記的維基百科文章通常品質較低，而且經常自我宣傳或偏向於有爭議性主題的特定觀點。

##### **Strategic Classification With Externalities**
2410.08032v1 by Yiling Chen, Safwan Hossain, Evi Micha, Ariel Procaccia

We propose a new variant of the strategic classification problem: a principal
reveals a classifier, and $n$ agents report their (possibly manipulated)
features to be classified. Motivated by real-world applications, our model
crucially allows the manipulation of one agent to affect another; that is, it
explicitly captures inter-agent externalities. The principal-agent interactions
are formally modeled as a Stackelberg game, with the resulting agent
manipulation dynamics captured as a simultaneous game. We show that under
certain assumptions, the pure Nash Equilibrium of this agent manipulation game
is unique and can be efficiently computed. Leveraging this result, PAC learning
guarantees are established for the learner: informally, we show that it is
possible to learn classifiers that minimize loss on the distribution, even when
a random number of agents are manipulating their way to a pure Nash
Equilibrium. We also comment on the optimization of such classifiers through
gradient-based approaches. This work sets the theoretical foundations for a
more realistic analysis of classifiers that are robust against multiple
strategic actors interacting in a common environment.

摘要：我們提出策略分類問題的新變體：一個委託人揭露一個分類器，而 $n$ 個代理報告他們（可能經過調整）的特性以進行分類。我們的模型受到真實世界應用的啟發，至關重要的是允許一個代理的調整影響另一個代理；也就是說，它明確地捕捉到代理間的外部性。委託-代理互動被正式建模為一個 Stackelberg 遊戲，並將由此產生的代理調整動態捕捉為一個同步遊戲。我們表明在某些假設下，這個代理調整遊戲的純納許均衡是唯一的，並且可以有效地計算。利用這個結果，為學習者建立了 PAC 學習保證：非正式地，我們表明有可能學習最小化分配損失的分類器，即使在隨機數量的代理正在調整他們的方式以達到純納許均衡時也是如此。我們還評論了通過基於梯度的途徑對此類分類器進行優化的問題。這項工作為對在共同環境中互動的、針對多個策略參與者具有魯棒性的分類器的更現實的分析奠定了理論基礎。

##### **Private Language Models via Truncated Laplacian Mechanism**
2410.08027v1 by Tianhao Huang, Tao Yang, Ivan Habernal, Lijie Hu, Di Wang

Deep learning models for NLP tasks are prone to variants of privacy attacks.
To prevent privacy leakage, researchers have investigated word-level
perturbations, relying on the formal guarantees of differential privacy (DP) in
the embedding space. However, many existing approaches either achieve
unsatisfactory performance in the high privacy regime when using the Laplacian
or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior
to the canonical DP in terms of privacy strength. This raises the question of
whether a new method for private word embedding can be designed to overcome
these limitations. In this paper, we propose a novel private embedding method
called the high dimensional truncated Laplacian mechanism. Specifically, we
introduce a non-trivial extension of the truncated Laplacian mechanism, which
was previously only investigated in one-dimensional space cases. Theoretically,
we show that our method has a lower variance compared to the previous private
word embedding methods. To further validate its effectiveness, we conduct
comprehensive experiments on private embedding and downstream tasks using three
datasets. Remarkably, even in the high privacy regime, our approach only incurs
a slight decrease in utility compared to the non-private scenario.

摘要：深度學習模型對於自然語言處理任務容易受到各種隱私攻擊。
為了防止隱私外洩，研究人員已經研究了詞級擾動，依賴於嵌入空間中差分隱私 (DP) 的正式保證。然而，許多現有方法在使用拉普拉斯或高斯機制時，在高隱私機制中無法達到令人滿意的效能，或訴諸於 DP 的較弱放寬，其隱私強度低於正規 DP。這引發了一個問題，即是否可以設計一種新的私有詞嵌入方法來克服這些限制。在本文中，我們提出了一種新的私有嵌入方法，稱為高維截斷拉普拉斯機制。具體來說，我們引入了截斷拉普拉斯機制的非平凡擴充，該機制以前僅在 одномерном 空間案例中進行研究。在理論上，我們表明與先前的私有詞嵌入方法相比，我們的方​​法具有較低的變異性。為了進一步驗證其有效性，我們使用三個資料集對私有嵌入和下游任務進行了全面的實驗。值得注意的是，即使在高隱私機制中，與非私有場景相比，我們的方​​法僅導致效用略有下降。

##### **The Computational Complexity of Circuit Discovery for Inner Interpretability**
2410.08025v1 by Federico Adolfi, Martina G. Vilas, Todd Wareham

Many proposed applications of neural networks in machine learning,
cognitive/brain science, and society hinge on the feasibility of inner
interpretability via circuit discovery. This calls for empirical and
theoretical explorations of viable algorithmic options. Despite advances in the
design and testing of heuristics, there are concerns about their scalability
and faithfulness at a time when we lack understanding of the complexity
properties of the problems they are deployed to solve. To address this, we
study circuit discovery with classical and parameterized computational
complexity theory: (1) we describe a conceptual scaffolding to reason about
circuit finding queries in terms of affordances for description, explanation,
prediction and control; (2) we formalize a comprehensive set of queries that
capture mechanistic explanation, and propose a formal framework for their
analysis; (3) we use it to settle the complexity of many query variants and
relaxations of practical interest on multi-layer perceptrons (part of, e.g.,
transformers). Our findings reveal a challenging complexity landscape. Many
queries are intractable (NP-hard, $\Sigma^p_2$-hard), remain fixed-parameter
intractable (W[1]-hard) when constraining model/circuit features (e.g., depth),
and are inapproximable under additive, multiplicative, and probabilistic
approximation schemes. To navigate this landscape, we prove there exist
transformations to tackle some of these hard problems (NP- vs.
$\Sigma^p_2$-complete) with better-understood heuristics, and prove the
tractability (PTIME) or fixed-parameter tractability (FPT) of more modest
queries which retain useful affordances. This framework allows us to understand
the scope and limits of interpretability queries, explore viable options, and
compare their resource demands among existing and future architectures.

摘要：許多神經網路在機器學習、認知/腦科學和社會中的應用提案，都取決於透過電路發現來進行內在可解釋性的可行性。這需要對可行的演算法選項進行實證和理論上的探討。儘管在啟發法的設計和測試方面有進展，但我們對其可擴展性和忠實度仍有疑慮，因為我們目前還缺乏對其用於解決問題的複雜性屬性的理解。為了解決這個問題，我們使用古典和參數化計算複雜度理論來研究電路發現：(1) 我們描述一個概念性支架，以描述、說明、預測和控制的便利性來推論電路尋找查詢；(2) 我們正式化一個全面的查詢集，以捕捉機制性說明，並提出一個正式的分析架構；(3) 我們使用它來解決多層感知器（例如Transformer的一部分）中許多查詢變體和放寬實際興趣的複雜性。我們的發現揭示了一個具有挑戰性的複雜性格局。許多查詢是難以處理的 (NP-hard、$\Sigma^p_2$-hard)，當約束模型/電路特徵（例如深度）時，仍然是固定參數難以處理的 (W[1]-hard)，並且在加法、乘法和機率近似方案下是不可近似的。為了應對這種情況，我們證明存在轉換來解決一些這些困難的問題 (NP- 對比 $\Sigma^p_2$-complete) 使用更易於理解的啟發法，並證明了更適度的查詢的可處理性 (PTIME) 或固定參數可處理性 (FPT)，這些查詢保留了有用的便利性。這個架構讓我們能夠了解可解釋性查詢的範圍和限制，探索可行的選項，並比較它們在現有和未來架構中的資源需求。

##### **Pretraining Graph Transformers with Atom-in-a-Molecule Quantum Properties for Improved ADMET Modeling**
2410.08024v1 by Alessio Fallani, Ramil Nugmanov, Jose Arjona-Medina, Jörg Kurt Wegner, Alexandre Tkatchenko, Kostiantyn Chernichenko

We evaluate the impact of pretraining Graph Transformer architectures on
atom-level quantum-mechanical features for the modeling of absorption,
distribution, metabolism, excretion, and toxicity (ADMET) properties of
drug-like compounds. We compare this pretraining strategy with two others: one
based on molecular quantum properties (specifically the HOMO-LUMO gap) and one
using a self-supervised atom masking technique. After fine-tuning on
Therapeutic Data Commons ADMET datasets, we evaluate the performance
improvement in the different models observing that models pretrained with
atomic quantum mechanical properties produce in general better results. We then
analyse the latent representations and observe that the supervised strategies
preserve the pretraining information after finetuning and that different
pretrainings produce different trends in latent expressivity across layers.
Furthermore, we find that models pretrained on atomic quantum mechanical
properties capture more low-frequency laplacian eigenmodes of the input graph
via the attention weights and produce better representations of atomic
environments within the molecule. Application of the analysis to a much larger
non-public dataset for microsomal clearance illustrates generalizability of the
studied indicators. In this case the performances of the models are in
accordance with the representation analysis and highlight, especially for the
case of masking pretraining and atom-level quantum property pretraining, how
model types with similar performance on public benchmarks can have different
performances on large scale pharmaceutical data.

摘要：我們評估預訓練圖形轉換器架構對原子層級量子力學特徵的影響，用於建構類藥物化合物的吸收、分佈、代謝、排泄和毒性 (ADMET) 屬性模型。我們將此預訓練策略與其他兩個策略進行比較：一個基於分子量子屬性（特別是 HOMO-LUMO 差距），另一個使用自監督原子遮罩技術。在 Therapeutic Data Commons ADMET 資料集上微調後，我們評估不同模型的效能提升，觀察到使用原子量子力學屬性預訓練的模型通常會產生更好的結果。接著我們分析潛在表示，並觀察到監督策略在微調後會保留預訓練資訊，而且不同的預訓練會在不同層級產生潛在表達力的不同趨勢。此外，我們發現預訓練於原子量子力學屬性的模型會透過注意力權重擷取輸入圖形的更多低頻拉普拉斯特徵模式，並產生分子內原子環境的更佳表示。將分析應用於一個更大的非公開微體清除資料集，說明了所研究指標的概括性。在這種情況下，模型的效能符合表示分析，並特別強調遮罩預訓練和原子層級量子屬性預訓練的情況，說明在公開基準上效能相似的模型類型在大型藥品資料上可能會有不同的效能。

##### **GrabDAE: An Innovative Framework for Unsupervised Domain Adaptation Utilizing Grab-Mask and Denoise Auto-Encoder**
2410.08023v1 by Junzhou Chen, Xuan Wen, Ronghui Zhang, Bingtao Ren, Di Wu, Zhigang Xu, Danwei Wang

Unsupervised Domain Adaptation (UDA) aims to adapt a model trained on a
labeled source domain to an unlabeled target domain by addressing the domain
shift. Existing Unsupervised Domain Adaptation (UDA) methods often fall short
in fully leveraging contextual information from the target domain, leading to
suboptimal decision boundary separation during source and target domain
alignment. To address this, we introduce GrabDAE, an innovative UDA framework
designed to tackle domain shift in visual classification tasks. GrabDAE
incorporates two key innovations: the Grab-Mask module, which blurs background
information in target domain images, enabling the model to focus on essential,
domain-relevant features through contrastive learning; and the Denoising
Auto-Encoder (DAE), which enhances feature alignment by reconstructing features
and filtering noise, ensuring a more robust adaptation to the target domain.
These components empower GrabDAE to effectively handle unlabeled target domain
data, significantly improving both classification accuracy and robustness.
Extensive experiments on benchmark datasets, including VisDA-2017, Office-Home,
and Office31, demonstrate that GrabDAE consistently surpasses state-of-the-art
UDA methods, setting new performance benchmarks. By tackling UDA's critical
challenges with its novel feature masking and denoising approach, GrabDAE
offers both significant theoretical and practical advancements in domain
adaptation.

摘要：無監督域適應 (UDA) 旨在透過解決域轉移，將在標籤來源域上訓練的模型適應到未標籤目標域。現有的無監督域適應 (UDA) 方法通常無法充分利用來自目標域的上下文資訊，導致在來源和目標域比對期間，次佳決策邊界分離。為了解決這個問題，我們引入了 GrabDAE，這是一個創新的 UDA 架構，旨在處理視覺分類任務中的域轉移。GrabDAE 結合了兩項關鍵創新：Grab-Mask 模組，它模糊了目標域影像中的背景資訊，使模型能夠透過對比學習專注於必要的、與域相關的特徵；以及去雜訊自動編碼器 (DAE)，它透過重建特徵和過濾雜訊來增強特徵比對，確保更強健的目標域適應。這些元件賦予 GrabDAE 有效處理未標籤目標域資料的能力，大幅提升分類準確度和強健性。在包括 VisDA-2017、Office-Home 和 Office31 在內的基準資料集上進行的廣泛實驗證明，GrabDAE 持續超越最先進的 UDA 方法，設定新的效能基準。透過利用其新穎的特徵遮罩和去雜訊方法來解決 UDA 的關鍵挑戰，GrabDAE 在域適應中提供了重要的理論和實務進展。

##### **Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs**
2410.08020v1 by Jonas Hübotter, Sascha Bongni, Ido Hakimi, Andreas Krause

Recent efforts in fine-tuning language models often rely on automatic data
selection, commonly using Nearest Neighbors retrieval from large datasets.
However, we theoretically show that this approach tends to select redundant
data, limiting its effectiveness or even hurting performance. To address this,
we introduce SIFT, a data selection algorithm designed to reduce uncertainty
about the model's response given a prompt, which unifies ideas from retrieval
and active learning. Whereas Nearest Neighbor retrieval typically fails in the
presence of information duplication, SIFT accounts for information duplication
and optimizes the overall information gain of the selected examples. We focus
our evaluations on fine-tuning at test-time for prompt-specific language
modeling on the Pile dataset, and show that SIFT consistently outperforms
Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we
show that our uncertainty estimates can predict the performance gain of
test-time fine-tuning, and use this to develop an adaptive algorithm that
invests test-time compute proportional to realized performance gains. We
provide the $\texttt{activeft}$ (Active Fine-Tuning) library which can be used
as a drop-in replacement for Nearest Neighbor retrieval.

摘要：近来微调语言模型的努力经常仰赖自动数据选取，通常使用大型数据集中的最近邻检索。然而，我们在理论上证明，此方法倾向于选取冗余数据，限制其效能，甚至损害效能。为了解决这个问题，我们引入了 SIFT，这是一种数据选取算法，旨在减少模型在给定提示时的响应不确定性，它统一了检索和主动学习的思想。最近邻检索在存在信息重复的情况下通常会失败，而 SIFT 则考虑了信息重复，并优化了所选范例的总体信息增益。我们专注于在测试时针对提示特定语言模型的微调在 Pile 数据集上进行评估，并表明 SIFT 始终优于最近邻检索，且计算开销最小。此外，我们表明，我们的不确定性估计可以预测测试时微调的性能增益，并利用此来开发一种自适应算法，该算法根据实现的性能增益成比例地投入测试时计算。我们提供了 $\texttt{activeft}$（主动微调）库，可以用作最近邻检索的替代方案。

##### **LLM Cascade with Multi-Objective Optimal Consideration**
2410.08014v1 by Kai Zhang, Liqian Peng, Congchao Wang, Alec Go, Xiaozhong Liu

Large Language Models (LLMs) have demonstrated exceptional capabilities in
understanding and generating natural language. However, their high deployment
costs often pose a barrier to practical applications, especially. Cascading
local and server models offers a promising solution to this challenge. While
existing studies on LLM cascades have primarily focused on the performance-cost
trade-off, real-world scenarios often involve more complex requirements. This
paper introduces a novel LLM Cascade strategy with Multi-Objective
Optimization, enabling LLM cascades to consider additional objectives (e.g.,
privacy) and better align with the specific demands of real-world applications
while maintaining their original cascading abilities. Extensive experiments on
three benchmarks validate the effectiveness and superiority of our approach.

摘要：大型語言模型 (LLM) 已展現出在理解和產生自然語言方面的卓越能力。然而，它們的高部署成本通常會對實際應用構成障礙，尤其是串接本地和伺服器模型為此挑戰提供了有希望的解決方案。雖然現有關於 LLM 串接的研究主要關注於效能成本權衡，但實際情況通常涉及更複雜的要求。本文介紹了一個新的 LLM 串接策略，具有多目標最佳化，讓 LLM 串接能夠考慮其他目標（例如隱私），並在維持其原始串接能力的同時，更好地符合實際應用程式的特定需求。在三個基準上進行的廣泛實驗驗證了我們方法的有效性和優越性。

##### **Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation**
2410.08001v1 by Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, Yu Qiao

The increasing demand for versatile robotic systems to operate in diverse and
dynamic environments has emphasized the importance of a generalist policy,
which leverages a large cross-embodiment data corpus to facilitate broad
adaptability and high-level reasoning. However, the generalist would struggle
with inefficient inference and cost-expensive training. The specialist policy,
instead, is curated for specific domain data and excels at task-level precision
with efficiency. Yet, it lacks the generalization capacity for a wide range of
applications. Inspired by these observations, we introduce RoboDual, a
synergistic dual-system that supplements the merits of both generalist and
specialist policy. A diffusion transformer-based specialist is devised for
multi-step action rollouts, exquisitely conditioned on the high-level task
understanding and discretized action output of a vision-language-action (VLA)
based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in
real-world setting and 12% gain on CALVIN by introducing a specialist policy
with merely 20M trainable parameters. It maintains strong performance with 5%
of demonstration data only, and enables a 3.8 times higher control frequency in
real-world deployment. Code would be made publicly available. Our project page
is hosted at: https://opendrivelab.com/RoboDual/

摘要：隨著對多功能機器人系統在多樣且動態環境中運作的需求日益增加，強調了通才政策的重要性，該政策利用大量的跨具現數據語料庫來促進廣泛的適應性和高層次推理。然而，通才在低效率的推論和成本昂貴的訓練中會遇到困難。相反地，專家政策是針對特定領域數據進行策劃，並在任務層級精確度和效率方面表現出色。然而，它缺乏廣泛應用的一般化能力。受到這些觀察的啟發，我們引入了 RoboDual，一個協同雙系統，它補充了通才和專家政策的優點。一個基於擴散Transformer的專家被設計用於多步驟動作展開，並根據基於視覺語言動作 (VLA) 的通才的高層次任務理解和離散化動作輸出進行精確調整。與 OpenVLA 相比，RoboDual 在現實世界設置中提升了 26.7%，並透過僅使用 20M 可訓練參數引入專家政策，在 CALVIN 上增加了 12%。它僅使用 5% 的示範數據就能維持強勁的效能，並在實際部署中實現了高出 3.8 倍的控制頻率。程式碼將公開提供。我們的專案頁面網址為：https://opendrivelab.com/RoboDual/

##### **Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets**
2410.07991v1 by Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci

The rise of online platforms exacerbated the spread of hate speech, demanding
scalable and effective detection. However, the accuracy of hate speech
detection systems heavily relies on human-labeled data, which is inherently
susceptible to biases. While previous work has examined the issue, the
interplay between the characteristics of the annotator and those of the target
of the hate are still unexplored. We fill this gap by leveraging an extensive
dataset with rich socio-demographic information of both annotators and targets,
uncovering how human biases manifest in relation to the target's attributes.
Our analysis surfaces the presence of widespread biases, which we
quantitatively describe and characterize based on their intensity and
prevalence, revealing marked differences. Furthermore, we compare human biases
with those exhibited by persona-based LLMs. Our findings indicate that while
persona-based LLMs do exhibit biases, these differ significantly from those of
human annotators. Overall, our work offers new and nuanced results on human
biases in hate speech annotations, as well as fresh insights into the design of
AI-driven hate speech detection systems.

摘要：線上平台的興起加劇了仇恨言論的散播，需要可擴充且有效的偵測機制。然而，仇恨言論偵測系統的準確性高度依賴於人工標記的資料，而這些資料本質上容易受到偏見的影響。雖然先前的研究已探討此議題，但標記者特徵與仇恨目標特徵之間的交互作用仍未被探討。我們透過利用包含標記者與目標豐富的社會人口統計資訊之廣泛資料集，填補了這項空白，揭露人類偏見如何與目標屬性相關聯。我們的分析浮現了廣泛偏見的存在，我們根據偏見的強度和普遍性，以量化的方式描述和描述偏見，揭露明顯的差異。此外，我們將人類偏見與基於角色的 LLM 所展現的偏見進行比較。我們的研究結果指出，雖然基於角色的 LLM 確實會展現偏見，但這些偏見與人工標記者的偏見有顯著的不同。總體而言，我們的研究針對仇恨言論標記中的人類偏見提供了新的且細緻的結果，並對 AI 驅動的仇恨言論偵測系統的設計提供了新的見解。

##### **Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models**
2410.07985v1 by Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, Baobao Chang

Recent advancements in large language models (LLMs) have led to significant
breakthroughs in mathematical reasoning capabilities. However, existing
benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,
OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for
truly challenging these models. To bridge this gap, we propose a comprehensive
and challenging benchmark specifically designed to assess LLMs' mathematical
reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,
our dataset focuses exclusively on mathematics and comprises a vast collection
of 4428 competition-level problems with rigorous human annotation. These
problems are meticulously categorized into over 33 sub-domains and span more
than 10 distinct difficulty levels, enabling a holistic assessment of model
performance in Olympiad-mathematical reasoning. Furthermore, we conducted an
in-depth analysis based on this benchmark. Our experimental results show that
even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle
with highly challenging Olympiad-level problems, with 60.54% and 52.55%
accuracy, highlighting significant challenges in Olympiad-level mathematical
reasoning.

摘要：大型語言模型 (LLM) 近期的進展已導致數學推理能力的重大突破。然而，現有的基準，例如 GSM8K 或 MATH，現在正以高準確度被解決（例如，OpenAI o1 在 MATH 資料集上達到 94.8%），表明它們不足以真正挑戰這些模型。為了彌合這一差距，我們提出了一個全面且具有挑戰性的基準，專門設計用於評估奧林匹克級別的 LLM 數學推理能力。與現有的奧林匹克相關基準不同，我們的資料集專注於數學，並包含大量 4428 個競賽級別問題，並附有人工仔細註解。這些問題被細緻地分為 33 個子領域，並跨越 10 個不同的難度級別，從而能夠全面評估模型在奧林匹克數學推理中的表現。此外，我們根據此基準進行了深入分析。我們的實驗結果表明，即使是最先進的模型 OpenAI o1-mini 和 OpenAI o1-preview，在極具挑戰性的奧林匹克級別問題上也難以應付，準確率分別為 60.54% 和 52.55%，突顯了奧林匹克級別數學推理中的重大挑戰。

##### **MolMix: A Simple Yet Effective Baseline for Multimodal Molecular Representation Learning**
2410.07981v1 by Andrei Manolache, Dragos Tantaru, Mathias Niepert

In this work, we propose a simple transformer-based baseline for multimodal
molecular representation learning, integrating three distinct modalities:
SMILES strings, 2D graph representations, and 3D conformers of molecules. A key
aspect of our approach is the aggregation of 3D conformers, allowing the model
to account for the fact that molecules can adopt multiple conformations-an
important factor for accurate molecular representation. The tokens for each
modality are extracted using modality-specific encoders: a transformer for
SMILES strings, a message-passing neural network for 2D graphs, and an
equivariant neural network for 3D conformers. The flexibility and modularity of
this framework enable easy adaptation and replacement of these encoders, making
the model highly versatile for different molecular tasks. The extracted tokens
are then combined into a unified multimodal sequence, which is processed by a
downstream transformer for prediction tasks. To efficiently scale our model for
large multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.
Despite its simplicity, our approach achieves state-of-the-art results across
multiple datasets, demonstrating its effectiveness as a strong baseline for
multimodal molecular representation learning.

摘要：在這項工作中，我們提出一個簡單的基於 Transformer 的基線，用於多模態分子表示學習，整合了三種不同的模態：SMILES 字符串、2D 圖形表示和分子的 3D 構象。我們方法的一個關鍵方面是 3D 構象的聚合，允許模型考慮分子可以採用多種構象的事實 - 這是準確分子表示的一個重要因素。每個模態的符號都是使用特定於模態的編碼器提取的：SMILES 字符串的 Transformer、2D 圖形的訊息傳遞神經網路和 3D 構象的等變神經網路。這個架構的靈活性與模組化允許輕鬆調整和替換這些編碼器，使模型對不同的分子任務具有高度通用性。提取的符號然後組合成一個統一的多模態序列，由下游 Transformer 處理以進行預測任務。為了有效地擴展我們的模型以適應大型多模態數據集，我們利用 Flash Attention 2 和 bfloat16 精度。儘管很簡單，但我們的做法在多個數據集上都取得了最先進的結果，證明了其作為多模態分子表示學習的強大基線的有效性。

##### **Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling**
2410.07974v1 by Yuanqi Du, Michael Plainer, Rob Brekelmans, Chenru Duan, Frank Noé, Carla P. Gomes, Alan Apsuru-Guzik, Kirill Neklyudov

Rare event sampling in dynamical systems is a fundamental problem arising in
the natural sciences, which poses significant computational challenges due to
an exponentially large space of trajectories. For settings where the dynamical
system of interest follows a Brownian motion with known drift, the question of
conditioning the process to reach a given endpoint or desired rare event is
definitively answered by Doob's h-transform. However, the naive estimation of
this transform is infeasible, as it requires simulating sufficiently many
forward trajectories to estimate rare event probabilities. In this work, we
propose a variational formulation of Doob's $h$-transform as an optimization
problem over trajectories between a given initial point and the desired ending
point. To solve this optimization, we propose a simulation-free training
objective with a model parameterization that imposes the desired boundary
conditions by design. Our approach significantly reduces the search space over
trajectories and avoids expensive trajectory simulation and inefficient
importance sampling estimators which are required in existing methods. We
demonstrate the ability of our method to find feasible transition paths on
real-world molecular simulation and protein folding tasks.

摘要：動力系統中的罕見事件取樣是一個自然科學中出現的基本問題，由於軌跡空間呈指數級增長，因此會造成重大的計算挑戰。對於興趣動力系統遵循已知漂移的布朗運動的設定，條件化處理以到達給定終點或所需罕見事件的問題，已明確由 Doob 的 h 轉換解答。然而，這個轉換的樸素估計不可行，因為它需要模擬足夠多的前向軌跡來估計罕見事件機率。在這項工作中，我們提出 Doob 的 h 轉換的變分公式，作為給定初始點和所需終點之間軌跡的最佳化問題。為了解決這個最佳化問題，我們提出一個無模擬訓練目標，其中模型參數化在設計上施加所需的邊界條件。我們的做法大幅減少軌跡上的搜尋空間，並避免現有方法中所需的昂貴軌跡模擬和低效率重要性取樣估計器。我們展示我們的方法在真實世界的分子模擬和蛋白質摺疊任務中找到可行轉換路徑的能力。

##### **Towards Assurance of LLM Adversarial Robustness using Ontology-Driven Argumentation**
2410.07962v1 by Tomas Bueno Momcilovic, Beat Buesser, Giulio Zizzo, Mark Purcell, Dian Balta

Despite the impressive adaptability of large language models (LLMs),
challenges remain in ensuring their security, transparency, and
interpretability. Given their susceptibility to adversarial attacks, LLMs need
to be defended with an evolving combination of adversarial training and
guardrails. However, managing the implicit and heterogeneous knowledge for
continuously assuring robustness is difficult. We introduce a novel approach
for assurance of the adversarial robustness of LLMs based on formal
argumentation. Using ontologies for formalization, we structure
state-of-the-art attacks and defenses, facilitating the creation of a
human-readable assurance case, and a machine-readable representation. We
demonstrate its application with examples in English language and code
translation tasks, and provide implications for theory and practice, by
targeting engineers, data scientists, users, and auditors.

摘要：儘管大型語言模型 (LLM) 具有令人印象深刻的適應性，
但在確保其安全性、透明度和
可解釋性方面仍存在挑戰。鑑於其容易受到對抗性攻擊，LLM
需要結合對抗性訓練和
護欄來進行防禦。然而，管理隱含且異質的知識以持續確保
穩健性很困難。我們引入了一種新方法，
用於基於形式化論證確保 LLM 的對抗性穩健性。使用本體論進行形式化，我們架構
最先進的攻擊和防禦，促進建立
人類可讀的保證案例和機器可讀的表示。我們
透過以工程師、資料科學家、使用者和審計員為目標，在英語語言和程式碼中展示其應用範例
翻譯任務，並提供對理論和實務的影響。

##### **COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act**
2410.07959v1 by Philipp Guldimann, Alexander Spiridonov, Robin Staab, Nikola Jovanović, Mark Vero, Velko Vechev, Anna Gueorguieva, Mislav Balunović, Nikola Konstantinov, Pavol Bielik, Petar Tsankov, Martin Vechev

The EU's Artificial Intelligence Act (AI Act) is a significant step towards
responsible AI development, but lacks clear technical interpretation, making it
difficult to assess models' compliance. This work presents COMPL-AI, a
comprehensive framework consisting of (i) the first technical interpretation of
the EU AI Act, translating its broad regulatory requirements into measurable
technical requirements, with the focus on large language models (LLMs), and
(ii) an open-source Act-centered benchmarking suite, based on thorough
surveying and implementation of state-of-the-art LLM benchmarks. By evaluating
12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in
existing models and benchmarks, particularly in areas like robustness, safety,
diversity, and fairness. This work highlights the need for a shift in focus
towards these aspects, encouraging balanced development of LLMs and more
comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the
first time demonstrates the possibilities and difficulties of bringing the
Act's obligations to a more concrete, technical level. As such, our work can
serve as a useful first step towards having actionable recommendations for
model providers, and contributes to ongoing efforts of the EU to enable
application of the Act, such as the drafting of the GPAI Code of Practice.

摘要：歐盟的人工智慧法案（AI 法案）是邁向負責任的 AI 開發的重要一步，但缺乏明確的技術詮釋，這使得評估模型的合規性變得困難。這項工作提出 COMPL-AI，一個全面的架構，包含 (i) 對歐盟 AI 法案的第一個技術詮釋，將其廣泛的監管要求轉化為可衡量的技術要求，重點關注大型語言模型 (LLM)，以及 (ii) 一個以法案為中心的開源基準測試套件，基於對最先進的 LLM 基準測試的徹底調查和實施。透過在 COMPL-AI 的背景下評估 12 個傑出的 LLM，我們揭露了現有模型和基準測試的缺點，特別是在健壯性、安全性、多樣性和公平性等領域。這項工作強調了需要將焦點轉移到這些方面，鼓勵 LLM 的平衡發展和更全面的法規對齊基準測試。同時，COMPL-AI 首次展示了將法案的義務具體化到技術層面的可能性和難度。因此，我們的研究可以作為邁向對模型供應商提出可行的建議的第一步，並有助於歐盟為實施法案所做的持續努力，例如起草 GPAI 行為守則。

##### **Disease Entity Recognition and Normalization is Improved with Large Language Model Derived Synthetic Normalized Mentions**
2410.07951v1 by Kuleen Sasse, Shinjitha Vadlakonda, Richard E. Kennedy, John D. Osborne

Background: Machine learning methods for clinical named entity recognition
and entity normalization systems can utilize both labeled corpora and Knowledge
Graphs (KGs) for learning. However, infrequently occurring concepts may have
few mentions in training corpora and lack detailed descriptions or synonyms,
even in large KGs. For Disease Entity Recognition (DER) and Disease Entity
Normalization (DEN), this can result in fewer high quality training examples
relative to the number of known diseases. Large Language Model (LLM) generation
of synthetic training examples could improve performance in these information
extraction tasks.
  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus
containing normalized mentions of concepts from the Unified Medical Language
System (UMLS) Disease Semantic Group. We measured overall and Out of
Distribution (OOD) performance for DER and DEN, with and without synthetic data
augmentation. We evaluated performance on 3 different disease corpora using 4
different data augmentation strategies, assessed using BioBERT for DER and
SapBERT and KrissBERT for DEN.
  Results: Our synthetic data yielded a substantial improvement for DEN, in all
3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by
3-9 points in overall performance and by 20-55 points in OOD data. A small
improvement (1-2 points) was also seen for DER in overall performance, but only
one dataset showed OOD improvement.
  Conclusion: LLM generation of normalized disease mentions can improve DEN
relative to normalization approaches that do not utilize LLMs to augment data
with synthetic mentions. Ablation studies indicate that performance gains for
DEN were only partially attributable to improvements in OOD performance. The
same approach has only a limited ability to improve DER. We make our software
and dataset publicly available.

摘要：<paragraph>背景：臨床命名實體識別的機器學習方法和實體正規化系統可以利用標記語料庫和知識圖譜 (KG) 來學習。然而，在訓練語料庫中很少出現的概念可能只有少數提及，即使在大型知識圖譜中也缺乏詳細的描述或同義詞。對於疾病實體識別 (DER) 和疾病實體正規化 (DEN)，相對於已知疾病的數量，這可能會導致較少的高品質訓練範例。大型語言模型 (LLM) 生成的合成訓練範例可以提升這些資訊擷取任務的效能。
方法：我們微調了一個 LLaMa-2 13B 聊天 LLM，以產生一個合成語料庫，其中包含來自統一醫學語言系統 (UMLS) 疾病語義群的標準化概念提及。我們衡量了 DER 和 DEN 的整體和分布外 (OOD) 效能，有和沒有合成資料擴充。我們使用 4 種不同的資料擴充策略評估了 3 個不同疾病語料庫的效能，使用 BioBERT 評估 DER，使用 SapBERT 和 KrissBERT 評估 DEN。
結果：我們的合成資料對 DEN 產生了顯著的改善，在所有 3 個訓練語料庫中，SapBERT 和 KrissBERT 的前 1 名準確率在整體效能上提高了 3-9 個百分點，在 OOD 資料中提高了 20-55 個百分點。在 DER 的整體效能上也看到了微小的改善（1-2 個百分點），但只有一組資料顯示出 OOD 改善。
結論：與不利用 LLM 擴充資料以合成提及的正規化方法相比，LLM 生成的標準化疾病提及可以改善 DEN。消融研究表明，DEN 的效能提升僅部分歸因於 OOD 效能的改善。相同的方法對於改善 DER 的能力有限。我們公開我們的軟體和資料集。</paragraph>

##### **The Function-Representation Unification Framework**
2410.07928v1 by Alfredo Ibias, Hector Antona, Guillem Ramirez-Miranda, Enric Guinovart, Eduard Alarcon

Cognitive Architectures are the forefront of our research into developing an
artificial cognition. However, they approach the problem from a separated
memory and program model of computation. This model of computation poses a
fundamental problem: the knowledge retrieval heuristic. In this paper we
propose to solve this problem by using a new model of computation, one where
the memory and the program are united: the Function-Representation. We propose
a whole framework about how to implement and use these
Function-Representations, and we explore their potential through mathematical
definitions and proofs. We also talk about different ways to organise multiple
Function-Representations, and explore the kind of functions that these
Function-Representations can implement. Finally, we also explore the
limitations of our proposal.

摘要：認知架構是我們開發人工認知研究的前沿。然而，他們從分離的記憶和程式計算模型來探討問題。這個計算模型提出了基本問題：知識擷取啟發法。在本文中，我們提議使用新的計算模型來解決這個問題，一個記憶體和程式結合的模型：函數表示。我們提出一個關於如何實作和使用這些函數表示的完整架構，並透過數學定義和證明來探討它們的潛力。我們也討論組織多個函數表示的不同方法，並探討這些函數表示可以實作的函數類型。最後，我們也探討我們提議的限制。

##### **InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions**
2410.07919v1 by Xiang Zhuang, Keyan Ding, Tianwen Lyu, Yinuo Jiang, Xiaotong Li, Zhuoyi Xiang, Zeyuan Wang, Ming Qin, Kehua Feng, Jike Wang, Qiang Zhang, Huajun Chen

Understanding and designing biomolecules, such as proteins and small
molecules, is central to advancing drug discovery, synthetic biology, and
enzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have
revolutionized biomolecular research, achieving remarkable accuracy in
biomolecular prediction and design. However, a critical gap remains between
AI's computational power and researchers' intuition, using natural language to
align molecular complexity with human intentions. Large Language Models (LLMs)
have shown potential to interpret human intentions, yet their application to
biomolecular research remains nascent due to challenges including specialized
knowledge requirements, multimodal data integration, and semantic alignment
between natural language and biomolecules. To address these limitations, we
present InstructBioMol, a novel LLM designed to bridge natural language and
biomolecules through a comprehensive any-to-any alignment of natural language,
molecules, and proteins. This model can integrate multimodal biomolecules as
input, and enable researchers to articulate design goals in natural language,
providing biomolecular outputs that meet precise biological needs. Experimental
results demonstrate InstructBioMol can understand and design biomolecules
following human instructions. Notably, it can generate drug molecules with a
10% improvement in binding affinity and design enzymes that achieve an ESP
Score of 70.4, making it the only method to surpass the enzyme-substrate
interaction threshold of 60.0 recommended by the ESP developer. This highlights
its potential to transform real-world biomolecular research.

摘要：<paragraph>了解和設計生物分子（例如蛋白質和小分子）對於推進藥物發現、合成生物學和酶工程至關重要。最近人工智能 (AI) 的突破徹底改變了生物分子研究，在生物分子預測和設計方面取得了顯著的準確性。然而，AI 的計算能力和研究人員的直覺之間仍然存在關鍵差距，使用自然語言將分子複雜性與人類意圖對齊。大型語言模型 (LLM) 已顯示出解釋人類意圖的潛力，但由於包括專業知識要求、多模式數據整合以及自然語言和生物分子之間的語義對齊在內的挑戰，它們在生物分子研究中的應用仍然處於萌芽階段。為了解決這些限制，我們提出了 InstructBioMol，這是一種新穎的 LLM，旨在通過自然語言、分子和蛋白質的全面任意對齊來橋接自然語言和生物分子。此模型可以整合多模式生物分子作為輸入，並使研究人員能夠用自然語言表達設計目標，提供滿足精確生物需求的生物分子輸出。實驗結果表明，InstructBioMol 可以理解和設計遵循人類指令的生物分子。值得注意的是，它可以生成結合親和力提高 10% 的藥物分子，並設計出 ESP 得分達到 70.4 的酶，使其成為唯一一種超越 ESP 開發人員推薦的 60.0 酶底物相互作用閾值的技術。這突顯了其轉變現實世界生物分子研究的潛力。</paragraph>

##### **ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation**
2410.07908v1 by Léo Machado, Hélène Philippe, Élodie Ferreres, Julien Khlaut, Julie Dupuis, Korentin Le Floch, Denis Habip Gatenyo, Pascal Roux, Jules Grégory, Maxime Ronot, Corentin Dancette, Daniel Tordjman, Pierre Manceron, Paul Hérent

Carcinogenesis is a proteiform phenomenon, with tumors emerging in various
locations and displaying complex, diverse shapes. At the crucial intersection
of research and clinical practice, it demands precise and flexible assessment.
However, current biomarkers, such as RECIST 1.1's long and short axis
measurements, fall short of capturing this complexity, offering an approximate
estimate of tumor burden and a simplistic representation of a more intricate
process. Additionally, existing supervised AI models face challenges in
addressing the variability in tumor presentations, limiting their clinical
utility. These limitations arise from the scarcity of annotations and the
models' focus on narrowly defined tasks.
  To address these challenges, we developed ONCOPILOT, an interactive
radiological foundation model trained on approximately 7,500 CT scans covering
the whole body, from both normal anatomy and a wide range of oncological cases.
ONCOPILOT performs 3D tumor segmentation using visual prompts like point-click
and bounding boxes, outperforming state-of-the-art models (e.g., nnUnet) and
achieving radiologist-level accuracy in RECIST 1.1 measurements. The key
advantage of this foundation model is its ability to surpass state-of-the-art
performance while keeping the radiologist in the loop, a capability that
previous models could not achieve. When radiologists interactively refine the
segmentations, accuracy improves further. ONCOPILOT also accelerates
measurement processes and reduces inter-reader variability, facilitating
volumetric analysis and unlocking new biomarkers for deeper insights.
  This AI assistant is expected to enhance the precision of RECIST 1.1
measurements, unlock the potential of volumetric biomarkers, and improve
patient stratification and clinical care, while seamlessly integrating into the
radiological workflow.

摘要：<paragraph>致癌作用是一種變形現象，腫瘤出現在各種位置，並呈現出複雜多樣的形狀。在研究和臨床實務的重要交會點，它需要精確且彈性的評估。然而，目前的生物標記，例如 RECIST 1.1 的長軸和短軸測量，並無法捕捉到這種複雜性，只能提供腫瘤負擔的近似估計，以及對更複雜過程的簡化表示。此外，現有的監督式 AI 模型在處理腫瘤表現的可變性時面臨挑戰，限制了它們的臨床效用。這些限制來自於註解的稀少性，以及模型專注於狹義定義的任務。
為了應對這些挑戰，我們開發了 ONCOPILOT，這是一個互動式放射學基礎模型，訓練於涵蓋全身的約 7,500 個電腦斷層掃描，包括正常解剖結構和廣泛的腫瘤病例。ONCOPILOT 使用視覺提示（例如點選和邊界框）執行 3D 腫瘤分割，優於最先進的模型（例如 nnUnet），並在 RECIST 1.1 測量中達到放射科醫師等級的準確度。這個基礎模型的主要優點是它能夠超越最先進的效能，同時讓放射科醫師參與其中，這是以前的模型無法達到的功能。當放射科醫師互動式地優化分割時，準確度會進一步提高。ONCOPILOT 還加速了測量過程，並減少了讀者間的變異性，促进了體積分析，並解鎖了新的生物標記，以獲得更深入的見解。
預計這個 AI 助理將提高 RECIST 1.1 測量的準確度，釋放體積生物標記的潛力，並改善患者分層和臨床照護，同時無縫整合到放射學工作流程中。</paragraph>

##### **Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines**
2410.07896v1 by Junyu Lai, Jiahe Xu, Yao Yang, Yunpeng Huang, Chun Cao, Jingwei Xu

Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of natural language processing and reasoning tasks. However, their
performance in the foundational domain of arithmetic remains unsatisfactory.
When dealing with arithmetic tasks, LLMs often memorize specific examples
rather than learning the underlying computational logic, limiting their ability
to generalize to new problems. In this paper, we propose a Composable
Arithmetic Execution Framework (CAEF) that enables LLMs to learn to execute
step-by-step computations by emulating Turing Machines, thereby gaining a
genuine understanding of computational logic. Moreover, the proposed framework
is highly scalable, allowing composing learned operators to significantly
reduce the difficulty of learning complex operators. In our evaluation, CAEF
achieves nearly 100% accuracy across seven common mathematical operations on
the LLaMA 3.1-8B model, effectively supporting computations involving operands
with up to 100 digits, a level where GPT-4o falls short noticeably in some
settings.

摘要：大型語言模型 (LLM) 在廣泛的自然語言處理和推理任務中展現出非凡的能力。然而，它們在算術基礎領域的表現仍不盡人意。在處理算術任務時，LLM 常常記住特定範例，而不是學習底層的運算邏輯，這限制了它們對新問題的概化能力。在本文中，我們提出了一個可組合運算執行框架 (CAEF)，它能讓 LLM 學習通過模擬圖靈機來執行循序漸進的運算，從而真正理解運算邏輯。此外，所提出的框架具有高度可擴充性，允許組合已學習的運算子，以顯著降低學習複雜運算子的難度。在我們的評估中，CAEF 在 LLaMA 3.1-8B 模型上對七項常見的數學運算達到了近 100% 的準確度，有效地支援涉及多達 100 位數運算元的運算，這是 GPT-4o 在某些設定中明顯不足的層級。

##### **Unsupervised Data Validation Methods for Efficient Model Training**
2410.07880v1 by Yurii Paniv

This paper investigates the challenges and potential solutions for improving
machine learning systems for low-resource languages. State-of-the-art models in
natural language processing (NLP), text-to-speech (TTS), speech-to-text (STT),
and vision-language models (VLM) rely heavily on large datasets, which are
often unavailable for low-resource languages. This research explores key areas
such as defining "quality data," developing methods for generating appropriate
data and enhancing accessibility to model training. A comprehensive review of
current methodologies, including data augmentation, multilingual transfer
learning, synthetic data generation, and data selection techniques, highlights
both advancements and limitations. Several open research questions are
identified, providing a framework for future studies aimed at optimizing data
utilization, reducing the required data quantity, and maintaining high-quality
model performance. By addressing these challenges, the paper aims to make
advanced machine learning models more accessible for low-resource languages,
enhancing their utility and impact across various sectors.

摘要：這篇論文探討了改善低資源語言機器學習系統的挑戰和潛在解決方案。自然語言處理 (NLP)、文字轉語音 (TTS)、語音轉文字 (STT) 和視覺語言模型 (VLM) 中的最新模型嚴重依賴大型資料集，而這些資料集通常無法用於低資源語言。本研究探討了幾個關鍵領域，例如定義「優質資料」、開發生成適當資料的方法，以及增強模型訓練的可存取性。對當前方法論的全面回顧，包括資料擴充、多語言轉移學習、合成資料生成和資料選擇技術，突出了進展和限制。確定了幾個開放的研究問題，為旨在最佳化資料利用、減少所需資料數量和維持高品質模型效能的未來研究提供了架構。透過解決這些挑戰，本文旨在讓進階機器學習模型更易於用於低資源語言，進而提升其在各個領域的實用性和影響力。

##### **Benchmarking Agentic Workflow Generation**
2410.07869v1 by Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen

Large Language Models (LLMs), with their exceptional ability to handle a wide
range of tasks, have driven significant advancements in tackling reasoning and
planning tasks, wherein decomposing complex problems into executable workflows
is a crucial step in this process. Existing workflow evaluation frameworks
either focus solely on holistic performance or suffer from limitations such as
restricted scenario coverage, simplistic workflow structures, and lax
evaluation standards. To this end, we introduce WorFBench, a unified workflow
generation benchmark with multi-faceted scenarios and intricate graph workflow
structures. Additionally, we present WorFEval, a systemic evaluation protocol
utilizing subsequence and subgraph matching algorithms to accurately quantify
the LLM agent's workflow generation capabilities. Through comprehensive
evaluations across different types of LLMs, we discover distinct gaps between
the sequence planning capabilities and graph planning capabilities of LLM
agents, with even GPT-4 exhibiting a gap of around 15%. We also train two
open-source models and evaluate their generalization abilities on held-out
tasks. Furthermore, we observe that the generated workflows can enhance
downstream tasks, enabling them to achieve superior performance with less time
during inference. Code and dataset will be available at
https://github.com/zjunlp/WorFBench.

摘要：大型語言模型 (LLM) 擁有處理各種任務的非凡能力，推動了解決推理和規劃任務的顯著進展，其中將複雜問題分解為可執行工作流程是此過程中至關重要的一步。現有的工作流程評估框架只專注於整體效能，或受到情境涵蓋範圍受限、工作流程結構簡化和評估標準寬鬆等限制。為此，我們引入了 WorFBench，一個統一的工作流程生成基準，具有多方面的場景和複雜的圖形工作流程結構。此外，我們提出了 WorFEval，一個利用子序列和子圖匹配演算法來準確量化 LLM 代理工作流程生成能力的系統性評估協定。透過對不同類型 LLM 的全面評估，我們發現 LLM 代理的序列規劃能力和圖形規劃能力之間存在明顯的差距，即使是 GPT-4 也表現出約 15% 的差距。我們還訓練了兩個開源模型，並評估了它們在保留任務上的泛化能力。此外，我們觀察到生成的的工作流程可以增強下游任務，讓它們在推理期間以更少的時間獲得更好的效能。程式碼和資料集將在 https://github.com/zjunlp/WorFBench 上提供。

##### **System-2 Reasoning via Generality and Adaptation**
2410.07866v1 by Sejin Kim, Sundong Kim

While significant progress has been made in task-specific applications,
current models struggle with deep reasoning, generality, and adaptation -- key
components of System-2 reasoning that are crucial for achieving Artificial
General Intelligence (AGI). Despite the promise of approaches such as program
synthesis, language models, and transformers, these methods often fail to
generalize beyond their training data and to adapt to novel tasks, limiting
their ability to perform human-like reasoning. This paper explores the
limitations of existing approaches in achieving advanced System-2 reasoning and
highlights the importance of generality and adaptation for AGI. Moreover, we
propose four key research directions to address these gaps: (1) learning human
intentions from action sequences, (2) combining symbolic and neural models, (3)
meta-learning for unfamiliar environments, and (4) reinforcement learning to
reason multi-step. Through these directions, we aim to advance the ability to
generalize and adapt, bringing computational models closer to the reasoning
capabilities required for AGI.

摘要：儘管在特定任務應用程式方面已取得重大進展，
目前的模型在深度推理、普遍性和適應性方面仍有困難，而這些是系統 2 推理中至關重要的關鍵組成部分，對於實現人工通用智慧 (AGI) 至關重要。儘管有程式合成、語言模型和轉換器等方法，但這些方法通常無法推廣到訓練資料之外並適應新任務，這限制了它們執行類人推理的能力。本文探討了現有方法在實現進階系統 2 推理方面的限制，並強調了普遍性和適應性對 AGI 的重要性。此外，我們提出了四個關鍵研究方向來解決這些差距：(1) 從動作序列中學習人類意圖，(2) 結合符號和神經模型，(3) 針對不熟悉的環境進行元學習，以及 (4) 透過強化學習進行多步驟推理。透過這些方向，我們旨在提升推廣和適應的能力，讓計算模型更接近 AGI 所需的推理能力。

##### **RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation**
2410.07864v1 by Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu

Bimanual manipulation is essential in robotics, yet developing foundation
models is extremely challenging due to the inherent complexity of coordinating
two robot arms (leading to multi-modal action distributions) and the scarcity
of training data. In this paper, we present the Robotics Diffusion Transformer
(RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT
builds on diffusion models to effectively represent multi-modality, with
innovative designs of a scalable Transformer to deal with the heterogeneity of
multi-modal inputs and to capture the nonlinearity and high frequency of
robotic data. To address data scarcity, we further introduce a Physically
Interpretable Unified Action Space, which can unify the action representations
of various robots while preserving the physical meanings of original actions,
facilitating learning transferrable physical knowledge. With these designs, we
managed to pre-train RDT on the largest collection of multi-robot datasets to
date and scaled it up to 1.2B parameters, which is the largest diffusion-based
foundation model for robotic manipulation. We finally fine-tuned RDT on a
self-created multi-task bimanual dataset with over 6K+ episodes to refine its
manipulation capabilities. Experiments on real robots demonstrate that RDT
significantly outperforms existing methods. It exhibits zero-shot
generalization to unseen objects and scenes, understands and follows language
instructions, learns new skills with just 1~5 demonstrations, and effectively
handles complex, dexterous tasks. We refer to
https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.

摘要：雙手操作在機器人技術中至關重要，然而由於協調兩個機器手臂（導致多模式動作分佈）的固有複雜性以及訓練數據的稀缺性，開發基礎模型極具挑戰性。在本文中，我們提出了機器人擴散Transformer (RDT)，這是一個用於雙手操作的開創性擴散基礎模型。RDT 建立在擴散模型之上，以有效表示多模態，並採用可擴充Transformer的創新設計來處理多模態輸入的異質性，並捕捉機器人數據的非線性和高頻率。為了解決數據稀缺問題，我們進一步引入了一個物理可解釋統一動作空間，它可以統一各種機器人的動作表示，同時保留原始動作的物理意義，促進學習可轉移的物理知識。有了這些設計，我們設法在迄今為止最大的多機器人數據集集合上對 RDT 進行預訓練，並將其擴展到 1.2B 參數，這是最大的基於擴散的機器人操作基礎模型。我們最終在一個自創的多任務雙手數據集上對 RDT 進行微調，該數據集包含超過 6K 個情節，以提升其操作能力。在真實機器人上的實驗表明，RDT 明顯優於現有方法。它對未見過的物體和場景表現出零次學習泛化，理解並遵循語言指令，只需 1~5 次示範就能學習新技能，並有效處理複雜的靈巧任務。我們參考 https://rdt-robotics.github.io/rdt-robotics/ 獲取代碼和影片。

##### **From Logits to Hierarchies: Hierarchical Clustering made Simple**
2410.07858v1 by Emanuele Palumbo, Moritz Vandenhirtz, Alain Ryser, Imant Daunhawer, Julia E. Vogt

The structure of many real-world datasets is intrinsically hierarchical,
making the modeling of such hierarchies a critical objective in both
unsupervised and supervised machine learning. Recently, novel approaches for
hierarchical clustering with deep architectures have been proposed. In this
work, we take a critical perspective on this line of research and demonstrate
that many approaches exhibit major limitations when applied to realistic
datasets, partly due to their high computational complexity. In particular, we
show that a lightweight procedure implemented on top of pre-trained
non-hierarchical clustering models outperforms models designed specifically for
hierarchical clustering. Our proposed approach is computationally efficient and
applicable to any pre-trained clustering model that outputs logits, without
requiring any fine-tuning. To highlight the generality of our findings, we
illustrate how our method can also be applied in a supervised setup, recovering
meaningful hierarchies from a pre-trained ImageNet classifier.

摘要：許多真實世界資料集的結構本質上是階層式的，
使得對此類階層建模成為無監督和監督機器學習中的重要目標。最近，
已經提出具有深度架構的階層式聚類新方法。在這個
工作中，我們對這條研究路線採取批判性的觀點，並證明
許多方法在應用於實際
資料集時表現出重大的限制，部分原因是它們的高計算複雜度。特別是，我們
表明在預先訓練的
非階層式聚類模型之上實作的輕量級程序優於專門為
階層式聚類設計的模型。我們提出的方法在計算上是有效的，並且
適用於任何輸出 logit 的預先訓練聚類模型，而無需
進行任何微調。為了強調我們發現的普遍性，我們
說明我們的模型如何也可以應用在有監督的設定中，從預先訓練的 ImageNet 分類器中恢復
有意義的階層。

##### **Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency**
2410.07839v1 by Tim Knappe, Ryan Li, Ayush Chauhan, Kaylee Chhua, Kevin Zhu, Sean O'Brien

While large language models (LLMs) have rapidly improved their performance on
a broad number of tasks, they still often fall short on reasoning tasks. As
LLMs become more integrated in diverse real-world tasks, advancing their
reasoning capabilities is crucial to their effectiveness in nuanced, complex
problems. Wang et al's self-consistency framework reveals that sampling
multiple rationales before taking a majority vote reliably improves model
performance across various closed-answer reasoning tasks. Standard methods
based on this framework aggregate the final decisions of these rationales but
fail to utilize the detailed step-by-step reasoning paths applied by these
paths. Our work enhances this approach by incorporating and analyzing both the
reasoning paths of these rationales in addition to their final decisions before
taking a majority vote. These methods not only improve the reliability of
reasoning paths but also cause more robust performance on complex reasoning
tasks.

摘要：儘管大型語言模型 (LLM) 在許多任務上的表現迅速提升，但它們在推理任務上仍經常表現不佳。隨著 LLM 更多元地整合在各種實際任務中，提升其推理能力對於它們在細微複雜問題中的成效至關重要。Wang 等人的自洽架構顯示，在進行多數決投票前對多個依據取樣，可以大幅提升模型在各種封閉式回答推理任務中的表現。基於此架構的標準方法會彙總這些依據的最終決定，但無法利用這些依據所應用的詳細逐步推理路徑。我們的研究透過在進行多數決投票前納入和分析這些依據的推理路徑，以及其最終決定，來強化此方法。這些方法不僅提升推理路徑的可靠性，也讓模型在複雜推理任務中的表現更強健。

##### **MinorityPrompt: Text to Minority Image Generation via Prompt Optimization**
2410.07838v1 by Soobin Um, Jong Chul Ye

We investigate the generation of minority samples using pretrained
text-to-image (T2I) latent diffusion models. Minority instances, in the context
of T2I generation, can be defined as ones living on low-density regions of
text-conditional data distributions. They are valuable for various applications
of modern T2I generators, such as data augmentation and creative AI.
Unfortunately, existing pretrained T2I diffusion models primarily focus on
high-density regions, largely due to the influence of guided samplers (like
CFG) that are essential for producing high-quality generations. To address
this, we present a novel framework to counter the high-density-focus of T2I
diffusion models. Specifically, we first develop an online prompt optimization
framework that can encourage the emergence of desired properties during
inference while preserving semantic contents of user-provided prompts. We
subsequently tailor this generic prompt optimizer into a specialized solver
that promotes the generation of minority features by incorporating a
carefully-crafted likelihood objective. Our comprehensive experiments,
conducted across various types of T2I models, demonstrate that our approach
significantly enhances the capability to produce high-quality minority
instances compared to existing samplers.

摘要：我們研究使用預訓練的文字轉圖像 (T2I) 潛在擴散模型生成少數樣本。在 T2I 生成的背景下，少數實例可以定義為存在於文字條件數據分佈的低密度區域中的實例。它們對於現代 T2I 生成器的各種應用很有價值，例如數據擴充和創意 AI。不幸的是，現有的預訓練 T2I 擴散模型主要關注高密度區域，這在很大程度上是受到引導取樣器（如 CFG）的影響，而引導取樣器對於產生高品質生成至關重要。為了解決這個問題，我們提出了一個新框架來應對 T2I 擴散模型的高密度關注。具體來說，我們首先開發一個線上提示最佳化框架，它可以在推理過程中鼓勵所需屬性的出現，同時保留使用者提供的提示的語義內容。我們隨後將這個通用提示最佳化器調整為一個專門的求解器，通過納入精心製作的似然目標來促進少數特徵的生成。我們在各種 T2I 模型上進行的綜合實驗表明，與現有的取樣器相比，我們的做法顯著增強了產生高品質少數實例的能力。

##### **Masked Generative Priors Improve World Models Sequence Modelling Capabilities**
2410.07836v1 by Cristian Meo, Mircea Lica, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels

Deep Reinforcement Learning (RL) has become the leading approach for creating
artificial agents in complex environments. Model-based approaches, which are RL
methods with world models that predict environment dynamics, are among the most
promising directions for improving data efficiency, forming a critical step
toward bridging the gap between research and real-world deployment. In
particular, world models enhance sample efficiency by learning in imagination,
which involves training a generative sequence model of the environment in a
self-supervised manner. Recently, Masked Generative Modelling has emerged as a
more efficient and superior inductive bias for modelling and generating token
sequences. Building on the Efficient Stochastic Transformer-based World Models
(STORM) architecture, we replace the traditional MLP prior with a Masked
Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our
model on two downstream tasks: reinforcement learning and video prediction.
GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari
100k benchmark. Moreover, we apply Transformer-based World Models to continuous
action environments for the first time, addressing a significant gap in prior
research. To achieve this, we employ a state mixer function that integrates
latent state representations with actions, enabling our model to handle
continuous control tasks. We validate this approach through qualitative and
quantitative analyses on the DeepMind Control Suite, showcasing the
effectiveness of Transformer-based World Models in this new domain. Our results
highlight the versatility and efficacy of the MaskGIT dynamics prior, paving
the way for more accurate world models and effective RL policies.

摘要：深度強化學習 (RL) 已成為在複雜環境中建立人工代理程式的主要方法。基於模型的方法（即具備預測環境動態的世界模型的 RL 方法）是改善資料效率的最有希望的方向之一，構成了縮小研究與實際部署之間差距的關鍵步驟。特別是，世界模型透過在想像中學習來增強樣本效率，其中涉及以自我監督的方式訓練環境的生成式序列模型。最近，遮罩生成模型已成為一種更有效率且更優越的歸納偏誤，用於建模和產生符號序列。在基於有效隨機變換器世界模型 (STORM) 架構的基礎上，我們用遮罩生成先驗 (例如 MaskGIT 先驗) 取代傳統的 MLP 先驗，並引入 GIT-STORM。我們在兩個下游任務中評估我們的模型：強化學習和影片預測。GIT-STORM 在 Atari 100k 基準測試中的 RL 任務中展現出顯著的效能提升。此外，我們首次將基於變換器的世界模型應用於連續動作環境，解決了先前研究中的重大差距。為此，我們採用一個狀態混合器函數，將潛在狀態表示與動作整合在一起，使我們的模型能夠處理連續控制任務。我們透過在 DeepMind Control Suite 上進行定性和定量分析來驗證此方法，展示了基於變換器的世界模型在此新領域中的有效性。我們的結果突顯了 MaskGIT 動態先驗的多功能性和有效性，為更準確的世界模型和有效的 RL 政策鋪平了道路。

##### **NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models**
2410.07830v1 by William Tan, Kevin Zhu

Large Language Models (LLMs) have demonstrated exceptional promise in
translation tasks for high-resource languages. However, their performance in
low-resource languages is limited by the scarcity of both parallel and
monolingual corpora, as well as the presence of noise. Consequently, such LLMs
suffer with alignment and have lagged behind State-of-The-Art (SoTA) neural
machine translation (NMT) models in these settings. This paper introduces
NusaMT-7B, an LLM-based machine translation model for low-resource Indonesian
languages, starting with Balinese and Minangkabau. Leveraging the pretrained
LLaMA2-7B, our approach integrates continued pre-training on monolingual data,
Supervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to
reduce noise in parallel sentences. In the FLORES-200 multilingual translation
benchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to
+6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms
by up to -3.38 spBLEU in translations into higher-resource languages. Our
results show that fine-tuned LLMs can enhance translation quality for
low-resource languages, aiding in linguistic preservation and cross-cultural
communication.

摘要：大型語言模型 (LLM) 已在高資源語言的翻譯任務中展現出非凡的潛力。然而，它們在低資源語言中的表現受到平行語料和單語語料庫的稀缺以及雜訊的存在所限制。因此，此類 LLM 在對齊方面存在問題，並且在這些設定中落後於最先進 (SoTA) 神經機器翻譯 (NMT) 模型。本文介紹 NusaMT-7B，一種基於 LLM 的機器翻譯模型，適用於低資源印尼語言，從巴厘語和米南卡保語開始。利用預先訓練的 LLaMA2-7B，我們的做法整合了對單語數據的持續預訓練、監督微調 (SFT)、自學以及基於 LLM 的數據清理器，以減少平行句子中的雜訊。在 FLORES-200 多語言翻譯基準中，NusaMT-7B 在 spBLEU 指標中優於 SoTA 模型，在翻譯成巴厘語和米南卡保語時，spBLEU 最高可提高 +6.69，但在翻譯成高資源語言時，spBLEU 最低可降低 -3.38。我們的結果表明，微調後的 LLM 可以提高低資源語言的翻譯品質，有助於語言保存和跨文化溝通。

##### **Why do objects have many names? A study on word informativeness in language use and lexical systems**
2410.07827v1 by Eleonora Gualdoni, Gemma Boleda

Human lexicons contain many different words that speakers can use to refer to
the same object, e.g., "purple" or "magenta" for the same shade of color. On
the one hand, studies on language use have explored how speakers adapt their
referring expressions to successfully communicate in context, without focusing
on properties of the lexical system. On the other hand, studies in language
evolution have discussed how competing pressures for informativeness and
simplicity shape lexical systems, without tackling in-context communication. We
aim at bridging the gap between these traditions, and explore why a soft
mapping between referents and words is a good solution for communication, by
taking into account both in-context communication and the structure of the
lexicon. We propose a simple measure of informativeness for words and lexical
systems, grounded in a visual space, and analyze color naming data for English
and Mandarin Chinese. We conclude that optimal lexical systems are those where
multiple words can apply to the same referent, conveying different amounts of
information. Such systems allow speakers to maximize communication accuracy and
minimize the amount of information they convey when communicating about
referents in contexts.

摘要：人类词汇包含许多不同的单词，说话者可以使用这些单词来指代同一个对象，例如，对于同一种颜色的色调，可以使用“紫色”或“洋红色”。一方面，语言使用研究探讨了说话者如何调整他们的指称表达，以便在上下文中成功地进行交流，而没有专注于词汇系统的属性。另一方面，语言演化研究讨论了信息性和简单性之间的竞争压力如何塑造词汇系统，而没有处理上下文中交流。我们的目标是弥合这些传统之间的差距，并探讨为什么指称物和单词之间的软映射是通过考虑上下文中的交流和词汇结构来进行交流的一个好解决方案。我们提出了一种简单的单词和词汇系统信息性度量，它基于视觉空间，并分析了英语和普通话的色彩命名数据。我们得出结论，最佳词汇系统是多个单词可以应用于同一指称物，传达不同数量的信息。这样的系统允许说话者最大化交流准确性，并在上下文中交流指称物时最小化他们传达的信息量。

##### **Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses**
2410.07826v1 by Pranav Senthilkumar, Visshwa Balasubramanian, Prisha Jain, Aneesa Maity, Jonathan Lu, Kevin Zhu

Language models often misinterpret human intentions due to their handling of
ambiguity, a limitation well-recognized in NLP research. While morally clear
scenarios are more discernible to LLMs, greater difficulty is encountered in
morally ambiguous contexts. In this investigation, we explored LLM calibration
to show that human and LLM judgments are poorly aligned in such scenarios. We
used two curated datasets from the Scruples project for evaluation: DILEMMAS,
which involves pairs of distinct moral scenarios to assess the model's ability
to compare and contrast ethical situations, and ANECDOTES, which presents
individual narratives to evaluate the model's skill in drawing out details,
interpreting, and analyzing distinct moral scenarios. Model answer
probabilities were extracted for all possible choices and compared with human
annotations to benchmark the alignment of three models: Llama-3.1-8b,
Zephyr-7b-beta, and Mistral-7b. Significant improvements were observed after
fine-tuning, with notable enhancements in both cross-entropy and Dirichlet
scores, particularly in the latter. Notably, after fine-tuning, the performance
of Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental
models that were examined were all still outperformed by the BERT and RoBERTa
models in terms of cross-entropy scores. Our fine-tuning approach, which
improves the model's understanding of text distributions in a text-to-text
format, effectively enhances performance and alignment in complex
decision-making contexts, underscoring the need for further research to refine
ethical reasoning techniques and capture human judgment nuances.

摘要：語言模型經常會誤解人類的意圖，因為它們處理歧義的方式，這在 NLP 研究中是一個廣為人知的限制。雖然對於 LLM 來說，道德明確的場景更容易辨識，但在道德模糊的語境中會遇到更大的困難。在此研究中，我們探討了 LLM 校正，以證明人類和 LLM 的判斷在這種場景中對齊得不好。我們使用了來自 Scruples 專案的兩個整理過的資料集進行評估：DILEMMAS，其中包含兩組不同的道德場景，用於評估模型比較和對比道德情境的的能力，以及 ANECDOTES，其中提供了個別敘述，用於評估模型在提取細節、詮釋和分析不同的道德場景方面的技能。模型答案機率會被提取出來，以供所有可能的選擇使用，並與人類註解進行比較，以基準測試三個模型的對齊程度：Llama-3.1-8b、Zephyr-7b-beta 和 Mistral-7b。在微調後觀察到顯著的進步，特別是在後者中，交叉熵和 Dirichlet 分數都有顯著的提升。特別值得注意的是，微調後，Mistral-7B-Instruct-v0.3 的效能與 GPT-4o 相當。然而，在交叉熵分數方面，所檢驗的實驗模型仍全都被 BERT 和 RoBERTa 模型超越。我們的微調方法改進了模型對文字到文字格式中文字分佈的理解，有效地提升了在複雜決策制定語境中的效能和對齊程度，強調了進一步研究以改善道德推理技術和捕捉人類判斷細微差别的必要性。

##### **Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models**
2410.07825v1 by Zhipeng Chen, Liang Song, Kun Zhou, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen

Multi-lingual ability transfer has become increasingly important for the
broad application of large language models (LLMs). Existing work highly relies
on training with the multi-lingual ability-related data, which may be not
available for low-resource languages. To solve it, we propose a Multi-lingual
Ability Extraction and Transfer approach, named as MAET. Our key idea is to
decompose and extract language-agnostic ability-related weights from LLMs, and
transfer them across different languages by simple addition and subtraction
operations without training. Specially, our MAET consists of the extraction and
transfer stages. In the extraction stage, we firstly locate key neurons that
are highly related to specific abilities, and then employ them to extract the
transferable ability-specific weights. In the transfer stage, we further select
the ability-related parameter tensors, and design the merging strategy based on
the linguistic and ability specific weights, to build the multi-lingual
ability-enhanced LLM. To demonstrate the effectiveness of our proposed
approach, we conduct extensive experiments on mathematical and scientific tasks
in both high-resource lingual and low-resource lingual scenarios. Experiment
results have shown that MAET can effectively and efficiently extract and
transfer the advanced abilities, and outperform training-based baseline
methods. Our code and data are available at
\url{https://github.com/RUCAIBox/MAET}.

摘要：多语言能力迁移对于大语言模型 (LLM) 的广泛应用变得越来越重要。现有工作高度依赖于使用与多语言能力相关的数据进行训练，这对于低资源语言来说可能不可用。为了解决这个问题，我们提出了一种名为 MAET 的多语言能力提取和迁移方法。我们的关键思想是从 LLM 中分解和提取与语言无关的能力相关权重，并通过简单的加法和减法运算在不同语言之间转移它们，而无需训练。特别地，我们的 MAET 包含提取和转移阶段。在提取阶段，我们首先定位与特定能力高度相关的关键神经元，然后使用它们来提取可转移的能力特定权重。在转移阶段，我们进一步选择与能力相关的参数张量，并根据语言和能力特定权重设计合并策略，以构建多语言能力增强的 LLM。为了证明我们提出的方法的有效性，我们在高资源语言和低资源语言场景中对数学和科学任务进行了广泛的实验。实验结果表明，MAET 可以有效地提取和转移高级能力，并且优于基于训练的基线方法。我们的代码和数据可在 \url{https://github.com/RUCAIBox/MAET} 获得。

##### **Mitigating Gender Bias in Code Large Language Models via Model Editing**
2410.07820v1 by Zhanyue Qin, Haochuan Wang, Zecheng Wang, Deyuan Liu, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Dianbo Sui

In recent years, with the maturation of large language model (LLM) technology
and the emergence of high-quality programming code datasets, researchers have
become increasingly confident in addressing the challenges of program synthesis
automatically. However, since most of the training samples for LLMs are
unscreened, it is inevitable that LLMs' performance may not align with
real-world scenarios, leading to the presence of social bias. To evaluate and
quantify the gender bias in code LLMs, we propose a dataset named CodeGenBias
(Gender Bias in the Code Generation) and an evaluation metric called FB-Score
(Factual Bias Score) based on the actual gender distribution of correlative
professions. With the help of CodeGenBias and FB-Score, we evaluate and analyze
the gender bias in eight mainstream Code LLMs. Previous work has demonstrated
that model editing methods that perform well in knowledge editing have the
potential to mitigate social bias in LLMs. Therefore, we develop a model
editing approach named MG-Editing (Multi-Granularity model Editing), which
includes the locating and editing phases. Our model editing method MG-Editing
can be applied at five different levels of model parameter granularity: full
parameters level, layer level, module level, row level, and neuron level.
Extensive experiments not only demonstrate that our MG-Editing can effectively
mitigate the gender bias in code LLMs while maintaining their general code
generation capabilities, but also showcase its excellent generalization. At the
same time, the experimental results show that, considering both the gender bias
of the model and its general code generation capability, MG-Editing is most
effective when applied at the row and neuron levels of granularity.

摘要：<paragraph>近年来，随着大语言模型 (LLM) 技术的成熟
以及高质量编程代码数据集的出现，研究人员对
自动解决程序合成的挑战越来越有信心。然而，由于 LLM 的大多数训练样本是
未经筛选的，因此 LLM 的性能不可避免地可能与
现实世界场景不一致，从而导致社会偏见的存在。为了评估和
量化代码 LLM 中的性别偏见，我们提出了一个名为 CodeGenBias 的数据集
（代码生成中的性别偏见）和一个基于相关职业的实际性别分布的评估指标，称为 FB-Score
（事实偏见分数）。在 CodeGenBias 和 FB-Score 的帮助下，我们评估和分析
了八个主流代码 LLM 中的性别偏见。以前的工作已经证明
在知识编辑中表现良好的模型编辑方法具有
减轻 LLM 中社会偏见。因此，我们开发了一个名为 MG-Editing 的模型
编辑方法（多粒度模型编辑），其中包括定位和编辑阶段。我们的模型编辑方法 MG-Editing
可以在模型参数粒度的五个不同级别上应用：完整
参数级别、层级别、模块级别、行级别和神经元级别。
大量的实验不仅证明了我们的 MG-Editing 可以有效
减轻代码 LLM 中的性别偏见，同时保持其通用代码
生成能力，但也展示了其出色的泛化能力。同时，实验结果表明，考虑模型的性别偏见
及其通用代码生成能力，MG-Editing 在行和神经元级别应用时最有效
粒度。</paragraph>

##### **Uncovering Overfitting in Large Language Model Editing**
2410.07819v1 by Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen

Knowledge editing has been proposed as an effective method for updating and
correcting the internal knowledge of Large Language Models (LLMs). However,
existing editing methods often struggle with complex tasks, such as multi-hop
reasoning. In this paper, we identify and investigate the phenomenon of Editing
Overfit, where edited models assign disproportionately high probabilities to
the edit target, hindering the generalization of new knowledge in complex
scenarios. We attribute this issue to the current editing paradigm, which
places excessive emphasis on the direct correspondence between the input prompt
and the edit target for each edit sample. To further explore this issue, we
introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge
Editing), along with fine-grained evaluation metrics. Through comprehensive
experiments and analysis, we demonstrate that Editing Overfit is prevalent in
current editing methods and that common overfitting mitigation strategies are
of limited effectiveness in knowledge editing. To overcome this, inspired by
LLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy
called Learn to Inference (LTI), which introduce a Multi-stage Inference
Constraint module to guide the edited models in recalling new knowledge
similarly to how unedited LLMs leverage knowledge through in-context learning.
Extensive experimental results across a wide range of tasks validate the
effectiveness of LTI in mitigating Editing Overfit.

摘要：知識編輯已被提議為更新和修正大型語言模型 (LLM) 內部知識的有效方法。然而，現有的編輯方法通常難以處理複雜的任務，例如多跳推理。在本文中，我們找出並研究編輯過度擬合的現象，其中已編輯模型會將不成比例的高機率分配給編輯目標，阻礙複雜場景中新知識的概化。我們將此問題歸因於目前的編輯範例，該範例過度強調輸入提示和每個編輯範例的編輯目標之間的直接對應關係。為了進一步探討這個問題，我們引入了一個新的基準 EVOKE（知識編輯中編輯過度擬合的評估），以及細粒度的評估指標。透過全面的實驗和分析，我們證明編輯過度擬合在目前的編輯方法中很普遍，而常見的過度擬合緩解策略在知識編輯中的效果有限。為了克服這個問題，受到 LLM 知識回溯機制的啟發，我們提出了一種新的即插即用策略，稱為學習推論 (LTI)，它引入了一個多階段推論約束模組，以引導已編輯的模型回溯新知識，類似於未編輯的 LLM 如何透過情境學習來利用知識。在各種任務中進行的廣泛實驗結果驗證了 LTI 在緩解編輯過度擬合方面的有效性。

##### **Temporal-Difference Variational Continual Learning**
2410.07812v1 by Luckeciano C. Melo, Alessandro Abate, Yarin Gal

A crucial capability of Machine Learning models in real-world applications is
the ability to continuously learn new tasks. This adaptability allows them to
respond to potentially inevitable shifts in the data-generating distribution
over time. However, in Continual Learning (CL) settings, models often struggle
to balance learning new tasks (plasticity) with retaining previous knowledge
(memory stability). Consequently, they are susceptible to Catastrophic
Forgetting, which degrades performance and undermines the reliability of
deployed systems. Variational Continual Learning methods tackle this challenge
by employing a learning objective that recursively updates the posterior
distribution and enforces it to stay close to the latest posterior estimate.
Nonetheless, we argue that these methods may be ineffective due to compounding
approximation errors over successive recursions. To mitigate this, we propose
new learning objectives that integrate the regularization effects of multiple
previous posterior estimations, preventing individual errors from dominating
future posterior updates and compounding over time. We reveal insightful
connections between these objectives and Temporal-Difference methods, a popular
learning mechanism in Reinforcement Learning and Neuroscience. We evaluate the
proposed objectives on challenging versions of popular CL benchmarks,
demonstrating that they outperform standard Variational CL methods and
non-variational baselines, effectively alleviating Catastrophic Forgetting.

摘要：機器學習模型在實際應用中的一項關鍵能力是
持續學習新任務的能力。這種適應性讓它們能夠
隨著時間推移對資料產生分佈中潛在的不可避免的轉變做出回應。然而，在持續學習 (CL) 設定中，模型通常難以在學習新任務（可塑性）與保留先前知識（記憶穩定性）之間取得平衡。因此，它們容易發生災難性遺忘，這會降低效能並損害已部署系統的可靠性。變異持續學習方法透過採用學習目標來應對這個挑戰，該目標會遞迴更新後驗分佈並強制其維持在最新的後驗估計值附近。儘管如此，我們認為這些方法可能因連續遞迴的近似誤差而無效。為了減輕這個問題，我們提出新的學習目標，結合多個先前後驗估計值的正則化效果，防止個別誤差主導未來的後驗更新並隨著時間推移而累積。我們揭示了這些目標與時序差分方法之間的深入關聯，時序差分方法是強化學習和神經科學中一種流行的學習機制。我們在熱門 CL 基準的挑戰版本上評估所提出的目標，證明它們優於標準的變異 CL 方法和非變異基準，有效地減輕了災難性遺忘。

##### **Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?**
2410.07809v1 by Gürkan Soykan, Gözde Gül Şahin

Multilingual language models often perform unevenly across different
languages due to limited generalization capabilities for some languages. This
issue is significant because of the growing interest in making universal
language models that work well for all languages. Instruction tuning with
multilingual instruction-response pairs has been used to improve model
performance across various languages. However, this approach is challenged by
high computational costs, a lack of quality tuning data for all languages, and
the "curse of multilinguality" -- the performance drop per language after
adding many languages. Recent studies have found that working with datasets
with few languages and a smaller number of instances can be beneficial. Yet,
there exists no systematic investigation into how choosing different languages
affects multilingual instruction tuning. Our study proposes a method to select
languages for instruction tuning in a linguistically informed way, aiming to
boost model performance across languages and tasks. We use a simple algorithm
to choose diverse languages and test their effectiveness on various benchmarks
and open-ended questions. Our results show that this careful selection
generally leads to better outcomes than choosing languages at random. We
suggest a new and simple way of enhancing multilingual models by selecting
diverse languages based on linguistic features that could help develop better
multilingual systems and guide dataset creation efforts. All resources,
including the code for language selection and multilingual instruction tuning,
are made available in our official repository at
https://github.com/GGLAB-KU/ling-informed-mit enabling reproducibility and
further research in this area.

摘要：多語言語言模型通常在不同語言之間表現不均，這是因為某些語言的概括能力有限。這個問題很重要，因為人們越來越有興趣建立對所有語言都有效的通用語言模型。透過多語言指令反應配對進行指令調整，已用於改善各種語言的模型效能。然而，這個方法受到高運算成本、缺乏所有語言的品質調整資料，以及「多語言的詛咒」的挑戰，也就是在加入許多語言後，每個語言的效能都會下降。最近的研究發現，使用語言較少且實例數量較小的資料集會很有幫助。然而，目前尚未系統性地探討選擇不同語言如何影響多語言指令調整。我們的研究提出了一種方法來以語言學為基礎的方式選擇指令調整的語言，目的是提升跨語言和任務的模型效能。我們使用一個簡單的演算法來選擇不同的語言，並在各種基準和開放式問題上測試其有效性。我們的結果顯示，這種仔細的選擇通常會比隨機選擇語言帶來更好的成果。我們建議一種新且簡單的方法，透過根據語言特徵選擇不同的語言來增強多語言模型，這有助於開發更好的多語言系統，並引導資料集建立工作。所有資源，包括語言選擇和多語言指令調整的程式碼，都可以在我們的官方存放庫中取得：https://github.com/GGLAB-KU/ling-informed-mit，以利於再現性，並進一步研究這個領域。

##### **Rewriting Conversational Utterances with Instructed Large Language Models**
2410.07797v1 by Elnara Galimzhanova, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Guido Rocchietti

Many recent studies have shown the ability of large language models (LLMs) to
achieve state-of-the-art performance on many NLP tasks, such as question
answering, text summarization, coding, and translation. In some cases, the
results provided by LLMs are on par with those of human experts. These models'
most disruptive innovation is their ability to perform tasks via zero-shot or
few-shot prompting. This capability has been successfully exploited to train
instructed LLMs, where reinforcement learning with human feedback is used to
guide the model to follow the user's requests directly. In this paper, we
investigate the ability of instructed LLMs to improve conversational search
effectiveness by rewriting user questions in a conversational setting. We study
which prompts provide the most informative rewritten utterances that lead to
the best retrieval performance. Reproducible experiments are conducted on
publicly-available TREC CAST datasets. The results show that rewriting
conversational utterances with instructed LLMs achieves significant
improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and
11.5% in Recall@500 over state-of-the-art techniques.

摘要：許多最近的研究顯示，大型語言模型 (LLM) 在許多 NLP 任務上展現出最先進的效能，例如問答、文字摘要、編碼和翻譯。在某些情況下，LLM 提供的結果與人類專家的結果不相上下。這些模型最具破壞性的創新是它們能夠透過零次學習或少次學習提示來執行任務。這種能力已成功用於訓練指示式 LLM，其中使用具有人類回饋的強化學習來引導模型直接遵循使用者的請求。在本文中，我們探討指示式 LLM 在對話式設定中改寫使用者問題，以提升對話式搜尋效能的能力。我們研究哪些提示提供了最多資訊的改寫語句，這些語句可帶來最佳的檢索效能。可在公開的 TREC CAST 資料集上進行可複製的實驗。結果顯示，使用指示式 LLM 改寫對話式語句，在 MRR 方面獲得高達 25.2% 的顯著提升，在 Precision@1 方面獲得 31.7% 的提升，在 NDCG@3 方面獲得 27% 的提升，在 Recall@500 方面獲得 11.5% 的提升，優於最先進的技術。

##### **Do Current Language Models Support Code Intelligence for R Programming Language?**
2410.07793v1 by ZiXiao Zhao, Fatemeh H. Fard

Recent advancements in developing Pre-trained Language Models for Code
(Code-PLMs) have urged many areas of Software Engineering (SE) and brought
breakthrough results for many SE tasks. Though these models have achieved the
state-of-the-art performance for SE tasks for many popular programming
languages, such as Java and Python, the Scientific Software and its related
languages like R programming language have rarely benefited or even been
evaluated with the Code-PLMs. Research has shown that R has many differences
with other programming languages and requires specific techniques. In this
study, we provide the first insights for code intelligence for R. For this
purpose, we collect and open source an R dataset, and evaluate Code-PLMs for
the two tasks of code summarization and method name prediction using several
settings and strategies, including the differences in two R styles, Tidy-verse
and Base R. Our results demonstrate that the studied models have experienced
varying degrees of performance degradation when processing R programming
language code, which is supported by human evaluation. Additionally, not all
models show performance improvement in R-specific tasks even after
multi-language fine-tuning. The dual syntax paradigms in R significantly impact
the models' performance, particularly in code summarization tasks. Furthermore,
the project-specific context inherent in R codebases significantly impacts the
performance when attempting cross-project training.

摘要：最近在開發程式碼預訓練語言模型 (Code-PLM) 方面取得的進展，已促使軟體工程 (SE) 的許多領域，並為許多 SE 任務帶來突破性的成果。儘管這些模型已針對許多熱門程式語言（例如 Java 和 Python）的 SE 任務達到了最先進的效能，但科學軟體及其相關語言（例如 R 程式語言）很少受益，甚至很少使用 Code-PLM 進行評估。研究表明，R 與其他程式語言有許多不同之處，需要特定的技術。在這項研究中，我們提供了 R 的程式碼智慧的第一個見解。為此，我們收集並開放原始碼 R 資料集，並使用多種設定和策略（包括兩種 R 風格的差異，Tidy-verse 和 Base R）來評估 Code-PLM 的程式碼摘要和方法名稱預測這兩個任務。我們的結果表明，在處理 R 程式語言程式碼時，所研究的模型經歷了不同程度的效能下降，這得到了人工評估的支援。此外，即使在多語言微調之後，並非所有模型在 R 特定任務中都顯示出效能提升。R 中的雙重語法範例顯著影響模型的效能，特別是在程式碼摘要任務中。此外，R 程式碼庫中固有的專案特定內容，在嘗試跨專案訓練時，會顯著影響效能。

##### **Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation**
2410.07779v1 by Sweta Agrawal, José G. C. de Souza, Ricardo Rei, António Farinhas, Gonçalo Faria, Patrick Fernandes, Nuno M Guerreiro, Andre Martins

Alignment with human preferences is an important step in developing accurate
and safe large language models. This is no exception in machine translation
(MT), where better handling of language nuances and context-specific variations
leads to improved quality. However, preference data based on human feedback can
be very expensive to obtain and curate at a large scale. Automatic metrics, on
the other hand, can induce preferences, but they might not match human
expectations perfectly. In this paper, we propose an approach that leverages
the best of both worlds. We first collect sentence-level quality assessments
from professional linguists on translations generated by multiple high-quality
MT systems and evaluate the ability of current automatic metrics to recover
these preferences. We then use this analysis to curate a new dataset, MT-Pref
(metric induced translation preference) dataset, which comprises 18k instances
covering 18 language directions, using texts sourced from multiple domains
post-2022. We show that aligning TOWER models on MT-Pref significantly improves
translation quality on WMT23 and FLORES benchmarks.

摘要：與人類偏好一致是開發準確且安全的巨量語言模型的重要步驟。機器翻譯 (MT) 也不例外，其中對語言差異細微差別和特定於脈絡的變化的更好處理會提升品質。然而，基於人類回饋的偏好資料在取得和整理上可能非常昂貴。另一方面，自動化指標可以誘發偏好，但它們可能無法完美符合人類的期望。在本文中，我們提出了一種結合兩全其美的做法。我們首先從專業語言學家收集由多個高品質 MT 系統產生的翻譯的句子層級品質評估，並評估當前自動化指標恢復這些偏好的能力。然後，我們使用此分析整理一個新的資料集 MT-Pref（指標誘發的翻譯偏好）資料集，其中包含 18k 個實例，涵蓋 18 種語言方向，使用來自 2022 年之後多個網域的文字。我們展示了在 MT-Pref 上調整 TOWER 模型會在 WMT23 和 FLORES 基準上顯著提升翻譯品質。

##### **Full-Rank No More: Low-Rank Weight Training for Modern Speech Recognition Models**
2410.07771v1 by Adriana Fernandez-Lopez, Shiwei Liu, Lu Yin, Stavros Petridis, Maja Pantic

This paper investigates the under-explored area of low-rank weight training
for large-scale Conformer-based speech recognition models from scratch. Our
study demonstrates the viability of this training paradigm for such models,
yielding several notable findings. Firstly, we discover that applying a
low-rank structure exclusively to the attention modules can unexpectedly
enhance performance, even with a significant rank reduction of 12%. In
contrast, feed-forward layers present greater challenges, as they begin to
exhibit performance degradation with a moderate 50% rank reduction.
Furthermore, we find that both initialization and layer-wise rank assignment
play critical roles in successful low-rank training. Specifically, employing
SVD initialization and linear layer-wise rank mapping significantly boosts the
efficacy of low-rank weight training. Building on these insights, we introduce
the Low-Rank Speech Model from Scratch (LR-SMS), an approach that achieves
performance parity with full-rank training while delivering substantial
reductions in parameters count (by at least 2x), and training time speedups (by
1.3x for ASR and 1.15x for AVSR).

摘要：本文探討了從頭開始訓練大型基於 Conformer 的語音識別模型的低秩權重訓練這個尚未充分探索的領域。我們的研究證明了這種訓練範例對此類模型的可行性，並產生了幾項值得注意的發現。首先，我們發現僅將低秩結構應用於注意力模組就能意外地提升效能，即使秩大幅降低了 12%。相比之下，前饋層會帶來更大的挑戰，因為它們會在秩適度降低 50% 的情況下開始表現出效能下降。此外，我們發現初始化和逐層秩指派在低秩訓練的成功中都扮演著關鍵角色。具體來說，採用 SVD 初始化和線性逐層秩對應會顯著提升低秩權重訓練的效能。基於這些見解，我們引入了從頭開始訓練的低秩語音模型 (LR-SMS)，這是一種方法，可以在參數數量大幅減少（至少 2 倍）和訓練時間加快（ASR 為 1.3 倍，AVSR 為 1.15 倍）的情況下，達到與全秩訓練相當的效能。

##### **Dialectical Behavior Therapy Approach to LLM Prompting**
2410.07768v1 by Oxana Vitman, Nika Amaglobeli, Paul Plachinda

Large language models demonstrated state-of-the-art results on various
reasoning tasks when applying the chain-of-thought (CoT) prompting technique.
CoT prompting guides the model into breaking tasks into a few intermediate
steps and provides step-by-step demonstrations. However, solving complex
reasoning tasks remains a challenge. In this paper, we propose a novel
prompting strategy inspired by Dialectical Behavioral Therapy (DBT). DBT, a
form of cognitive-behavioral therapy, aims to help individuals cope with stress
by developing a system of reasoning. We applied DBT's basic concepts of shaping
dialog to construct prompts and conducted experiments on different datasets and
LLMs with various numbers of parameters. Our results show that prompts crafted
with DBT techniques significantly improve results on smaller models, achieving
a 7% increase in accuracy on the StrategyQA, 4.8% on Aqua dataset using 8b
parameters model, and a 16.2% increase on the StrategyQA, 5.3% on GSM8K dataset
with 14b parameters model.

摘要：大型语言模型在应用思维链（CoT）提示技术时，在各种推理任务中展示了最先进的结果。CoT 提示引导模型将任务分解为几个中间步骤，并提供逐步演示。然而，解决复杂的推理任务仍然是一个挑战。在本文中，我们提出了一种受辩证行为疗法 (DBT) 启发的全新提示策略。DBT 是一种认知行为疗法，旨在通过建立推理系统来帮助个人应对压力。我们应用了 DBT 塑造对话的基本概念来构建提示，并在不同的数据集和具有不同数量参数的 LLM 上进行了实验。我们的结果表明，使用 DBT 技术精心设计的提示可以显着提高较小模型的结果，在 StrategyQA 上准确率提高了 7%，在使用 8b 参数模型的 Aqua 数据集上提高了 4.8%，在 StrategyQA 上提高了 16.2%，在使用 14b 参数模型的 GSM8K 数据集上提高了 5.3%。

##### **GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps**
2410.07765v1 by Muhammad Umair Nasir, Steven James, Julian Togelius

Large language models (LLMs) have recently demonstrated great success in
generating and understanding natural language. While they have also shown
potential beyond the domain of natural language, it remains an open question as
to what extent and in which way these LLMs can plan. We investigate their
planning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark
consisting of diverse 2D grid-based game maps. An LLM succeeds if it can
traverse through given objectives, with a minimum number of steps and a minimum
number of generation errors. We evaluate a number of LLMs on GTB and found that
GPT-4-Turbo achieved the highest score of 44.97% on GTB\_Score (GTBS), a
composite score that combines the three above criteria. Furthermore, we
preliminarily test large reasoning models, namely o1, which scores $67.84\%$ on
GTBS, indicating that the benchmark remains challenging for current models.
Code, data, and documentation are available at
https://github.com/umair-nasir14/Game-Traversal-Benchmark.

摘要：大型語言模型 (LLM) 最近在生成和理解自然語言方面取得了巨大的成功。儘管它們也展現了超越自然語言領域的潛力，但這些 LLM 能在多大程度上以及以何種方式進行規劃，仍然是一個開放性的問題。我們通過提出 GameTraversalBenchmark (GTB) 來探討它們的規劃能力，GTB 是由多種 2D 網格化遊戲地圖組成的基準。如果 LLM 能以最少的步驟數和最少的生成錯誤，穿過給定的目標，則表示它成功了。我們在 GTB 上評估了多個 LLM，發現 GPT-4-Turbo 在 GTB\_Score (GTBS) 上取得了 44.97% 的最高分，GTBS 是結合上述三個標準的綜合分數。此外，我們初步測試了大型推理模型，即 o1，它在 GTBS 上的得分為 67.84%，這表示該基準對於目前的模型來說仍然具有挑戰性。可以在 https://github.com/umair-nasir14/Game-Traversal-Benchmark 找到程式碼、資料和文件。

##### **HARIVO: Harnessing Text-to-Image Models for Video Generation**
2410.07763v1 by Mingi Kwon, Seoung Wug Oh, Yang Zhou, Difan Liu, Joon-Young Lee, Haoran Cai, Baqiao Liu, Feng Liu, Youngjung Uh

We present a method to create diffusion-based video models from pretrained
Text-to-Image (T2I) models. Recently, AnimateDiff proposed freezing the T2I
model while only training temporal layers. We advance this method by proposing
a unique architecture, incorporating a mapping network and frame-wise tokens,
tailored for video generation while maintaining the diversity and creativity of
the original T2I model. Key innovations include novel loss functions for
temporal smoothness and a mitigating gradient sampling technique, ensuring
realistic and temporally consistent video generation despite limited public
video data. We have successfully integrated video-specific inductive biases
into the architecture and loss functions. Our method, built on the frozen
StableDiffusion model, simplifies training processes and allows for seamless
integration with off-the-shelf models like ControlNet and DreamBooth. project
page: https://kwonminki.github.io/HARIVO

摘要：我們提出一個方法，可以從預訓練的文字轉圖像 (T2I) 模型中建立基於擴散的影片模型。最近，AnimateDiff 提議凍結 T2I 模型，同時僅訓練時間層。我們透過提出一個獨特的架構來推進此方法，結合一個對應網路和逐幀代幣，專門用於影片生成，同時維持原始 T2I 模型的多樣性和創造力。關鍵創新包括時間平滑度的新損失函數和緩解梯度取樣技術，確保逼真且時間一致的影片生成，儘管受限於公開影片資料。我們已成功將影片特定的歸納偏差整合到架構和損失函數中。我們的模型建立在凍結的 StableDiffusion 模型上，簡化了訓練流程，並允許與 ControlNet 和 DreamBooth 等現成模型無縫整合。專案頁面：https://kwonminki.github.io/HARIVO

##### **$\textit{Jump Your Steps}$: Optimizing Sampling Schedule of Discrete Diffusion Models**
2410.07761v1 by Yong-Hyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, Yuki Mitsufuji

Diffusion models have seen notable success in continuous domains, leading to
the development of discrete diffusion models (DDMs) for discrete variables.
Despite recent advances, DDMs face the challenge of slow sampling speeds. While
parallel sampling methods like $\tau$-leaping accelerate this process, they
introduce $\textit{Compounding Decoding Error}$ (CDE), where discrepancies
arise between the true distribution and the approximation from parallel token
generation, leading to degraded sample quality. In this work, we present
$\textit{Jump Your Steps}$ (JYS), a novel approach that optimizes the
allocation of discrete sampling timesteps by minimizing CDE without extra
computational cost. More precisely, we derive a practical upper bound on CDE
and propose an efficient algorithm for searching for the optimal sampling
schedule. Extensive experiments across image, music, and text generation show
that JYS significantly improves sampling quality, establishing it as a
versatile framework for enhancing DDM performance for fast sampling.

摘要：擴散模型在連續域中已獲得顯著的成功，進而促成了離散變數的離散擴散模型 (DDM) 的發展。儘管有近期的進展，DDM 面臨著取樣速度緩慢的挑戰。雖然像 $\tau$-躍遷等並行取樣方法加速了這個過程，但它們引入了$\textit{複合解碼誤差}$ (CDE)，其中真實分佈與並行符號產生的近似值之間出現差異，導致樣本品質下降。在這項工作中，我們提出了$\textit{跳躍你的步驟}$ (JYS)，這是一種新方法，透過最小化 CDE 來最佳化離散取樣時間步長的配置，而無需額外的運算成本。更精確地說，我們推導出 CDE 的實用上限，並提出了一種用於搜尋最佳取樣時程的有效演算法。跨越影像、音樂和文字生成的廣泛實驗顯示，JYS 大幅改善了取樣品質，確立了它作為一個通用的框架，用於增強 DDM 效能以進行快速取樣。

##### **Learning Low-Level Causal Relations using a Simulated Robotic Arm**
2410.07751v1 by Miroslav Cibula, Matthias Kerzel, Igor Farkaš

Causal learning allows humans to predict the effect of their actions on the
known environment and use this knowledge to plan the execution of more complex
actions. Such knowledge also captures the behaviour of the environment and can
be used for its analysis and the reasoning behind the behaviour. This type of
knowledge is also crucial in the design of intelligent robotic systems with
common sense. In this paper, we study causal relations by learning the forward
and inverse models based on data generated by a simulated robotic arm involved
in two sensorimotor tasks. As a next step, we investigate feature attribution
methods for the analysis of the forward model, which reveals the low-level
causal effects corresponding to individual features of the state vector related
to both the arm joints and the environment features. This type of analysis
provides solid ground for dimensionality reduction of the state
representations, as well as for the aggregation of knowledge towards the
explainability of causal effects at higher levels.

摘要：因果學習讓人們能夠預測其行為對已知環境的影響，並利用此知識來規劃執行更複雜的行為。此類知識也捕捉環境的行為，可供其分析和行為背後的推理使用。此類型的知識在具備常識的智慧型機器人系統設計中也至關重要。在本文中，我們透過學習基於模擬機器手臂參與兩項感測運動任務所產生資料的正向和反向模型來研究因果關係。作為下一步，我們調查正向模型分析的功能屬性方法，揭示與手臂關節和環境特徵相關的狀態向量的個別特徵對應的低階因果效應。此類型的分析為狀態表示的維度縮減提供了堅實的基礎，並為更高層級因果效應的可解釋性彙整知識。

##### **StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs**
2410.07745v1 by Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, Min Zhang

Despite having powerful reasoning and inference capabilities, Large Language
Models (LLMs) still need external tools to acquire real-time information
retrieval or domain-specific expertise to solve complex tasks, which is
referred to as tool learning. Existing tool learning methods primarily rely on
tuning with expert trajectories, focusing on token-sequence learning from a
linguistic perspective. However, there are several challenges: 1) imitating
static trajectories limits their ability to generalize to new tasks. 2) even
expert trajectories can be suboptimal, and better solution paths may exist. In
this work, we introduce StepTool, a novel step-grained reinforcement learning
framework to improve tool learning in LLMs. It consists of two components:
Step-grained Reward Shaping, which assigns rewards at each tool interaction
based on tool invocation success and its contribution to the task, and
Step-grained Optimization, which uses policy gradient methods to optimize the
model in a multi-step manner. Experimental results demonstrate that StepTool
significantly outperforms existing methods in multi-step, tool-based tasks,
providing a robust solution for complex task environments. Codes are available
at https://github.com/yuyq18/StepTool.

摘要：儘管大型語言模型 (LLM) 擁有強大的推理和推論能力，但仍需要外部工具來獲取即時資訊檢索或特定領域的專業知識，以解決複雜任務，這被稱為工具學習。現有的工具學習方法主要依賴於專家軌跡的調整，專注於從語言學的角度進行令牌序列學習。然而，有幾個挑戰：1) 模仿靜態軌跡限制了它們泛化到新任務的能力。2) 即使是專家軌跡也可能不是最佳的，並且可能存在更好的解決方案路徑。在這項工作中，我們介紹了 StepTool，這是一個新穎的步驟化強化學習框架，用於改進 LLM 中的工具學習。它包含兩個組成部分：步驟化獎勵調整，它根據工具調用成功及其對任務的貢獻在每次工具互動時分配獎勵，以及步驟化最佳化，它使用策略梯度方法以多步驟的方式最佳化模型。實驗結果表明，StepTool 在基於工具的多步驟任務中顯著優於現有方法，為複雜任務環境提供了一個穩健的解決方案。程式碼可在 https://github.com/yuyq18/StepTool 獲得。

##### **SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture**
2410.07739v1 by Jiayi Han, Liang Du, Hongwei Du, Xiangguo Zhou, Yiwen Wu, Weibo Zheng, Donghong Han

Although many efforts have been made, it is still a challenge to balance the
training budget, downstream performance, and the general capabilities of the
LLMs in many applications. Training the whole model for downstream tasks is
expensive, and could easily result in catastrophic forgetting. By introducing
parameter-efficient fine-tuning (PEFT), the training cost could be reduced, but
it still suffers from forgetting, and limits the learning on the downstream
tasks. To efficiently fine-tune the LLMs with less limitation to their
downstream performance while mitigating the forgetting of general capabilities,
we propose a novel mixture of expert (MoE) framework based on Soft LoRA and
Identity Mixture (SLIM), that allows dynamic routing between LoRA adapters and
skipping connection, enables the suppression of forgetting. We adopt
weight-yielding with sliding clustering for better out-of-domain distinguish to
enhance the routing. We also propose to convert the mixture of low-rank
adapters to the model merging formulation and introduce fast dynamic merging of
LoRA adapters to keep the general capabilities of the base model. Extensive
experiments demonstrate that the proposed SLIM is comparable to the
state-of-the-art PEFT approaches on the downstream tasks while achieving the
leading performance in mitigating catastrophic forgetting.

摘要：儘管已做出許多努力，但在許多應用中平衡 LLM 的訓練預算、下游效能和一般功能仍是一項挑戰。針對下游任務訓練整個模型很昂貴，而且很容易導致災難性遺忘。透過引入參數有效微調 (PEFT)，訓練成本可以降低，但它仍然會遺忘，並限制下游任務的學習。為了有效微調 LLM，同時減少對其下游效能的限制，並減輕一般功能的遺忘，我們提出一個基於 Soft LoRA 和身分混合 (SLIM) 的專家混合 (MoE) 框架，允許在 LoRA 適配器和跳接連接之間進行動態路由，並抑制遺忘。我們採用帶有滑動聚類的權重讓步，以獲得更好的網域外區分，以增強路由。我們還建議將低階適配器的混合轉換為模型合併公式，並引入 LoRA 適配器的快速動態合併，以保持基礎模型的一般功能。廣泛的實驗證明，所提出的 SLIM 在下游任務上可與最先進的 PEFT 方法相媲美，同時在減輕災難性遺忘方面取得領先的效能。

##### **Enhancing Federated Domain Adaptation with Multi-Domain Prototype-Based Federated Fine-Tuning**
2410.07738v1 by Jingyuan Zhang, Yiyang Duan, Shuaicheng Niu, Yang Cao, Wei Yang Bryan Lim

Federated Domain Adaptation (FDA) is a Federated Learning (FL) scenario where
models are trained across multiple clients with unique data domains but a
shared category space, without transmitting private data. The primary challenge
in FDA is data heterogeneity, which causes significant divergences in gradient
updates when using conventional averaging-based aggregation methods, reducing
the efficacy of the global model. This further undermines both in-domain and
out-of-domain performance (within the same federated system but outside the
local client). To address this, we propose a novel framework called
\textbf{M}ulti-domain \textbf{P}rototype-based \textbf{F}ederated
Fine-\textbf{T}uning (MPFT). MPFT fine-tunes a pre-trained model using
multi-domain prototypes, i.e., pretrained representations enriched with
domain-specific information from category-specific local data. This enables
supervised learning on the server to derive a globally optimized adapter that
is subsequently distributed to local clients, without the intrusion of data
privacy. Empirical results show that MPFT significantly improves both in-domain
and out-of-domain accuracy over conventional methods, enhancing knowledge
preservation and adaptation in FDA. Notably, MPFT achieves convergence within a
single communication round, greatly reducing computation and communication
costs. To ensure privacy, MPFT applies differential privacy to protect the
prototypes. Additionally, we develop a prototype-based feature space hijacking
attack to evaluate robustness, confirming that raw data samples remain
unrecoverable even after extensive training epochs. The complete implementation
of MPFL is available at \url{https://anonymous.4open.science/r/DomainFL/}.

摘要：<paragraph>聯邦域適應（FDA）是一種聯邦學習（FL）場景，其中
模型在具有唯一數據域但共享類別空間的多個客戶端上進行訓練，而不會傳輸私人數據。FDA 中的主要挑戰是數據異質性，這會導致使用傳統基於平均的聚合方法時梯度更新出現顯著差異，從而降低全局模型的效能。這進一步損害了域內和域外效能（在同一個聯邦系統中，但在本地客戶端之外）。為了解決這個問題，我們提出了一個名為
\textbf{M}ulti-domain \textbf{P}rototype-based \textbf{F}ederated
Fine-\textbf{T}uning (MPFT) 的新框架。MPFT 使用多域原型微調預訓練模型，即預訓練表示，其中包含來自類別特定本地數據的特定於域的信息。這使得伺服器上的監督學習能夠推導出一個全局最佳化的適配器，隨後將其分發到本地客戶端，而不會侵犯數據隱私。經驗結果表明，MPFT 在域內和域外準確度方面都顯著優於傳統方法，增強了 FDA 中的知識保留和適應。值得注意的是，MPFT 在單次通信回合內實現收斂，大大降低了計算和通信成本。為了確保隱私，MPFT 應用差分隱私來保護原型。此外，我們開發了一個基於原型的特徵空間劫持攻擊來評估魯棒性，確認即使經過大量的訓練時期，原始數據樣本仍然無法恢復。MPFL 的完整實作可在 \url{https://anonymous.4open.science/r/DomainFL/} 中取得。</paragraph>

##### **On the Generalization Properties of Deep Learning for Aircraft Fuel Flow Estimation Models**
2410.07717v1 by Gabriel Jarry, Ramon Dalmau, Philippe Very, Junzi Sun

Accurately estimating aircraft fuel flow is essential for evaluating new
procedures, designing next-generation aircraft, and monitoring the
environmental impact of current aviation practices. This paper investigates the
generalization capabilities of deep learning models in predicting fuel
consumption, focusing particularly on their performance for aircraft types
absent from the training data. We propose a novel methodology that integrates
neural network architectures with domain generalization techniques to enhance
robustness and reliability across a wide range of aircraft. A comprehensive
dataset containing 101 different aircraft types, separated into training and
generalization sets, with each aircraft type set containing 1,000 flights. We
employed the base of aircraft data (BADA) model for fuel flow estimates,
introduced a pseudo-distance metric to assess aircraft type similarity, and
explored various sampling strategies to optimize model performance in
data-sparse regions. Our results reveal that for previously unseen aircraft
types, the introduction of noise into aircraft and engine parameters improved
model generalization. The model is able to generalize with acceptable mean
absolute percentage error between 2\% and 10\% for aircraft close to existing
aircraft, while performance is below 1\% error for known aircraft in the
training set. This study highlights the potential of combining domain-specific
insights with advanced machine learning techniques to develop scalable,
accurate, and generalizable fuel flow estimation models.

摘要：準確估計飛機燃料流量對於評估新程序、設計新世代飛機以及監控現行航空實務的環境影響至關重要。本文探討深度學習模型在預測燃料消耗方面的泛化能力，特別關注它們在訓練資料中沒有的飛機類型的效能。我們提出一個創新的方法論，將神經網路架構與領域泛化技術整合，以增強各種飛機的穩健性和可靠性。一個包含 101 種不同飛機類型的綜合資料集，分為訓練集和泛化集，每個飛機類型集包含 1,000 個航班。我們採用飛機資料庫 (BADA) 模型來估計燃料流量，引進一個偽距離量度來評估飛機類型的相似性，並探索各種取樣策略，以最佳化資料稀疏區域的模型效能。我們的結果顯示，對於以前未見的飛機類型，在飛機和引擎參數中引入雜訊可以改善模型泛化。對於接近現有飛機的飛機，該模型能夠以 2% 到 10% 之間的可接受平均絕對百分比誤差進行泛化，而對於訓練集中已知的飛機，效能低於 1% 的誤差。這項研究強調了結合特定領域見解與進階機器學習技術以開發可擴充、準確且可泛化的燃料流量估計模型的潛力。

##### **Learning Tree Pattern Transformations**
2410.07708v1 by Daniel Neider, Leif Sabellek, Johannes Schmidt, Fabian Vehlken, Thomas Zeume

Explaining why and how a tree $t$ structurally differs from another tree
$t^*$ is a question that is encountered throughout computer science, including
in understanding tree-structured data such as XML or JSON data. In this
article, we explore how to learn explanations for structural differences
between pairs of trees from sample data: suppose we are given a set $\{(t_1,
t_1^*),\dots, (t_n, t_n^*)\}$ of pairs of labelled, ordered trees; is there a
small set of rules that explains the structural differences between all pairs
$(t_i, t_i^*)$? This raises two research questions: (i) what is a good notion
of "rule" in this context?; and (ii) how can sets of rules explaining a data
set be learnt algorithmically?
  We explore these questions from the perspective of database theory by (1)
introducing a pattern-based specification language for tree transformations;
(2) exploring the computational complexity of variants of the above algorithmic
problem, e.g. showing NP-hardness for very restricted variants; and (3)
discussing how to solve the problem for data from CS education research using
SAT solvers.

摘要：<paragraph>解釋樹 $t$ 在結構上與另一棵樹 $t^*$ 有何不同，以及原因為何，是電腦科學領域中經常遇到的問題，包括在理解樹狀結構資料（例如 XML 或 JSON 資料）時。在本文中，我們探討如何從範例資料中學習樹對之間結構差異的解釋：假設我們給定一組標籤、有序樹對 $\{(t_1, t_1^*),\dots, (t_n, t_n^*)\}$；是否存在一組小規則，可以解釋所有對 $(t_i, t_i^*)$ 之間的結構差異？這提出了兩個研究問題：(i) 在這個脈絡中，「規則」的良好概念是什麼？(ii) 如何演算法學習解釋資料集的規則集？
我們從資料庫理論的角度探討這些問題，方法是：(1) 介紹樹狀轉換的基於模式的規範語言；(2) 探討上述演算法問題變體的計算複雜度，例如顯示非常受限變體的 NP 難度；以及 (3) 討論如何使用 SAT 求解器解決來自電腦科學教育研究的資料問題。</paragraph>

##### **AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories**
2410.07706v1 by Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, Sujian Li

Fine-tuning on agent-environment interaction trajectory data holds
significant promise for surfacing generalized agent capabilities in open-source
large language models (LLMs). In this work, we introduce AgentBank, by far the
largest trajectory tuning data collection featuring more than 50k diverse
high-quality interaction trajectories which comprises 16 tasks covering five
distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are
able to scale the annotated trajectories and generate a trajectory dataset with
minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a
series of agent models, Samoyed. Our comparative experiments demonstrate the
effectiveness of scaling the interaction trajectory data to acquire generalized
agent capabilities. Additional studies also reveal some key observations
regarding trajectory tuning and agent skill generalization.

摘要：在代理環境互動軌跡資料上進行微調，對於在開源大型語言模型 (LLM) 中浮現廣義的代理功能具有顯著的希望。在這個工作中，我們介紹了 AgentBank，目前為止最大的軌跡調校資料收集，具有超過 50k 個多樣化的優質互動軌跡，其中包含涵蓋五個不同代理技能維度的 16 個任務。利用新穎的註釋管道，我們能夠擴展註釋的軌跡並生成具有最小化難度偏差的軌跡資料集。此外，我們在 AgentBank 上微調 LLM 以獲得一系列代理模型，Samoyed。我們的比較實驗證明了擴展互動軌跡資料以獲取廣義代理功能的有效性。其他研究也揭示了一些關於軌跡調校和代理技能概括化的關鍵觀察。

##### **Multi-Facet Counterfactual Learning for Content Quality Evaluation**
2410.07693v1 by Jiasheng Zheng, Hongyu Lin, Boxi Cao, Meng Liao, Yaojie Lu, Xianpei Han, Le Sun

Evaluating the quality of documents is essential for filtering valuable
content from the current massive amount of information. Conventional approaches
typically rely on a single score as a supervision signal for training content
quality evaluators, which is inadequate to differentiate documents with quality
variations across multiple facets. In this paper, we propose Multi-facet
cOunterfactual LEarning (MOLE), a framework for efficiently constructing
evaluators that perceive multiple facets of content quality evaluation. Given a
specific scenario, we prompt large language models to generate counterfactual
content that exhibits variations in critical quality facets compared to the
original document. Furthermore, we leverage a joint training strategy based on
contrastive learning and supervised learning to enable the evaluator to
distinguish between different quality facets, resulting in more accurate
predictions of content quality scores. Experimental results on 2 datasets
across different scenarios demonstrate that our proposed MOLE framework
effectively improves the correlation of document content quality evaluations
with human judgments, which serve as a valuable toolkit for effective
information acquisition.

摘要：評估文件品質對於從當前大量的資訊中過濾出有價值的內容至關重要。傳統方法通常依賴單一評分作為訓練內容品質評估員的監督訊號，這不足以區分在多個面向具有品質差異的文件。在本文中，我們提出多面向反事實學習 (MOLE)，一個用於有效建構評估員的架構，該評估員能感知內容品質評估的多個面向。針對特定情境，我們提示大型語言模型產生反事實內容，與原始文件相比，這些內容在關鍵品質面向中表現出差異。此外，我們利用基於對比學習和監督學習的聯合訓練策略，讓評估員能夠區分不同的品質面向，進而更準確地預測內容品質評分。在不同情境中的 2 個資料集上的實驗結果表明，我們提出的 MOLE 框架有效地改善了文件內容品質評估與人類判斷之間的相關性，這是一個用於有效資訊獲取的有價值工具包。

##### **Smart Audit System Empowered by LLM**
2410.07677v1 by Xu Yao, Xiaoxu Wu, Xi Li, Huan Xu, Chenlei Li, Ping Huang, Si Li, Xiaoning Ma, Jiulong Shan

Manufacturing quality audits are pivotal for ensuring high product standards
in mass production environments. Traditional auditing processes, however, are
labor-intensive and reliant on human expertise, posing challenges in
maintaining transparency, accountability, and continuous improvement across
complex global supply chains. To address these challenges, we propose a smart
audit system empowered by large language models (LLMs). Our approach introduces
three innovations: a dynamic risk assessment model that streamlines audit
procedures and optimizes resource allocation; a manufacturing compliance
copilot that enhances data processing, retrieval, and evaluation for a
self-evolving manufacturing knowledge base; and a Re-act framework commonality
analysis agent that provides real-time, customized analysis to empower
engineers with insights for supplier improvement. These enhancements elevate
audit efficiency and effectiveness, with testing scenarios demonstrating an
improvement of over 24%.

摘要：製造品質稽核對於確保大量生產環境中的高產品標準至關重要。然而，傳統的稽核程序需要大量人力，且依賴於人為專業知識，在維持透明度、責任制和跨複雜全球供應鏈的持續改進方面構成挑戰。為了應對這些挑戰，我們提出一個由大型語言模型 (LLM) 賦能的智慧稽核系統。我們的做法引進了三項創新：簡化稽核程序並最佳化資源配置的動態風險評估模型；增強資料處理、擷取和評估以建立自演化製造知識庫的製造合規輔助系統；以及提供即時、客製化分析以賦予工程師供應商改善見解的 Re-act 架構共性分析代理。這些強化提升了稽核效率和效能，測試情境顯示改善幅度超過 24%。

##### **Adversarial Robustness Overestimation and Instability in TRADES**
2410.07675v1 by Jonathan Weiping Li, Ren-Wei Liang, Cheng-Han Yeh, Cheng-Chang Tsai, Kuanchun Yu, Chun-Shien Lu, Shang-Tse Chen

This paper examines the phenomenon of probabilistic robustness overestimation
in TRADES, a prominent adversarial training method. Our study reveals that
TRADES sometimes yields disproportionately high PGD validation accuracy
compared to the AutoAttack testing accuracy in the multiclass classification
task. This discrepancy highlights a significant overestimation of robustness
for these instances, potentially linked to gradient masking. We further analyze
the parameters contributing to unstable models that lead to overestimation. Our
findings indicate that smaller batch sizes, lower beta values (which control
the weight of the robust loss term in TRADES), larger learning rates, and
higher class complexity (e.g., CIFAR-100 versus CIFAR-10) are associated with
an increased likelihood of robustness overestimation. By examining metrics such
as the First-Order Stationary Condition (FOSC), inner-maximization, and
gradient information, we identify the underlying cause of this phenomenon as
gradient masking and provide insights into it. Furthermore, our experiments
show that certain unstable training instances may return to a state without
robust overestimation, inspiring our attempts at a solution. In addition to
adjusting parameter settings to reduce instability or retraining when
overestimation occurs, we recommend incorporating Gaussian noise in inputs when
the FOSC score exceed the threshold. This method aims to mitigate robustness
overestimation of TRADES and other similar methods at its source, ensuring more
reliable representation of adversarial robustness during evaluation.

摘要：這篇論文探討了 TRADES 中機率穩健性高估的現象，TRADES 是一種著名的對抗性訓練方法。我們的研究顯示，在多類別分類任務中，與 AutoAttack 測試準確度相比，TRADES 有時會產生不成比例的高 PGD 驗證準確度。這種差異突顯了對這些實例的穩健性高估，可能與梯度遮蔽有關。我們進一步分析了導致高估的不穩定模型的參數。我們的發現表明，較小的批次大小、較低的 beta 值（控制 TRADES 中穩健損失項的權重）、較高的學習率和較高的類別複雜度（例如，CIFAR-100 與 CIFAR-10）與穩健性高估的可能性增加有關。通過檢查一階平穩條件 (FOSC)、內部最大化和梯度資訊等指標，我們將這種現象的根本原因確定為梯度遮蔽，並對其提供了見解。此外，我們的實驗表明，某些不穩定的訓練實例可能會返回到沒有穩健高估的狀態，這激發了我們嘗試解決方案。除了調整參數設定以減少不穩定性或在發生高估時重新訓練之外，我們建議在 FOSC 分數超過閾值時在輸入中加入高斯噪聲。這種方法旨在從源頭減輕 TRADES 和其他類似方法的穩健性高估，確保在評估過程中更可靠地表示對抗性穩健性。

##### **Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference**
2410.07673v1 by Jianxing Yu, Shiqi Wang, Han Yin, Zhenlong Sun, Ruobing Xie, Bo Zhang, Yanghui Rao

This paper focuses on detecting clickbait posts on the Web. These posts often
use eye-catching disinformation in mixed modalities to mislead users to click
for profit. That affects the user experience and thus would be blocked by
content provider. To escape detection, malicious creators use tricks to add
some irrelevant non-bait content into bait posts, dressing them up as legal to
fool the detector. This content often has biased relations with non-bait
labels, yet traditional detectors tend to make predictions based on simple
co-occurrence rather than grasping inherent factors that lead to malicious
behavior. This spurious bias would easily cause misjudgments. To address this
problem, we propose a new debiased method based on causal inference. We first
employ a set of features in multiple modalities to characterize the posts.
Considering these features are often mixed up with unknown biases, we then
disentangle three kinds of latent factors from them, including the invariant
factor that indicates intrinsic bait intention; the causal factor which
reflects deceptive patterns in a certain scenario, and non-causal noise. By
eliminating the noise that causes bias, we can use invariant and causal factors
to build a robust model with good generalization ability. Experiments on three
popular datasets show the effectiveness of our approach.

摘要：本論文專注於偵測網路上的聳動標題貼文。這些貼文通常使用引人注目的錯誤資訊，並以混合模式誤導使用者點擊以獲利。這會影響使用者體驗，因此會被內容提供者封鎖。為了逃避偵測，惡意創作者會使用技巧將一些無關的非誘餌內容加入誘餌貼文中，將它們偽裝成合法的貼文以愚弄偵測器。這些內容通常與非誘餌標籤有偏誤的關聯，但傳統的偵測器傾向於根據簡單的共現，而不是掌握導致惡意行為的內在因素來做出預測。這種虛假的偏誤很容易造成錯誤判斷。為了解決這個問題，我們提出了一種新的基於因果推論的去偏方法。我們首先使用多種模式中的一組特徵來描述貼文。考慮到這些特徵通常與未知的偏誤混在一起，我們接著從中解開三種類型的潛在因素，包括表示內在誘餌意圖的不變因素；在特定場景中反映欺騙模式的因果因素，以及非因果噪音。透過消除造成偏誤的噪音，我們可以使用不變和因果因素來建立一個具有良好泛化能力的強健模型。在三個熱門資料集上的實驗顯示了我們方法的有效性。

##### **MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization**
2410.07672v1 by Yougang Lyu, Lingyong Yan, Zihan Wang, Dawei Yin, Pengjie Ren, Maarten de Rijke, Zhaochun Ren

As large language models (LLMs) are rapidly advancing and achieving
near-human capabilities, aligning them with human values is becoming more
urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong
alignment problem where we need to effectively align strong student LLMs
through weak supervision generated by weak teachers. Existing alignment methods
mainly focus on strong-to-weak alignment and self-alignment settings, and it is
impractical to adapt them to the much harder weak-to-strong alignment setting.
To fill this gap, we propose a multi-agent contrastive preference optimization
(MACPO) framework. MACPO facilitates weak teachers and strong students to learn
from each other by iteratively reinforcing unfamiliar positive behaviors while
penalizing familiar negative ones. To get this, we devise a mutual positive
behavior augmentation strategy to encourage weak teachers and strong students
to learn from each other's positive behavior and further provide higher quality
positive behavior for the next iteration. Additionally, we propose a hard
negative behavior construction strategy to induce weak teachers and strong
students to generate familiar negative behavior by fine-tuning on negative
behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets,
evaluated using both automatic metrics and human judgments, demonstrate that
MACPO simultaneously improves the alignment performance of strong students and
weak teachers. Moreover, as the number of weak teachers increases, MACPO
achieves better weak-to-strong alignment performance through more iteration
optimization rounds.

摘要：隨著大型語言模型 (LLM) 快速進步並達到接近人類的能力，使其與人類價值觀保持一致變得更加迫切。在 LLM 優於人類的場景中，我們面臨一個弱對強的一致性問題，我們需要通過弱教師產生的弱監督來有效地調整強學生 LLM。現有的一致性方法主要關注強對弱的一致性和自對齊設定，而且將它們適應到更困難的弱對強的一致性設定是不切實際的。為了填補這一空白，我們提出了一個多主體對比偏好最佳化 (MACPO) 框架。MACPO 促進弱教師和強學生通過反覆加強不熟悉的正面行為並懲罰熟悉的負面行為來相互學習。為了實現這一目標，我們設計了一個相互的正面行為增強策略來鼓勵弱教師和強學生相互學習彼此的正面行為，並進一步為下一次迭代提供更高品質的正面行為。此外，我們提出了一個硬負面行為構建策略，以誘導弱教師和強學生通過對負面行為數據進行微調來產生熟悉的負面行為。在 HH-RLHF 和 PKU-SafeRLHF 數據集上的實驗結果，使用自動指標和人類判斷進行評估，表明 MACPO 同時提高了強學生和弱教師的一致性表現。此外，隨著弱教師的數量增加，MACPO 通過更多的迭代最佳化回合實現了更好的弱對強的一致性表現。

##### **DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for Interpretable Job Recommendation**
2410.07671v1 by Xiaoshan Yu, Chuan Qin, Qi Zhang, Chen Zhu, Haiping Ma, Xingyi Zhang, Hengshu Zhu

The rapid development of online recruitment platforms has created
unprecedented opportunities for job seekers while concurrently posing the
significant challenge of quickly and accurately pinpointing positions that
align with their skills and preferences. Job recommendation systems have
significantly alleviated the extensive search burden for job seekers by
optimizing user engagement metrics, such as clicks and applications, thus
achieving notable success. In recent years, a substantial amount of research
has been devoted to developing effective job recommendation models, primarily
focusing on text-matching based and behavior modeling based methods. While
these approaches have realized impressive outcomes, it is imperative to note
that research on the explainability of recruitment recommendations remains
profoundly unexplored. To this end, in this paper, we propose DISCO, a
hierarchical Disentanglement based Cognitive diagnosis framework, aimed at
flexibly accommodating the underlying representation learning model for
effective and interpretable job recommendations. Specifically, we first design
a hierarchical representation disentangling module to explicitly mine the
hierarchical skill-related factors implied in hidden representations of job
seekers and jobs. Subsequently, we propose level-aware association modeling to
enhance information communication and robust representation learning both
inter- and intra-level, which consists of the interlevel knowledge influence
module and the level-wise contrastive learning. Finally, we devise an
interaction diagnosis module incorporating a neural diagnosis function for
effectively modeling the multi-level recruitment interaction process between
job seekers and jobs, which introduces the cognitive measurement theory.

摘要：線上招募平台的快速發展為求職者創造了前所未有的機會，同時也帶來了快速且準確找出與其技能和偏好相符職位的重大挑戰。求職推薦系統透過最佳化使用者參與度指標（例如點擊和應徵），大幅減輕求職者的廣泛搜尋負擔，因而獲得顯著的成功。近年來，大量研究致力於開發有效的求職推薦模型，主要著重於基於文字比對和基於行為建模的方法。儘管這些方法已實現令人印象深刻的成果，但必須注意的是，對於招募推薦的可解釋性研究仍未深入探討。為此，我們在本文中提出 DISCO，一個基於階層式解開的認知診斷架構，旨在靈活容納基礎表徵學習模型，以提供有效且可解釋的求職推薦。具體來說，我們首先設計一個階層式表徵解開模組，以明確找出隱含在求職者和職務的隱藏表徵中的階層式技能相關因素。隨後，我們提出層級感知關聯建模，以增強資訊傳遞和強健表徵學習，包括層級間知識影響模組和層級對比學習。最後，我們設計了一個互動診斷模組，結合神經診斷函數，以有效建模求職者與職務之間的多層次招募互動過程，並引入認知測量理論。

##### **StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models**
2410.07652v1 by Minchan Kwon, Gaeun Kim, Jongsuk Kim, Haeil Lee, Junmo Kim

Finding appropriate prompts for the specific task has become an important
issue as the usage of Large Language Models (LLM) has expanded. Reinforcement
Learning (RL) is widely used for prompt tuning, but its inherent instability
and environmental dependency make it difficult to use in practice. In this
paper, we propose StablePrompt, which strikes a balance between training
stability and search space, mitigating the instability of RL and producing
high-performance prompts. We formulate prompt tuning as an online RL problem
between the agent and target LLM and introduce Adaptive Proximal Policy
Optimization (APPO). APPO introduces an LLM anchor model to adaptively adjust
the rate of policy updates. This allows for flexible prompt search while
preserving the linguistic ability of the pre-trained LLM. StablePrompt
outperforms previous methods on various tasks including text classification,
question answering, and text generation. Our code can be found in github.

摘要：隨著大型語言模型 (LLM) 的使用擴大，找到特定任務的適當提示已成為一個重要議題。強化學習 (RL) 廣泛用於提示調整，但其內在的不穩定性與環境依賴性使其難以在實務中使用。在本文中，我們提出 StablePrompt，它在訓練穩定性與搜尋空間之間取得平衡，減輕 RL 的不穩定性並產生高性能提示。我們將提示調整制定為代理與目標 LLM 之間的線上 RL 問題，並引入適應性近端策略最佳化 (APPO)。APPO 引入 LLM 錨定模型來適應性調整策略更新的速率。這允許靈活的提示搜尋，同時保留預先訓練 LLM 的語言能力。StablePrompt 在各種任務上優於先前的各種方法，包括文字分類、問題解答和文字生成。我們的程式碼可以在 github 中找到。

##### **Almost Minimax Optimal Best Arm Identification in Piecewise Stationary Linear Bandits**
2410.07638v1 by Yunlong Hou, Vincent Y. F. Tan, Zixin Zhong

We propose a {\em novel} piecewise stationary linear bandit (PSLB) model,
where the environment randomly samples a context from an unknown probability
distribution at each changepoint, and the quality of an arm is measured by its
return averaged over all contexts. The contexts and their distribution, as well
as the changepoints are unknown to the agent. We design {\em
Piecewise-Stationary $\varepsilon$-Best Arm Identification$^+$}
(PS$\varepsilon$BAI$^+$), an algorithm that is guaranteed to identify an
$\varepsilon$-optimal arm with probability $\ge 1-\delta$ and with a minimal
number of samples. PS$\varepsilon$BAI$^+$ consists of two subroutines,
PS$\varepsilon$BAI and {\sc Na\"ive $\varepsilon$-BAI} (N$\varepsilon$BAI),
which are executed in parallel. PS$\varepsilon$BAI actively detects
changepoints and aligns contexts to facilitate the arm identification process.
When PS$\varepsilon$BAI and N$\varepsilon$BAI are utilized judiciously in
parallel, PS$\varepsilon$BAI$^+$ is shown to have a finite expected sample
complexity. By proving a lower bound, we show the expected sample complexity of
PS$\varepsilon$BAI$^+$ is optimal up to a logarithmic factor. We compare
PS$\varepsilon$BAI$^+$ to baseline algorithms using numerical experiments which
demonstrate its efficiency. Both our analytical and numerical results
corroborate that the efficacy of PS$\varepsilon$BAI$^+$ is due to the delicate
change detection and context alignment procedures embedded in
PS$\varepsilon$BAI.

摘要：<paragraph>我們提出一個{\em 新穎的}分段平穩線性賭徒機 (PSLB) 模型，
其中環境在每個變更點從未知機率分佈中隨機抽樣一個情境，而手臂的品質則由其在所有情境中平均回報來衡量。情境及其分佈以及變更點對代理人而言都是未知的。我們設計了{\em 分段平穩 $\varepsilon$-最佳手臂辨識$^+$}
(PS$\varepsilon$BAI$^+$)，這是一個演算法，保證能以機率 $\ge 1-\delta$ 且最少的取樣數目找出一個 $\varepsilon$-最優手臂。PS$\varepsilon$BAI$^+$ 包含兩個子常式，PS$\varepsilon$BAI 和 {\sc 樸素 $\varepsilon$-BAI} (N$\varepsilon$BAI)，它們並行執行。PS$\varepsilon$BAI 主動偵測變更點並調整情境以利於手臂辨識流程。當 PS$\varepsilon$BAI 和 N$\varepsilon$BAI 並行明智地使用時，PS$\varepsilon$BAI$^+$ 已顯示具有有限預期取樣複雜度。透過證明下界，我們顯示 PS$\varepsilon$BAI$^+$ 的預期取樣複雜度最佳，最多差一個對數因子。我們使用數值實驗將 PS$\varepsilon$BAI$^+$ 與基線演算法進行比較，證明其效率。我們的分析和數值結果都證實 PS$\varepsilon$BAI$^+$ 的效能歸因於 PS$\varepsilon$BAI 中嵌入的精細變更偵測和情境調整程序。</paragraph>

##### **Automatic Curriculum Expert Iteration for Reliable LLM Reasoning**
2410.07627v1 by Zirui Zhao, Hanze Dong, Amrita Saha, Caiming Xiong, Doyen Sahoo

Hallucinations (i.e., generating plausible but inaccurate content) and
laziness (i.e. excessive refusals or defaulting to "I don't know") persist as
major challenges in LLM reasoning. Current efforts to reduce hallucinations
primarily focus on factual errors in knowledge-grounded tasks, often neglecting
hallucinations related to faulty reasoning. Meanwhile, some approaches render
LLMs overly conservative, limiting their problem-solving capabilities. To
mitigate hallucination and laziness in reasoning tasks, we propose Automatic
Curriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align
responses to the model's capabilities--assertively answering within its limits
and declining when tasks exceed them. In our method, Expert Iteration explores
the reasoning trajectories near the LLM policy, guiding incorrect paths back on
track to reduce compounding errors and improve robustness; it also promotes
appropriate "I don't know" responses after sufficient reasoning attempts. The
curriculum automatically adjusts rewards, incentivizing extended reasoning
before acknowledging incapability, thereby pushing the limits of LLM reasoning
and aligning its behaviour with these limits. We compare Auto-CEI with various
SOTA baselines across logical reasoning, mathematics, and planning tasks, where
Auto-CEI achieves superior alignment by effectively balancing assertiveness and
conservativeness.

摘要：幻覺（例如產生似是而非但有誤的內容）和懶惰（例如過度拒絕或預設為「我不知道」）仍然是 LLM 推理中的主要挑戰。目前減少幻覺的努力主要集中於基於知識的任務中的事實錯誤，而經常忽略與錯誤推理相關的幻覺。同時，某些方法會讓 LLM 過於保守，限制其問題解決能力。為了減輕推理任務中的幻覺和懶惰，我們提出自動課程專家迭代 (Auto-CEI) 來增強 LLM 推理，並讓回應與模型的能力保持一致——在其限制範圍內自信地回答，並在任務超出其能力範圍時拒絕。在我們的模型中，專家迭代探索 LLM 策略附近的推理軌跡，引導不正確的路線回到正軌，以減少累積錯誤並提高穩健性；它還適當地在經過足夠的推理嘗試後促進「我不知道」的回應。課程會自動調整獎勵，在承認無能為力之前激勵延伸推理，從而擴展 LLM 推理的限制，並讓其行為與這些限制保持一致。我們在邏輯推理、數學和規劃任務中將 Auto-CEI 與各種 SOTA 基準進行比較，結果發現 Auto-CEI 透過有效平衡自信和保守態度，達到了卓越的一致性。

##### **Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation**
2410.07618v1 by Kaiyuan Liu, Jiahao Mei, Hengyu Zhang, Yihuai Zhang, Xingjiao Wu, Daoguo Dong, Liang He

Although Chinese calligraphy generation has achieved style transfer,
generating calligraphy by specifying the calligrapher, font, and character
style remains challenging. To address this, we propose a new Chinese
calligraphy generation model 'Moyun' , which replaces the Unet in the Diffusion
model with Vision Mamba and introduces the TripleLabel control mechanism to
achieve controllable calligraphy generation. The model was tested on our
large-scale dataset 'Mobao' of over 1.9 million images, and the results
demonstrate that 'Moyun' can effectively control the generation process and
produce calligraphy in the specified style. Even for calligraphy the
calligrapher has not written, 'Moyun' can generate calligraphy that matches the
style of the calligrapher.

摘要：儘管中文書法生成已達成風格轉移，但透過指定書法家、字型和字體風格來生成書法仍具挑戰性。為了解決這個問題，我們提出一個新的中文書法生成模型「墨雲」，它用 Vision Mamba 取代 Diffusion 模型中的 Unet，並引入 TripleLabel 控制機制來達成可控的書法生成。該模型已在我們超過 190 萬張圖片的大規模資料集「墨寶」中進行測試，結果表明「墨雲」可以有效地控制生成過程，並產生指定風格的書法。即使對於書法家未寫過的書法，「墨雲」也能生成與書法家風格相符的書法。

