{"2406.18193": {"publish_time": "2024-06-26", "title": "MammothModa: Multi-Modal Large Language Model", "paper_summary": "In this report, we introduce MammothModa, yet another multi-modal large\nlanguage model (MLLM) designed to achieve state-of-the-art performance starting\nfrom an elementary baseline. We focus on three key design insights: (i)\nIntegrating Visual Capabilities while Maintaining Complex Language\nUnderstanding: In addition to the vision encoder, we incorporated the Visual\nAttention Experts into the LLM to enhance its visual capabilities. (ii)\nExtending Context Window for High-Resolution and Long-Duration Visual Feature:\nWe explore the Visual Merger Module to effectively reduce the token number of\nhigh-resolution images and incorporated frame position ids to avoid position\ninterpolation. (iii) High-Quality Bilingual Datasets: We meticulously curated\nand filtered a high-quality bilingual multimodal dataset to reduce visual\nhallucinations. With above recipe we build MammothModa that consistently\noutperforms the state-of-the-art models, e.g., LLaVA-series, across main\nreal-world visual language benchmarks without bells and whistles.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 MammothModa\uff0c\u53e6\u4e00\u500b\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM)\uff0c\u65e8\u5728\u5f9e\u57fa\u790e\u57fa\u6e96\u958b\u59cb\u5be6\u73fe\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u4e09\u500b\u95dc\u9375\u8a2d\u8a08\u898b\u89e3\uff1a(i) \u6574\u5408\u8996\u89ba\u529f\u80fd\uff0c\u540c\u6642\u7dad\u6301\u8907\u96dc\u8a9e\u8a00\u7406\u89e3\uff1a\u9664\u4e86\u8996\u89ba\u7de8\u78bc\u5668\u4e4b\u5916\uff0c\u6211\u5011\u5c07\u8996\u89ba\u6ce8\u610f\u529b\u5c08\u5bb6\u7d0d\u5165 LLM\uff0c\u4ee5\u589e\u5f37\u5176\u8996\u89ba\u529f\u80fd\u3002(ii) \u64f4\u5c55\u5167\u5bb9\u8996\u7a97\u4ee5\u7372\u5f97\u9ad8\u89e3\u6790\u5ea6\u548c\u9577\u6642\u7a0b\u8996\u89ba\u7279\u5fb5\uff1a\u6211\u5011\u63a2\u8a0e\u8996\u89ba\u5408\u4f75\u6a21\u7d44\uff0c\u4ee5\u6709\u6548\u6e1b\u5c11\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u7684\u4ee3\u5e63\u6578\uff0c\u4e26\u7d0d\u5165\u5f71\u50cf\u4f4d\u7f6e ID\uff0c\u4ee5\u907f\u514d\u4f4d\u7f6e\u5167\u63d2\u3002(iii) \u9ad8\u54c1\u8cea\u96d9\u8a9e\u8cc7\u6599\u96c6\uff1a\u6211\u5011\u4ed4\u7d30\u7b56\u5283\u4e26\u904e\u6ffe\u9ad8\u54c1\u8cea\u96d9\u8a9e\u591a\u6a21\u614b\u8cc7\u6599\u96c6\uff0c\u4ee5\u6e1b\u5c11\u8996\u89ba\u5e7b\u89ba\u3002\u900f\u904e\u4e0a\u8ff0\u65b9\u6cd5\uff0c\u6211\u5011\u5efa\u69cb\u51fa MammothModa\uff0c\u5b83\u5728\u4e3b\u8981\u7684\u771f\u5be6\u4e16\u754c\u8996\u89ba\u8a9e\u8a00\u57fa\u6e96\u4e2d\uff0c\u59cb\u7d42\u512a\u65bc\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u4f8b\u5982 LLaVA \u7cfb\u5217\uff0c\u800c\u4e14\u6c92\u6709\u82b1\u4fcf\u7684\u6280\u5de7\u3002", "author": "Qi She et.al.", "authors": "Qi She, Junwen Pan, Xin Wan, Rui Zhang, Dawei Lu, Kai Huang", "id": "2406.18193v1", "paper_url": "http://arxiv.org/abs/2406.18193v1", "repo": "null"}}