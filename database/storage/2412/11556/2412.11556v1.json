{"2412.11556": {"publish_time": "2024-12-16", "title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs", "paper_summary": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.", "paper_summary_zh": "\u5f9e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u63d0\u53d6\u53e5\u5b50\u5d4c\u5165\u662f\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u5411\uff0c\u56e0\u70ba LLM \u5df2\u5c55\u73fe\u51fa\u66f4\u5f37\u5927\u7684\u8a9e\u7fa9\u7406\u89e3\u80fd\u529b\u3002\u5148\u524d\u7684\u7814\u7a76\u901a\u5e38\u5c08\u6ce8\u65bc\u63d0\u793a\u5de5\u7a0b\uff0c\u4ee5\u900f\u904e\u63d0\u793a\u6a21\u578b\u5c07\u53e5\u5b50\u8cc7\u8a0a\u7de8\u78bc\u5230\u6700\u5f8c\u4e00\u500b\u8a18\u865f\u7684\u5d4c\u5165\u4e2d\uff0c\u4f86\u5f9e LLM \u5f15\u51fa\u53e5\u5b50\u5d4c\u5165\u3002\u7136\u800c\uff0cLLM \u4e3b\u8981\u662f\u5177\u6709\u56e0\u679c\u6ce8\u610f\u529b\u7684\u50c5\u89e3\u78bc\u5668\u6a21\u578b\uff0c\u4e14\u53e5\u5b50\u4e2d\u8f03\u65e9\u7684\u8a18\u865f\u7121\u6cd5\u6ce8\u610f\u5f8c\u9762\u7684\u8a18\u865f\uff0c\u5c0e\u81f4\u53e5\u5b50\u8cc7\u8a0a\u7684\u7de8\u78bc\u6709\u504f\u5dee\uff0c\u4e26\u5c0d\u6700\u5f8c\u89e3\u78bc\u7684\u8a18\u865f\u7522\u751f\u9023\u9396\u6548\u61c9\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u8a18\u865f\u524d\u7f6e\uff08TP\uff09\u6280\u8853\uff0c\u5c07\u6bcf\u4e00\u5c64\u7684\u89e3\u78bc\u53e5\u5b50\u5d4c\u5165\u524d\u7f6e\u5230\u4e0b\u4e00\u5c64\u8f38\u5165\u53e5\u5b50\u7684\u958b\u982d\uff0c\u8b93\u8f03\u65e9\u7684\u8a18\u865f\u53ef\u4ee5\u5728\u56e0\u679c\u6ce8\u610f\u529b\u6a5f\u5236\u4e0b\u6ce8\u610f\u5b8c\u6574\u7684\u53e5\u5b50\u8cc7\u8a0a\u3002\u6240\u63d0\u51fa\u7684 TP \u6280\u8853\u662f\u4e00\u7a2e\u5373\u63d2\u5373\u7528\u4e14\u514d\u8a13\u7df4\u7684\u6280\u8853\uff0c\u9019\u8868\u793a\u5b83\u53ef\u4ee5\u7121\u7e2b\u5730\u6574\u5408\u5230\u5404\u7a2e\u57fa\u65bc\u63d0\u793a\u7684\u53e5\u5b50\u5d4c\u5165\u65b9\u6cd5\u548c\u81ea\u8ff4\u6b78 LLM \u4e2d\u3002\u5728\u5404\u7a2e\u8a9e\u7fa9\u6587\u5b57\u76f8\u4f3c\u5ea6\uff08STS\uff09\u4efb\u52d9\u548c\u4e0b\u6e38\u5206\u985e\u4efb\u52d9\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684 TP \u6280\u8853\u53ef\u4ee5\u986f\u8457\u6539\u5584\u73fe\u6709\u57fa\u65bc\u63d0\u793a\u7684\u53e5\u5b50\u5d4c\u5165\u65b9\u6cd5\u5728\u4e0d\u540c LLM \u4e2d\u7684\u6548\u80fd\uff0c\u540c\u6642\u7522\u751f\u53ef\u5ffd\u7565\u4e0d\u8a08\u7684\u984d\u5916\u63a8\u8ad6\u6210\u672c\u3002", "author": "Yuchen Fu et.al.", "authors": "Yuchen Fu, Zifeng Cheng, Zhiwei Jiang, Zhonghui Wang, Yafeng Yin, Zhengliang Li, Qing Gu", "id": "2412.11556v1", "paper_url": "http://arxiv.org/abs/2412.11556v1", "repo": "null"}}