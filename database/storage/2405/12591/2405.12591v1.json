{"2405.12591": {"publish_time": "2024-05-21", "title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression", "paper_summary": "Key-value~(KV) caching is an important technique to accelerate the inference\nof large language models~(LLMs), but incurs significant memory overhead. To\ncompress the size of KV cache, existing methods often compromise precision or\nrequire extra data for calibration, limiting their practicality in LLM\ndeployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free\nlow-bit quantization technique based on tensor decomposition methods, to\neffectively compress KV cache. Our core idea is to adjust the outlier\ndistribution of the original matrix by performing tensor decomposition, so that\nthe quantization difficulties are migrated from the matrix to decomposed local\ntensors. Specially, we find that outliers mainly concentrate on small local\ntensors, while large tensors tend to have a narrower value range. Based on this\nfinding, we propose to apply low-bit quantization to the large tensor, while\nmaintaining high-precision representation for the small tensor. Furthermore, we\nutilize the proposed quantization method to compress the KV cache of LLMs to\naccelerate the inference and develop an efficient dequantization kernel\ntailored specifically for DecoQuant. Through extensive experiments, DecoQuant\ndemonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\%\nreduction in memory footprint while maintaining comparable generation quality.", "paper_summary_zh": "\u95dc\u9375\u503c~(KV) \u5feb\u53d6\u662f\u4e00\u7a2e\u52a0\u901f\u5927\u578b\u8a9e\u8a00\u6a21\u578b~(LLM) \u63a8\u8ad6\u7684\u91cd\u8981\u6280\u8853\uff0c\u4f46\u6703\u9020\u6210\u986f\u8457\u7684\u8a18\u61b6\u9ad4\u958b\u92b7\u3002\u70ba\u4e86\u58d3\u7e2e KV \u5feb\u53d6\u7684\u5927\u5c0f\uff0c\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u6703\u6298\u8877\u7cbe\u5ea6\u6216\u9700\u8981\u984d\u5916\u7684\u6821\u6b63\u8cc7\u6599\uff0c\u9019\u6703\u9650\u5236\u5b83\u5011\u5728 LLM \u90e8\u7f72\u4e2d\u7684\u5be6\u7528\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 \\textbf{DecoQuant}\uff0c\u9019\u662f\u4e00\u7a2e\u57fa\u65bc\u5f35\u91cf\u5206\u89e3\u65b9\u6cd5\u7684\u65b0\u578b\u7121\u8cc7\u6599\u4f4e\u4f4d\u5143\u91cf\u5316\u6280\u8853\uff0c\u53ef\u6709\u6548\u58d3\u7e2e KV \u5feb\u53d6\u3002\u6211\u5011\u7684\u6838\u5fc3\u6982\u5ff5\u662f\u900f\u904e\u57f7\u884c\u5f35\u91cf\u5206\u89e3\u4f86\u8abf\u6574\u539f\u59cb\u77e9\u9663\u7684\u7570\u5e38\u503c\u5206\u4f48\uff0c\u4ee5\u4fbf\u5c07\u91cf\u5316\u96e3\u5ea6\u5f9e\u77e9\u9663\u8f49\u79fb\u5230\u5206\u89e3\u7684\u5c40\u90e8\u5f35\u91cf\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u767c\u73fe\u7570\u5e38\u503c\u4e3b\u8981\u96c6\u4e2d\u5728\u5c0f\u578b\u5c40\u90e8\u5f35\u91cf\u4e0a\uff0c\u800c\u5927\u578b\u5f35\u91cf\u5f80\u5f80\u5177\u6709\u8f03\u7a84\u7684\u503c\u57df\u3002\u6839\u64da\u9019\u500b\u767c\u73fe\uff0c\u6211\u5011\u5efa\u8b70\u5c07\u4f4e\u4f4d\u5143\u91cf\u5316\u61c9\u7528\u65bc\u5927\u578b\u5f35\u91cf\uff0c\u540c\u6642\u7dad\u6301\u5c0f\u578b\u5f35\u91cf\u7684\u7cbe\u78ba\u5ea6\u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528\u6240\u63d0\u51fa\u7684\u91cf\u5316\u65b9\u6cd5\u4f86\u58d3\u7e2e LLM \u7684 KV \u5feb\u53d6\uff0c\u4ee5\u52a0\u901f\u63a8\u8ad6\uff0c\u4e26\u958b\u767c\u4e86\u4e00\u500b\u5c08\u9580\u91dd\u5c0d DecoQuant \u7684\u9ad8\u6548\u53bb\u91cf\u5316\u6838\u5fc3\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0cDecoQuant \u5c55\u793a\u4e86\u986f\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u5c55\u793a\u51fa\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86\u7d04 $\\sim$75%\uff0c\u540c\u6642\u7dad\u6301\u4e86\u53ef\u6bd4\u8f03\u7684\u7522\u751f\u54c1\u8cea\u3002", "author": "Peiyu Liu et.al.", "authors": "Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Yipeng Ma, Tao Wang, Ji-Rong Wen", "id": "2405.12591v1", "paper_url": "http://arxiv.org/abs/2405.12591v1", "repo": "null"}}