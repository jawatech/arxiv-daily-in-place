{"2407.18581": {"publish_time": "2024-07-26", "title": "Dynamic Language Group-Based MoE: Enhancing Efficiency and Flexibility for Code-Switching Speech Recognition", "paper_summary": "The Mixture of Experts (MoE) approach is ideally suited for tackling\nmultilingual and code-switching (CS) challenges due to its multi-expert\narchitecture. This work introduces the DLG-MoE, which is optimized for\nbilingual and CS scenarios. Our novel Dynamic Language Group-based MoE layer\nfeatures a language router with shared weights for explicit language modeling,\nwhile independent unsupervised routers within the language group handle\nattributes beyond language. This structure not only enhances expert extension\ncapabilities but also supports dynamic top-k training, allowing for flexible\ninference across various top-k values and improving overall performance. The\nmodel requires no pre-training and supports streaming recognition, achieving\nstate-of-the-art (SOTA) results with unmatched flexibility compared to other\nmethods. The Code will be released.", "paper_summary_zh": "\u5c08\u5bb6\u6df7\u5408 (MoE) \u65b9\u6cd5\u7531\u65bc\u5176\u591a\u5c08\u5bb6\u67b6\u69cb\uff0c\u975e\u5e38\u9069\u5408\u89e3\u6c7a\u591a\u8a9e\u8a00\u548c\u8a9e\u78bc\u8f49\u63db (CS) \u7684\u6311\u6230\u3002\u9019\u9805\u5de5\u4f5c\u5f15\u5165\u4e86 DLG-MoE\uff0c\u5b83\u91dd\u5c0d\u96d9\u8a9e\u548c CS \u5834\u666f\u9032\u884c\u4e86\u6700\u4f73\u5316\u3002\u6211\u5011\u5275\u65b0\u7684\u57fa\u65bc\u52d5\u614b\u8a9e\u8a00\u7fa4\u7d44\u7684 MoE \u5c64\u5177\u6709\u8a9e\u8a00\u8def\u7531\u5668\uff0c\u5177\u6709\u7528\u65bc\u660e\u78ba\u8a9e\u8a00\u5efa\u6a21\u7684\u5171\u4eab\u6b0a\u91cd\uff0c\u800c\u8a9e\u8a00\u7fa4\u7d44\u5167\u7684\u7368\u7acb\u975e\u76e3\u7763\u8def\u7531\u5668\u8655\u7406\u8a9e\u8a00\u4e4b\u5916\u7684\u5c6c\u6027\u3002\u6b64\u7d50\u69cb\u4e0d\u50c5\u589e\u5f37\u4e86\u5c08\u5bb6\u64f4\u5145\u529f\u80fd\uff0c\u9084\u652f\u63f4\u52d5\u614b top-k \u8a13\u7df4\uff0c\u5141\u8a31\u8de8\u5404\u7a2e top-k \u503c\u9032\u884c\u9748\u6d3b\u63a8\u8ad6\u4e26\u6539\u5584\u6574\u9ad4\u6548\u80fd\u3002\u8a72\u6a21\u578b\u4e0d\u9700\u8981\u9810\u8a13\u7df4\u4e26\u652f\u63f4\u4e32\u6d41\u8fa8\u8b58\uff0c\u8207\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5be6\u73fe\u4e86\u6700\u5148\u9032 (SOTA) \u7684\u7d50\u679c\uff0c\u4e26\u5177\u6709\u7121\u8207\u502b\u6bd4\u7684\u9748\u6d3b\u6027\u3002\u7a0b\u5f0f\u78bc\u5c07\u6703\u91cb\u51fa\u3002", "author": "Hukai Huang et.al.", "authors": "Hukai Huang, Shenghui Lu, Yahui Shan, He Qu, Wenhao Guan, Qingyang Hong, Lin Li", "id": "2407.18581v1", "paper_url": "http://arxiv.org/abs/2407.18581v1", "repo": "null"}}