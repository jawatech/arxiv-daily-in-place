{"2406.02601": {"publish_time": "2024-06-02", "title": "Multimodal Deep Learning for Low-Resource Settings: A Vector Embedding Alignment Approach for Healthcare Applications", "paper_summary": "Large-scale multi-modal deep learning models have revolutionized domains such\nas healthcare, highlighting the importance of computational power. However, in\nresource-constrained regions like Low and Middle-Income Countries (LMICs),\nlimited access to GPUs and data poses significant challenges, often leaving\nCPUs as the sole resource. To address this, we advocate for leveraging vector\nembeddings to enable flexible and efficient computational methodologies,\ndemocratizing multimodal deep learning across diverse contexts.\n  Our paper investigates the efficiency and effectiveness of using vector\nembeddings from single-modal foundation models and multi-modal Vision-Language\nModels (VLMs) for multimodal deep learning in low-resource environments,\nparticularly in healthcare. Additionally, we propose a simple yet effective\ninference-time method to enhance performance by aligning image-text embeddings.\nComparing these approaches with traditional methods, we assess their impact on\ncomputational efficiency and model performance using metrics like accuracy,\nF1-score, inference time, training time, and memory usage across three medical\nmodalities: BRSET (ophthalmology), HAM10000 (dermatology), and SatelliteBench\n(public health).\n  Our findings show that embeddings reduce computational demands without\ncompromising model performance. Furthermore, our alignment method improves\nperformance in medical tasks. This research promotes sustainable AI practices\nby optimizing resources in constrained environments, highlighting the potential\nof embedding-based approaches for efficient multimodal learning. Vector\nembeddings democratize multimodal deep learning in LMICs, particularly in\nhealthcare, enhancing AI adaptability in varied use cases.", "paper_summary_zh": "<paragraph>\u5927\u578b\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u533b\u7597\u4fdd\u5065\u7b49\u9886\u57df\uff0c\u7a81\u51fa\u4e86\u8ba1\u7b97\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\u7136\u800c\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5730\u533a\uff0c\u5982\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6 (LMIC)\uff0c\u53d7\u9650\u7684 GPU \u548c\u6570\u636e\u8bbf\u95ee\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u901a\u5e38\u53ea\u5269\u4e0b CPU \u4f5c\u4e3a\u552f\u4e00\u8d44\u6e90\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u5021\u5229\u7528\u5411\u91cf\u5d4c\u5165\u6765\u5b9e\u73b0\u7075\u6d3b\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8ba9\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u6c11\u4e3b\u5316\u3002\n\u6211\u4eec\u7684\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u6765\u81ea\u5355\u6a21\u6001\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u5411\u91cf\u5d4c\u5165\u8fdb\u884c\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u63a8\u7406\u65f6\u95f4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u56fe\u50cf\u6587\u672c\u5d4c\u5165\u6765\u589e\u5f3a\u6027\u80fd\u3002\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u4e0e\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5b83\u4eec\u5bf9\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u51c6\u786e\u5ea6\u3001F1 \u5206\u6570\u3001\u63a8\u7406\u65f6\u95f4\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u7b49\u6307\u6807\uff0c\u8de8\u8d8a\u4e09\u4e2a\u533b\u5b66\u6a21\u6001\uff1aBRSET\uff08\u773c\u79d1\uff09\u3001HAM10000\uff08\u76ae\u80a4\u75c5\u5b66\uff09\u548c SatelliteBench\uff08\u516c\u5171\u536b\u751f\uff09\u3002\n\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5d4c\u5165\u53ef\u4ee5\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5bf9\u9f50\u65b9\u6cd5\u63d0\u9ad8\u4e86\u533b\u5b66\u4efb\u52a1\u7684\u6027\u80fd\u3002\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8d44\u6e90\uff0c\u4fc3\u8fdb\u4e86\u53ef\u6301\u7eed\u7684 AI \u5b9e\u8df5\uff0c\u7a81\u51fa\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u5728\u9ad8\u6548\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002\u5411\u91cf\u5d4c\u5165\u8ba9 LMIC \u4e2d\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u6c11\u4e3b\u5316\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u589e\u5f3a\u4e86 AI \u5728\u5404\u79cd\u7528\u4f8b\u4e2d\u7684\u9002\u5e94\u6027\u3002</paragraph>", "author": "David Restrepo et.al.", "authors": "David Restrepo, Chenwei Wu, Sebasti\u00e1n Andr\u00e9s Cajas, Luis Filipe Nakayama, Leo Anthony Celi, Diego M L\u00f3pez", "id": "2406.02601v1", "paper_url": "http://arxiv.org/abs/2406.02601v1", "repo": "null"}}