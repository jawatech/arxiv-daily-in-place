{"2410.08876": {"publish_time": "2024-10-11", "title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models", "paper_summary": "Current vision-language models (VLMs) still exhibit inferior performance on\nknowledge-intensive tasks, primarily due to the challenge of accurately\nencoding all the associations between visual objects and scenes to their\ncorresponding entities and background knowledge. While retrieval augmentation\nmethods offer an efficient way to integrate external knowledge, extending them\nto vision-language domain presents unique challenges in (1) precisely\nretrieving relevant information from external sources due to the inherent\ndiscrepancy within the multimodal queries, and (2) being resilient to the\nirrelevant, extraneous and noisy information contained in the retrieved\nmultimodal knowledge snippets. In this work, we introduce RORA-VLM, a novel and\nrobust retrieval augmentation framework specifically tailored for VLMs, with\ntwo key innovations: (1) a 2-stage retrieval process with image-anchored\ntextual-query expansion to synergistically combine the visual and textual\ninformation in the query and retrieve the most relevant multimodal knowledge\nsnippets; and (2) a robust retrieval augmentation method that strengthens the\nresilience of VLMs against irrelevant information in the retrieved multimodal\nknowledge by injecting adversarial noises into the retrieval-augmented training\nprocess, and filters out extraneous visual information, such as unrelated\nentities presented in images, via a query-oriented visual token refinement\nstrategy. We conduct extensive experiments to validate the effectiveness and\nrobustness of our proposed methods on three widely adopted benchmark datasets.\nOur results demonstrate that with a minimal amount of training instance,\nRORA-VLM enables the base model to achieve significant performance improvement\nand constantly outperform state-of-the-art retrieval-augmented VLMs on all\nbenchmarks while also exhibiting a novel zero-shot domain transfer capability.", "paper_summary_zh": "<paragraph>\u76ee\u524d\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u4e2d\u4ecd\u8868\u73fe\u51fa\u8f03\u5dee\u7684\u6548\u80fd\uff0c\u9019\u4e3b\u8981\u662f\u56e0\u70ba\u6e96\u78ba\u7de8\u78bc\u8996\u89ba\u7269\u4ef6\u548c\u5834\u666f\u8207\u5176\u5c0d\u61c9\u5be6\u9ad4\u548c\u80cc\u666f\u77e5\u8b58\u4e4b\u9593\u7684\u6240\u6709\u95dc\u806f\u7684\u6311\u6230\u3002\u96d6\u7136\u6aa2\u7d22\u64f4\u5145\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6574\u5408\u5916\u90e8\u77e5\u8b58\u7684\u6709\u6548\u65b9\u5f0f\uff0c\u4f46\u5c07\u5b83\u5011\u64f4\u5c55\u5230\u8996\u89ba\u8a9e\u8a00\u9818\u57df\u6703\u5e36\u4f86\u7368\u7279\u7684\u6311\u6230\uff0c\u5305\u62ec\uff1a(1) \u7531\u65bc\u591a\u6a21\u614b\u67e5\u8a62\u4e2d\u56fa\u6709\u7684\u5dee\u7570\uff0c\u5f9e\u5916\u90e8\u4f86\u6e90\u6e96\u78ba\u6aa2\u7d22\u76f8\u95dc\u8cc7\u8a0a\uff0c\u4ee5\u53ca (2) \u5c0d\u6aa2\u7d22\u5230\u7684\u591a\u6a21\u614b\u77e5\u8b58\u7247\u6bb5\u4e2d\u5305\u542b\u7684\u4e0d\u76f8\u95dc\u3001\u7121\u95dc\u548c\u6709\u96dc\u8a0a\u7684\u8cc7\u8a0a\u5177\u6709\u5f48\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 RORA-VLM\uff0c\u9019\u662f\u4e00\u500b\u5c08\u9580\u70ba VLM \u91cf\u8eab\u6253\u9020\u7684\u65b0\u7a4e\u4e14\u5f37\u5927\u7684\u6aa2\u7d22\u64f4\u5145\u67b6\u69cb\uff0c\u5177\u6709\u5169\u9805\u95dc\u9375\u5275\u65b0\uff1a(1) \u4e00\u500b 2 \u968e\u6bb5\u6aa2\u7d22\u7a0b\u5e8f\uff0c\u5177\u6709\u5f71\u50cf\u9328\u5b9a\u7684\u6587\u5b57\u67e5\u8a62\u64f4\u5145\uff0c\u53ef\u4ee5\u5354\u540c\u7d50\u5408\u67e5\u8a62\u4e2d\u7684\u8996\u89ba\u548c\u6587\u5b57\u8cc7\u8a0a\uff0c\u4e26\u6aa2\u7d22\u6700\u76f8\u95dc\u7684\u591a\u6a21\u614b\u77e5\u8b58\u7247\u6bb5\uff1b\u4ee5\u53ca (2) \u4e00\u500b\u5f37\u5927\u7684\u6aa2\u7d22\u64f4\u5145\u65b9\u6cd5\uff0c\u900f\u904e\u5728\u6aa2\u7d22\u64f4\u5145\u8a13\u7df4\u904e\u7a0b\u4e2d\u6ce8\u5165\u5c0d\u6297\u6027\u96dc\u8a0a\uff0c\u5f37\u5316 VLM \u5c0d\u6aa2\u7d22\u5230\u7684\u591a\u6a21\u614b\u77e5\u8b58\u4e2d\u4e0d\u76f8\u95dc\u8cc7\u8a0a\u7684\u5f48\u6027\uff0c\u4e26\u900f\u904e\u4ee5\u67e5\u8a62\u70ba\u5c0e\u5411\u7684\u8996\u89ba\u6a19\u8a18\u7cbe\u7149\u7b56\u7565\uff0c\u904e\u6ffe\u6389\u7121\u95dc\u7684\u8996\u89ba\u8cc7\u8a0a\uff0c\u4f8b\u5982\u5f71\u50cf\u4e2d\u5448\u73fe\u7684\u4e0d\u76f8\u95dc\u5be6\u9ad4\u3002\u6211\u5011\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u9a57\u8b49\u6211\u5011\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u500b\u5ee3\u6cdb\u63a1\u7528\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u6709\u6548\u6027\u548c\u5f37\u5065\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0c\u900f\u904e\u6700\u5c11\u91cf\u7684\u8a13\u7df4\u5be6\u4f8b\uff0cRORA-VLM \u80fd\u8b93\u57fa\u790e\u6a21\u578b\u7372\u5f97\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u4e26\u5728\u6240\u6709\u57fa\u6e96\u4e0a\u6301\u7e8c\u512a\u65bc\u6700\u5148\u9032\u7684\u6aa2\u7d22\u64f4\u5145 VLM\uff0c\u540c\u6642\u4e5f\u5c55\u73fe\u51fa\u65b0\u7a4e\u7684\u96f6\u6b21\u5b78\u7fd2\u9818\u57df\u8f49\u79fb\u80fd\u529b\u3002</paragraph>", "author": "Jingyuan Qi et.al.", "authors": "Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, Jing Di, Yu Cheng, Qifan Wang, Lifu Huang", "id": "2410.08876v1", "paper_url": "http://arxiv.org/abs/2410.08876v1", "repo": "null"}}