{"2408.01402": {"publish_time": "2024-08-02", "title": "Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer", "paper_summary": "Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.", "paper_summary_zh": "\u6c7a\u7b56Transformer (DT) \u5df2\u6210\u70ba\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2 (RL) \u4efb\u52d9\u4e2d\u4e00\u7a2e\u6709\u524d\u9014\u7684\u6f14\u7b97\u6cd5\u985e\u5225\uff0c\u5b83\u5229\u7528\u9810\u5148\u6536\u96c6\u7684\u8cc7\u6599\u96c6\u548cTransformer\u5c0d\u9577\u5e8f\u5217\u5efa\u6a21\u7684\u80fd\u529b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c07\u8a13\u7df4\u4efb\u52d9\u4e2d\u90e8\u5206\u8ecc\u8de1\u7528\u4f5c DT \u4e2d\u7684\u63d0\u793a\uff0c\u53ef\u4ee5\u63d0\u5347\u5176\u5728\u672a\u898b\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\uff0c\u7531\u6b64\u7522\u751f\u4e86\u63d0\u793a DT \u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u8a31\u591a\u5834\u666f\u4e2d\uff0c\u5f9e\u7279\u5b9a\u74b0\u5883\u6536\u96c6\u8cc7\u6599\u65e2\u6602\u8cb4\u53c8\u4e0d\u5b89\u5168\uff0c\u9019\u5c0e\u81f4\u4e86\u6b21\u512a\u7684\u8868\u73fe\u548c\u6709\u9650\u7684\u5c11\u6b21\u63d0\u793a\u80fd\u529b\uff0c\u56e0\u70ba\u57fa\u65bcTransformer\u7684\u6a21\u578b\u5177\u6709\u8cc7\u6599\u5bc6\u96c6\u7684\u7279\u6027\u3002\u6b64\u5916\uff0c\u9810\u8a13\u7df4\u4e2d\u4f7f\u7528\u7684\u6709\u9650\u8cc7\u6599\u96c6\u4f7f\u5f97\u63d0\u793a DT \u985e\u578b\u7684\u6a21\u578b\u50c5\u900f\u904e\u63d0\u793a\u5c31\u96e3\u4ee5\u5340\u5206\u5404\u7a2e RL \u4efb\u52d9\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u57fa\u65bc\u8a9e\u8a00\u6a21\u578b\u7684\u63d0\u793a\u6c7a\u7b56Transformer (LPDT)\uff0c\u5b83\u5229\u7528\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u5143 RL \u4efb\u52d9\uff0c\u4e26\u4f7f\u7528\u4f4e\u79e9\u9069\u61c9 (LoRA) \u5fae\u8abf\u6a21\u578b\u3002\u6211\u5011\u9032\u4e00\u6b65\u6574\u5408\u63d0\u793a\u6b63\u898f\u5316\uff0c\u4ee5\u6839\u64da\u63d0\u793a\u7279\u5fb5\u8868\u793a\u6709\u6548\u5340\u5206\u4efb\u52d9\u3002\u6211\u5011\u7684\u505a\u6cd5\u7121\u7e2b\u6574\u5408\u4e86\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\u548c RL \u4efb\u52d9\u3002\u5ee3\u6cdb\u7684\u5be6\u8b49\u7814\u7a76\u8868\u660e\uff0c\u8207\u57fa\u7dda\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u521d\u59cb\u5316\u986f\u8457\u63d0\u5347\u4e86\u63d0\u793a DT \u5728\u672a\u898b\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002", "author": "Yu Yang et.al.", "authors": "Yu Yang, Pan Xu", "id": "2408.01402v1", "paper_url": "http://arxiv.org/abs/2408.01402v1", "repo": "null"}}