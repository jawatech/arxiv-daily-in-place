{"2501.17343": {"publish_time": "2025-01-28", "title": "Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines", "paper_summary": "Quantizing deep neural networks ,reducing the precision (bit-width) of their\ncomputations, can remarkably decrease memory usage and accelerate processing,\nmaking these models more suitable for large-scale medical imaging applications\nwith limited computational resources. However, many existing methods studied\n\"fake quantization\", which simulates lower precision operations during\ninference, but does not actually reduce model size or improve real-world\ninference speed. Moreover, the potential of deploying real 3D low-bit\nquantization on modern GPUs is still unexplored. In this study, we introduce a\nreal post-training quantization (PTQ) framework that successfully implements\ntrue 8-bit quantization on state-of-the-art (SOTA) 3D medical segmentation\nmodels, i.e., U-Net, SegResNet, SwinUNETR, nnU-Net, UNesT, TransUNet,\nST-UNet,and VISTA3D. Our approach involves two main steps. First, we use\nTensorRT to perform fake quantization for both weights and activations with\nunlabeled calibration dataset. Second, we convert this fake quantization into\nreal quantization via TensorRT engine on real GPUs, resulting in real-world\nreductions in model size and inference latency. Extensive experiments\ndemonstrate that our framework effectively performs 8-bit quantization on GPUs\nwithout sacrificing model performance. This advancement enables the deployment\nof efficient deep learning models in medical imaging applications where\ncomputational resources are constrained. The code and models have been\nreleased, including U-Net, TransUNet pretrained on the BTCV dataset for\nabdominal (13-label) segmentation, UNesT pretrained on the Whole Brain Dataset\nfor whole brain (133-label) segmentation, and nnU-Net, SegResNet, SwinUNETR and\nVISTA3D pretrained on TotalSegmentator V2 for full body (104-label)\nsegmentation. https://github.com/hrlblab/PTQ.", "paper_summary_zh": "<paragraph>\u91cf\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u964d\u4f4e\u5176\u8ba1\u7b97\u7684\u7cbe\u5ea6\uff08\u4f4d\u5bbd\uff09\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u91cf\u5e76\u52a0\u901f\u5904\u7406\uff0c\u4f7f\u8fd9\u4e9b\u6a21\u578b\u66f4\u9002\u5408\u4e8e\u5177\u6709\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u7684\u5927\u89c4\u6a21\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u3002\u7136\u800c\uff0c\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u7814\u7a76\u4e86\u201c\u4f2a\u91cf\u5316\u201d\uff0c\u5b83\u5728\u63a8\u7406\u671f\u95f4\u6a21\u62df\u8f83\u4f4e\u7cbe\u5ea6\u7684\u64cd\u4f5c\uff0c\u4f46\u5b9e\u9645\u4e0a\u5e76\u6ca1\u6709\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u6216\u63d0\u9ad8\u5b9e\u9645\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u5728\u73b0\u4ee3 GPU \u4e0a\u90e8\u7f72\u771f\u6b63\u7684 3D \u4f4e\u4f4d\u91cf\u5316\u7684\u6f5c\u529b\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u771f\u6b63\u7684\u8bad\u7ec3\u540e\u91cf\u5316 (PTQ) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6210\u529f\u5730\u5728\u6700\u5148\u8fdb\u7684 (SOTA) 3D \u533b\u5b66\u5206\u5272\u6a21\u578b\uff08\u5373 U-Net\u3001SegResNet\u3001SwinUNETR\u3001nnU-Net\u3001UNesT\u3001TransUNet\u3001ST-UNet \u548c VISTA3D\uff09\u4e0a\u5b9e\u73b0\u4e86\u771f\u6b63\u7684 8 \u4f4d\u91cf\u5316\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\u3002\u9996\u5148\uff0c\u6211\u4eec\u4f7f\u7528 TensorRT \u5bf9\u6743\u91cd\u548c\u6fc0\u6d3b\u8fdb\u884c\u4f2a\u91cf\u5316\uff0c\u5e76\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u6821\u51c6\u6570\u636e\u96c6\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u4f2a\u91cf\u5316\u901a\u8fc7\u771f\u5b9e GPU \u4e0a\u7684 TensorRT \u5f15\u64ce\u8f6c\u6362\u4e3a\u771f\u6b63\u7684\u91cf\u5316\uff0c\u4ece\u800c\u5728\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u5b9e\u73b0\u4e86\u5b9e\u9645\u7684\u51cf\u5c11\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728 GPU \u4e0a\u6709\u6548\u5730\u6267\u884c 8 \u4f4d\u91cf\u5316\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6a21\u578b\u6027\u80fd\u3002\u8fd9\u4e00\u8fdb\u6b65\u4f7f\u5f97\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u90e8\u7f72\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u7ecf\u53d1\u5e03\uff0c\u5305\u62ec U-Net\u3001TransUNET\uff0c\u5728 BTCV \u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7528\u4e8e\u8179\u90e8\uff0813 \u6807\u7b7e\uff09\u5206\u5272\uff0cUNesT \u5728 Whole Brain \u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7528\u4e8e\u5168\u8111\uff08133 \u6807\u7b7e\uff09\u5206\u5272\uff0c\u4ee5\u53ca nnU-Net\u3001SegResNet\u3001SwinUNETR \u548c VISTA3D \u5728 TotalSegmentator V2 \u4e0a\u9884\u8bad\u7ec3\u7528\u4e8e\u5168\u8eab\uff08104 \u6807\u7b7e\uff09\u5206\u5272\u3002https://github.com/hrlblab/PTQ\u3002</paragraph>", "author": "Chongyu Qu et.al.", "authors": "Chongyu Qu, Ritchie Zhao, Ye Yu, Bin Liu, Tianyuan Yao, Junchao Zhu, Bennett A. Landman, Yucheng Tang, Yuankai Huo", "id": "2501.17343v1", "paper_url": "http://arxiv.org/abs/2501.17343v1", "repo": "null"}}