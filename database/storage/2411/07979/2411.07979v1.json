{"2411.07979": {"publish_time": "2024-11-12", "title": "Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization", "paper_summary": "Second-order optimization has been shown to accelerate the training of deep\nneural networks in many applications, often yielding faster progress per\niteration on the training loss compared to first-order optimizers.However, the\ngeneralization properties of second-order methods are still being debated.\nTheoretical investigations have proved difficult to carry out outside the\ntractable settings of heavily simplified model classes -- thus, the relevance\nof existing theories to practical deep learning applications remains unclear.\nSimilarly, empirical studies in large-scale models and real datasets are\nsignificantly confounded by the necessity to approximate second-order updates\nin practice. It is often unclear whether the observed generalization behaviour\narises specifically from the second-order nature of the parameter updates, or\ninstead reflects the specific structured (e.g.\\ Kronecker) approximations used\nor any damping-based interpolation towards first-order updates. Here, we show\nfor the first time that exact Gauss-Newton (GN) updates take on a tractable\nform in a class of deep reversible architectures that are sufficiently\nexpressive to be meaningfully applied to common benchmark datasets. We exploit\nthis novel setting to study the training and generalization properties of the\nGN optimizer. We find that exact GN generalizes poorly. In the mini-batch\ntraining setting, this manifests as rapidly saturating progress even on the\n\\emph{training} loss, with parameter updates found to overfit each\nmini-batchatch without producing the features that would support generalization\nto other mini-batches. We show that our experiments run in the ``lazy'' regime,\nin which the neural tangent kernel (NTK) changes very little during the course\nof training. This behaviour is associated with having no significant changes in\nneural representations, explaining the lack of generalization.", "paper_summary_zh": "<paragraph>\u4e8c\u6b21\u512a\u5316\u5df2\u88ab\u8b49\u660e\u53ef\u4ee5\u52a0\u901f\u8a31\u591a\u61c9\u7528\u4e2d\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7684\u8a13\u7df4\uff0c\u8207\u4e00\u968e\u6700\u4f73\u5316\u5668\u76f8\u6bd4\uff0c\u901a\u5e38\u5728\u8a13\u7df4\u640d\u5931\u4e0a\u6bcf\u6b21\u53cd\u8986\u904b\u7b97\u90fd\u80fd\u7522\u751f\u66f4\u5feb\u7684\u9032\u5ea6\u3002\u7136\u800c\uff0c\u4e8c\u968e\u65b9\u6cd5\u7684\u6cdb\u5316\u7279\u6027\u4ecd\u6709\u722d\u8b70\u3002\u7406\u8ad6\u7814\u7a76\u5df2\u8b49\u660e\u5728\u5927\u91cf\u7c21\u5316\u6a21\u578b\u985e\u5225\u7684\u53ef\u8655\u7406\u8a2d\u5b9a\u4e4b\u5916\u96e3\u4ee5\u9032\u884c\uff0c\u56e0\u6b64\uff0c\u73fe\u6709\u7406\u8ad6\u8207\u5be6\u969b\u6df1\u5ea6\u5b78\u7fd2\u61c9\u7528\u4e4b\u9593\u7684\u95dc\u806f\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u540c\u6a23\u5730\uff0c\u7531\u65bc\u5728\u5be6\u52d9\u4e0a\u9700\u8981\u8fd1\u4f3c\u4e8c\u968e\u66f4\u65b0\uff0c\u56e0\u6b64\u5927\u578b\u6a21\u578b\u548c\u771f\u5be6\u8cc7\u6599\u96c6\u4e2d\u7684\u5be6\u8b49\u7814\u7a76\u6703\u53d7\u5230\u986f\u8457\u7684\u6df7\u6dc6\u3002\u901a\u5e38\u4e0d\u6e05\u695a\u89c0\u5bdf\u5230\u7684\u6cdb\u5316\u884c\u70ba\u662f\u5426\u7279\u5225\u4f86\u81ea\u65bc\u53c3\u6578\u66f4\u65b0\u7684\u4e8c\u968e\u6027\u8cea\uff0c\u6216\u8005\u53cd\u6620\u4e86\u6240\u4f7f\u7528\u7684\u7279\u5b9a\u7d50\u69cb\u5316\uff08\u4f8b\u5982\u514b\u7f85\u5167\u514b\uff09\u8fd1\u4f3c\u503c\u6216\u4efb\u4f55\u57fa\u65bc\u963b\u5c3c\u7684\u63d2\u503c\u671d\u5411\u4e00\u968e\u66f4\u65b0\u3002\u5728\u6b64\uff0c\u6211\u5011\u9996\u6b21\u5c55\u793a\u78ba\u5207\u7684 Gauss-Newton (GN) \u66f4\u65b0\u5728\u6df1\u5ea6\u53ef\u9006\u67b6\u69cb\u985e\u5225\u4e2d\u63a1\u7528\u53ef\u8655\u7406\u7684\u5f62\u5f0f\uff0c\u800c\u9019\u4e9b\u67b6\u69cb\u8db3\u5920\u5177\u6709\u8868\u73fe\u529b\uff0c\u53ef\u4ee5\u6709\u610f\u7fa9\u5730\u61c9\u7528\u65bc\u5e38\u898b\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\u3002\u6211\u5011\u5229\u7528\u9019\u500b\u65b0\u7a4e\u7684\u8a2d\u5b9a\u4f86\u7814\u7a76 GN \u6700\u4f73\u5316\u5668\u7684\u8a13\u7df4\u548c\u6cdb\u5316\u7279\u6027\u3002\u6211\u5011\u767c\u73fe\u78ba\u5207\u7684 GN \u6cdb\u5316\u6548\u679c\u4e0d\u4f73\u3002\u5728\u5c0f\u6279\u6b21\u8a13\u7df4\u8a2d\u5b9a\u4e2d\uff0c\u9019\u8868\u73fe\u70ba\u5373\u4f7f\u5728\\emph{\u8a13\u7df4}\u640d\u5931\u4e0a\u4e5f\u8fc5\u901f\u98fd\u548c\u9032\u5ea6\uff0c\u767c\u73fe\u53c3\u6578\u66f4\u65b0\u904e\u5ea6\u64ec\u5408\u6bcf\u500b\u5c0f\u6279\u6b21\uff0c\u800c\u6c92\u6709\u7522\u751f\u652f\u63f4\u6cdb\u5316\u5230\u5176\u4ed6\u5c0f\u6279\u6b21\u7684\u7279\u6027\u3002\u6211\u5011\u5c55\u793a\u6211\u5011\u7684\u5be6\u9a57\u5728\u300c\u60f0\u6027\u300d\u6a21\u5f0f\u4e0b\u57f7\u884c\uff0c\u5176\u4e2d\u795e\u7d93\u5207\u7dda\u6838 (NTK) \u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u8b8a\u5316\u5f88\u5c0f\u3002\u9019\u7a2e\u884c\u70ba\u8207\u795e\u7d93\u8868\u5fb5\u6c92\u6709\u986f\u8457\u8b8a\u5316\u6709\u95dc\uff0c\u9019\u89e3\u91cb\u4e86\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002</paragraph>", "author": "Davide Buffelli et.al.", "authors": "Davide Buffelli, Jamie McGowan, Wangkun Xu, Alexandru Cioba, Da-shan Shiu, Guillaume Hennequin, Alberto Bernacchia", "id": "2411.07979v1", "paper_url": "http://arxiv.org/abs/2411.07979v1", "repo": "null"}}