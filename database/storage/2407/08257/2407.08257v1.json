{"2407.08257": {"publish_time": "2024-07-11", "title": "Knowledge distillation to effectively attain both region-of-interest and global semantics from an image where multiple objects appear", "paper_summary": "Models based on convolutional neural networks (CNN) and transformers have\nsteadily been improved. They also have been applied in various computer vision\ndownstream tasks. However, in object detection tasks, accurately localizing and\nclassifying almost infinite categories of foods in images remains challenging.\nTo address these problems, we first segmented the food as the\nregion-of-interest (ROI) by using the segment-anything model (SAM) and masked\nthe rest of the region except ROI as black pixels. This process simplified the\nproblems into a single classification for which annotation and training were\nmuch simpler than object detection. The images in which only the ROI was\npreserved were fed as inputs to fine-tune various off-the-shelf models that\nencoded their own inductive biases. Among them, Data-efficient image\nTransformers (DeiTs) had the best classification performance. Nonetheless, when\nfoods' shapes and textures were similar, the contextual features of the\nROI-only images were not enough for accurate classification. Therefore, we\nintroduced a novel type of combined architecture, RveRNet, which consisted of\nROI, extra-ROI, and integration modules that allowed it to account for both the\nROI's and global contexts. The RveRNet's F1 score was 10% better than other\nindividual models when classifying ambiguous food images. If the RveRNet's\nmodules were DeiT with the knowledge distillation from the CNN, performed the\nbest. We investigated how architectures can be made robust against input noise\ncaused by permutation and translocation. The results indicated that there was a\ntrade-off between how much the CNN teacher's knowledge could be distilled to\nDeiT and DeiT's innate strength. Code is publicly available at:\nhttps://github.com/Seonwhee-Genome/RveRNet.", "paper_summary_zh": "<paragraph>\u57fa\u65bc\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u548c Transformer \u7684\u6a21\u578b\u5df2\u6301\u7e8c\u7372\u5f97\u6539\u5584\u3002\u5b83\u5011\u4e5f\u5df2\u61c9\u7528\u65bc\u5404\u7a2e\u96fb\u8166\u8996\u89ba\u4e0b\u6e38\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5728\u7269\u4ef6\u5075\u6e2c\u4efb\u52d9\u4e2d\uff0c\u6e96\u78ba\u5b9a\u4f4d\u548c\u5206\u985e\u5f71\u50cf\u4e2d\u5e7e\u4e4e\u7121\u9650\u985e\u5225\u7684\u98df\u7269\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u5206\u5272\u4efb\u4f55\u6771\u897f\u6a21\u578b (SAM) \u5c07\u98df\u7269\u5206\u5272\u70ba\u611f\u8208\u8da3\u5340\u57df (ROI)\uff0c\u4e26\u5c07 ROI \u4ee5\u5916\u7684\u5340\u57df\u906e\u7f69\u70ba\u9ed1\u8272\u50cf\u7d20\u3002\u6b64\u7a0b\u5e8f\u5c07\u554f\u984c\u7c21\u5316\u70ba\u55ae\u4e00\u5206\u985e\uff0c\u5176\u8a3b\u89e3\u548c\u8a13\u7df4\u6bd4\u7269\u4ef6\u5075\u6e2c\u7c21\u55ae\u8a31\u591a\u3002\u50c5\u4fdd\u7559 ROI \u7684\u5f71\u50cf\u88ab\u63d0\u4f9b\u70ba\u8f38\u5165\uff0c\u7528\u65bc\u5fae\u8abf\u5404\u7a2e\u5167\u5efa\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u7de8\u78bc\u4e86\u5176\u81ea\u8eab\u7684\u6b78\u7d0d\u504f\u5dee\u3002\u5176\u4e2d\uff0c\u8cc7\u6599\u6709\u6548\u7387\u7684\u5f71\u50cf Transformer (DeiTs) \u5177\u6709\u6700\u4f73\u7684\u5206\u985e\u6548\u80fd\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u7576\u98df\u7269\u7684\u5f62\u72c0\u548c\u8cea\u5730\u76f8\u4f3c\u6642\uff0c\u50c5 ROI \u5f71\u50cf\u7684\u8108\u7d61\u7279\u5fb5\u4e0d\u8db3\u4ee5\u9032\u884c\u6e96\u78ba\u5206\u985e\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u8907\u5408\u67b6\u69cb RveRNet\uff0c\u5b83\u5305\u542b ROI\u3001ROI \u5916\u90e8\u548c\u6574\u5408\u6a21\u7d44\uff0c\u4f7f\u5176\u80fd\u5920\u8003\u91cf ROI \u548c\u5168\u57df\u8108\u7d61\u3002RveRNet \u7684 F1 \u5206\u6578\u6bd4\u5176\u4ed6\u500b\u5225\u6a21\u578b\u9ad8\u51fa 10%\uff0c\u7528\u65bc\u5206\u985e\u6a21\u7a1c\u5169\u53ef\u7684\u98df\u7269\u5f71\u50cf\u3002\u5982\u679c RveRNet \u7684\u6a21\u7d44\u662f\u5177\u5099 CNN \u77e5\u8b58\u8403\u53d6\u7684 DeiT\uff0c\u5247\u8868\u73fe\u6700\u4f73\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u5982\u4f55\u8b93\u67b6\u69cb\u5c0d\u7531\u6392\u5217\u548c\u8f49\u4f4d\u9020\u6210\u7684\u8f38\u5165\u96dc\u8a0a\u5177\u6709\u7a69\u5065\u6027\u3002\u7d50\u679c\u8868\u660e\uff0cCNN \u6559\u5e2b\u7684\u77e5\u8b58\u80fd\u8403\u53d6\u5230 DeiT \u7684\u7a0b\u5ea6\u8207 DeiT \u7684\u5167\u5728\u5f37\u5ea6\u4e4b\u9593\u5b58\u5728\u6b0a\u8861\u3002\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc\uff1ahttps://github.com/Seonwhee-Genome/RveRNet\u3002</paragraph>", "author": "Seonwhee Jin et.al.", "authors": "Seonwhee Jin", "id": "2407.08257v1", "paper_url": "http://arxiv.org/abs/2407.08257v1", "repo": "https://github.com/seonwhee-genome/rvernet"}}