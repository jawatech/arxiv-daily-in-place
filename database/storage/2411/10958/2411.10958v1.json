{"2411.10958": {"publish_time": "2024-11-17", "title": "SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration", "paper_summary": "Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. SageAttention utilizes\n8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit\naccumulator, and precision-enhancing methods, implementing an accurate and 2x\nspeedup kernel compared to FlashAttention2. To further enhance the efficiency\nof attention computation while maintaining precision, we propose\nSageAttention2, which utilizes significantly faster 4-bit matrix multiplication\n(Matmul) alongside additional precision-enhancing techniques. First, we propose\nto quantize matrixes $(Q, K)$ to INT4 in a warp-level granularity and quantize\nmatrixes $(\\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$\nand $V$, enhancing the accuracy of attention with INT4 $QK$ and FP8 $PV$.\nThird, we analyze the quantization accuracy across timesteps and layers, then\npropose an adaptive quantization method to ensure the end-to-end metrics over\nvarious models. The operations per second (OPS) of SageAttention2 surpass\nFlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively.\nComprehensive experiments confirm that our approach incurs negligible\nend-to-end metrics loss across diverse models, including those for large\nlanguage processing, image generation, and video generation. The codes are\navailable at https://github.com/thu-ml/SageAttention.", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u7dda\u6027\u5c64\u7684\u91cf\u5316\u5df2\u5ee3\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u52a0\u901f\u6ce8\u610f\u529b\u8655\u7406\u4e0a\u7684\u61c9\u7528\u4ecd\u7136\u6709\u9650\u3002SageAttention \u4f7f\u7528 8 \u4f4d\u5143\u77e9\u9663\u4e58\u6cd5\u300116 \u4f4d\u5143\u77e9\u9663\u4e58\u6cd5\u642d\u914d 16 \u4f4d\u5143\u7d2f\u52a0\u5668\uff0c\u4ee5\u53ca\u7cbe\u6e96\u5ea6\u63d0\u5347\u65b9\u6cd5\uff0c\u5be6\u4f5c\u4e00\u500b\u7cbe\u6e96\u4e14\u901f\u5ea6\u70ba FlashAttention2 2 \u500d\u7684\u6838\u3002\u70ba\u4e86\u5728\u7dad\u6301\u7cbe\u6e96\u5ea6\u7684\u540c\u6642\u9032\u4e00\u6b65\u63d0\u5347\u6ce8\u610f\u529b\u904b\u7b97\u7684\u6548\u7387\uff0c\u6211\u5011\u63d0\u51fa SageAttention2\uff0c\u5b83\u4f7f\u7528\u986f\u8457\u66f4\u5feb\u7684 4 \u4f4d\u5143\u77e9\u9663\u4e58\u6cd5 (Matmul) \u642d\u914d\u5176\u4ed6\u7cbe\u6e96\u5ea6\u63d0\u5347\u6280\u8853\u3002\u9996\u5148\uff0c\u6211\u5011\u63d0\u51fa\u5c07\u77e9\u9663 $(Q, K)$ \u91cf\u5316\u70ba INT4\uff0c\u4ee5 warp \u5c64\u7d1a\u7684\u9846\u7c92\u5ea6\uff0c\u4e26\u5c07\u77e9\u9663 $(\\widetilde P, V)$ \u91cf\u5316\u70ba FP8\u3002\u5176\u6b21\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b9\u6cd5\u4f86\u5e73\u6ed1 $Q$ \u548c $V$\uff0c\u4f7f\u7528 INT4 $QK$ \u548c FP8 $PV$ \u63d0\u5347\u6ce8\u610f\u529b\u7684\u7cbe\u6e96\u5ea6\u3002\u7b2c\u4e09\uff0c\u6211\u5011\u5206\u6790\u4e86\u6642\u9593\u6b65\u9577\u548c\u5c64\u7d1a\u4e4b\u9593\u7684\u91cf\u5316\u7cbe\u6e96\u5ea6\uff0c\u7136\u5f8c\u63d0\u51fa\u4e00\u500b\u81ea\u9069\u61c9\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u78ba\u4fdd\u5404\u7a2e\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6307\u6a19\u3002SageAttention2 \u7684\u6bcf\u79d2\u904b\u7b97\u6b21\u6578 (OPS) \u5206\u5225\u6bd4 RTX4090 \u4e0a\u7684 FlashAttention2 \u548c xformers \u5feb\u7d04 3 \u500d\u548c 5 \u500d\u3002\u5168\u9762\u7684\u5be6\u9a57\u8b49\u5be6\uff0c\u6211\u5011\u7684\u505a\u6cd5\u6703\u9020\u6210\u5404\u7a2e\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6307\u6a19\u640d\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\uff0c\u5305\u62ec\u5927\u578b\u8a9e\u8a00\u8655\u7406\u3001\u5f71\u50cf\u751f\u6210\u548c\u5f71\u7247\u751f\u6210\u7684\u6a21\u578b\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/thu-ml/SageAttention \u53d6\u5f97\u3002</paragraph>", "author": "Jintao Zhang et.al.", "authors": "Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen", "id": "2411.10958v1", "paper_url": "http://arxiv.org/abs/2411.10958v1", "repo": "https://github.com/thu-ml/SageAttention"}}