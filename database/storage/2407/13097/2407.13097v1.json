{"2407.13097": {"publish_time": "2024-07-18", "title": "AlcLaM: Arabic Dialectal Language Model", "paper_summary": "Pre-trained Language Models (PLMs) are integral to many modern natural\nlanguage processing (NLP) systems. Although multilingual models cover a wide\nrange of languages, they often grapple with challenges like high inference\ncosts and a lack of diverse non-English training data. Arabic-specific PLMs are\ntrained predominantly on modern standard Arabic, which compromises their\nperformance on regional dialects. To tackle this, we construct an Arabic\ndialectal corpus comprising 3.4M sentences gathered from social media\nplatforms. We utilize this corpus to expand the vocabulary and retrain a\nBERT-based model from scratch. Named AlcLaM, our model was trained using only\n13 GB of text, which represents a fraction of the data used by existing models\nsuch as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%,\nrespectively. Remarkably, AlcLaM demonstrates superior performance on a variety\nof Arabic NLP tasks despite the limited training data. AlcLaM is available at\nGitHub https://github.com/amurtadha/Alclam and HuggingFace\nhttps://huggingface.co/rahbi.", "paper_summary_zh": "\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u662f\u8a31\u591a\u73fe\u4ee3\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7cfb\u7d71\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\u3002\u5118\u7ba1\u591a\u8a9e\u8a00\u6a21\u578b\u6db5\u84cb\u5ee3\u6cdb\u7684\u8a9e\u8a00\uff0c\u4f46\u5b83\u5011\u7d93\u5e38\u6703\u9047\u5230\u9ad8\u63a8\u7406\u6210\u672c\u548c\u7f3a\u4e4f\u591a\u6a23\u5316\u975e\u82f1\u8a9e\u8a13\u7df4\u8cc7\u6599\u7b49\u6311\u6230\u3002\u963f\u62c9\u4f2f\u8a9e\u7279\u5b9a PLM \u4e3b\u8981\u91dd\u5c0d\u73fe\u4ee3\u6a19\u6e96\u963f\u62c9\u4f2f\u8a9e\u9032\u884c\u8a13\u7df4\uff0c\u9019\u6703\u640d\u5bb3\u5b83\u5011\u5728\u5340\u57df\u65b9\u8a00\u4e2d\u7684\u8868\u73fe\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u963f\u62c9\u4f2f\u65b9\u8a00\u8a9e\u6599\u5eab\uff0c\u5176\u4e2d\u5305\u542b\u5f9e\u793e\u7fa4\u5a92\u9ad4\u5e73\u53f0\u6536\u96c6\u7684 340 \u842c\u500b\u53e5\u5b50\u3002\u6211\u5011\u5229\u7528\u9019\u500b\u8a9e\u6599\u5eab\u4f86\u64f4\u5145\u8a5e\u5f59\u91cf\uff0c\u4e26\u5f9e\u982d\u958b\u59cb\u91cd\u65b0\u8a13\u7df4 BERT \u6a21\u578b\u3002\u6211\u5011\u7684\u6a21\u578b\u540d\u70ba AlcLaM\uff0c\u50c5\u4f7f\u7528 13 GB \u7684\u6587\u5b57\u9032\u884c\u8a13\u7df4\uff0c\u50c5\u4f54\u73fe\u6709\u6a21\u578b\uff08\u4f8b\u5982 CAMeL\u3001MARBERT \u548c ArBERT\uff09\u6240\u7528\u8cc7\u6599\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u5206\u5225\u70ba 7.8%\u300110.2% \u548c 21.3%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5118\u7ba1\u8a13\u7df4\u8cc7\u6599\u6709\u9650\uff0c\u4f46 AlcLaM \u5728\u5404\u7a2e\u963f\u62c9\u4f2f\u8a9e NLP \u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\u3002AlcLaM \u53ef\u5728 GitHub https://github.com/amurtadha/Alclam \u548c HuggingFace https://huggingface.co/rahbi \u53d6\u5f97\u3002", "author": "Murtadha Ahmed et.al.", "authors": "Murtadha Ahmed, Saghir Alfasly, Bo Wen, Jamaal Qasem, Mohammed Ahmed, Yunfeng Liu", "id": "2407.13097v1", "paper_url": "http://arxiv.org/abs/2407.13097v1", "repo": "https://github.com/amurtadha/alclam"}}