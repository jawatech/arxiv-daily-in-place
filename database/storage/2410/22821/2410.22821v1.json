{"2410.22821": {"publish_time": "2024-10-30", "title": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations", "paper_summary": "How to evaluate Large Language Models (LLMs) in code generation remains an\nopen question. Existing benchmarks have two limitations - data leakage and lack\nof domain-specific evaluation. The former hurts the fairness of benchmarks, and\nthe latter hinders practitioners from selecting superior LLMs for specific\nprogramming domains. To address these two limitations, we propose a new\nbenchmark - EvoCodeBench, which has the following advances: (1) Evolving data.\nEvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid\ndata leakage. This paper releases the first version - EvoCodeBench-2403,\ncontaining 275 samples from 25 repositories. (2) A domain taxonomy and domain\nlabels. Based on the statistics of open-source communities, we design a\nprogramming domain taxonomy consisting of 10 popular domains. Based on the\ntaxonomy, we annotate each sample in EvoCodeBench with a domain label. (3)\nDomain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific\nImprovement (DSI) and define LLMs' comfort and strange domains. These\nevaluations help practitioners select superior LLMs in specific domains and\ndiscover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g.,\ngpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights.\nEvoCodeBench reveals the actual abilities of these LLMs in real-world\nrepositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is\nonly 20.74%. Besides, we evaluate LLMs in different domains and discover their\ncomfort and strange domains. For example, gpt-4 performs best in most domains\nbut falls behind others in the Internet domain. StarCoder 2-15B unexpectedly\nperforms well in the Database domain and even outperforms 33B LLMs.\nEvoCodeBench has been released.", "paper_summary_zh": "<paragraph>\u5982\u4f55\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u4e2d\u7684\u8868\u73fe\u4ecd\u7136\u662f\u4e00\u500b\u958b\u653e\u6027\u7684\u554f\u984c\u3002\u73fe\u6709\u7684\u57fa\u6e96\u6709\u5169\u500b\u9650\u5236 - \u8cc7\u6599\u5916\u6d29\u548c\u7f3a\u4e4f\u7279\u5b9a\u9818\u57df\u7684\u8a55\u4f30\u3002\u524d\u8005\u640d\u5bb3\u4e86\u57fa\u6e96\u7684\u516c\u5e73\u6027\uff0c\u800c\u5f8c\u8005\u963b\u7919\u4e86\u5f9e\u696d\u8005\u70ba\u7279\u5b9a\u7a0b\u5f0f\u8a2d\u8a08\u9818\u57df\u9078\u64c7\u512a\u8d8a\u7684 LLM\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u5169\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u6e96 - EvoCodeBench\uff0c\u5b83\u5177\u6709\u4ee5\u4e0b\u9032\u5c55\uff1a(1) \u6f14\u9032\u8cc7\u6599\u3002EvoCodeBench \u5c07\u6bcf\u9694\u4e00\u6bb5\u6642\u9593\uff08\u4f8b\u5982 6 \u500b\u6708\uff09\u52d5\u614b\u66f4\u65b0\uff0c\u4ee5\u907f\u514d\u8cc7\u6599\u5916\u6d29\u3002\u672c\u6587\u767c\u5e03\u4e86\u7b2c\u4e00\u500b\u7248\u672c - EvoCodeBench-2403\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea 25 \u500b\u5132\u5b58\u5eab\u7684 275 \u500b\u7bc4\u4f8b\u3002(2) \u9818\u57df\u5206\u985e\u6cd5\u548c\u9818\u57df\u6a19\u7c64\u3002\u6839\u64da\u958b\u6e90\u793e\u7fa4\u7684\u7d71\u8a08\u8cc7\u6599\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u5305\u542b 10 \u500b\u71b1\u9580\u9818\u57df\u7684\u7a0b\u5f0f\u8a2d\u8a08\u9818\u57df\u5206\u985e\u6cd5\u3002\u6839\u64da\u5206\u985e\u6cd5\uff0c\u6211\u5011\u4f7f\u7528\u9818\u57df\u6a19\u7c64\u8a3b\u89e3 EvoCodeBench \u4e2d\u7684\u6bcf\u500b\u7bc4\u4f8b\u3002(3) \u9818\u57df\u7279\u5b9a\u7684\u8a55\u4f30\u3002\u9664\u4e86 Pass@k\uff0c\u6211\u5011\u8a08\u7b97\u4e86\u7279\u5b9a\u9818\u57df\u7684\u6539\u9032 (DSI) \u4e26\u5b9a\u7fa9\u4e86 LLM \u7684\u8212\u9069\u9818\u57df\u548c\u964c\u751f\u9818\u57df\u3002\u9019\u4e9b\u8a55\u4f30\u6709\u52a9\u65bc\u5f9e\u696d\u8005\u5728\u7279\u5b9a\u9818\u57df\u4e2d\u9078\u64c7\u512a\u8d8a\u7684 LLM\uff0c\u4e26\u767c\u73fe\u73fe\u6709 LLM \u7684\u7f3a\u9ede\u3002\u6211\u5011\u5728 EvoCodeBench \u4e0a\u8a55\u4f30\u4e86 8 \u500b\u6d41\u884c\u7684 LLM\uff08\u4f8b\u5982 gpt-4\u3001DeepSeek Coder\uff09\uff0c\u4e26\u7e3d\u7d50\u4e86\u4e00\u4e9b\u898b\u89e3\u3002EvoCodeBench \u63ed\u793a\u4e86\u9019\u4e9b LLM \u5728\u771f\u5be6\u4e16\u754c\u5132\u5b58\u5eab\u4e2d\u7684\u5be6\u969b\u80fd\u529b\u3002\u4f8b\u5982\uff0cgpt-4 \u5728 EvoCodeBench-2403 \u4e0a\u7684\u6700\u9ad8 Pass@1 \u50c5\u70ba 20.74%\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u4e0d\u540c\u9818\u57df\u4e2d\u7684 LLM\uff0c\u4e26\u767c\u73fe\u4e86\u5b83\u5011\u7684\u8212\u9069\u9818\u57df\u548c\u964c\u751f\u9818\u57df\u3002\u4f8b\u5982\uff0cgpt-4 \u5728\u5927\u591a\u6578\u9818\u57df\u4e2d\u8868\u73fe\u6700\u4f73\uff0c\u4f46\u5728\u7db2\u969b\u7db2\u8def\u9818\u57df\u843d\u5f8c\u65bc\u5176\u4ed6\u9818\u57df\u3002StarCoder 2-15B \u5728\u8cc7\u6599\u5eab\u9818\u57df\u8868\u73fe\u51fa\u4e4e\u610f\u6599\u5730\u597d\uff0c\u751a\u81f3\u512a\u65bc 33B LLM\u3002EvoCodeBench \u5df2\u767c\u5e03\u3002</paragraph>", "author": "Jia Li et.al.", "authors": "Jia Li, Ge Li, Xuanming Zhang, Yunfei Zhao, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li", "id": "2410.22821v1", "paper_url": "http://arxiv.org/abs/2410.22821v1", "repo": "null"}}