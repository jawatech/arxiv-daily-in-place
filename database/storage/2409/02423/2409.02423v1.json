{"2409.02423": {"publish_time": "2024-09-04", "title": "Accelerating Large Language Model Training with Hybrid GPU-based Compression", "paper_summary": "Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP)\nare the three strategies widely adopted to enable fast and efficient Large\nLanguage Model (LLM) training. However, these approaches rely on data-intensive\ncommunication routines to collect, aggregate, and re-distribute gradients,\nactivations, and other important model information, which pose significant\noverhead. Co-designed with GPU-based compression libraries, MPI libraries have\nbeen proven to reduce message size significantly, and leverage interconnect\nbandwidth, thus increasing training efficiency while maintaining acceptable\naccuracy.\n  In this work, we investigate the efficacy of compression-assisted MPI\ncollectives under the context of distributed LLM training using 3D parallelism\nand ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen\nsupercomputer. First, we enabled a na\\\"ive compression scheme across all\ncollectives and observed a 22.5\\% increase in TFLOPS per GPU and a 23.6\\%\nincrease in samples per second for GPT-NeoX-20B training. Nonetheless, such a\nstrategy ignores the sparsity discrepancy among messages communicated in each\nparallelism degree, thus introducing more errors and causing degradation in\ntraining loss. Therefore, we incorporated hybrid compression settings toward\neach parallel dimension and adjusted the compression intensity accordingly.\nGiven their low-rank structure (arXiv:2301.02654), we apply aggressive\ncompression on gradients when performing DP All-reduce. We adopt milder\ncompression to preserve precision while communicating activations, optimizer\nstates, and model parameters in TP and PP. Using the adjusted hybrid\ncompression scheme, we demonstrate a 17.3\\% increase in TFLOPS per GPU and a\n12.7\\% increase in samples per second while reaching baseline loss convergence.", "paper_summary_zh": "\u8cc7\u6599\u5e73\u884c\uff08DP\uff09\u3001\u5f35\u91cf\u5e73\u884c\uff08TP\uff09\u548c\u7ba1\u9053\u5e73\u884c\uff08PP\uff09\u662f\u5ee3\u6cdb\u63a1\u7528\u7684\u4e09\u7a2e\u7b56\u7565\uff0c\u7528\u65bc\u5be6\u73fe\u5feb\u901f\u4e14\u9ad8\u6548\u7684\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8a13\u7df4\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u8cc7\u6599\u5bc6\u96c6\u578b\u901a\u8a0a\u4f8b\u7a0b\u4f86\u6536\u96c6\u3001\u5f59\u7e3d\u548c\u91cd\u65b0\u5206\u914d\u68af\u5ea6\u3001\u6fc0\u6d3b\u548c\u5176\u4ed6\u91cd\u8981\u7684\u6a21\u578b\u8cc7\u8a0a\uff0c\u9019\u6703\u9020\u6210\u986f\u8457\u7684\u984d\u5916\u8ca0\u64d4\u3002MPI \u5eab\u8207\u57fa\u65bc GPU \u7684\u58d3\u7e2e\u5eab\u5171\u540c\u8a2d\u8a08\uff0c\u5df2\u8b49\u660e\u53ef\u4ee5\u986f\u8457\u6e1b\u5c11\u8a0a\u606f\u5927\u5c0f\uff0c\u4e26\u5229\u7528\u4e92\u9023\u983b\u5bec\uff0c\u5f9e\u800c\u63d0\u9ad8\u8a13\u7df4\u6548\u7387\uff0c\u540c\u6642\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u6e96\u78ba\u5ea6\u3002\n\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5728\u4f7f\u7528 3D \u5e73\u884c\u548c ZeRO \u6700\u4f73\u5316\u9032\u884c\u5206\u6563\u5f0f LLM \u8a13\u7df4\u7684\u80cc\u666f\u4e0b\uff0c\u58d3\u7e2e\u8f14\u52a9 MPI \u96c6\u5408\u7684\u529f\u6548\u3002\u6211\u5011\u5728 Lassen \u8d85\u7d1a\u96fb\u8166\u4e0a\u64f4\u5145\u5230 192 \u500b V100 GPU\u3002\u9996\u5148\uff0c\u6211\u5011\u5728\u6240\u6709\u96c6\u5408\u4e2d\u555f\u7528\u4e86\u4e00\u500b\u300c\u5929\u771f\u7684\u300d\u58d3\u7e2e\u65b9\u6848\uff0c\u4e26\u89c0\u5bdf\u5230 GPT-NeoX-20B \u8a13\u7df4\u7684\u6bcf\u79d2 TFLOPS \u589e\u52a0 22.5%\uff0c\u6bcf\u79d2\u6a23\u672c\u589e\u52a0 23.6%\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u9019\u7a2e\u7b56\u7565\u5ffd\u7565\u4e86\u5728\u6bcf\u500b\u5e73\u884c\u5ea6\u4e2d\u50b3\u9054\u7684\u8a0a\u606f\u4e4b\u9593\u7684\u7a00\u758f\u5dee\u7570\uff0c\u5f9e\u800c\u5f15\u5165\u4e86\u66f4\u591a\u932f\u8aa4\u4e26\u5c0e\u81f4\u8a13\u7df4\u640d\u5931\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u6211\u5011\u91dd\u5c0d\u6bcf\u500b\u5e73\u884c\u7dad\u5ea6\u7d0d\u5165\u4e86\u6df7\u5408\u58d3\u7e2e\u8a2d\u5b9a\uff0c\u4e26\u76f8\u61c9\u8abf\u6574\u58d3\u7e2e\u5f37\u5ea6\u3002\n\u9451\u65bc\u5b83\u5011\u7684\u4f4e\u79e9\u7d50\u69cb\uff08arXiv:2301.02654\uff09\uff0c\u6211\u5011\u5728\u57f7\u884c DP \u5168\u90e8\u6e1b\u5c11\u6642\u5c0d\u68af\u5ea6\u61c9\u7528\u6fc0\u9032\u58d3\u7e2e\u3002\u6211\u5011\u63a1\u7528\u8f03\u6eab\u548c\u7684\u58d3\u7e2e\u4f86\u4fdd\u7559\u7cbe\u5ea6\uff0c\u540c\u6642\u5728 TP \u548c PP \u4e2d\u50b3\u9054\u6fc0\u6d3b\u3001\u6700\u4f73\u5316\u5668\u72c0\u614b\u548c\u6a21\u578b\u53c3\u6578\u3002\u4f7f\u7528\u8abf\u6574\u5f8c\u7684\u6df7\u5408\u58d3\u7e2e\u65b9\u6848\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6bcf\u79d2 TFLOPS \u589e\u52a0 17.3%\uff0c\u6bcf\u79d2\u6a23\u672c\u589e\u52a0 12.7%\uff0c\u540c\u6642\u9054\u5230\u57fa\u7dda\u640d\u5931\u6536\u6582\u3002", "author": "Lang Xu et.al.", "authors": "Lang Xu, Quentin Anthony, Qinghua Zhou, Nawras Alnaasan, Radha R. Gulhane, Aamir Shafi, Hari Subramoni, Dhabaleswar K. Panda", "id": "2409.02423v1", "paper_url": "http://arxiv.org/abs/2409.02423v1", "repo": "null"}}