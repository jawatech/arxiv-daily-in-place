{"2410.05361": {"publish_time": "2024-10-07", "title": "RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction", "paper_summary": "The high incidence and mortality rates associated with respiratory diseases\nunderscores the importance of early screening. Machine learning models can\nautomate clinical consultations and auscultation, offering vital support in\nthis area. However, the data involved, spanning demographics, medical history,\nsymptoms, and respiratory audio, are heterogeneous and complex. Existing\napproaches are insufficient and lack generalizability, as they typically rely\non limited training data, basic fusion techniques, and task-specific models. In\nthis paper, we propose RespLLM, a novel multimodal large language model (LLM)\nframework that unifies text and audio representations for respiratory health\nprediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs\nand enables effective audio-text fusion through cross-modal attentions.\nInstruction tuning is employed to integrate diverse data from multiple sources,\nensuring generalizability and versatility of the model. Experiments on five\nreal-world datasets demonstrate that RespLLM outperforms leading baselines by\nan average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates\nzero-shot predictions for new tasks. Our work lays the foundation for\nmultimodal models that can perceive, listen to, and understand heterogeneous\ndata, paving the way for scalable respiratory health diagnosis.", "paper_summary_zh": "\u9ad8\u767c\u751f\u7387\u548c\u6b7b\u4ea1\u7387\u7684\u547c\u5438\u9053\u75be\u75c5\u7a81\u986f\u4e86\u65e9\u671f\u7be9\u6aa2\u7684\u91cd\u8981\u6027\u3002\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u53ef\u4ee5\u81ea\u52d5\u5316\u81e8\u5e8a\u8aee\u8a62\u548c\u807d\u8a3a\uff0c\u5728\u6b64\u9818\u57df\u63d0\u4f9b\u91cd\u8981\u7684\u652f\u63f4\u3002\u7136\u800c\uff0c\u6240\u6d89\u53ca\u7684\u8cc7\u6599\u6db5\u84cb\u4eba\u53e3\u7d71\u8a08\u3001\u75c5\u53f2\u3001\u75c7\u72c0\u548c\u547c\u5438\u97f3\u8a0a\uff0c\u65e2\u7570\u8cea\u53c8\u8907\u96dc\u3002\u73fe\u6709\u7684\u65b9\u6cd5\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u6982\u62ec\u6027\uff0c\u56e0\u70ba\u5b83\u5011\u901a\u5e38\u4f9d\u8cf4\u65bc\u6709\u9650\u7684\u8a13\u7df4\u8cc7\u6599\u3001\u57fa\u672c\u7684\u878d\u5408\u6280\u8853\u548c\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa RespLLM\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6846\u67b6\uff0c\u5b83\u7d71\u4e00\u4e86\u6587\u672c\u548c\u97f3\u8a0a\u8868\u793a\uff0c\u4ee5\u9032\u884c\u547c\u5438\u9053\u5065\u5eb7\u9810\u6e2c\u3002RespLLM \u5229\u7528\u9810\u8a13\u7df4 LLM \u7684\u5ee3\u6cdb\u5148\u9a57\u77e5\u8b58\uff0c\u4e26\u900f\u904e\u8de8\u6a21\u614b\u6ce8\u610f\u529b\u5be6\u73fe\u6709\u6548\u7684\u97f3\u8a0a\u6587\u672c\u878d\u5408\u3002\u6307\u793a\u8abf\u6574\u7528\u65bc\u6574\u5408\u4f86\u81ea\u591a\u500b\u4f86\u6e90\u7684\u4e0d\u540c\u8cc7\u6599\uff0c\u78ba\u4fdd\u6a21\u578b\u7684\u6982\u62ec\u6027\u548c\u591a\u529f\u80fd\u6027\u3002\u5728\u4e94\u500b\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cRespLLM \u5728\u8a13\u7df4\u4efb\u52d9\u4e0a\u6bd4\u9818\u5148\u7684\u57fa\u6e96\u9ad8\u51fa\u5e73\u5747 4.6%\uff0c\u5728\u672a\u898b\u8cc7\u6599\u96c6\u4e0a\u9ad8\u51fa 7.9%\uff0c\u4e26\u4fc3\u9032\u65b0\u4efb\u52d9\u7684\u96f6\u6b21\u5b78\u7fd2\u9810\u6e2c\u3002\u6211\u5011\u7684\u7814\u7a76\u70ba\u591a\u6a21\u614b\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u790e\uff0c\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u611f\u77e5\u3001\u8046\u807d\u548c\u7406\u89e3\u7570\u8cea\u8cc7\u6599\uff0c\u70ba\u53ef\u64f4\u5145\u7684\u547c\u5438\u9053\u5065\u5eb7\u8a3a\u65b7\u92ea\u5e73\u9053\u8def\u3002", "author": "Yuwei Zhang et.al.", "authors": "Yuwei Zhang, Tong Xia, Aaqib Saeed, Cecilia Mascolo", "id": "2410.05361v1", "paper_url": "http://arxiv.org/abs/2410.05361v1", "repo": "null"}}