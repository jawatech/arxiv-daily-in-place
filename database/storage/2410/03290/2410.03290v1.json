{"2410.03290": {"publish_time": "2024-10-04", "title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models", "paper_summary": "Video Large Language Models (Video-LLMs) have demonstrated remarkable\ncapabilities in coarse-grained video understanding, however, they struggle with\nfine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,\na novel Video-LLM adept at perceiving and reasoning over specific video moments\nin a fine-grained manner. We identify that current Video-LLMs have limitations\nfor fine-grained video understanding since they lack effective temporal\nmodeling and timestamp representation. In light of this, we sharpen our model\nby incorporating (1) an additional temporal stream to encode the relationships\nbetween frames and (2) discrete temporal tokens enriched with specific time\nknowledge to represent timestamps. To optimize the training of\nGrounded-VideoLLM, we employ a multi-stage training scheme, beginning with\nsimple video-captioning tasks and progressively introducing video temporal\ngrounding tasks of increasing complexity. To further enhance\nGrounded-VideoLLM's temporal reasoning capability, we also curate a grounded\nVideoQA dataset by an automatic annotation pipeline. Extensive experiments\ndemonstrate that Grounded-VideoLLM not only excels in fine-grained grounding\ntasks such as temporal sentence grounding, dense video captioning, and grounded\nVideoQA, but also shows great potential as a versatile video assistant for\ngeneral video understanding.", "paper_summary_zh": "\u5f71\u7247\u5927\u578b\u8a9e\u8a00\u6a21\u578b (Video-LLM) \u5df2\u5c55\u73fe\u51fa\u5728\u7c97\u7565\u5f71\u7247\u7406\u89e3\u65b9\u9762\u7684\u975e\u51e1\u80fd\u529b\uff0c\u7136\u800c\uff0c\u5b83\u5011\u5728\u7d30\u7dfb\u7684\u6642\u9593\u4f9d\u64da\u65b9\u9762\u537b\u9762\u81e8\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 Grounded-VideoLLM\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684 Video-LLM\uff0c\u64c5\u9577\u4ee5\u7d30\u7dfb\u7684\u65b9\u5f0f\u611f\u77e5\u548c\u63a8\u7406\u7279\u5b9a\u5f71\u7247\u6642\u523b\u3002\u6211\u5011\u767c\u73fe\u76ee\u524d\u7684 Video-LLM \u5728\u7d30\u7dfb\u5f71\u7247\u7406\u89e3\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u56e0\u70ba\u5b83\u5011\u7f3a\u4e4f\u6709\u6548\u7684\u6642\u9593\u5efa\u6a21\u548c\u6642\u9593\u6233\u8868\u793a\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u900f\u904e\u6574\u5408 (1) \u4e00\u500b\u984d\u5916\u7684\u6642\u9593\u4e32\u6d41\u4f86\u7de8\u78bc\u5e40\u4e4b\u9593\u7684\u95dc\u4fc2\uff0c\u4ee5\u53ca (2) \u5177\u6709\u7279\u5b9a\u6642\u9593\u77e5\u8b58\u7684\u96e2\u6563\u6642\u9593\u6a19\u8a18\u4f86\u8868\u793a\u6642\u9593\u6233\uff0c\u9032\u800c\u5f37\u5316\u6211\u5011\u7684\u6a21\u578b\u3002\u70ba\u4e86\u6700\u4f73\u5316 Grounded-VideoLLM \u7684\u8a13\u7df4\uff0c\u6211\u5011\u63a1\u7528\u591a\u968e\u6bb5\u8a13\u7df4\u67b6\u69cb\uff0c\u5f9e\u7c21\u55ae\u7684\u5f71\u7247\u5b57\u5e55\u4efb\u52d9\u958b\u59cb\uff0c\u4e26\u9010\u6b65\u5f15\u5165\u8d8a\u4f86\u8d8a\u8907\u96dc\u7684\u5f71\u7247\u6642\u9593\u4f9d\u64da\u4efb\u52d9\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u589e\u5f37 Grounded-VideoLLM \u7684\u6642\u9593\u63a8\u7406\u80fd\u529b\uff0c\u6211\u5011\u9084\u900f\u904e\u81ea\u52d5\u8a3b\u89e3\u7ba1\u9053\u6574\u7406\u4e86\u4e00\u500b\u4f9d\u64da\u7684 VideoQA \u8cc7\u6599\u96c6\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cGrounded-VideoLLM \u4e0d\u50c5\u5728\u7d30\u7dfb\u4f9d\u64da\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f8b\u5982\u6642\u9593\u53e5\u5b50\u4f9d\u64da\u3001\u5bc6\u96c6\u5f71\u7247\u5b57\u5e55\u548c\u4f9d\u64da VideoQA\uff0c\u800c\u4e14\u9084\u5c55\u73fe\u51fa\u4f5c\u70ba\u901a\u7528\u5f71\u7247\u52a9\u7406\u5728\u4e00\u822c\u5f71\u7247\u7406\u89e3\u65b9\u9762\u5177\u6709\u6975\u5927\u7684\u6f5b\u529b\u3002", "author": "Haibo Wang et.al.", "authors": "Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, Lifu Huang", "id": "2410.03290v1", "paper_url": "http://arxiv.org/abs/2410.03290v1", "repo": "https://github.com/whb139426/grounded-video-llm"}}