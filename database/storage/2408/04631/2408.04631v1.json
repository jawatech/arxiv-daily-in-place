{"2408.04631": {"publish_time": "2024-08-08", "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics", "paper_summary": "We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa Puppet-Master\uff0c\u4e00\u500b\u4e92\u52d5\u5f0f\u5f71\u7247\u751f\u6210\u6a21\u578b\uff0c\u53ef\u7528\u4f5c\u90e8\u5206\u5c64\u7d1a\u52d5\u614b\u7684\u52d5\u4f5c\u5148\u9a57\u3002\u5728\u6e2c\u8a66\u6642\uff0c\u7d66\u5b9a\u55ae\u4e00\u5f71\u50cf\u548c\u4e00\u7d44\u7a00\u758f\u7684\u52d5\u4f5c\u8ecc\u8de1\uff08\u5373\u62d6\u66f3\uff09\uff0cPuppet-Master \u53ef\u4ee5\u5408\u6210\u5f71\u7247\uff0c\u63cf\u7e6a\u51fa\u7b26\u5408\u7d66\u5b9a\u62d6\u66f3\u4e92\u52d5\u7684\u903c\u771f\u90e8\u5206\u5c64\u7d1a\u52d5\u4f5c\u3002\u9019\u662f\u900f\u904e\u5fae\u8abf\u5927\u578b\u9810\u5148\u8a13\u7df4\u5f71\u7247\u64f4\u6563\u6a21\u578b\u4f86\u5be6\u73fe\u7684\uff0c\u6211\u5011\u70ba\u6b64\u63d0\u51fa\u65b0\u7684\u5236\u7d04\u67b6\u69cb\uff0c\u4ee5\u6709\u6548\u6ce8\u5165\u62d6\u66f3\u63a7\u5236\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u5f15\u5165\u5168\u5c0d\u4e00\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u9019\u662f\u5ee3\u6cdb\u63a1\u7528\u7684\u7a7a\u9593\u6ce8\u610f\u529b\u6a21\u7d44\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u900f\u904e\u89e3\u6c7a\u73fe\u6709\u6a21\u578b\u4e2d\u7684\u5916\u89c0\u548c\u80cc\u666f\u554f\u984c\uff0c\u986f\u8457\u6539\u5584\u751f\u6210\u54c1\u8cea\u3002\u8207\u5176\u4ed6\u5728\u91ce\u5916\u5f71\u7247\u4e0a\u8a13\u7df4\u4e14\u4e3b\u8981\u79fb\u52d5\u6574\u500b\u7269\u9ad4\u7684\u52d5\u4f5c\u689d\u4ef6\u5f71\u7247\u751f\u6210\u5668\u4e0d\u540c\uff0cPuppet-Master \u662f\u5f9e Objaverse-Animation-HQ\uff08\u4e00\u7a2e\u7d93\u904e\u6574\u7406\u7684\u90e8\u5206\u5c64\u7d1a\u52d5\u4f5c\u7247\u6bb5\u7684\u65b0\u8cc7\u6599\u96c6\uff09\u5b78\u7fd2\u7684\u3002\u6211\u5011\u63d0\u51fa\u7b56\u7565\uff0c\u81ea\u52d5\u904e\u6ffe\u6389\u6b21\u4f73\u52d5\u756b\uff0c\u4e26\u4f7f\u7528\u6709\u610f\u7fa9\u7684\u52d5\u4f5c\u8ecc\u8de1\u64f4\u5145\u5408\u6210\u6e32\u67d3\u3002Puppet-Master \u5728\u5404\u7a2e\u985e\u5225\u7684\u771f\u5be6\u5f71\u50cf\u4e2d\u90fd\u80fd\u5f88\u597d\u5730\u6982\u62ec\uff0c\u4e26\u5728\u771f\u5be6\u4e16\u754c\u7684\u57fa\u6e96\u4e0a\u4ee5\u96f6\u6b21\u5b78\u7fd2\u7684\u65b9\u5f0f\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u8acb\u53c3\u95b1\u6211\u5011\u7684\u5c08\u6848\u9801\u9762\u4ee5\u53d6\u5f97\u66f4\u591a\u7d50\u679c\uff1avgg-puppetmaster.github.io\u3002", "author": "Ruining Li et.al.", "authors": "Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi", "id": "2408.04631v1", "paper_url": "http://arxiv.org/abs/2408.04631v1", "repo": "null"}}