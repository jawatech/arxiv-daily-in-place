{"2410.22884": {"publish_time": "2024-10-30", "title": "Stealing User Prompts from Mixture of Experts", "paper_summary": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.", "paper_summary_zh": "\u6df7\u5408\u4e13\u5bb6 (MoE) \u6a21\u578b\u900f\u904e\u5c07\u6bcf\u500b\u7b26\u865f\u8def\u7531\u5230\u6bcf\u4e00\u5c64\u7684\u5c11\u6578\u5c08\u5bb6\uff0c\u9032\u800c\u63d0\u5347\u5bc6\u96c6\u8a9e\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u53ef\u64f4\u5145\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4e00\u500b\u5c0d\u624b\u5982\u4f55\u5b89\u6392\u4ed6\u5011\u7684\u67e5\u8a62\u51fa\u73fe\u5728\u8207\u53d7\u5bb3\u8005\u67e5\u8a62\u76f8\u540c\u7684\u7bc4\u4f8b\u6279\u6b21\u4e2d\uff0c\u53ef\u4ee5\u5229\u7528\u5c08\u5bb6\u9078\u64c7\u8def\u7531\u4f86\u5b8c\u5168\u63ed\u9732\u53d7\u5bb3\u8005\u7684\u63d0\u793a\u3002\u6211\u5011\u6210\u529f\u5c55\u793a\u4e86\u9019\u7a2e\u653b\u64ca\u5728\u96d9\u5c64 Mixtral \u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5229\u7528\u4e86\u706b\u70ac topk CUDA \u5be6\u4f5c\u7684\u5e73\u624b\u8655\u7406\u884c\u70ba\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u53ef\u4ee5\u4f7f\u7528 $O({VM}^2)$ \u500b\u67e5\u8a62\uff08\u8a5e\u5f59\u91cf\u5927\u5c0f\u70ba $V$\uff0c\u63d0\u793a\u9577\u5ea6\u70ba $M$\uff09\u6216\u5728\u6211\u5011\u8003\u616e\u7684\u8a2d\u5b9a\u4e2d\u5e73\u5747\u6bcf\u500b\u7b26\u865f 100 \u500b\u67e5\u8a62\u4f86\u63d0\u53d6\u6574\u500b\u63d0\u793a\u3002\u9019\u662f\u7b2c\u4e00\u500b\u5229\u7528\u67b6\u69cb\u7f3a\u9677\u4f86\u63d0\u53d6\u4f7f\u7528\u8005\u63d0\u793a\u7684\u653b\u64ca\uff0c\u5f15\u5165\u4e86 LLM \u6f0f\u6d1e\u7684\u65b0\u985e\u5225\u3002", "author": "Itay Yona et.al.", "authors": "Itay Yona, Ilia Shumailov, Jamie Hayes, Nicholas Carlini", "id": "2410.22884v1", "paper_url": "http://arxiv.org/abs/2410.22884v1", "repo": "null"}}