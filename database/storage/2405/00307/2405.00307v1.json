{"2405.00307": {"publish_time": "2024-05-01", "title": "Active Learning with Task Adaptation Pre-training for Speech Emotion Recognition", "paper_summary": "Speech emotion recognition (SER) has garnered increasing attention due to its\nwide range of applications in various fields, including human-machine\ninteraction, virtual assistants, and mental health assistance. However,\nexisting SER methods often overlook the information gap between the\npre-training speech recognition task and the downstream SER task, resulting in\nsub-optimal performance. Moreover, current methods require much time for\nfine-tuning on each specific speech dataset, such as IEMOCAP, which limits\ntheir effectiveness in real-world scenarios with large-scale noisy data. To\naddress these issues, we propose an active learning (AL)-based fine-tuning\nframework for SER, called \\textsc{After}, that leverages task adaptation\npre-training (TAPT) and AL methods to enhance performance and efficiency.\nSpecifically, we first use TAPT to minimize the information gap between the\npre-training speech recognition task and the downstream speech emotion\nrecognition task. Then, AL methods are employed to iteratively select a subset\nof the most informative and diverse samples for fine-tuning, thereby reducing\ntime consumption. Experiments demonstrate that our proposed method\n\\textsc{After}, using only 20\\% of samples, improves accuracy by 8.45\\% and\nreduces time consumption by 79\\%. The additional extension of \\textsc{After}\nand ablation studies further confirm its effectiveness and applicability to\nvarious real-world scenarios. Our source code is available on Github for\nreproducibility. (https://github.com/Clearloveyuan/AFTER).", "paper_summary_zh": "\u8a9e\u97f3\u60c5\u611f\u8fa8\u8b58 (SER) \u7531\u65bc\u5176\u5728\u5404\u7a2e\u9818\u57df\u7684\u5ee3\u6cdb\u61c9\u7528\u800c\u5099\u53d7\u95dc\u6ce8\uff0c\u5305\u62ec\u4eba\u6a5f\u4e92\u52d5\u3001\u865b\u64ec\u52a9\u7406\u548c\u5fc3\u7406\u5065\u5eb7\u5354\u52a9\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 SER \u65b9\u6cd5\u7d93\u5e38\u5ffd\u7565\u9810\u8a13\u7df4\u8a9e\u97f3\u8fa8\u8b58\u4efb\u52d9\u548c\u4e0b\u6e38 SER \u4efb\u52d9\u4e4b\u9593\u7684\u8cc7\u8a0a\u5dee\u8ddd\uff0c\u5c0e\u81f4\u6b21\u4f73\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u76ee\u524d\u7684\u8a31\u591a\u65b9\u6cd5\u9700\u8981\u82b1\u8cbb\u5927\u91cf\u6642\u9593\u5fae\u8abf\u6bcf\u500b\u7279\u5b9a\u7684\u8a9e\u97f3\u8cc7\u6599\u96c6\uff0c\u4f8b\u5982 IEMOCAP\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u5728\u5177\u6709\u5927\u898f\u6a21\u96dc\u8a0a\u8cc7\u6599\u7684\u771f\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u4e3b\u52d5\u5b78\u7fd2 (AL) \u7684 SER \u5fae\u8abf\u67b6\u69cb\uff0c\u7a31\u70ba \\textsc{After}\uff0c\u5b83\u5229\u7528\u4efb\u52d9\u9069\u61c9\u9810\u8a13\u7df4 (TAPT) \u548c AL \u65b9\u6cd5\u4f86\u589e\u5f37\u6548\u80fd\u548c\u6548\u7387\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528 TAPT \u4f86\u6700\u5c0f\u5316\u9810\u8a13\u7df4\u8a9e\u97f3\u8fa8\u8b58\u4efb\u52d9\u548c\u4e0b\u6e38\u8a9e\u97f3\u60c5\u611f\u8fa8\u8b58\u4efb\u52d9\u4e4b\u9593\u7684\u8cc7\u8a0a\u5dee\u8ddd\u3002\u7136\u5f8c\uff0c\u63a1\u7528 AL \u65b9\u6cd5\u53cd\u8986\u9078\u64c7\u6700\u5177\u8cc7\u8a0a\u6027\u548c\u591a\u6a23\u6027\u7684\u5b50\u96c6\u6a23\u672c\u9032\u884c\u5fae\u8abf\uff0c\u5f9e\u800c\u6e1b\u5c11\u6642\u9593\u6d88\u8017\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5 \\textsc{After}\uff0c\u50c5\u4f7f\u7528 20% \u7684\u6a23\u672c\uff0c\u5c31\u80fd\u5c07\u6e96\u78ba\u5ea6\u63d0\u9ad8 8.45%\uff0c\u4e26\u5c07\u6642\u9593\u6d88\u8017\u6e1b\u5c11 79%\u3002\\textsc{After} \u7684\u984d\u5916\u5ef6\u4f38\u548c\u6d88\u878d\u7814\u7a76\u9032\u4e00\u6b65\u8b49\u5be6\u4e86\u5176\u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u9069\u7528\u6027\u3002\u6211\u5011\u7684\u539f\u59cb\u7a0b\u5f0f\u78bc\u53ef\u5728 Github \u4e0a\u53d6\u5f97\uff0c\u4ee5\u4f9b\u91cd\u73fe\u3002(https://github.com/Clearloveyuan/AFTER)\u3002", "author": "Dongyuan Li et.al.", "authors": "Dongyuan Li, Ying Zhang, Yusong Wang, Funakoshi Kataro, Manabu Okumura", "id": "2405.00307v1", "paper_url": "http://arxiv.org/abs/2405.00307v1", "repo": "https://github.com/clearloveyuan/after"}}