{"2412.17574": {"publish_time": "2024-12-23", "title": "HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data", "paper_summary": "In the domain of Multimodal Large Language Models (MLLMs), achieving\nhuman-centric video understanding remains a formidable challenge. Existing\nbenchmarks primarily emphasize object and action recognition, often neglecting\nthe intricate nuances of human emotions, behaviors, and speech visual alignment\nwithin video content. We present HumanVBench, an innovative benchmark\nmeticulously crafted to bridge these gaps in the evaluation of video MLLMs.\nHumanVBench comprises 17 carefully designed tasks that explore two primary\ndimensions: inner emotion and outer manifestations, spanning static and\ndynamic, basic and complex, as well as single-modal and cross-modal aspects.\nWith two advanced automated pipelines for video annotation and\ndistractor-included QA generation, HumanVBench utilizes diverse\nstate-of-the-art (SOTA) techniques to streamline benchmark data synthesis and\nquality assessment, minimizing human annotation dependency tailored to\nhuman-centric multimodal attributes. A comprehensive evaluation across 16 SOTA\nvideo MLLMs reveals notable limitations in current performance, especially in\ncross-modal and temporal alignment, underscoring the necessity for further\nrefinement toward achieving more human-like understanding. HumanVBench is\nopen-sourced to facilitate future advancements and real-world applications in\nvideo MLLMs.", "paper_summary_zh": "\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u9886\u57df\uff0c\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u672c\u7684\u89c6\u9891\u7406\u89e3\u4ecd\u7136\u662f\u4e00\u9879\u8270\u5de8\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u5f3a\u8c03\u5bf9\u8c61\u548c\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u5e38\u5ffd\u7565\u4e86\u89c6\u9891\u5185\u5bb9\u4e2d\u4eba\u7c7b\u60c5\u611f\u3001\u884c\u4e3a\u548c\u8a00\u8bed\u89c6\u89c9\u5bf9\u9f50\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u6211\u4eec\u63d0\u51fa\u4e86 HumanVBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u521b\u65b0\u57fa\u51c6\uff0c\u65e8\u5728\u5f25\u5408\u89c6\u9891 MLLM \u8bc4\u4f30\u4e2d\u7684\u8fd9\u4e9b\u5dee\u8ddd\u3002HumanVBench \u5305\u542b 17 \u9879\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u63a2\u7d22\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff1a\u5185\u5728\u60c5\u611f\u548c\u5916\u5728\u8868\u73b0\uff0c\u6db5\u76d6\u9759\u6001\u548c\u52a8\u6001\u3001\u57fa\u672c\u548c\u590d\u6742\u4ee5\u53ca\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u65b9\u9762\u3002\u501f\u52a9\u4e24\u4e2a\u7528\u4e8e\u89c6\u9891\u6ce8\u91ca\u548c\u5305\u542b\u5e72\u6270\u9879\u7684 QA \u751f\u6210\u7684\u5148\u8fdb\u81ea\u52a8\u5316\u7ba1\u9053\uff0cHumanVBench \u5229\u7528\u4e86\u591a\u79cd\u6700\u5148\u8fdb (SOTA) \u6280\u672f\u6765\u7b80\u5316\u57fa\u51c6\u6570\u636e\u5408\u6210\u548c\u8d28\u91cf\u8bc4\u4f30\uff0c\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u4e86\u9488\u5bf9\u4ee5\u4eba\u4e3a\u672c\u7684\u591a\u6a21\u6001\u5c5e\u6027\u7684\u4eba\u5de5\u6ce8\u91ca\u4f9d\u8d56\u6027\u3002\u5bf9 16 \u4e2a SOTA \u89c6\u9891 MLLM \u7684\u5168\u9762\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524d\u6027\u80fd\u7684\u663e\u7740\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8de8\u6a21\u6001\u548c\u65f6\u95f4\u5bf9\u9f50\u65b9\u9762\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u5b8c\u5584\u4ee5\u5b9e\u73b0\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u7406\u89e3\u7684\u5fc5\u8981\u6027\u3002HumanVBench \u662f\u5f00\u6e90\u7684\uff0c\u65e8\u5728\u4fc3\u8fdb\u89c6\u9891 MLLM \u4e2d\u7684\u672a\u6765\u8fdb\u6b65\u548c\u5b9e\u9645\u5e94\u7528\u3002", "author": "Ting Zhou et.al.", "authors": "Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen", "id": "2412.17574v1", "paper_url": "http://arxiv.org/abs/2412.17574v1", "repo": "null"}}