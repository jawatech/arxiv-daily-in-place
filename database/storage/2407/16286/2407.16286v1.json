{"2407.16286": {"publish_time": "2024-07-23", "title": "A deeper look at depth pruning of LLMs", "paper_summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e0d\u50c5\u8a13\u7df4\u8cc7\u6e90\u5bc6\u96c6\uff0c\u90e8\u7f72\u81f3\u751f\u7522\u74b0\u5883\u7684\u6210\u672c\u66f4\u9ad8\u3002\u56e0\u6b64\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5617\u8a66\u6839\u64da\u4f30\u8a08\u5340\u584a\u91cd\u8981\u6027\u7684\u5ec9\u50f9\u4ee3\u7406\u4f86\u4fee\u526a LLM \u5340\u584a\uff0c\u6709\u6548\u5730\u79fb\u9664\u4e86\u8a13\u7df4\u826f\u597d\u7684 LLaMa-2 \u548c Mistral 7b \u6a21\u578b\u4e2d 10% \u7684\u5340\u584a\uff0c\u800c\u4e0d\u6703\u986f\u8457\u964d\u4f4e\u4e0b\u6e38\u6307\u6a19\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9664\u4e86\u63a2\u8a0e\u5148\u524d\u7814\u7a76\u4e2d\u63a2\u7d22\u7684\u975c\u614b\u6307\u6a19\u5916\uff0c\u9084\u8003\u616e\u4e86 Shapley \u503c\u7b49\u81ea\u9069\u61c9\u6307\u6a19\uff0c\u4f86\u63a2\u7d22\u4e0d\u540c\u7684\u5340\u584a\u91cd\u8981\u6027\u6307\u6a19\u3002\u6211\u5011\u5c55\u793a\u4e86\u81ea\u9069\u61c9\u6307\u6a19\u5728\u4efb\u52d9\u4e4b\u9593\u7684\u6027\u80fd\u6b0a\u8861\uff0c\u5373\u5728\u4e00\u500b\u4efb\u52d9\u4e0a\u7684\u6539\u9032\u53ef\u80fd\u6703\u7531\u65bc\u8a08\u7b97\u7684\u5340\u584a\u5f71\u97ff\u7684\u5dee\u7570\u800c\u964d\u4f4e\u5728\u53e6\u4e00\u500b\u4efb\u52d9\u4e0a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u6b64\u5206\u6790\u5f9e\u4e00\u500b\u5b8c\u6574\u5340\u584a\u64f4\u5c55\u5230\u500b\u5225\u81ea\u6ce8\u610f\u529b\u548c\u524d\u994b\u5c64\uff0c\u7a81\u51fa\u4e86\u81ea\u6ce8\u610f\u529b\u5c64\u66f4\u5bb9\u6613\u9032\u884c\u4fee\u526a\u7684\u50be\u5411\uff0c\u751a\u81f3\u5141\u8a31\u79fb\u9664\u591a\u9054 33% \u7684\u81ea\u6ce8\u610f\u529b\u5c64\uff0c\u800c\u4e0d\u6703\u5c0d Mistral 7b \u7684 MMLU \u9020\u6210\u4efb\u4f55\u6027\u80fd\u4e0b\u964d\uff08\u5927\u5e45\u6e1b\u5c11 KV \u5feb\u53d6\u7684\u6602\u8cb4\u7dad\u8b77\uff09\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u7c21\u55ae\u7684\u6027\u80fd\u6062\u5fa9\u6280\u8853\uff0c\u901a\u904e\u8a13\u7df4\u8f15\u91cf\u7d1a\u52a0\u6027\u504f\u5dee\u6216\u4f4e\u79e9\u7dda\u6027\u9069\u914d\u5668\u4f86\u6a21\u64ec\u4fee\u526a\u7684\u5c64\u3002\u4f7f\u7528\u6a21\u64ec\u66f4\u65b0\u7684\u6027\u80fd\u6062\u5fa9\u907f\u514d\u4e86\u521d\u59cb\u5340\u584a\u7684\u6027\u80fd\u4e0b\u964d\uff08\u5728 MMLU \u4e0a\u7d55\u5c0d\u63d0\u5347\u9ad8\u9054 5%\uff09\uff0c\u9019\u8207\u57fa\u65bc\u5b78\u7fd2\u7684\u6280\u8853\u76f8\u6bd4\u5177\u6709\u7af6\u722d\u529b\u6216\u512a\u52e2\u3002", "author": "Shoaib Ahmed Siddiqui et.al.", "authors": "Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz, David Krueger, Pavlo Molchanov", "id": "2407.16286v1", "paper_url": "http://arxiv.org/abs/2407.16286v1", "repo": "https://github.com/shoaibahmed/llm_depth_pruning"}}