{"2502.15609": {"publish_time": "2025-02-21", "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification", "paper_summary": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u8a9e\u5883\u5b78\u7fd2\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u9810\u6e2c\u53ef\u80fd\u56e0\u4e8b\u5be6\u6b63\u78ba\u7684\u8a9e\u5883\u800c\u4e2d\u65b7\uff0c\u9019\u7a2e\u73fe\u8c61\u7a31\u70ba\u8a9e\u5883\u52ab\u6301\uff0c\u63ed\u793a\u4e86\u4e00\u500b\u91cd\u5927\u7684\u7a69\u5065\u6027\u554f\u984c\u3002\u70ba\u4e86\u5728\u7406\u8ad6\u4e0a\u7406\u89e3\u9019\u7a2e\u73fe\u8c61\uff0c\u6211\u5011\u57fa\u65bc\u7dda\u6027Transformer\u6700\u8fd1\u7684\u9032\u5c55\uff0c\u63a2\u8a0e\u4e86\u4e00\u500b\u8a9e\u5883\u7dda\u6027\u5206\u985e\u554f\u984c\u3002\u5728\u6211\u5011\u7684\u8a2d\u5b9a\u4e2d\uff0c\u8a9e\u5883\u6a19\u8a18\u88ab\u8a2d\u8a08\u6210\u4e8b\u5be6\u6b63\u78ba\u7684\u554f\u7b54\u5c0d\uff0c\u5176\u4e2d\u67e5\u8a62\u8207\u6700\u7d42\u67e5\u8a62\u76f8\u4f3c\uff0c\u4f46\u6a19\u7c64\u76f8\u53cd\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c0d\u7dda\u6027Transformer\u7684\u7a69\u5065\u6027\u9032\u884c\u4e86\u4e00\u822c\u7406\u8ad6\u5206\u6790\uff0c\u8a72\u5206\u6790\u88ab\u8868\u8ff0\u70ba\u6a21\u578b\u6df1\u5ea6\u3001\u8a13\u7df4\u8a9e\u5883\u9577\u5ea6\u548c\u52ab\u6301\u8a9e\u5883\u6a19\u8a18\u6578\u7684\u51fd\u6578\u3002\u4e00\u500b\u95dc\u9375\u767c\u73fe\u662f\uff0c\u8a13\u7df4\u826f\u597d\u7684\u8f03\u6df1Transformer\u53ef\u4ee5\u5be6\u73fe\u66f4\u9ad8\u7684\u7a69\u5065\u6027\uff0c\u9019\u8207\u7d93\u9a57\u89c0\u5bdf\u4e00\u81f4\u3002\u6211\u5011\u8868\u660e\uff0c\u9019\u7a2e\u6539\u9032\u4e4b\u6240\u4ee5\u51fa\u73fe\uff0c\u662f\u56e0\u70ba\u66f4\u6df1\u7684\u5c64\u6b21\u53ef\u4ee5\u5be6\u73fe\u66f4\u7d30\u7c92\u5ea6\u7684\u512a\u5316\u6b65\u9a5f\uff0c\u6709\u6548\u5730\u6e1b\u8f15\u4e86\u8a9e\u5883\u52ab\u6301\u7684\u5e72\u64fe\u3002\u9019\u4e5f\u5f97\u5230\u4e86\u6211\u5011\u7684\u6578\u503c\u5be6\u9a57\u7684\u5145\u5206\u652f\u6301\u3002\u6211\u5011\u7684\u767c\u73fe\u70ba\u66f4\u6df1\u5c64\u67b6\u69cb\u7684\u512a\u9ede\u63d0\u4f9b\u4e86\u7406\u8ad6\u898b\u89e3\uff0c\u4e26\u6709\u52a9\u65bc\u589e\u5f37\u5c0dTransformer\u67b6\u69cb\u7684\u7406\u89e3\u3002", "author": "Tianle Li et.al.", "authors": "Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, Difan Zou", "id": "2502.15609v1", "paper_url": "http://arxiv.org/abs/2502.15609v1", "repo": "null"}}