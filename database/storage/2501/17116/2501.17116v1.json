{"2501.17116": {"publish_time": "2025-01-28", "title": "Optimizing Large Language Model Training Using FP4 Quantization", "paper_summary": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8a13\u7df4\u4e0d\u65b7\u589e\u9577\u7684\u8a08\u7b97\u9700\u6c42\uff0c\u9700\u8981\u66f4\u6709\u6548\u7387\u7684\u65b9\u6cd5\u3002\u91cf\u5316\u8a13\u7df4\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u900f\u904e\u555f\u7528\u4f4e\u4f4d\u5143\u7b97\u8853\u904b\u7b97\u4f86\u964d\u4f4e\u9019\u4e9b\u6210\u672c\u3002\u96d6\u7136 FP8 \u7cbe\u5ea6\u5df2\u8b49\u660e\u53ef\u884c\uff0c\u4f46\u7531\u65bc\u91cf\u5316\u8aa4\u5dee\u986f\u8457\u548c\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u56e0\u6b64\u5229\u7528 FP4 \u4ecd\u7136\u662f\u4e00\u500b\u6311\u6230\u3002\u9019\u9805\u5de5\u4f5c\u5f15\u5165\u4e86 LLM \u7684\u7b2c\u4e00\u500b FP4 \u8a13\u7df4\u67b6\u69cb\uff0c\u900f\u904e\u5169\u500b\u95dc\u9375\u5275\u65b0\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff1a\u4e00\u500b\u53ef\u5fae\u5206\u91cf\u5316\u4f30\u8a08\u5668\uff0c\u7528\u65bc\u7cbe\u78ba\u6b0a\u91cd\u66f4\u65b0\uff0c\u4ee5\u53ca\u4e00\u500b\u7570\u5e38\u503c\u7b9d\u5236\u548c\u88dc\u511f\u7b56\u7565\uff0c\u4ee5\u9632\u6b62\u6fc0\u6d3b\u5d29\u6f70\u3002\u70ba\u4e86\u78ba\u4fdd\u7a69\u5b9a\u6027\uff0c\u8a72\u67b6\u69cb\u6574\u5408\u4e86\u4e00\u500b\u6df7\u5408\u7cbe\u5ea6\u8a13\u7df4\u65b9\u6848\u548c\u5411\u91cf\u5316\u91cf\u5316\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684 FP4 \u67b6\u69cb\u9054\u5230\u4e86\u8207 BF16 \u548c FP8 \u76f8\u7576\u7684\u6e96\u78ba\u5ea6\uff0c\u4e14\u6709\u6700\u5c0f\u7684\u8870\u6e1b\uff0c\u53ef\u6709\u6548\u64f4\u5145\u5230\u5728\u591a\u9054 100B \u500b\u7b26\u865f\u4e0a\u8a13\u7df4\u7684 13B \u53c3\u6578 LLM\u3002\u96a8\u8457\u652f\u63f4 FP4 \u7684\u65b0\u4e00\u4ee3\u786c\u9ad4\u51fa\u73fe\uff0c\u6211\u5011\u7684\u67b6\u69cb\u70ba\u9ad8\u6548\u7684\u8d85\u4f4e\u7cbe\u5ea6\u8a13\u7df4\u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Ruizhe Wang et.al.", "authors": "Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng", "id": "2501.17116v1", "paper_url": "http://arxiv.org/abs/2501.17116v1", "repo": "null"}}