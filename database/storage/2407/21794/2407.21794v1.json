{"2407.21794": {"publish_time": "2024-07-31", "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey", "paper_summary": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.", "paper_summary_zh": "\u5075\u6e2c\u7570\u5e38\u6a23\u672c (OOD) \u5c0d\u65bc\u78ba\u4fdd\u6a5f\u5668\u5b78\u7fd2\u7cfb\u7d71\u7684\u5b89\u5168\u6027\u81f3\u95dc\u91cd\u8981\uff0c\u4e26\u5f62\u5851\u4e86 OOD \u5075\u6e2c\u9818\u57df\u3002\u540c\u6642\uff0c\u9084\u6709\u8a31\u591a\u5176\u4ed6\u554f\u984c\u8207 OOD \u5075\u6e2c\u606f\u606f\u76f8\u95dc\uff0c\u5305\u62ec\u7570\u5e38\u5075\u6e2c (AD)\u3001\u65b0\u7a4e\u6027\u5075\u6e2c (ND)\u3001\u958b\u653e\u96c6\u8b58\u5225 (OSR) \u548c\u96e2\u7fa4\u503c\u5075\u6e2c (OD)\u3002\u70ba\u4e86\u7d71\u4e00\u9019\u4e9b\u554f\u984c\uff0c\u63d0\u51fa\u4e86\u5ee3\u7fa9\u7684 OOD \u5075\u6e2c\u67b6\u69cb\uff0c\u5c07\u9019\u4e94\u500b\u554f\u984c\u5206\u985e\u3002\u7136\u800c\uff0c\u50cf CLIP \u7b49\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5df2\u5927\u5e45\u6539\u8b8a\u5178\u7bc4\uff0c\u4e26\u6a21\u7cca\u4e86\u9019\u4e9b\u9818\u57df\u4e4b\u9593\u7684\u754c\u7dda\uff0c\u518d\u6b21\u8b93\u7814\u7a76\u4eba\u54e1\u611f\u5230\u56f0\u60d1\u3002\u5728\u9019\u9805\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u5ee3\u7fa9\u7684 OOD \u5075\u6e2c v2\uff0c\u6982\u62ec\u4e86 AD\u3001ND\u3001OSR\u3001OOD \u5075\u6e2c\u548c OD \u5728 VLM \u6642\u4ee3\u7684\u6f14\u9032\u3002\u6211\u5011\u7684\u67b6\u69cb\u63ed\u793a\uff0c\u7531\u65bc\u67d0\u4e9b\u9818\u57df\u7684\u4e0d\u6d3b\u8e8d\u548c\u6574\u5408\uff0c\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\u5df2\u6210\u70ba OOD \u5075\u6e2c\u548c AD\u3002\u6b64\u5916\uff0c\u6211\u5011\u4e5f\u91cd\u9ede\u8aaa\u660e\u5b9a\u7fa9\u3001\u554f\u984c\u8a2d\u5b9a\u548c\u57fa\u6e96\u7684\u91cd\u5927\u8f49\u8b8a\uff1b\u56e0\u6b64\uff0c\u6211\u5011\u5c0d OOD \u5075\u6e2c\u7684\u65b9\u6cd5\u8ad6\u9032\u884c\u5168\u9762\u6aa2\u8996\uff0c\u5305\u62ec\u8a0e\u8ad6\u5176\u4ed6\u76f8\u95dc\u4efb\u52d9\u4ee5\u91d0\u6e05\u5b83\u5011\u8207 OOD \u5075\u6e2c\u7684\u95dc\u4fc2\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63a2\u8a0e\u65b0\u8208\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLM) \u6642\u4ee3\u7684\u9032\u5c55\uff0c\u4f8b\u5982 GPT-4V\u3002\u6211\u5011\u4ee5\u958b\u653e\u6311\u6230\u548c\u672a\u4f86\u65b9\u5411\u4f5c\u70ba\u9019\u9805\u8abf\u67e5\u7684\u7d50\u8ad6\u3002", "author": "Atsuyuki Miyai et.al.", "authors": "Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa", "id": "2407.21794v1", "paper_url": "http://arxiv.org/abs/2407.21794v1", "repo": "null"}}