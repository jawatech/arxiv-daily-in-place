{"2412.04003": {"publish_time": "2024-12-05", "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement", "paper_summary": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8fd1\u5e74\u4f86\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff1b\u7136\u800c\uff0c\u5176\u5353\u8d8a\u7684\u6548\u80fd\u4ecd\u7136\u4e3b\u8981\u4fb7\u9650\u65bc\u4e3b\u8981\u4e16\u754c\u8a9e\u8a00\uff0c\u7279\u5225\u662f\u82f1\u8a9e\u3002\u8a31\u591a LLM \u6301\u7e8c\u9762\u81e8\u591a\u8a9e\u8a00\u4efb\u52d9\u7684\u6311\u6230\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u65b9\u9762\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 Marco-LLM\uff1a\u7528\u65bc\u8de8\u8a9e\u8a00\u589e\u5f37 LLM \u7684\u5927\u578b\u591a\u8a9e\u8a00\u8a13\u7df4\u3002\u6211\u5011\u6536\u96c6\u4e86\u5927\u91cf\u4f86\u81ea\u591a\u7a2e\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7684\u591a\u8a9e\u8a00\u8cc7\u6599\uff0c\u4e26\u4f7f\u7528 Qwen2 \u6a21\u578b\u9032\u884c\u5ee3\u6cdb\u7684\u6301\u7e8c\u9810\u8a13\u7df4\u3002\u9019\u9805\u5de5\u4f5c\u7522\u751f\u4e86\u4e00\u500b\u540d\u70ba Marco-LLM \u7684\u591a\u8a9e\u8a00 LLM\u3002\u900f\u904e\u5728\u5404\u7a2e\u591a\u8a9e\u8a00\u57fa\u6e96\u4e0a\u9032\u884c\u5168\u9762\u8a55\u4f30\uff0c\u5305\u62ec MMMLU\u3001AGIEval\u3001Belebele\u3001Flores-200\u3001XCOPA \u4ee5\u53ca\u8a31\u591a\u5176\u4ed6\u57fa\u6e96\uff0cMarco-LLM \u5df2\u8b49\u660e\u6bd4\u73fe\u6709\u7684 LLM \u6709\u986f\u8457\u7684\u9032\u6b65\u3002\u6b64\u5916\uff0cMarco-LLM \u5728\u4efb\u4f55\u5230\u4efb\u4f55\u6a5f\u5668\u7ffb\u8b6f\u4efb\u52d9\u4e2d\u90fd\u7372\u5f97\u4e86\u986f\u8457\u7684\u589e\u5f37\uff0c\u986f\u793a\u4e86\u6211\u5011\u591a\u8a9e\u8a00 LLM \u7684\u6709\u6548\u6027\u3002Marco-LLM \u662f\u4e00\u500b\u5275\u65b0\u7684\u591a\u8a9e\u8a00 LLM\uff0c\u4e0d\u50c5\u5728\u591a\u8a9e\u8a00\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u5305\u62ec\u4f4e\u8cc7\u6e90\u8a9e\u8a00\uff0c\u800c\u4e14\u5728\u82f1\u8a9e\u548c\u5176\u4ed6\u4e3b\u8981\u8a9e\u8a00\u4e2d\u4e5f\u80fd\u4fdd\u6301\u5f37\u52c1\u7684\u6548\u80fd\uff0c\u7e2e\u5c0f\u4e86\u9ad8\u8cc7\u6e90\u548c\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u80fd\u529b\u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u3002\u900f\u904e\u5efa\u7acb\u8a9e\u8a00\u6a4b\u6881\uff0c\u9019\u9805\u5de5\u4f5c\u8b49\u660e\u4e86\u6211\u5011\u81f4\u529b\u65bc\u78ba\u4fdd LLM \u80fd\u6e96\u78ba\u5730\u8de8\u8d8a\u5404\u7a2e\u8a9e\u8a00\u904b\u4f5c\u3002", "author": "Lingfeng Ming et.al.", "authors": "Lingfeng Ming, Bo Zeng, Chenyang Lyu, Tianqi Shi, Yu Zhao, Xue Yang, Yefeng Liu, Yiyu Wang, Linlong Xu, Yangyang Liu, Xiaohu Zhao, Hao Wang, Heng Liu, Hao Zhou, Huifeng Yin, Zifu Shang, Haijun Li, Longyue Wang, Weihua Luo, Kaifu Zhang", "id": "2412.04003v1", "paper_url": "http://arxiv.org/abs/2412.04003v1", "repo": "null"}}