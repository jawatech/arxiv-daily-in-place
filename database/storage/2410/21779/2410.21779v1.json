{"2410.21779": {"publish_time": "2024-10-29", "title": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach", "paper_summary": "Large Language Models (LLMs) have exhibited remarkable potential across a\nwide array of reasoning tasks, including logical reasoning. Although massive\nefforts have been made to empower the logical reasoning ability of LLMs via\nexternal logical symbolic solvers, crucial challenges of the poor\ngeneralization ability to questions with different features and inevitable\nquestion information loss of symbolic solver-driven approaches remain\nunresolved. To mitigate these issues, we introduce LINA, a LLM-driven\nneuro-symbolic approach for faithful logical reasoning. By enabling an LLM to\nautonomously perform the transition from propositional logic extraction to\nsophisticated logical reasoning, LINA not only bolsters the resilience of the\nreasoning process but also eliminates the dependency on external solvers.\nAdditionally, through its adoption of a hypothetical-deductive reasoning\nparadigm, LINA effectively circumvents the expansive search space challenge\nthat plagues traditional forward reasoning methods. Empirical evaluations\ndemonstrate that LINA substantially outperforms both established propositional\nlogic frameworks and conventional prompting techniques across a spectrum of\nfive logical reasoning tasks. Specifically, LINA achieves an improvement of\n24.34% over LINC on the FOLIO dataset, while also surpassing prompting\nstrategies like CoT and CoT-SC by up to 24.02%. Our code is available at\nhttps://github.com/wufeiwuwoshihua/nshy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5ee3\u6cdb\u7684\u63a8\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6f5b\u529b\uff0c\u5305\u62ec\u908f\u8f2f\u63a8\u7406\u3002\u5118\u7ba1\u5df2\u6295\u5165\u5927\u91cf\u5fc3\u529b\u900f\u904e\u5916\u90e8\u908f\u8f2f\u7b26\u865f\u6c42\u89e3\u5668\u63d0\u5347 LLM \u7684\u908f\u8f2f\u63a8\u7406\u80fd\u529b\uff0c\u7b26\u865f\u6c42\u89e3\u5668\u9a45\u52d5\u65b9\u6cd5\u7684\u95dc\u9375\u6311\u6230\u5728\u65bc\u5c0d\u5177\u5099\u4e0d\u540c\u7279\u5fb5\u554f\u984c\u7684\u6982\u5316\u80fd\u529b\u4e0d\u4f73\uff0c\u4ee5\u53ca\u4e0d\u53ef\u907f\u514d\u7684\u984c\u76ee\u8cc7\u8a0a\u907a\u5931\uff0c\u9019\u4e9b\u554f\u984c\u4ecd\u672a\u7372\u5f97\u89e3\u6c7a\u3002\u70ba\u4e86\u7de9\u89e3\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 LINA\uff0c\u4e00\u7a2e\u7531 LLM \u9a45\u52d5\u7684\u795e\u7d93\u7b26\u865f\u65b9\u6cd5\uff0c\u7528\u65bc\u5fe0\u5be6\u7684\u908f\u8f2f\u63a8\u7406\u3002\u900f\u904e\u8b93 LLM \u81ea\u4e3b\u57f7\u884c\u5f9e\u547d\u984c\u908f\u8f2f\u8403\u53d6\u5230\u7cbe\u5bc6\u7684\u908f\u8f2f\u63a8\u7406\u7684\u8f49\u63db\uff0cLINA \u4e0d\u50c5\u589e\u5f37\u4e86\u63a8\u7406\u904e\u7a0b\u7684\u5fa9\u539f\u529b\uff0c\u4e5f\u6d88\u9664\u4e86\u5c0d\u5916\u90e8\u6c42\u89e3\u5668\u7684\u4f9d\u8cf4\u6027\u3002\u6b64\u5916\uff0c\u900f\u904e\u63a1\u7528\u5047\u8a2d\u6f14\u7e79\u63a8\u7406\u7bc4\u4f8b\uff0cLINA \u6709\u6548\u5730\u8ff4\u907f\u4e86\u56f0\u64fe\u50b3\u7d71\u524d\u5411\u63a8\u7406\u65b9\u6cd5\u7684\u5ee3\u6cdb\u641c\u5c0b\u7a7a\u9593\u6311\u6230\u3002\u5be6\u8b49\u8a55\u4f30\u986f\u793a\uff0cLINA \u5728\u4e94\u9805\u908f\u8f2f\u63a8\u7406\u4efb\u52d9\u4e2d\u90fd\u5927\u5e45\u512a\u65bc\u65e2\u6709\u7684\u547d\u984c\u908f\u8f2f\u67b6\u69cb\u548c\u6163\u7528\u7684\u63d0\u793a\u6280\u8853\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cLINA \u5728 FOLIO \u8cc7\u6599\u96c6\u4e0a\u6bd4 LINC \u63d0\u5347\u4e86 24.34%\uff0c\u540c\u6642\u4e5f\u6bd4 CoT \u548c CoT-SC \u7b49\u63d0\u793a\u7b56\u7565\u9ad8\u51fa 24.02%\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/wufeiwuwoshihua/nshy \u53d6\u5f97\u3002", "author": "Qingchuan Li et.al.", "authors": "Qingchuan Li, Jiatong Li, Tongxuan Liu, Yuting Zeng, Mingyue Cheng, Weizhe Huang, Qi Liu", "id": "2410.21779v1", "paper_url": "http://arxiv.org/abs/2410.21779v1", "repo": "https://github.com/wufeiwuwoshihua/nshy"}}