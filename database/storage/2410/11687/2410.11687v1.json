{"2410.11687": {"publish_time": "2024-10-15", "title": "State-space models can learn in-context by gradient descent", "paper_summary": "Deep state-space models (Deep SSMs) have shown capabilities for in-context\nlearning on autoregressive tasks, similar to transformers. However, the\narchitectural requirements and mechanisms enabling this in recurrent networks\nremain unclear. This study demonstrates that state-space model architectures\ncan perform gradient-based learning and use it for in-context learning. We\nprove that a single structured state-space model layer, augmented with local\nself-attention, can reproduce the outputs of an implicit linear model with\nleast squares loss after one step of gradient descent. Our key insight is that\nthe diagonal linear recurrent layer can act as a gradient accumulator, which\ncan be `applied' to the parameters of the implicit regression model. We\nvalidate our construction by training randomly initialized augmented SSMs on\nsimple linear regression tasks. The empirically optimized parameters match the\ntheoretical ones, obtained analytically from the implicit model construction.\nExtensions to multi-step linear and non-linear regression yield consistent\nresults. The constructed SSM encompasses features of modern deep state-space\nmodels, with the potential for scalable training and effectiveness even in\ngeneral tasks. The theoretical construction elucidates the role of local\nself-attention and multiplicative interactions in recurrent architectures as\nthe key ingredients for enabling the expressive power typical of foundation\nmodels.", "paper_summary_zh": "\u6df1\u5ea6\u72c0\u614b\u7a7a\u9593\u6a21\u578b (Deep SSM) \u5df2\u5c55\u73fe\u51fa\u985e\u4f3c\u65bc\u8f49\u63db\u5668\u7684\u81ea\u52d5\u8ff4\u6b78\u4efb\u52d9\u7684\u8a9e\u5883\u5b78\u7fd2\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u905e\u8ff4\u7db2\u8def\u4e2d\u555f\u7528\u6b64\u529f\u80fd\u7684\u67b6\u69cb\u9700\u6c42\u548c\u6a5f\u5236\u4ecd\u4e0d\u660e\u78ba\u3002\u672c\u7814\u7a76\u8b49\u660e\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u67b6\u69cb\u53ef\u4ee5\u57f7\u884c\u57fa\u65bc\u68af\u5ea6\u7684\u5b78\u7fd2\uff0c\u4e26\u5c07\u5176\u7528\u65bc\u8a9e\u5883\u5b78\u7fd2\u3002\u6211\u5011\u8b49\u660e\uff0c\u4e00\u500b\u55ae\u4e00\u7684\u7d50\u69cb\u5316\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u5c64\uff0c\u52a0\u4e0a\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u5728\u68af\u5ea6\u4e0b\u964d\u4e00\u6b65\u5f8c\uff0c\u4f7f\u7528\u6700\u5c0f\u5e73\u65b9\u640d\u5931\u8907\u88fd\u96b1\u5f0f\u7dda\u6027\u6a21\u578b\u7684\u8f38\u51fa\u3002\u6211\u5011\u7684\u95dc\u9375\u898b\u89e3\u662f\uff0c\u5c0d\u89d2\u7dda\u6027\u905e\u8ff4\u5c64\u53ef\u4ee5\u5145\u7576\u68af\u5ea6\u7d2f\u52a0\u5668\uff0c\u53ef\u4ee5\u5c07\u5176\u300c\u61c9\u7528\u300d\u5230\u96b1\u5f0f\u56de\u6b78\u6a21\u578b\u7684\u53c3\u6578\u3002\u6211\u5011\u901a\u904e\u5728\u7c21\u55ae\u7dda\u6027\u56de\u6b78\u4efb\u52d9\u4e0a\u8a13\u7df4\u96a8\u6a5f\u521d\u59cb\u5316\u7684\u64f4\u5145 SSM \u4f86\u9a57\u8b49\u6211\u5011\u7684\u5efa\u69cb\u3002\u7d93\u9a57\u512a\u5316\u7684\u53c3\u6578\u8207\u7406\u8ad6\u53c3\u6578\u76f8\u5339\u914d\uff0c\u5f9e\u96b1\u5f0f\u6a21\u578b\u5efa\u69cb\u4e2d\u5206\u6790\u7372\u5f97\u3002\u5c0d\u591a\u6b65\u7dda\u6027\u548c\u975e\u7dda\u6027\u56de\u6b78\u7684\u64f4\u5c55\u7522\u751f\u4e00\u81f4\u7684\u7d50\u679c\u3002\u5efa\u69cb\u7684 SSM \u6db5\u84cb\u4e86\u73fe\u4ee3\u6df1\u5ea6\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u7684\u7279\u5fb5\uff0c\u5373\u4f7f\u5728\u4e00\u822c\u4efb\u52d9\u4e2d\u4e5f\u5177\u6709\u53ef\u64f4\u5c55\u8a13\u7df4\u548c\u6709\u6548\u6027\u7684\u6f5b\u529b\u3002\u7406\u8ad6\u5efa\u69cb\u95e1\u660e\u4e86\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\u548c\u905e\u8ff4\u67b6\u69cb\u4e2d\u7684\u4e58\u6cd5\u4ea4\u4e92\u4f5c\u7528\u4f5c\u70ba\u555f\u7528\u57fa\u790e\u6a21\u578b\u5178\u578b\u8868\u9054\u80fd\u529b\u7684\u95dc\u9375\u8981\u7d20\u3002", "author": "Neeraj Mohan Sushma et.al.", "authors": "Neeraj Mohan Sushma, Yudou Tian, Harshvardhan Mestha, Nicolo Colombo, David Kappel, Anand Subramoney", "id": "2410.11687v1", "paper_url": "http://arxiv.org/abs/2410.11687v1", "repo": "null"}}