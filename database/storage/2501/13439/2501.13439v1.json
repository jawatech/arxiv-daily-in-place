{"2501.13439": {"publish_time": "2025-01-23", "title": "One-cycle Structured Pruning with Stability Driven Structure Search", "paper_summary": "Existing structured pruning typically involves multi-stage training\nprocedures that often demand heavy computation. Pruning at initialization,\nwhich aims to address this limitation, reduces training costs but struggles\nwith performance. To address these challenges, we propose an efficient\nframework for one-cycle structured pruning without compromising model\nperformance. In this approach, we integrate pre-training, pruning, and\nfine-tuning into a single training cycle, referred to as the `one cycle\napproach'. The core idea is to search for the optimal sub-network during the\nearly stages of network training, guided by norm-based group saliency criteria\nand structured sparsity regularization. We introduce a novel pruning indicator\nthat determines the stable pruning epoch by assessing the similarity between\nevolving pruning sub-networks across consecutive training epochs. Also, group\nsparsity regularization helps to accelerate the pruning process and results in\nspeeding up the entire process. Extensive experiments on datasets, including\nCIFAR-10/100, and ImageNet, using VGGNet, ResNet, MobileNet, and ViT\narchitectures, demonstrate that our method achieves state-of-the-art accuracy\nwhile being one of the most efficient pruning frameworks in terms of training\ntime. The source code will be made publicly available.", "paper_summary_zh": "\u73fe\u6709\u7684\u7d50\u69cb\u5316\u526a\u679d\u901a\u5e38\u6d89\u53ca\u591a\u968e\u6bb5\u8a13\u7df4\u7a0b\u5e8f\uff0c\u9019\u4e9b\u7a0b\u5e8f\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u3002\u521d\u59cb\u5316\u6642\u9032\u884c\u526a\u679d\uff0c\u65e8\u5728\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u53ef\u6e1b\u5c11\u8a13\u7df4\u6210\u672c\uff0c\u4f46\u6703\u5f71\u97ff\u6548\u80fd\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6709\u6548\u7387\u7684\u6846\u67b6\uff0c\u7528\u65bc\u9032\u884c\u4e00\u5faa\u74b0\u7d50\u69cb\u5316\u526a\u679d\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u6a21\u578b\u6548\u80fd\u3002\u5728\u6b64\u65b9\u6cd5\u4e2d\uff0c\u6211\u5011\u5c07\u9810\u8a13\u7df4\u3001\u526a\u679d\u548c\u5fae\u8abf\u6574\u5408\u5230\u4e00\u500b\u8a13\u7df4\u9031\u671f\u4e2d\uff0c\u7a31\u70ba\u300c\u4e00\u5faa\u74b0\u65b9\u6cd5\u300d\u3002\u6838\u5fc3\u6982\u5ff5\u662f\u5728\u7db2\u8def\u8a13\u7df4\u7684\u65e9\u671f\u968e\u6bb5\u641c\u5c0b\u6700\u4f73\u5b50\u7db2\u8def\uff0c\u4e26\u4ee5\u57fa\u65bc\u6a19\u6e96\u7684\u7fa4\u7d44\u986f\u8457\u6027\u6e96\u5247\u548c\u7d50\u69cb\u5316\u7a00\u758f\u6b63\u898f\u5316\u4f5c\u70ba\u6307\u5c0e\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u526a\u679d\u6307\u6a19\uff0c\u900f\u904e\u8a55\u4f30\u9023\u7e8c\u8a13\u7df4\u9031\u671f\u4e2d\u4e0d\u65b7\u6f14\u5316\u7684\u526a\u679d\u5b50\u7db2\u8def\u4e4b\u9593\u7684\u76f8\u4f3c\u6027\uff0c\u4f86\u78ba\u5b9a\u7a69\u5b9a\u7684\u526a\u679d\u9031\u671f\u3002\u6b64\u5916\uff0c\u7fa4\u7d44\u7a00\u758f\u6b63\u898f\u5316\u6709\u52a9\u65bc\u52a0\u901f\u526a\u679d\u6d41\u7a0b\uff0c\u4e26\u52a0\u5feb\u6574\u500b\u6d41\u7a0b\u3002\u5728\u8cc7\u6599\u96c6\uff08\u5305\u62ec\u4f7f\u7528 VGGNet\u3001ResNet\u3001MobileNet \u548c ViT \u67b6\u69cb\u7684 CIFAR-10/100 \u548c ImageNet\uff09\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u8a13\u7df4\u6642\u9593\u65b9\u9762\u662f\u6700\u6709\u6548\u7387\u7684\u526a\u679d\u6846\u67b6\u4e4b\u4e00\uff0c\u540c\u6642\u9054\u5230\u6700\u5148\u9032\u7684\u6e96\u78ba\u5ea6\u3002\u539f\u59cb\u7a0b\u5f0f\u78bc\u5c07\u516c\u958b\u63d0\u4f9b\u3002", "author": "Deepak Ghimire et.al.", "authors": "Deepak Ghimire, Dayoung Kil, Seonghwan Jeong, Jaesik Park, Seong-heum Kim", "id": "2501.13439v1", "paper_url": "http://arxiv.org/abs/2501.13439v1", "repo": "null"}}