{"2412.10117": {"publish_time": "2024-12-13", "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models", "paper_summary": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.", "paper_summary_zh": "\u5728\u6211\u5011\u4e4b\u524d\u7684\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CosyVoice\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u76e3\u7763\u96e2\u6563\u8a9e\u97f3\u7b26\u865f\u7684\u591a\u8a9e\u8a00\u8a9e\u97f3\u5408\u6210\u6a21\u578b\u3002\u901a\u904e\u4f7f\u7528\u5169\u500b\u6d41\u884c\u7684\u751f\u6210\u6a21\u578b\uff08\u8a9e\u8a00\u6a21\u578b (LM) \u548c\u6d41\u5339\u914d\uff09\u4f86\u63a1\u7528\u6f38\u9032\u8a9e\u7fa9\u89e3\u78bc\uff0cCosyVoice \u5c55\u793a\u4e86\u8a9e\u5883\u5b78\u7fd2\u4e2d\u8a9e\u8abf\u7684\u81ea\u7136\u6027\u3001\u5167\u5bb9\u7684\u4e00\u81f4\u6027\u548c\u8aaa\u8a71\u8005\u7684\u76f8\u4f3c\u6027\u3002\u6700\u8fd1\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\uff0c\u5176\u4e2d\u8a9e\u97f3\u5408\u6210\u7684\u97ff\u61c9\u5ef6\u9072\u548c\u5be6\u6642\u56e0\u7d20\u5728\u4e92\u52d5\u9ad4\u9a57\u4e2d\u767c\u63ee\u8457\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6539\u9032\u7684\u4e32\u6d41\u8a9e\u97f3\u5408\u6210\u6a21\u578b CosyVoice 2\uff0c\u5b83\u5305\u542b\u4e86\u5168\u9762\u548c\u7cfb\u7d71\u6027\u7684\u6700\u4f73\u5316\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6709\u9650\u6a19\u91cf\u91cf\u5316\u4f86\u6539\u5584\u8a9e\u97f3\u7b26\u865f\u7684\u78bc\u672c\u5229\u7528\u7387\u3002\u5c0d\u65bc\u6587\u5b57\u8a9e\u97f3 LM\uff0c\u6211\u5011\u7c21\u5316\u4e86\u6a21\u578b\u67b6\u69cb\uff0c\u5141\u8a31\u76f4\u63a5\u4f7f\u7528\u9810\u8a13\u7df4\u7684 LLM \u4f5c\u70ba\u4e3b\u5e79\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u5206\u584a\u611f\u77e5\u56e0\u679c\u6d41\u5339\u914d\u6a21\u578b\u4f86\u652f\u63f4\u5404\u7a2e\u5408\u6210\u5834\u666f\uff0c\u5728\u55ae\u4e00\u6a21\u578b\u4e2d\u5be6\u73fe\u4e32\u6d41\u548c\u975e\u4e32\u6d41\u5408\u6210\u3002\u901a\u904e\u5728\u4e00\u500b\u5927\u898f\u6a21\u591a\u8a9e\u8a00\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\uff0cCosyVoice 2 \u5728\u4e32\u6d41\u6a21\u5f0f\u4e0b\u5be6\u73fe\u4e86\u4eba\u985e\u540c\u7b49\u7684\u81ea\u7136\u6027\u3001\u6700\u5c0f\u7684\u97ff\u61c9\u5ef6\u9072\u548c\u5e7e\u4e4e\u7121\u640d\u7684\u5408\u6210\u54c1\u8cea\u3002\u6211\u5011\u9080\u8acb\u8b80\u8005\u5728 https://funaudiollm.github.io/cosyvoice2 \u4e0a\u6536\u807d\u793a\u7bc4\u3002", "author": "Zhihao Du et.al.", "authors": "Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, Jingren Zhou", "id": "2412.10117v1", "paper_url": "http://arxiv.org/abs/2412.10117v1", "repo": "null"}}