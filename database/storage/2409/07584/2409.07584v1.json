{"2409.07584": {"publish_time": "2024-09-11", "title": "DS-ViT: Dual-Stream Vision Transformer for Cross-Task Distillation in Alzheimer's Early Diagnosis", "paper_summary": "In the field of Alzheimer's disease diagnosis, segmentation and\nclassification tasks are inherently interconnected. Sharing knowledge between\nmodels for these tasks can significantly improve training efficiency,\nparticularly when training data is scarce. However, traditional knowledge\ndistillation techniques often struggle to bridge the gap between segmentation\nand classification due to the distinct nature of tasks and different model\narchitectures. To address this challenge, we propose a dual-stream pipeline\nthat facilitates cross-task and cross-architecture knowledge sharing. Our\napproach introduces a dual-stream embedding module that unifies feature\nrepresentations from segmentation and classification models, enabling\ndimensional integration of these features to guide the classification model. We\nvalidated our method on multiple 3D datasets for Alzheimer's disease diagnosis,\ndemonstrating significant improvements in classification performance,\nespecially on small datasets. Furthermore, we extended our pipeline with a\nresidual temporal attention mechanism for early diagnosis, utilizing images\ntaken before the atrophy of patients' brain mass. This advancement shows\npromise in enabling diagnosis approximately six months earlier in mild and\nasymptomatic stages, offering critical time for intervention.", "paper_summary_zh": "\u5728\u963f\u8332\u6d77\u9ed8\u75c7\u8a3a\u65b7\u9818\u57df\u4e2d\uff0c\u5206\u5272\u548c\u5206\u985e\u4efb\u52d9\u672c\u8cea\u4e0a\u662f\u76f8\u4e92\u95dc\u806f\u7684\u3002\u5728\u9019\u4e9b\u4efb\u52d9\u7684\u6a21\u578b\u4e4b\u9593\u5171\u4eab\u77e5\u8b58\u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u8a13\u7df4\u6548\u7387\uff0c\u7279\u5225\u662f\u5728\u8a13\u7df4\u8cc7\u6599\u7a00\u5c11\u7684\u60c5\u6cc1\u4e0b\u3002\u7136\u800c\uff0c\u7531\u65bc\u4efb\u52d9\u7684\u6027\u8cea\u4e0d\u540c\u548c\u6a21\u578b\u67b6\u69cb\u4e0d\u540c\uff0c\u50b3\u7d71\u7684\u77e5\u8b58\u84b8\u993e\u6280\u8853\u901a\u5e38\u96e3\u4ee5\u5f4c\u5408\u5206\u5272\u548c\u5206\u985e\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u96d9\u6d41\u7ba1\u7dda\uff0c\u5b83\u4fc3\u9032\u4e86\u8de8\u4efb\u52d9\u548c\u8de8\u67b6\u69cb\u7684\u77e5\u8b58\u5171\u4eab\u3002\u6211\u5011\u7684\u505a\u6cd5\u5f15\u5165\u4e86\u4e00\u500b\u96d9\u6d41\u5d4c\u5165\u6a21\u7d44\uff0c\u5b83\u7d71\u4e00\u4e86\u5206\u5272\u548c\u5206\u985e\u6a21\u578b\u4e2d\u7684\u7279\u5fb5\u8868\u793a\uff0c\u5f9e\u800c\u80fd\u5920\u5c0d\u9019\u4e9b\u7279\u5fb5\u9032\u884c\u7dad\u5ea6\u6574\u5408\uff0c\u4ee5\u6307\u5c0e\u5206\u985e\u6a21\u578b\u3002\u6211\u5011\u5728\u591a\u500b\u963f\u8332\u6d77\u9ed8\u75c7\u8a3a\u65b7\u7684 3D \u8cc7\u6599\u96c6\u4e0a\u9a57\u8b49\u4e86\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u8b49\u660e\u4e86\u5206\u985e\u6548\u80fd\u6709\u986f\u8457\u7684\u63d0\u5347\uff0c\u7279\u5225\u662f\u5728\u5c0f\u578b\u8cc7\u6599\u96c6\u4e0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528\u5728\u60a3\u8005\u8166\u8cea\u840e\u7e2e\u524d\u62cd\u651d\u7684\u5f71\u50cf\uff0c\u5c07\u6211\u5011\u7684\u7ba1\u7dda\u64f4\u5145\u5957\u4ef6\u4e86\u4e00\u500b\u6b98\u5dee\u6642\u9593\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4ee5\u9032\u884c\u65e9\u671f\u8a3a\u65b7\u3002\u9019\u4e00\u9032\u5c55\u986f\u793a\u51fa\u5728\u8f15\u5ea6\u548c\u7121\u75c7\u72c0\u968e\u6bb5\u63d0\u524d\u5927\u7d04\u516d\u500b\u6708\u9032\u884c\u8a3a\u65b7\u7684\u53ef\u80fd\u6027\uff0c\u70ba\u5e72\u9810\u63d0\u4f9b\u4e86\u95dc\u9375\u6642\u9593\u3002", "author": "Ke Chen et.al.", "authors": "Ke Chen, Yifeng Wang, Yufei Zhou, Haohan Wang", "id": "2409.07584v1", "paper_url": "http://arxiv.org/abs/2409.07584v1", "repo": "null"}}