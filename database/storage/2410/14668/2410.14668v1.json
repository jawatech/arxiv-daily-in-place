{"2410.14668": {"publish_time": "2024-10-18", "title": "MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps", "paper_summary": "Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.", "paper_summary_zh": "\u591a\u6a21\u614b\u601d\u7dad\u93c8\uff08MCoT\uff09\u662f\u4e00\u7a2e\u6d41\u884c\u7684\u63d0\u793a\u7b56\u7565\uff0c\u7528\u65bc\u63d0\u5347\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5404\u7a2e\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002\u5118\u7ba1\u5b83\u5f88\u53d7\u6b61\u8fce\uff0c\u4f46\u5c0d\u65bc\u8a55\u4f30 MCoT \u4e2d\u63a8\u7406\u6b65\u9a5f\u54c1\u8cea\u7684\u81ea\u52d5\u5316\u65b9\u6cd5\u537b\u660e\u986f\u4e0d\u8db3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u591a\u6a21\u614b\u601d\u7dad\u93c8\u8a55\u4f30\uff08MiCEval\uff09\uff0c\u4e00\u500b\u65e8\u5728\u8a55\u4f30\u63a8\u7406\u93c8\u6b63\u78ba\u6027\u7684\u6846\u67b6\uff0c\u65b9\u6cd5\u662f\u8a55\u4f30\u63cf\u8ff0\u548c\u6bcf\u500b\u63a8\u7406\u6b65\u9a5f\u7684\u54c1\u8cea\u3002\u63cf\u8ff0\u7d44\u6210\u7684\u8a55\u4f30\u91cd\u9ede\u5728\u65bc\u5f71\u50cf\u63cf\u8ff0\u7684\u6e96\u78ba\u6027\uff0c\u800c\u63a8\u7406\u6b65\u9a5f\u5247\u8a55\u4f30\u6bcf\u500b\u6b65\u9a5f\u5728\u6839\u64da\u524d\u5e8f\u6b65\u9a5f\u6709\u689d\u4ef6\u7522\u751f\u7684\u54c1\u8cea\u3002MiCEval \u5efa\u7acb\u5728\u4e00\u500b\u7d30\u7dfb\u7684\u8cc7\u6599\u96c6\u4e0a\uff0c\u5176\u4e2d\u5305\u542b\u6839\u64da\u6b63\u78ba\u6027\u3001\u76f8\u95dc\u6027\u548c\u8cc7\u8a0a\u6027\u5c0d\u6bcf\u500b\u6b65\u9a5f\u9032\u884c\u8a55\u5206\u7684\u8a3b\u89e3\u3002\u91dd\u5c0d\u56db\u500b\u6700\u5148\u9032\u7684 MLLM \u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0c\u4f7f\u7528 MiCEval \u9032\u884c\u7684\u9010\u6b65\u8a55\u4f30\u8207\u4eba\u985e\u7684\u5224\u65b7\u66f4\u70ba\u4e00\u81f4\uff0c\u8207\u57fa\u65bc\u9918\u5f26\u76f8\u4f3c\u6027\u6216\u5fae\u8abf\u65b9\u6cd5\u7684\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\u3002MiCEval \u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/alenai97/MiCEval \u4e2d\u627e\u5230\u3002", "author": "Xiongtao Zhou et.al.", "authors": "Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen", "id": "2410.14668v1", "paper_url": "http://arxiv.org/abs/2410.14668v1", "repo": "https://github.com/alenai97/miceval"}}