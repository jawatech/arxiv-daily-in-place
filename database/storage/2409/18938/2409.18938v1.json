{"2409.18938": {"publish_time": "2024-09-27", "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding", "paper_summary": "The integration of Large Language Models (LLMs) with visual encoders has\nrecently shown promising performance in visual understanding tasks, leveraging\ntheir inherent capability to comprehend and generate human-like text for visual\nreasoning. Given the diverse nature of visual data, MultiModal Large Language\nModels (MM-LLMs) exhibit variations in model designing and training for\nunderstanding images, short videos, and long videos. Our paper focuses on the\nsubstantial differences and unique challenges posed by long video understanding\ncompared to static image and short video understanding. Unlike static images,\nshort videos encompass sequential frames with both spatial and within-event\ntemporal information, while long videos consist of multiple events with\nbetween-event and long-term temporal information. In this survey, we aim to\ntrace and summarize the advancements of MM-LLMs from image understanding to\nlong video understanding. We review the differences among various visual\nunderstanding tasks and highlight the challenges in long video understanding,\nincluding more fine-grained spatiotemporal details, dynamic events, and\nlong-term dependencies. We then provide a detailed summary of the advancements\nin MM-LLMs in terms of model design and training methodologies for\nunderstanding long videos. Finally, we compare the performance of existing\nMM-LLMs on video understanding benchmarks of various lengths and discuss\npotential future directions for MM-LLMs in long video understanding.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u8996\u89ba\u7de8\u78bc\u5668\u7684\u6574\u5408\u6700\u8fd1\u5728\u8996\u89ba\u7406\u89e3\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u826f\u597d\u7684\u6548\u80fd\uff0c\u5229\u7528\u5176\u7406\u89e3\u548c\u751f\u6210\u985e\u4eba\u6587\u672c\u7684\u5167\u5728\u80fd\u529b\u9032\u884c\u8996\u89ba\u63a8\u7406\u3002\u8003\u91cf\u5230\u8996\u89ba\u8cc7\u6599\u7684\u591a\u5143\u6027\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MM-LLM) \u5728\u6a21\u578b\u8a2d\u8a08\u548c\u8a13\u7df4\u4e0a\u5c55\u73fe\u51fa\u5dee\u7570\uff0c\u4ee5\u7406\u89e3\u5f71\u50cf\u3001\u77ed\u5f71\u7247\u548c\u9577\u5f71\u7247\u3002\u6211\u5011\u7684\u8ad6\u6587\u91cd\u9ede\u95dc\u6ce8\u8207\u975c\u614b\u5f71\u50cf\u548c\u77ed\u5f71\u7247\u7406\u89e3\u76f8\u6bd4\uff0c\u9577\u5f71\u7247\u7406\u89e3\u6240\u5e36\u4f86\u7684\u986f\u8457\u5dee\u7570\u548c\u7368\u7279\u6311\u6230\u3002\u8207\u975c\u614b\u5f71\u50cf\u4e0d\u540c\uff0c\u77ed\u5f71\u7247\u5305\u542b\u5177\u6709\u7a7a\u9593\u548c\u4e8b\u4ef6\u5167\u6642\u9593\u8cc7\u8a0a\u7684\u9023\u7e8c\u756b\u9762\uff0c\u800c\u9577\u5f71\u7247\u5247\u5305\u542b\u5177\u6709\u4e8b\u4ef6\u9593\u548c\u9577\u671f\u6642\u9593\u8cc7\u8a0a\u7684\u591a\u500b\u4e8b\u4ef6\u3002\u5728\u672c\u6b21\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u8ffd\u8e64\u548c\u7e3d\u7d50 MM-LLM \u5f9e\u5f71\u50cf\u7406\u89e3\u5230\u9577\u5f71\u7247\u7406\u89e3\u7684\u9032\u5c55\u3002\u6211\u5011\u6aa2\u8996\u5404\u7a2e\u8996\u89ba\u7406\u89e3\u4efb\u52d9\u4e4b\u9593\u7684\u5dee\u7570\uff0c\u4e26\u5f37\u8abf\u9577\u5f71\u7247\u7406\u89e3\u4e2d\u7684\u6311\u6230\uff0c\u5305\u62ec\u66f4\u7d30\u7dfb\u7684\u6642\u7a7a\u7d30\u7bc0\u3001\u52d5\u614b\u4e8b\u4ef6\u548c\u9577\u671f\u4f9d\u8cf4\u6027\u3002\u7136\u5f8c\uff0c\u6211\u5011\u91dd\u5c0d\u6a21\u578b\u8a2d\u8a08\u548c\u8a13\u7df4\u65b9\u6cd5\u8a73\u7d30\u7e3d\u7d50 MM-LLM \u5728\u7406\u89e3\u9577\u5f71\u7247\u65b9\u9762\u7684\u9032\u5c55\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6bd4\u8f03\u73fe\u6709 MM-LLM \u5728\u4e0d\u540c\u9577\u5ea6\u7684\u5f71\u7247\u7406\u89e3\u57fa\u6e96\u4e0a\u7684\u6548\u80fd\uff0c\u4e26\u8a0e\u8ad6 MM-LLM \u5728\u9577\u5f71\u7247\u7406\u89e3\u4e2d\u7684\u6f5b\u5728\u672a\u4f86\u65b9\u5411\u3002", "author": "Heqing Zou et.al.", "authors": "Heqing Zou, Tianze Luo, Guiyang Xie, Victor, Zhang, Fengmao Lv, Guangcong Wang, Juanyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang", "id": "2409.18938v1", "paper_url": "http://arxiv.org/abs/2409.18938v1", "repo": "null"}}