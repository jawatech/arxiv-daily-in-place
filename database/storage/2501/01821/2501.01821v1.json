{"2501.01821": {"publish_time": "2025-01-03", "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents", "paper_summary": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex goal-oriented social\ndialogues. Direct Preference Optimization (DPO) has proven effective in\naligning LLM behavior with human preferences across a variety of agent tasks.\nExisting DPO-based approaches for multi-turn interactions are divided into\nturn-level and session-level methods. The turn-level method is overly\nfine-grained, focusing exclusively on individual turns, while session-level\nmethods are too coarse-grained, often introducing training noise. To address\nthese limitations, we propose Segment-Level Direct Preference Optimization\n(SDPO), which focuses on specific key segments within interactions to optimize\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u9a45\u52d5\u7684\u793e\u4ea4\u4ee3\u7406\u53ef\u4ee5\u6a21\u64ec\u4eba\u985e\u793e\u4ea4\u884c\u70ba\uff0c\u4f46\u5728\u8655\u7406\u8907\u96dc\u76ee\u6a19\u5c0e\u5411\u7684\u793e\u4ea4\u5c0d\u8a71\u6642\u537b\u529b\u6709\u672a\u902e\u3002\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff08DPO\uff09\u5df2\u88ab\u8b49\u660e\u80fd\u6709\u6548\u5730\u5c07 LLM \u884c\u70ba\u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\uff0c\u9069\u7528\u65bc\u5404\u7a2e\u4ee3\u7406\u4efb\u52d9\u3002\u73fe\u6709\u7684\u57fa\u65bc DPO \u7684\u591a\u8f2a\u4e92\u52d5\u65b9\u6cd5\u5206\u70ba\u8f2a\u6b21\u7d1a\u5225\u548c\u6703\u8a71\u7d1a\u5225\u65b9\u6cd5\u3002\u8f2a\u6b21\u7d1a\u5225\u65b9\u6cd5\u904e\u65bc\u7d30\u7dfb\uff0c\u50c5\u5c08\u6ce8\u65bc\u500b\u5225\u8f2a\u6b21\uff0c\u800c\u6703\u8a71\u7d1a\u5225\u65b9\u6cd5\u5247\u904e\u65bc\u7c97\u7565\uff0c\u901a\u5e38\u6703\u5f15\u5165\u8a13\u7df4\u566a\u97f3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5340\u6bb5\u7d1a\u5225\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff08SDPO\uff09\uff0c\u5b83\u5c08\u6ce8\u65bc\u4e92\u52d5\u4e2d\u7684\u7279\u5b9a\u95dc\u9375\u5340\u6bb5\uff0c\u4ee5\u6700\u4f73\u5316\u591a\u8f2a\u4ee3\u7406\u884c\u70ba\uff0c\u540c\u6642\u6700\u5c0f\u5316\u8a13\u7df4\u566a\u97f3\u3002\u5728 SOTOPIA \u57fa\u6e96\u4e0a\u7684\u8a55\u4f30\u8868\u660e\uff0c\u7d93\u904e SDPO \u8abf\u6574\u7684\u4ee3\u7406\u59cb\u7d42\u512a\u65bc\u73fe\u6709\u7684\u57fa\u65bc DPO \u7684\u65b9\u6cd5\u548c GPT-4o \u7b49\u5c08\u6709 LLM\uff0c\u9019\u51f8\u986f\u4e86 SDPO \u5728\u63d0\u5347\u57fa\u65bc LLM \u7684\u4ee3\u7406\u7684\u793e\u4ea4\u667a\u80fd\u65b9\u9762\u7684\u6f5b\u529b\u3002\u6211\u5011\u5728 https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO \u4e0a\u767c\u5e03\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u3002", "author": "Aobo Kong et.al.", "authors": "Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, Fei Huang", "id": "2501.01821v1", "paper_url": "http://arxiv.org/abs/2501.01821v1", "repo": "null"}}