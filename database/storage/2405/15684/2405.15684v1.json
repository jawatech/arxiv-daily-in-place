{"2405.15684": {"publish_time": "2024-05-24", "title": "Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models", "paper_summary": "To bridge the gap between vision and language modalities, Multimodal Large\nLanguage Models (MLLMs) usually learn an adapter that converts visual inputs to\nunderstandable tokens for Large Language Models (LLMs). However, most adapters\ngenerate consistent visual tokens, regardless of the specific objects of\ninterest mentioned in the prompt. Since these adapters distribute equal\nattention to every detail in the image and focus on the entire scene, they may\nincrease the cognitive load for LLMs, particularly when processing complex\nscenes. To alleviate this problem, we propose prompt-aware adapters. These\nadapters are designed with the capability to dynamically embed visual inputs\nbased on the specific focus of the prompt. Specifically, prompt-aware adapters\nutilize both global and local textual features to capture the most relevant\nvisual clues from the prompt at both coarse and fine granularity levels. This\napproach significantly enhances the ability of LLMs to understand and interpret\nvisual content. Experiments on various visual question answering tasks, such as\ncounting and position reasoning, demonstrate the effectiveness of prompt-aware\nadapters.", "paper_summary_zh": "\u70ba\u4e86\u5f4c\u5408\u8996\u89ba\u8207\u8a9e\u8a00\u6a21\u614b\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u901a\u5e38\u6703\u5b78\u7fd2\u4e00\u500b\u8f49\u63a5\u5668\uff0c\u5c07\u8996\u89ba\u8f38\u5165\u8f49\u63db\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u7406\u89e3\u7684\u4ee3\u78bc\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u8f49\u63a5\u5668\u6703\u7522\u751f\u4e00\u81f4\u7684\u8996\u89ba\u4ee3\u78bc\uff0c\u800c\u4e0d\u7ba1\u63d0\u793a\u4e2d\u63d0\u5230\u7684\u7279\u5b9a\u611f\u8208\u8da3\u5c0d\u8c61\u70ba\u4f55\u3002\u7531\u65bc\u9019\u4e9b\u8f49\u63a5\u5668\u5c07\u76f8\u540c\u7684\u6ce8\u610f\u529b\u5206\u914d\u7d66\u5f71\u50cf\u4e2d\u7684\u6bcf\u500b\u7d30\u7bc0\uff0c\u4e26\u5c08\u6ce8\u65bc\u6574\u500b\u5834\u666f\uff0c\u56e0\u6b64\u53ef\u80fd\u6703\u589e\u52a0 LLM \u7684\u8a8d\u77e5\u8ca0\u8f09\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\u8907\u96dc\u5834\u666f\u6642\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u63d0\u793a\u611f\u77e5\u8f49\u63a5\u5668\u3002\u9019\u4e9b\u8f49\u63a5\u5668\u88ab\u8a2d\u8a08\u70ba\u5177\u5099\u6839\u64da\u63d0\u793a\u7684\u7279\u5b9a\u7126\u9ede\u52d5\u614b\u5d4c\u5165\u8996\u89ba\u8f38\u5165\u7684\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u63d0\u793a\u611f\u77e5\u8f49\u63a5\u5668\u540c\u6642\u5229\u7528\u5168\u5c40\u548c\u5c40\u90e8\u6587\u5b57\u7279\u5fb5\uff0c\u4ee5\u5728\u7c97\u7565\u548c\u7cbe\u7d30\u7c92\u5ea6\u5c64\u7d1a\u5f9e\u63d0\u793a\u4e2d\u64f7\u53d6\u6700\u76f8\u95dc\u7684\u8996\u89ba\u7dda\u7d22\u3002\u9019\u7a2e\u65b9\u6cd5\u986f\u8457\u589e\u5f37\u4e86 LLM \u7406\u89e3\u548c\u8a6e\u91cb\u8996\u89ba\u5167\u5bb9\u7684\u80fd\u529b\u3002\u5728\u5404\u7a2e\u8996\u89ba\u554f\u984c\u56de\u7b54\u4efb\u52d9\uff08\u4f8b\u5982\u8a08\u6578\u548c\u4f4d\u7f6e\u63a8\u7406\uff09\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u63d0\u793a\u611f\u77e5\u8f49\u63a5\u5668\u7684\u6709\u6548\u6027\u3002", "author": "Yue Zhang et.al.", "authors": "Yue Zhang, Hehe Fan, Yi Yang", "id": "2405.15684v1", "paper_url": "http://arxiv.org/abs/2405.15684v1", "repo": "null"}}