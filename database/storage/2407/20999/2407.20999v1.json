{"2407.20999": {"publish_time": "2024-07-30", "title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning", "paper_summary": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in a wide range of tasks. Typically, an LLM is pre-trained on\nlarge corpora and subsequently fine-tuned on task-specific datasets. However,\nduring finetuning, LLMs may forget the knowledge acquired in the pretraining\nstage, leading to a decline in general capabilities. To address this issue, we\npropose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).\nThe key idea of MoFO is to iteratively select and update the model parameters\nwith the largest momentum magnitudes. Compared to full-parameter training, MoFO\nachieves similar fine-tuning performance while keeping parameters closer to the\npre-trained model, thereby mitigating knowledge forgetting. Unlike most\nexisting methods for forgetting mitigation, MoFO combines the following two\nadvantages. First, MoFO does not require access to pre-training data. This\nmakes MoFO particularly suitable for fine-tuning scenarios where pre-training\ndata is unavailable, such as fine-tuning checkpoint-only open-source LLMs.\nSecond, MoFO does not alter the original loss function. This could avoid\nimpairing the model performance on the fine-tuning tasks. We validate MoFO\nthrough rigorous convergence analysis and extensive experiments, demonstrating\nits superiority over existing methods in mitigating forgetting and enhancing\nfine-tuning performance.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u901a\u5e38\uff0cLLM \u5728\u5927\u578b\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u7136\u800c\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0cLLM \u53ef\u80fd\u4f1a\u5fd8\u8bb0\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u83b7\u5f97\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u5bfc\u81f4\u4e00\u822c\u80fd\u529b\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u7b97\u6cd5\uff0c\u79f0\u4e3a\u52a8\u91cf\u6ee4\u6ce2\u4f18\u5316\u5668 (MoFO)\u3002MoFO \u7684\u5173\u952e\u601d\u60f3\u662f\u8fed\u4ee3\u9009\u62e9\u548c\u66f4\u65b0\u5177\u6709\u6700\u5927\u52a8\u91cf\u5e45\u5ea6\u7684\u6a21\u578b\u53c2\u6570\u3002\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u76f8\u6bd4\uff0cMoFO \u5728\u4fdd\u6301\u53c2\u6570\u66f4\u63a5\u8fd1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u77e5\u8bc6\u9057\u5fd8\u3002\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u7f13\u89e3\u9057\u5fd8\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cMoFO \u7ed3\u5408\u4e86\u4ee5\u4e0b\u4e24\u4e2a\u4f18\u70b9\u3002\u9996\u5148\uff0cMoFO \u4e0d\u9700\u8981\u8bbf\u95ee\u9884\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u4f7f\u5f97 MoFO \u7279\u522b\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u4e0d\u53ef\u7528\u7684\u5fae\u8c03\u573a\u666f\uff0c\u4f8b\u5982\u5fae\u8c03\u4ec5\u9650\u4e8e\u68c0\u67e5\u70b9\u7684\u5f00\u6e90 LLM\u3002\u5176\u6b21\uff0cMoFO \u4e0d\u4f1a\u6539\u53d8\u539f\u59cb\u635f\u5931\u51fd\u6570\u3002\u8fd9\u53ef\u4ee5\u907f\u514d\u635f\u5bb3\u6a21\u578b\u5728\u5fae\u8c03\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u4e25\u683c\u7684\u6536\u655b\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 MoFO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u51cf\u8f7b\u9057\u5fd8\u548c\u589e\u5f3a\u5fae\u8c03\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "author": "Yupeng Chen et.al.", "authors": "Yupeng Chen, Senmiao Wang, Zhihang Lin, Zeyu Qin, Yushun Zhang, Tian Ding, Ruoyu Sun", "id": "2407.20999v1", "paper_url": "http://arxiv.org/abs/2407.20999v1", "repo": "null"}}