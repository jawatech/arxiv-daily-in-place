{"2501.12231": {"publish_time": "2025-01-21", "title": "InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models", "paper_summary": "The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification.", "paper_summary_zh": "\u751f\u6210\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\u6709\u52a9\u4e8e\u6784\u5efa\u5229\u7528\u8bed\u8a00\u4e4b\u5916\u7684\u591a\u6a21\u6001\u865a\u62df\u52a9\u624b\u3002\u901a\u8fc7\u89c2\u5bdf\u4eba\u7c7b\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\uff0c\u53ef\u4ee5\u6784\u5efa\u5bf9\u6b63\u5728\u6267\u884c\u7684\u52a8\u4f5c\u548c\u4efb\u52a1\u6709\u60c5\u5883\u611f\u77e5\u7684\u52a9\u624b\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u6839\u636e\u8fd9\u79cd\u7406\u89e3\u63d0\u4f9b\u5e2e\u52a9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5177\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6307\u4ee4\u4efb\u52a1\u52a9\u624b (InsTALL)\uff0c\u8be5\u52a9\u624b\u5229\u7528\u5728\u7ebf\u89c6\u89c9\u6d41\uff08\u4f8b\u5982\u7528\u6237\u7684\u5c4f\u5e55\u5171\u4eab\u6216\u89c6\u9891\u5f55\u5236\uff09\uff0c\u5e76\u5b9e\u65f6\u54cd\u5e94\u4e0e\u624b\u5934\u4efb\u52a1\u76f8\u5173\u7684\u7528\u6237\u67e5\u8be2\u3002\u4e3a\u4e86\u63d0\u4f9b\u6709\u7528\u7684\u5e2e\u52a9\uff0cInsTALL 1) \u5728\u4efb\u52a1\u89c6\u9891\u548c\u914d\u5bf9\u6587\u672c\u6570\u636e\u4e0a\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ee5\u53ca 2) \u4ece\u89c6\u9891\u6570\u636e\u4e2d\u81ea\u52a8\u63d0\u53d6\u4efb\u52a1\u56fe\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u5229\u7528\u5b83\u3002\u6211\u4eec\u5c55\u793a\u4e86 InsTALL \u5728\u8003\u8651\u7528\u4e8e\u591a\u6a21\u6001\u6d3b\u52a8\u7406\u89e3\u7684\u63d0\u8bae\u5b50\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u2014\u2014\u4efb\u52a1\u8bc6\u522b (TR)\u3001\u52a8\u4f5c\u8bc6\u522b (AR)\u3001\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u9884\u6d4b (AP) \u548c\u8ba1\u5212\u9884\u6d4b (PP)\u2014\u2014\u5e76\u4e14\u5728\u4e0e\u81ea\u52a8\u9519\u8bef\u8bc6\u522b\u76f8\u5173\u7684\u4e24\u4e2a\u65b0\u5b50\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u3002", "author": "Pha Nguyen et.al.", "authors": "Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min", "id": "2501.12231v1", "paper_url": "http://arxiv.org/abs/2501.12231v1", "repo": "null"}}