{"2407.07342": {"publish_time": "2024-07-10", "title": "Multilingual Blending: LLM Safety Alignment Evaluation with Language Mixture", "paper_summary": "As safety remains a crucial concern throughout the development lifecycle of\nLarge Language Models (LLMs), researchers and industrial practitioners have\nincreasingly focused on safeguarding and aligning LLM behaviors with human\npreferences and ethical standards. LLMs, trained on extensive multilingual\ncorpora, exhibit powerful generalization abilities across diverse languages and\ndomains. However, current safety alignment practices predominantly focus on\nsingle-language scenarios, which leaves their effectiveness in complex\nmultilingual contexts, especially for those complex mixed-language formats,\nlargely unexplored. In this study, we introduce Multilingual Blending, a\nmixed-language query-response scheme designed to evaluate the safety alignment\nof various state-of-the-art LLMs (e.g., GPT-4o, GPT-3.5, Llama3) under\nsophisticated, multilingual conditions. We further investigate language\npatterns such as language availability, morphology, and language family that\ncould impact the effectiveness of Multilingual Blending in compromising the\nsafeguards of LLMs. Our experimental results show that, without meticulously\ncrafted prompt templates, Multilingual Blending significantly amplifies the\ndetriment of malicious queries, leading to dramatically increased bypass rates\nin LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding\nthose of single-language baselines. Moreover, the performance of Multilingual\nBlending varies notably based on intrinsic linguistic properties, with\nlanguages of different morphology and from diverse families being more prone to\nevading safety alignments. These findings underscore the necessity of\nevaluating LLMs and developing corresponding safety alignment strategies in a\ncomplex, multilingual context to align with their superior cross-language\ngeneralization capabilities.", "paper_summary_zh": "\u7531\u65bc\u5b89\u5168\u6027\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6574\u500b\u958b\u767c\u9031\u671f\u4e2d\u4ecd\u7136\u662f\u4e00\u500b\u81f3\u95dc\u91cd\u8981\u7684\u554f\u984c\uff0c\u56e0\u6b64\u7814\u7a76\u4eba\u54e1\u548c\u7522\u696d\u5f9e\u696d\u8005\u8d8a\u4f86\u8d8a\u5c08\u6ce8\u65bc\u4fdd\u969c\u548c\u8abf\u6574 LLM \u884c\u70ba\uff0c\u4ee5\u7b26\u5408\u4eba\u985e\u504f\u597d\u548c\u9053\u5fb7\u6a19\u6e96\u3002LLM \u5728\u5ee3\u6cdb\u7684\u591a\u8a9e\u8a00\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\uff0c\u5c55\u73fe\u51fa\u8de8\u8d8a\u4e0d\u540c\u8a9e\u8a00\u548c\u9818\u57df\u7684\u5f37\u5927\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u5b89\u5168\u6027\u8abf\u6574\u5be6\u52d9\u4e3b\u8981\u5c08\u6ce8\u65bc\u55ae\u4e00\u8a9e\u8a00\u5834\u666f\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u5728\u8907\u96dc\u7684\u591a\u8a9e\u8a00\u74b0\u5883\u4e2d\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u8907\u96dc\u7684\u6df7\u5408\u8a9e\u8a00\u683c\u5f0f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u8a0e\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u8a9e\u8a00\u6df7\u5408\uff0c\u9019\u662f\u4e00\u7a2e\u6df7\u5408\u8a9e\u8a00\u67e5\u8a62\u56de\u61c9\u6a5f\u5236\uff0c\u65e8\u5728\u8a55\u4f30\u5404\u7a2e\u6700\u5148\u9032\u7684 LLM\uff08\u4f8b\u5982 GPT-4o\u3001GPT-3.5\u3001Llama3\uff09\u5728\u8907\u96dc\u7684\u591a\u8a9e\u8a00\u689d\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\u8abf\u6574\u3002\u6211\u5011\u9032\u4e00\u6b65\u7814\u7a76\u4e86\u8a9e\u8a00\u6a21\u5f0f\uff0c\u4f8b\u5982\u8a9e\u8a00\u53ef\u7528\u6027\u3001\u5f62\u614b\u548c\u8a9e\u8a00\u7cfb\uff0c\u9019\u4e9b\u6a21\u5f0f\u53ef\u80fd\u6703\u5f71\u97ff\u591a\u8a9e\u8a00\u6df7\u5408\u5728\u7834\u58de LLM \u4fdd\u969c\u63aa\u65bd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5728\u6c92\u6709\u7cbe\u5fc3\u88fd\u4f5c\u7684\u63d0\u793a\u7bc4\u672c\u7684\u60c5\u6cc1\u4e0b\uff0c\u591a\u8a9e\u8a00\u6df7\u5408\u6703\u986f\u8457\u653e\u5927\u60e1\u610f\u67e5\u8a62\u7684\u640d\u5bb3\uff0c\u5c0e\u81f4 LLM \u5b89\u5168\u6027\u8abf\u6574\u7684\u7e5e\u904e\u7387\u5927\u5e45\u589e\u52a0\uff08GPT-3.5 \u4e0a\u70ba 67.23%\uff0cGPT-4o \u4e0a\u70ba 40.34%\uff09\uff0c\u9060\u9060\u8d85\u904e\u55ae\u4e00\u8a9e\u8a00\u57fa\u7dda\u3002\u6b64\u5916\uff0c\u591a\u8a9e\u8a00\u6df7\u5408\u7684\u6548\u80fd\u6703\u6839\u64da\u5167\u5728\u8a9e\u8a00\u7279\u6027\u800c\u6709\u986f\u8457\u5dee\u7570\uff0c\u4e0d\u540c\u5f62\u614b\u548c\u4f86\u81ea\u4e0d\u540c\u8a9e\u8a00\u7cfb\u7684\u8a9e\u8a00\u66f4\u5bb9\u6613\u898f\u907f\u5b89\u5168\u6027\u8abf\u6574\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86\u5728\u8907\u96dc\u7684\u591a\u8a9e\u8a00\u74b0\u5883\u4e2d\u8a55\u4f30 LLM \u4e26\u5236\u5b9a\u76f8\u61c9\u7684\u5b89\u5168\u6027\u8abf\u6574\u7b56\u7565\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u7b26\u5408\u5b83\u5011\u5353\u8d8a\u7684\u8de8\u8a9e\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "author": "Jiayang Song et.al.", "authors": "Jiayang Song, Yuheng Huang, Zhehua Zhou, Lei Ma", "id": "2407.07342v1", "paper_url": "http://arxiv.org/abs/2407.07342v1", "repo": "null"}}