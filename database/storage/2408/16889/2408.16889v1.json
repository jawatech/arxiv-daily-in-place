{"2408.16889": {"publish_time": "2024-08-29", "title": "LLaVA-Chef: A Multi-modal Generative Model for Food Recipes", "paper_summary": "In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches.", "paper_summary_zh": "<paragraph>\u5728\u5168\u7403\u5316\u7684\u80cc\u666f\u4e0b\uff0c\u5728\u7ebf\u98df\u8c31\u5206\u4eab\u9886\u57df\u8fc5\u901f\u53d1\u5c55\uff0c\u4eba\u4eec\u5bf9\u7406\u89e3\u548c\u751f\u6210\u98df\u8c31\u7684\u7814\u7a76\u6fc0\u589e\u3002GPT-2 \u548c LLaVA \u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u65b9\u6cd5\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u53ef\u4ee5\u6df1\u5165\u7814\u7a76\u4e0e\u98df\u7269\u76f8\u5173\u7684\u4efb\u52a1\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec\u6210\u5206\u8bc6\u522b\u548c\u5168\u9762\u98df\u8c31\u751f\u6210\u3002\u5c3d\u7ba1 LLM \u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u548c\u591a\u6a21\u6001\u9002\u5e94\u6027\uff0c\u4f46\u7279\u5b9a\u9886\u57df\u7684\u57f9\u8bad\u4ecd\u7136\u662f\u5176\u6709\u6548\u5e94\u7528\u7684\u5173\u952e\u3002\u8fd9\u9879\u5de5\u4f5c\u8bc4\u4f30\u4e86\u73b0\u6709\u7684 LLM \u7528\u4e8e\u98df\u8c31\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86 LLaVA-Chef\uff0c\u8fd9\u662f\u4e00\u79cd\u7ecf\u8fc7\u591a\u9636\u6bb5\u65b9\u6cd5\u5728\u7cbe\u9009\u7684\u591a\u6837\u5316\u98df\u8c31\u63d0\u793a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u65b0\u6a21\u578b\u3002\u9996\u5148\uff0c\u6211\u4eec\u6539\u8fdb\u4e86\u89c6\u89c9\u98df\u7269\u56fe\u50cf\u5d4c\u5165\u5230\u8bed\u8a00\u7a7a\u95f4\u7684\u6620\u5c04\u3002\u5176\u6b21\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u76f8\u5173\u7684\u98df\u8c31\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5c06 LLaVA \u8c03\u6574\u5230\u98df\u54c1\u9886\u57df\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5229\u7528\u5404\u79cd\u63d0\u793a\u6765\u589e\u5f3a\u6a21\u578b\u7684\u98df\u8c31\u7406\u89e3\u529b\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u60e9\u7f5a\u6a21\u578b\u6765\u63d0\u9ad8\u751f\u6210\u98df\u8c31\u7684\u8bed\u8a00\u8d28\u91cf\u3002LLaVA-Chef \u5c55\u793a\u4e86\u5bf9\u9884\u8bad\u7ec3 LLM \u548c\u5148\u524d\u5de5\u4f5c\u7684\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6539\u8fdb\u3002\u8be6\u7ec6\u7684\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cLLaVA-Chef \u751f\u6210\u4e86\u66f4\u8be6\u7ec6\u7684\u98df\u8c31\uff0c\u5e76\u7cbe\u786e\u63d0\u5230\u4e86\u6210\u5206\u3002</paragraph>", "author": "Fnu Mohbat et.al.", "authors": "Fnu Mohbat, Mohammed J. Zaki", "id": "2408.16889v1", "paper_url": "http://arxiv.org/abs/2408.16889v1", "repo": "https://github.com/mohbattharani/LLaVA-Chef"}}