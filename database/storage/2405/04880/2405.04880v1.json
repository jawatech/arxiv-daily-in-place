{"2405.04880": {"publish_time": "2024-05-08", "title": "The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio", "paper_summary": "With the proliferation of Audio Language Model (ALM) based deepfake audio,\nthere is an urgent need for effective detection methods. Unlike traditional\ndeepfake audio generation, which often involves multi-step processes\nculminating in vocoder usage, ALM directly utilizes neural codec methods to\ndecode discrete codes into audio. Moreover, driven by large-scale data, ALMs\nexhibit remarkable robustness and versatility, posing a significant challenge\nto current audio deepfake detection (ADD) models. To effectively detect\nALM-based deepfake audio, we focus on the mechanism of the ALM-based audio\ngeneration method, the conversion from neural codec to waveform. We initially\nconstruct the Codecfake dataset, an open-source large-scale dataset, including\ntwo languages, millions of audio samples, and various test conditions, tailored\nfor ALM-based audio detection. Additionally, to achieve universal detection of\ndeepfake audio and tackle domain ascent bias issue of original SAM, we propose\nthe CSAM strategy to learn a domain balanced and generalized minima. Experiment\nresults demonstrate that co-training on Codecfake dataset and vocoded dataset\nwith CSAM strategy yield the lowest average Equal Error Rate (EER) of 0.616%\nacross all test conditions compared to baseline models.", "paper_summary_zh": "\u96a8\u8457\u57fa\u65bc\u97f3\u8a0a\u8a9e\u8a00\u6a21\u578b (ALM) \u7684\u6df1\u5ea6\u507d\u9020\u97f3\u8a0a\u7684\u6fc0\u589e\uff0c\u6025\u9700\u6709\u6548\u7684\u5075\u6e2c\u65b9\u6cd5\u3002\u8207\u50b3\u7d71\u7684\u6df1\u5ea6\u507d\u9020\u97f3\u8a0a\u751f\u6210\u4e0d\u540c\uff0c\u5f8c\u8005\u901a\u5e38\u6d89\u53ca\u591a\u6b65\u9a5f\u6d41\u7a0b\uff0c\u6700\u7d42\u4f7f\u7528\u8a9e\u97f3\u7de8\u78bc\u5668\uff0c\u800c ALM \u76f4\u63a5\u5229\u7528\u795e\u7d93\u7de8\u89e3\u78bc\u5668\u65b9\u6cd5\u5c07\u96e2\u6563\u4ee3\u78bc\u89e3\u78bc\u70ba\u97f3\u8a0a\u3002\u6b64\u5916\uff0c\u5728\u5927\u91cf\u8cc7\u6599\u7684\u9a45\u52d5\u4e0b\uff0cALM \u5c55\u73fe\u51fa\u986f\u8457\u7684\u7a69\u5065\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u5c0d\u76ee\u524d\u7684\u97f3\u8a0a\u6df1\u5ea6\u507d\u9020\u5075\u6e2c (ADD) \u6a21\u578b\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u6709\u6548\u5075\u6e2c\u57fa\u65bc ALM \u7684\u6df1\u5ea6\u507d\u9020\u97f3\u8a0a\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u57fa\u65bc ALM \u7684\u97f3\u8a0a\u751f\u6210\u65b9\u6cd5\u7684\u6a5f\u5236\uff0c\u5373\u5f9e\u795e\u7d93\u7de8\u89e3\u78bc\u5668\u8f49\u63db\u70ba\u6ce2\u5f62\u3002\u6211\u5011\u6700\u521d\u5efa\u69cb Codecfake \u8cc7\u6599\u96c6\uff0c\u4e00\u500b\u958b\u653e\u539f\u59cb\u78bc\u7684\u5927\u898f\u6a21\u8cc7\u6599\u96c6\uff0c\u5305\u62ec\u5169\u7a2e\u8a9e\u8a00\u3001\u6578\u767e\u842c\u500b\u97f3\u8a0a\u7bc4\u4f8b\u548c\u5404\u7a2e\u6e2c\u8a66\u689d\u4ef6\uff0c\u5c08\u9580\u7528\u65bc\u57fa\u65bc ALM \u7684\u97f3\u8a0a\u5075\u6e2c\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u5be6\u73fe\u6df1\u5ea6\u507d\u9020\u97f3\u8a0a\u7684\u901a\u7528\u5075\u6e2c\u4e26\u89e3\u6c7a\u539f\u59cb SAM \u7684\u9818\u57df\u63d0\u5347\u504f\u5dee\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa CSAM \u7b56\u7565\u4f86\u5b78\u7fd2\u9818\u57df\u5e73\u8861\u4e14\u5ee3\u6cdb\u7684\u6700\u5c0f\u503c\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5728 Codecfake \u8cc7\u6599\u96c6\u548c\u4f7f\u7528 CSAM \u7b56\u7565\u9032\u884c\u8a9e\u97f3\u7de8\u78bc\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u806f\u5408\u8a13\u7df4\uff0c\u5728\u6240\u6709\u6e2c\u8a66\u689d\u4ef6\u4e0b\u7684\u5e73\u5747\u7b49\u932f\u8aa4\u7387 (EER) \u6700\u4f4e\uff0c\u70ba 0.616%\uff0c\u512a\u65bc\u57fa\u6e96\u6a21\u578b\u3002", "author": "Yuankun Xie et.al.", "authors": "Yuankun Xie, Yi Lu, Ruibo Fu, Zhengqi Wen, Zhiyong Wang, Jianhua Tao, Xin Qi, Xiaopeng Wang, Yukun Liu, Haonan Cheng, Long Ye, Yi Sun", "id": "2405.04880v1", "paper_url": "http://arxiv.org/abs/2405.04880v1", "repo": "null"}}