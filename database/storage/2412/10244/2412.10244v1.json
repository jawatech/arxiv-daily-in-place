{"2412.10244": {"publish_time": "2024-12-13", "title": "Efficient Continual Pre-training of LLMs for Low-resource Languages", "paper_summary": "Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families.", "paper_summary_zh": "\u958b\u653e\u539f\u59cb\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (OsLLM) \u63a8\u52d5\u81ea\u7136\u8a9e\u8a00\u7814\u7a76\u7684\u6c11\u4e3b\u5316\uff0c\u8b93\u6a21\u578b\u53c3\u6578\u53ef\u4ee5\u9748\u6d3b\u5730\u64f4\u5145\u6216\u66f4\u65b0\u4ee5\u63d0\u5347\u6548\u80fd\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u8207\u5c08\u6709 LLM \u985e\u4f3c\uff0c\u7531\u65bc\u8a13\u7df4\u8cc7\u6599\u91cf\u8f03\u5c11\u4e14\u8a5e\u5f59\u91cf\u4e0d\u8db3\uff0cOs-LLM \u5728\u4f4e\u8cc7\u6e90\u8a9e\u8a00 (LRL) \u4e0a\u7684\u8868\u73fe\u6bd4\u9ad8\u8cc7\u6e90\u8a9e\u8a00 (HRL) \u5dee\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u91dd\u5c0d\u7279\u5b9a\u8a9e\u8a00\u8cc7\u6599\u9032\u884c\u6301\u7e8c\u9810\u8a13\u7df4 (CPT) \u5728\u8cc7\u6599\u53d6\u5f97\u548c\u904b\u7b97\u8cc7\u6e90\u65b9\u9762\u662f\u4e00\u9805\u6602\u8cb4\u7684\u63d0\u8b70\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u5927\u5e45\u964d\u4f4e CPT \u6210\u672c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u9996\u5148\u958b\u767c\u4e00\u7a2e\u65b0\u6f14\u7b97\u6cd5\uff0c\u5f9e\u8f03\u5927\u7684\u8a9e\u6599\u5eab\u4e2d\u9078\u53d6\u4e00\u500b\u5b50\u96c6\u7684\u6587\u5b57\u3002\u6211\u5011\u4f7f\u7528\u6975\u5c11\u7684 CPT \u8cc7\u6599\u4f86\u5c55\u793a\u6211\u5011\u6280\u8853\u7684\u6709\u6548\u6027\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u6539\u9032\uff0c\u6211\u5011\u8a2d\u8a08\u4e00\u7a2e\u65b0\u6f14\u7b97\u6cd5\u4f86\u9078\u53d6\u8981\u5305\u542b\u5728 LLM \u8a5e\u5f59\u4e2d\u7684\u8a5e\u5f59\u3002\u6211\u5011\u4f7f\u7528\u6700\u65b0\u7684 Llama-3 \u6a21\u578b\u548c\u4e5d\u7a2e\u5370\u5ea6\u8a9e\u8a00\u9032\u884c\u5be6\u9a57\uff0c\u9019\u4e9b\u8a9e\u8a00\u5177\u6709\u4e0d\u540c\u7684\u6587\u5b57\u7cfb\u7d71\u548c\u8cc7\u6e90\u53ef\u7528\u6027\u7a0b\u5ea6\u3002\u5728\u8a55\u4f30\u65b9\u9762\uff0c\u6211\u5011\u4f7f\u7528 IndicGenBench\uff0c\u9019\u662f\u4e00\u500b\u91dd\u5c0d\u5370\u5ea6\u8a9e\u8a00\u7684\u7522\u751f\u4efb\u52d9\u57fa\u6e96\u8cc7\u6599\u96c6\u3002\u6211\u5011\u4f7f\u7528\u5404\u7a2e CPT \u8a9e\u6599\u5eab\u548c\u64f4\u5145\u7684\u8a5e\u5f59\u91cf\u9032\u884c\u5be6\u9a57\uff0c\u4e26\u63d0\u4f9b\u8de8\u8a9e\u8a00\u7cfb\u5217\u7684\u898b\u89e3\u3002", "author": "Arijit Nag et.al.", "authors": "Arijit Nag, Soumen Chakrabarti, Animesh Mukherjee, Niloy Ganguly", "id": "2412.10244v1", "paper_url": "http://arxiv.org/abs/2412.10244v1", "repo": "null"}}