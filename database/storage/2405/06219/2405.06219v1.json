{"2405.06219": {"publish_time": "2024-05-10", "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models", "paper_summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u73fe\u5728\u53ef\u4ee5\u8655\u7406\u66f4\u9577\u7684\u7b26\u865f\u5e8f\u5217\uff0c\n\u57f7\u884c\u8907\u96dc\u7684\u4efb\u52d9\uff0c\u4f8b\u5982\u7406\u89e3\u66f8\u7c4d\u548c\u7522\u751f\u5197\u9577\u7684\u7ae0\u7bc0\u3002\n\u7136\u800c\uff0cLLM \u6240\u9700\u7684\u9375\u503c (KV) \u5feb\u53d6\u6703\u96a8\u8457\u5167\u5bb9\u9577\u5ea6\u7684\u589e\u52a0\u800c\u6d88\u8017\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\uff0c\n\u6210\u70ba\u90e8\u7f72\u7684\u74f6\u9838\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba SKVQ \u7684\u7b56\u7565\uff0c\u4ee3\u8868\u6ed1\u52d5\u8996\u7a97 KV \u5feb\u53d6\u91cf\u5316\uff0c\n\u4ee5\u89e3\u6c7a\u6975\u4f4e\u4f4d\u5143\u5bec\u5ea6 KV \u5feb\u53d6\u91cf\u5316\u7684\u554f\u984c\u3002\u70ba\u6b64\uff0cSKVQ \u91cd\u65b0\u6392\u5217 KV \u5feb\u53d6\u7684\u901a\u9053\uff0c\n\u4ee5\u63d0\u9ad8\u91cf\u5316\u7d44\u4e2d\u901a\u9053\u7684\u76f8\u4f3c\u6027\uff0c\u4e26\u5728\u7d44\u7d1a\u5225\u5957\u7528\u88c1\u526a\u52d5\u614b\u91cf\u5316\u3002\u6b64\u5916\uff0cSKVQ \u78ba\u4fdd\nKV \u5feb\u53d6\u4e2d\u6700\u65b0\u7684\u8996\u7a97\u7b26\u865f\u4ee5\u9ad8\u7cbe\u5ea6\u4fdd\u7559\u3002\u9019\u6709\u52a9\u65bc\u7dad\u8b77 KV \u5feb\u53d6\u4e2d\u4e00\u5c0f\u90e8\u5206\u4f46\u91cd\u8981\u7684\u90e8\u5206\u7684\u6e96\u78ba\u6027\u3002\nSKVQ \u5728\u7dad\u6301\u6e96\u78ba\u6027\u7684\u540c\u6642\u5be6\u73fe\u4e86\u9ad8\u58d3\u7e2e\u7387\u3002\u6211\u5011\u5c0d LLM \u7684\u8a55\u4f30\u8868\u660e\uff0cSKVQ \u8d85\u8d8a\u4e86\u5148\u524d\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\n\u5141\u8a31\u5c07 KV \u5feb\u53d6\u91cf\u5316\u70ba 2 \u4f4d\u5143\u9375\u548c 1.5 \u4f4d\u5143\u503c\uff0c\u6e96\u78ba\u5ea6\u640d\u5931\u6975\u5c0f\u3002\u4f7f\u7528 SKVQ\uff0c\u53ef\u4ee5\u5728 80GB \u8a18\u61b6\u9ad4 GPU \u4e0a\u8655\u7406\u9577\u9054 1M \u7684\u5167\u5bb9\u9577\u5ea6\uff0c\u9069\u7528\u65bc 7b \u6a21\u578b\uff0c\u4e14\u89e3\u78bc\u901f\u5ea6\u6700\u9ad8\u53ef\u63d0\u5347 7 \u500d\u3002", "author": "Haojie Duanmu et.al.", "authors": "Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin", "id": "2405.06219v1", "paper_url": "http://arxiv.org/abs/2405.06219v1", "repo": "null"}}