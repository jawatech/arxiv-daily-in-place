{"2406.12742": {"publish_time": "2024-06-18", "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning", "paper_summary": "The advancement of large language models (LLMs) has significantly broadened\nthe scope of applications in natural language processing, with multi-modal LLMs\nextending these capabilities to integrate and interpret visual data. However,\nexisting benchmarks for visual language models (VLMs) predominantly focus on\nsingle-image inputs, neglecting the crucial aspect of multi-image\nunderstanding. In this paper, we introduce a Multi-Image Relational Benchmark\nMIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across\nmultiple images. Our benchmark encompasses four categories: perception, visual\nworld knowledge, reasoning, and multi-hop reasoning. Through a comprehensive\nevaluation of a wide range of open-source and closed-source models, we\ndemonstrate that while open-source VLMs were shown to approach the performance\nof GPT-4V in single-image tasks, a significant performance gap remains in\nmulti-image reasoning tasks. Our findings also reveal that even the\nstate-of-the-art GPT-4V model struggles with our benchmark, underscoring the\nneed for further research and development in this area. We believe our\ncontribution of MIRB could serve as a testbed for developing the\nnext-generation multi-modal models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9032\u6b65\u5927\u5e45\u64f4\u5c55\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u61c9\u7528\u7bc4\u570d\uff0c\u800c\u591a\u6a21\u614b LLM \u5247\u5c07\u9019\u4e9b\u529f\u80fd\u64f4\u5c55\u5230\u6574\u5408\u548c\u8a6e\u91cb\u8996\u89ba\u8cc7\u6599\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u57fa\u6e96\u4e3b\u8981\u95dc\u6ce8\u55ae\u4e00\u5f71\u50cf\u8f38\u5165\uff0c\u5ffd\u7565\u4e86\u591a\u5f71\u50cf\u7406\u89e3\u7684\u95dc\u9375\u9762\u5411\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u5f71\u50cf\u95dc\u4fc2\u57fa\u6e96 MIRB\uff0c\u65e8\u5728\u8a55\u4f30 VLM \u5728\u591a\u5f35\u5f71\u50cf\u4e2d\u6bd4\u8f03\u3001\u5206\u6790\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u57fa\u6e96\u5305\u542b\u56db\u500b\u985e\u5225\uff1a\u611f\u77e5\u3001\u8996\u89ba\u4e16\u754c\u77e5\u8b58\u3001\u63a8\u7406\u548c\u591a\u8df3\u63a8\u7406\u3002\u900f\u904e\u5c0d\u5ee3\u6cdb\u7684\u958b\u6e90\u548c\u9589\u6e90\u6a21\u578b\u9032\u884c\u5168\u9762\u8a55\u4f30\uff0c\u6211\u5011\u8b49\u660e\u4e86\u96d6\u7136\u958b\u6e90 VLM \u5df2\u88ab\u8b49\u660e\u80fd\u63a5\u8fd1 GPT-4V \u5728\u55ae\u4e00\u5f71\u50cf\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\uff0c\u4f46\u5728\u591a\u5f71\u50cf\u63a8\u7406\u4efb\u52d9\u4e2d\u4ecd\u5b58\u5728\u986f\u8457\u7684\u6548\u80fd\u5dee\u8ddd\u3002\u6211\u5011\u7684\u767c\u73fe\u4e5f\u63ed\u793a\u4e86\u5373\u4f7f\u662f\u73fe\u4eca\u6700\u5148\u9032\u7684 GPT-4V \u6a21\u578b\u5728\u6211\u5011\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\u4ecd\u6709\u56f0\u96e3\uff0c\u9019\u51f8\u986f\u4e86\u9032\u4e00\u6b65\u7814\u7a76\u548c\u958b\u767c\u6b64\u9818\u57df\u7684\u5fc5\u8981\u6027\u3002\u6211\u5011\u76f8\u4fe1 MIRB \u7684\u8ca2\u737b\u53ef\u4ee5\u4f5c\u70ba\u958b\u767c\u4e0b\u4e00\u4ee3\u591a\u6a21\u614b\u6a21\u578b\u7684\u6e2c\u8a66\u5e73\u53f0\u3002", "author": "Bingchen Zhao et.al.", "authors": "Bingchen Zhao, Yongshuo Zong, Letian Zhang, Timothy Hospedales", "id": "2406.12742v1", "paper_url": "http://arxiv.org/abs/2406.12742v1", "repo": "https://github.com/dtennant/mirb_eval"}}