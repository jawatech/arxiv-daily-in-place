{"2411.04996": {"publish_time": "2024-11-07", "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models", "paper_summary": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u767c\u5c55\u5df2\u64f4\u5c55\u5230\u591a\u6a21\u614b\u7cfb\u7d71\uff0c\u80fd\u5920\u5728\u7d71\u4e00\u7684\u67b6\u69cb\u5167\u8655\u7406\u6587\u5b57\u3001\u5f71\u50cf\u548c\u8a9e\u97f3\u3002\u8a13\u7df4\u9019\u4e9b\u6a21\u578b\u9700\u8981\u6bd4\u50c5\u6587\u5b57\u7684 LLM \u5927\u5f97\u591a\u7684\u8cc7\u6599\u96c6\u548c\u904b\u7b97\u8cc7\u6e90\u3002\u70ba\u4e86\u61c9\u5c0d\u64f4\u5145\u6311\u6230\uff0c\u6211\u5011\u5f15\u9032\u6df7\u5408Transformer (MoT)\uff0c\u9019\u662f\u4e00\u7a2e\u7a00\u758f\u591a\u6a21\u614bTransformer\u67b6\u69cb\uff0c\u53ef\u5927\u5e45\u6e1b\u5c11\u9810\u8a13\u7df4\u7684\u904b\u7b97\u6210\u672c\u3002MoT \u900f\u904e\u6a21\u614b\u89e3\u8026\u6a21\u578b\u7684\u975e\u5d4c\u5165\u53c3\u6578\uff0c\u5305\u62ec\u524d\u994b\u7db2\u8def\u3001\u6ce8\u610f\u529b\u77e9\u9663\u548c\u5c64\u6b21\u6a19\u6e96\u5316\uff0c\u4e26\u5728\u5b8c\u6574\u7684\u8f38\u5165\u5e8f\u5217\u4e0a\u555f\u7528\u5177\u5099\u5168\u5c40\u81ea\u6211\u6ce8\u610f\u529b\u7684\u6a21\u614b\u7279\u5b9a\u8655\u7406\u3002\u6211\u5011\u5728\u591a\u7a2e\u8a2d\u5b9a\u548c\u6a21\u578b\u898f\u6a21\u4e2d\u8a55\u4f30 MoT\u3002\u5728\u8b8a\u8272\u9f8d 7B \u8a2d\u5b9a\uff08\u81ea\u8ff4\u6b78\u6587\u5b57\u548c\u5f71\u50cf\u7522\u751f\uff09\u4e2d\uff0cMoT \u50c5\u4f7f\u7528 55.8% \u7684\u6d6e\u9ede\u904b\u7b97\u6b21\u6578 (FLOP) \u5c31\u9054\u5230\u5bc6\u96c6\u57fa\u7dda\u7684\u6548\u80fd\u3002\u7576\u64f4\u5145\u5230\u5305\u542b\u8a9e\u97f3\u6642\uff0cMoT \u9054\u5230\u7684\u8a9e\u97f3\u6548\u80fd\u53ef\u8207\u5bc6\u96c6\u57fa\u7dda\u76f8\u6bd4\uff0c\u4f46\u50c5\u4f7f\u7528 37.2% \u7684\u6d6e\u9ede\u904b\u7b97\u6b21\u6578\u3002\u5728\u8f38\u8840\u8a2d\u5b9a\u4e2d\uff0c\u6587\u5b57\u548c\u5f71\u50cf\u4f7f\u7528\u4e0d\u540c\u7684\u76ee\u6a19\u9032\u884c\u8a13\u7df4\uff0c7B MoT \u6a21\u578b\u7684\u5f71\u50cf\u6a21\u614b\u6548\u80fd\u8207\u5bc6\u96c6\u57fa\u7dda\u76f8\u7576\uff0c\u4f46\u6d6e\u9ede\u904b\u7b97\u6b21\u6578\u53ea\u6709\u4e09\u5206\u4e4b\u4e00\uff0c\u800c 760M MoT \u6a21\u578b\u5247\u5728\u95dc\u9375\u5f71\u50cf\u7522\u751f\u6307\u6a19\u4e0a\u512a\u65bc 1.4B \u5bc6\u96c6\u57fa\u7dda\u3002\u7cfb\u7d71\u5206\u6790\u9032\u4e00\u6b65\u7a81\u986f\u4e86 MoT \u7684\u5be6\u969b\u6548\u76ca\uff0c\u5728 47.2% \u7684\u5be6\u969b\u6642\u9593\u5167\u9054\u6210\u5bc6\u96c6\u57fa\u7dda\u5f71\u50cf\u54c1\u8cea\uff0c\u5728 75.6% \u7684\u5be6\u969b\u6642\u9593\u5167\u9054\u6210\u6587\u5b57\u54c1\u8cea\uff08\u5728\u914d\u5099 NVIDIA A100 GPU \u7684 AWS p4de.24xlarge \u5be6\u4f8b\u4e0a\u6e2c\u91cf\uff09\u3002", "author": "Weixin Liang et.al.", "authors": "Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin", "id": "2411.04996v1", "paper_url": "http://arxiv.org/abs/2411.04996v1", "repo": "null"}}