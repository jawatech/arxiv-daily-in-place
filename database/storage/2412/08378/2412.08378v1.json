{"2412.08378": {"publish_time": "2024-12-11", "title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "paper_summary": "Recently, there has been growing interest in the capability of multimodal\nlarge language models (MLLMs) to process high-resolution images. A common\napproach currently involves dynamically cropping the original high-resolution\nimage into smaller sub-images, which are then fed into a vision encoder that\nwas pre-trained on lower-resolution images. However, this cropping approach\noften truncates objects and connected areas in the original image, causing\nsemantic breaks. To address this limitation, we introduce HyViLM, designed to\nprocess images of any resolution while retaining the overall context during\nencoding. Specifically, we: (i) Design a new visual encoder called Hybrid\nEncoder that not only encodes individual sub-images but also interacts with\ndetailed global visual features, significantly improving the model's ability to\nencode high-resolution images. (ii) Propose an optimal feature fusion strategy\nfor the dynamic cropping approach, effectively leveraging information from\ndifferent layers of the vision encoder. Compared with the state-of-the-art\nMLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out\nof ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance\non the TextVQA task and a 6.9% enhancement on the DocVQA task.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u4eba\u4eec\u5bf9\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u80fd\u529b\u8d8a\u6765\u8d8a\u611f\u5174\u8da3\u3002\u76ee\u524d\uff0c\u4e00\u79cd\u5e38\u89c1\u7684\u65b9\u6cd5\u6d89\u53ca\u5c06\u539f\u59cb\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u52a8\u6001\u88c1\u526a\u6210\u8f83\u5c0f\u7684\u5b50\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u5176\u8f93\u5165\u5230\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u88c1\u526a\u65b9\u6cd5\u901a\u5e38\u4f1a\u622a\u65ad\u539f\u59cb\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u548c\u8fde\u63a5\u533a\u57df\uff0c\u4ece\u800c\u5bfc\u81f4\u8bed\u4e49\u4e2d\u65ad\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 HyViLM\uff0c\u5b83\u65e8\u5728\u5904\u7406\u4efb\u4f55\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u5728\u7f16\u7801\u671f\u95f4\u4fdd\u7559\u6574\u4f53\u4e0a\u4e0b\u6587\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\uff1a(i) \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u79f0\u4e3a\u6df7\u5408\u7f16\u7801\u5668\uff0c\u5b83\u4e0d\u4ec5\u53ef\u4ee5\u5bf9\u5404\u4e2a\u5b50\u56fe\u50cf\u8fdb\u884c\u7f16\u7801\uff0c\u8fd8\u53ef\u4ee5\u4e0e\u8be6\u7ec6\u7684\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8fdb\u884c\u7f16\u7801\u7684\u80fd\u529b\u3002(ii) \u4e3a\u52a8\u6001\u88c1\u526a\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u4f73\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u5730\u5229\u7528\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u4e0d\u540c\u5c42\u7684\u4fe1\u606f\u3002\u4e0e\u5728\u76f8\u540c\u8bbe\u7f6e\u4e0b\u7684\u6700\u5148\u8fdb\u7684 MLLM \u76f8\u6bd4\uff0c\u6211\u4eec\u7684 HyViLM \u5728\u5341\u5206\u4e4b\u4e5d\u7684\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684 MLLM\u3002\u5177\u4f53\u6765\u8bf4\uff0cHyViLM \u5728 TextVQA \u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 9.6%\uff0c\u5728 DocVQA \u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 6.9%\u3002</paragraph>", "author": "Shiding Zhu et.al.", "authors": "Shiding Zhu, Wenhui Dong, Jun Song, Yanan Guo, Bo Zheng", "id": "2412.08378v1", "paper_url": "http://arxiv.org/abs/2412.08378v1", "repo": "null"}}