{"2408.00923": {"publish_time": "2024-08-01", "title": "Reclaiming Residual Knowledge: A Novel Paradigm to Low-Bit Quantization", "paper_summary": "This paper explores a novel paradigm in low-bit (i.e. 4-bits or lower)\nquantization, differing from existing state-of-the-art methods, by framing\noptimal quantization as an architecture search problem within convolutional\nneural networks (ConvNets). Our framework, dubbed \\textbf{CoRa} (Optimal\nQuantization Residual \\textbf{Co}nvolutional Operator Low-\\textbf{Ra}nk\nAdaptation), is motivated by two key aspects. Firstly, quantization residual\nknowledge, i.e. the lost information between floating-point weights and\nquantized weights, has long been neglected by the research community.\nReclaiming the critical residual knowledge, with an infinitesimal extra\nparameter cost, can reverse performance degradation without training. Secondly,\nstate-of-the-art quantization frameworks search for optimal quantized weights\nto address the performance degradation. Yet, the vast search spaces in weight\noptimization pose a challenge for the efficient optimization in large models.\nFor example, state-of-the-art BRECQ necessitates $2 \\times 10^4$ iterations to\nquantize models. Fundamentally differing from existing methods, \\textbf{CoRa}\nsearches for the optimal architectures of low-rank adapters, reclaiming\ncritical quantization residual knowledge, within the search spaces smaller\ncompared to the weight spaces, by many orders of magnitude. The low-rank\nadapters approximate the quantization residual weights, discarded in previous\nmethods. We evaluate our approach over multiple pre-trained ConvNets on\nImageNet. \\textbf{CoRa} achieves comparable performance against both\nstate-of-the-art quantization-aware training and post-training quantization\nbaselines, in $4$-bit and $3$-bit quantization, by using less than $250$\niterations on a small calibration set with $1600$ images. Thus, \\textbf{CoRa}\nestablishes a new state-of-the-art in terms of the optimization efficiency in\nlow-bit quantization.", "paper_summary_zh": "<paragraph>\u672c\u6587\u63a2\u8a0e\u4f4e\u4f4d\u5143\uff08\u5373 4 \u4f4d\u5143\u6216\u66f4\u4f4e\uff09\u91cf\u5316\u4e2d\u7684\u4e00\u7a2e\u65b0\u6a21\u5f0f\uff0c\u5b83\u4e0d\u540c\u65bc\u73fe\u6709\u7684\u6700\u5148\u9032\u65b9\u6cd5\uff0c\u800c\u662f\u5c07\u6700\u4f73\u91cf\u5316\u8a2d\u5b9a\u70ba\u5377\u7a4d\u795e\u7d93\u7db2\u8def\uff08ConvNets\uff09\u4e2d\u7684\u67b6\u69cb\u641c\u5c0b\u554f\u984c\u3002\u6211\u5011\u7684\u6846\u67b6\u7a31\u70ba \\textbf{CoRa}\uff08\u6700\u4f73\u91cf\u5316\u6b98\u5dee\\textbf{Co}nvolutional \u7b97\u5b50\u4f4e\\textbf{Ra}nk \u9069\u61c9\uff09\uff0c\u5176\u52d5\u6a5f\u4f86\u81ea\u5169\u500b\u95dc\u9375\u65b9\u9762\u3002\u9996\u5148\uff0c\u91cf\u5316\u6b98\u5dee\u77e5\u8b58\uff0c\u5373\u6d6e\u9ede\u6b0a\u91cd\u548c\u91cf\u5316\u6b0a\u91cd\u4e4b\u9593\u7684\u907a\u5931\u8cc7\u8a0a\uff0c\u9577\u671f\u4ee5\u4f86\u4e00\u76f4\u88ab\u7814\u7a76\u793e\u7fa4\u6240\u5ffd\u8996\u3002\u4ee5\u6975\u5c0f\u7684\u984d\u5916\u53c3\u6578\u6210\u672c\u56de\u6536\u91cd\u8981\u7684\u6b98\u5dee\u77e5\u8b58\uff0c\u53ef\u4ee5\u5728\u4e0d\u9032\u884c\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\u626d\u8f49\u6548\u80fd\u4e0b\u964d\u3002\u5176\u6b21\uff0c\u6700\u5148\u9032\u7684\u91cf\u5316\u6846\u67b6\u6703\u641c\u5c0b\u6700\u4f73\u91cf\u5316\u6b0a\u91cd\u4f86\u89e3\u6c7a\u6548\u80fd\u4e0b\u964d\u554f\u984c\u3002\u7136\u800c\uff0c\u6b0a\u91cd\u6700\u4f73\u5316\u4e2d\u7684\u9f90\u5927\u641c\u5c0b\u7a7a\u9593\u5c0d\u5927\u578b\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6700\u4f73\u5316\u69cb\u6210\u6311\u6230\u3002\u4f8b\u5982\uff0c\u6700\u5148\u9032\u7684 BRECQ \u9700\u8981 $2 \\times 10^4$ \u6b21\u53cd\u8986\u904b\u7b97\u624d\u80fd\u91cf\u5316\u6a21\u578b\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u6709\u6839\u672c\u4e0a\u7684\u4e0d\u540c\uff0c\\textbf{CoRa} \u6703\u641c\u5c0b\u4f4e\u79e9\u9069\u914d\u5668\u7684\u6700\u4f73\u67b6\u69cb\uff0c\u56de\u6536\u91cd\u8981\u7684\u91cf\u5316\u6b98\u5dee\u77e5\u8b58\uff0c\u5728\u8207\u6b0a\u91cd\u7a7a\u9593\u76f8\u6bd4\u5c0f\u5f88\u591a\u500b\u6578\u91cf\u7d1a\u7684\u641c\u5c0b\u7a7a\u9593\u4e2d\u3002\u4f4e\u79e9\u9069\u914d\u5668\u8fd1\u4f3c\u65bc\u91cf\u5316\u6b98\u5dee\u6b0a\u91cd\uff0c\u9019\u5728\u5148\u524d\u7684\u6a21\u578b\u4e2d\u6703\u88ab\u6368\u68c4\u3002\u6211\u5011\u91dd\u5c0d ImageNet \u4e0a\u7684\u6578\u500b\u9810\u5148\u8a13\u7df4\u7684 ConvNets \u8a55\u4f30\u6211\u5011\u7684\u505a\u6cd5\u3002\\textbf{CoRa} \u5728 4 \u4f4d\u5143\u548c 3 \u4f4d\u5143\u91cf\u5316\u4e2d\uff0c\u4f7f\u7528\u5c0f\u65bc $250$ \u6b21\u53cd\u8986\u904b\u7b97\uff0c\u5728\u4e00\u500b\u5305\u542b $1600$ \u5f35\u5f71\u50cf\u7684\u5c0f\u6821\u6b63\u96c6\u5408\u4e0a\uff0c\u9054\u5230\u8207\u6700\u5148\u9032\u7684\u91cf\u5316\u611f\u77e5\u8a13\u7df4\u548c\u8a13\u7df4\u5f8c\u91cf\u5316\u57fa\u6e96\u76f8\u7576\u7684\u6548\u80fd\u3002\u56e0\u6b64\uff0c\\textbf{CoRa} \u5728\u4f4e\u4f4d\u5143\u91cf\u5316\u7684\u6700\u4f73\u5316\u6548\u7387\u65b9\u9762\u6a39\u7acb\u4e86\u65b0\u7684\u6700\u5148\u9032\u6a19\u6e96\u3002</paragraph>", "author": "R\u00f3is\u00edn Luo et.al.", "authors": "R\u00f3is\u00edn Luo, Alexandru Drimbarean, James McDermott, Colm O'Riordan", "id": "2408.00923v1", "paper_url": "http://arxiv.org/abs/2408.00923v1", "repo": "null"}}