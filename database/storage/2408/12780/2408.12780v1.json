{"2408.12780": {"publish_time": "2024-08-23", "title": "Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation", "paper_summary": "Despite the recent popularity of Large Language Models (LLMs) in Machine\nTranslation (MT), their performance in low-resource translation still lags\nsignificantly behind Neural Machine Translation (NMT) models. In this paper, we\nexplore what it would take to adapt LLMs for low-resource settings. In\nparticular, we re-examine the role of two factors: a) the importance and\napplication of parallel data, and b) diversity in Supervised Fine-Tuning (SFT).\nRecently, parallel data has been shown to be less important for MT using LLMs\nthan in previous MT research. Similarly, diversity during SFT has been shown to\npromote significant transfer in LLMs across languages and tasks. However, for\nlow-resource LLM-MT, we show that the opposite is true for both of these\nconsiderations: a) parallel data is critical during both pretraining and SFT,\nand b) diversity tends to cause interference, not transfer. Our experiments,\nconducted with 3 LLMs across 2 low-resourced language groups - indigenous\nAmerican and North-East Indian - reveal consistent patterns in both cases,\nunderscoring the generalizability of our findings. We believe these insights\nwill be valuable for scaling to massively multilingual LLM-MT models that can\neffectively serve lower-resource languages.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6a5f\u5668\u7ffb\u8b6f (MT) \u4e2d\u8fd1\u671f\u5f88\u53d7\u6b61\u8fce\uff0c\u5b83\u5011\u5728\u4f4e\u8cc7\u6e90\u7ffb\u8b6f\u4e2d\u7684\u8868\u73fe\u4ecd\u9060\u9060\u843d\u5f8c\u65bc\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (NMT) \u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u9069\u61c9\u4f4e\u8cc7\u6e90\u74b0\u5883\u7684 LLM \u9700\u8981\u5177\u5099\u4ec0\u9ebc\u689d\u4ef6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u91cd\u65b0\u5be9\u8996\u4e86\u5169\u500b\u56e0\u7d20\u7684\u4f5c\u7528\uff1aa) \u5e73\u884c\u8cc7\u6599\u7684\u91cd\u8981\u6027\u53ca\u5176\u61c9\u7528\uff0c\u4ee5\u53ca b) \u76e3\u7763\u5fae\u8abf (SFT) \u4e2d\u7684\u591a\u6a23\u6027\u3002\u6700\u8fd1\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5c0d\u65bc\u4f7f\u7528 LLM \u7684 MT \u800c\u8a00\uff0c\u5e73\u884c\u8cc7\u6599\u7684\u91cd\u8981\u6027\u4f4e\u65bc\u5148\u524d\u7684 MT \u7814\u7a76\u3002\u985e\u4f3c\u5730\uff0c\u5728 SFT \u904e\u7a0b\u4e2d\uff0c\u591a\u6a23\u6027\u5df2\u88ab\u8b49\u660e\u53ef\u4ee5\u4fc3\u9032 LLM \u5728\u8a9e\u8a00\u548c\u4efb\u52d9\u4e4b\u9593\u7684\u986f\u8457\u8f49\u79fb\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u4f4e\u8cc7\u6e90 LLM-MT\uff0c\u6211\u5011\u8868\u660e\u9019\u5169\u500b\u8003\u91cf\u6070\u6070\u76f8\u53cd\uff1aa) \u5e73\u884c\u8cc7\u6599\u5728\u9810\u8a13\u7df4\u548c SFT \u904e\u7a0b\u4e2d\u81f3\u95dc\u91cd\u8981\uff0cb) \u591a\u6a23\u6027\u50be\u5411\u65bc\u9020\u6210\u5e72\u64fe\uff0c\u800c\u4e0d\u662f\u8f49\u79fb\u3002\u6211\u5011\u7684\u5be6\u9a57\u4f7f\u7528 3 \u500b LLM \u5728 2 \u500b\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7d44\uff08\u7f8e\u6d32\u539f\u4f4f\u6c11\u548c\u5370\u5ea6\u6771\u5317\u90e8\uff09\u4e2d\u9032\u884c\uff0c\u5728\u5169\u7a2e\u60c5\u6cc1\u4e0b\u90fd\u63ed\u793a\u4e86\u4e00\u81f4\u7684\u6a21\u5f0f\uff0c\u7a81\u986f\u4e86\u6211\u5011\u767c\u73fe\u7684\u666e\u904d\u6027\u3002\u6211\u5011\u76f8\u4fe1\u9019\u4e9b\u898b\u89e3\u5c0d\u65bc\u64f4\u5c55\u5230\u80fd\u5920\u6709\u6548\u670d\u52d9\u65bc\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7684\u5927\u898f\u6a21\u591a\u8a9e\u8a00 LLM-MT \u6a21\u578b\u5f88\u6709\u50f9\u503c\u3002", "author": "Vivek Iyer et.al.", "authors": "Vivek Iyer, Bhavitvya Malik, Pavel Stepachev, Pinzhen Chen, Barry Haddow, Alexandra Birch", "id": "2408.12780v1", "paper_url": "http://arxiv.org/abs/2408.12780v1", "repo": "null"}}