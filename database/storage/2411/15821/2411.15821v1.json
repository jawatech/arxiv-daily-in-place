{"2411.15821": {"publish_time": "2024-11-24", "title": "Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?", "paper_summary": "This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all.", "paper_summary_zh": "\u672c\u7814\u7a76\u8abf\u67e5\u8a13\u7df4\u8cc7\u6599\u54c1\u8cea\u76f8\u5c0d\u65bc\u6578\u91cf\u5c0d\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u6548\u80fd\u7684\u76f8\u5c0d\u5f71\u97ff\uff0c\u4e26\u5229\u7528 TinyStories \u8cc7\u6599\u96c6\u9032\u884c\u5be6\u8b49\u5206\u6790\u3002\u5206\u6790\u8cc7\u6599\u96c6\u8b8a\u7570\uff0c\u5305\u62ec\u5927\u5c0f\uff08\u539f\u59cb\u5927\u5c0f\u7684 25% \u548c 50%\uff09\u548c\u91cd\u8907\uff08\u53d7\u63a7\u6bd4\u7387 25%\u300150%\u300175% \u548c 100%\uff09\u3002\u6a21\u578b\u6548\u80fd\u6839\u64da\u9a57\u8b49\u640d\u5931\u3001\u6e96\u78ba\u5ea6\u548c\u56f0\u60d1\u5ea6\u6307\u6a19\u9032\u884c\u8a55\u4f30\u3002\u7d50\u679c\u986f\u793a\u8a13\u7df4\u8cc7\u6599\u54c1\u8cea\u5728 SLM \u7684\u6574\u9ad4\u6548\u80fd\u4e2d\u626e\u6f14\u66f4\u91cd\u8981\u7684\u89d2\u8272\uff0c\u7279\u5225\u662f\u5728\u6b64\u5be6\u9a57\u7684\u898f\u6a21\u4e0b\u3002\u6700\u5c0f\u7684\u91cd\u8907\u5c0d\u6a21\u578b\u6e96\u78ba\u5ea6\u7522\u751f\u6b63\u9762\u5f71\u97ff\uff08\u91cd\u8907\u7387 25% \u6642\u6e96\u78ba\u5ea6\u589e\u52a0 +0.87%\uff09\uff0c\u4e14\u4e0d\u6703\u986f\u8457\u589e\u52a0\u56f0\u60d1\u5ea6\uff08\u5f9e 0% \u5230 25% \u91cd\u8907\u6642\u589e\u52a0 +0.52%\uff09\uff0c\u4f46\u904e\u5ea6\u91cd\u8907\u6703\u5c0e\u81f4\u6548\u80fd\u986f\u8457\u4e0b\u964d\uff08\u91cd\u8907\u7387 100% \u6642\u6e96\u78ba\u5ea6\u4e0b\u964d -40%\uff09\u3002\u6b64\u63a2\u8a0e\u7684\u610f\u7fa9\u4e0d\u53ea\u5728\u65bc\u6a21\u578b\u6548\u80fd\uff1b\u8a13\u7df4\u5927\u578b\u6a21\u578b\u6703\u9020\u6210\u986f\u8457\u7684\u8ca1\u52d9\u548c\u904b\u7b97\u8ca0\u64d4\uff0c\u5c0d\u7d44\u7e54\u3001\u500b\u4eba\u548c\u5ee3\u5927\u6c11\u773e\u800c\u8a00\u53ef\u80fd\u96e3\u4ee5\u8ca0\u64d4\uff0c\u5c24\u5176\u662f\u5728\u958b\u767c\u4e2d\u570b\u5bb6\u3002\u6b64\u5916\uff0c\u8207\u5927\u578b\u8a13\u7df4\u76f8\u95dc\u7684\u80fd\u6e90\u6d88\u8017\u4e5f\u5f15\u767c\u74b0\u5883\u554f\u984c\u3002\u4e86\u89e3\u8cc7\u6599\u54c1\u8cea\u76f8\u5c0d\u65bc\u6578\u91cf\u7684\u76f8\u5c0d\u91cd\u8981\u6027\u53ef\u4ee5\u4f7f AI \u6280\u8853\u6c11\u4e3b\u5316\uff0c\u8b93\u5148\u9032\u6a21\u578b\u66f4\u6613\u65bc\u53d6\u5f97\uff0c\u4e14\u5c0d\u6240\u6709\u4eba\u800c\u8a00\u66f4\u5177\u6c38\u7e8c\u6027\u3002", "author": "Aryan Sajith et.al.", "authors": "Aryan Sajith, Krishna Chaitanya Rao Kathala", "id": "2411.15821v1", "paper_url": "http://arxiv.org/abs/2411.15821v1", "repo": "https://github.com/aryan-sajith/urv-data_quantity_vs_data_quality-research"}}