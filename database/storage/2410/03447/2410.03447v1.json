{"2410.03447": {"publish_time": "2024-10-04", "title": "How Language Models Prioritize Contextual Grammatical Cues?", "paper_summary": "Transformer-based language models have shown an excellent ability to\neffectively capture and utilize contextual information. Although various\nanalysis techniques have been used to quantify and trace the contribution of\nsingle contextual cues to a target task such as subject-verb agreement or\ncoreference resolution, scenarios in which multiple relevant cues are available\nin the context remain underexplored. In this paper, we investigate how language\nmodels handle gender agreement when multiple gender cue words are present, each\ncapable of independently disambiguating a target gender pronoun. We analyze two\nwidely used Transformer-based models: BERT, an encoder-based, and GPT-2, a\ndecoder-based model. Our analysis employs two complementary approaches: context\nmixing analysis, which tracks information flow within the model, and a variant\nof activation patching, which measures the impact of cues on the model's\nprediction. We find that BERT tends to prioritize the first cue in the context\nto form both the target word representations and the model's prediction, while\nGPT-2 relies more on the final cue. Our findings reveal striking differences in\nhow encoder-based and decoder-based models prioritize and use contextual\ninformation for their predictions.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u8a9e\u8a00\u6a21\u578b\u5df2\u5c55\u73fe\u51fa\u6975\u4f73\u7684\u80fd\u529b\uff0c\u80fd\u6709\u6548\u64f7\u53d6\u548c\u5229\u7528\u8108\u7d61\u8cc7\u8a0a\u3002\u5118\u7ba1\u5df2\u4f7f\u7528\u5404\u7a2e\u5206\u6790\u6280\u8853\u4f86\u91cf\u5316\u548c\u8ffd\u8e64\u55ae\u4e00\u8108\u7d61\u7dda\u7d22\u5c0d\u76ee\u6a19\u4efb\u52d9\uff08\u4f8b\u5982\u4e3b\u8a5e\u52d5\u8a5e\u4e00\u81f4\u6216\u5171\u540c\u53c3\u7167\u89e3\u6790\uff09\u7684\u8ca2\u737b\uff0c\u4f46\u5c0d\u65bc\u5728\u8108\u7d61\u4e2d\u6709\u591a\u500b\u76f8\u95dc\u7dda\u7d22\u53ef\u7528\u7684\u5834\u666f\u4ecd\u672a\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u8a9e\u8a00\u6a21\u578b\u5728\u5b58\u5728\u591a\u500b\u6027\u5225\u7dda\u7d22\u5b57\u8a5e\u6642\u5982\u4f55\u8655\u7406\u6027\u5225\u4e00\u81f4\uff0c\u6bcf\u500b\u7dda\u7d22\u5b57\u8a5e\u90fd\u80fd\u7368\u7acb\u6d88\u9664\u76ee\u6a19\u6027\u5225\u4ee3\u540d\u8a5e\u7684\u6b67\u7fa9\u3002\u6211\u5011\u5206\u6790\u5169\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u65bc Transformer \u7684\u6a21\u578b\uff1aBERT\uff08\u57fa\u65bc\u7de8\u78bc\u5668\uff09\u548c GPT-2\uff08\u57fa\u65bc\u89e3\u78bc\u5668\uff09\u6a21\u578b\u3002\u6211\u5011\u7684\u5206\u6790\u63a1\u7528\u5169\u7a2e\u4e92\u88dc\u65b9\u6cd5\uff1a\u8108\u7d61\u6df7\u5408\u5206\u6790\uff08\u8ffd\u8e64\u6a21\u578b\u5167\u7684\u8cc7\u8a0a\u6d41\u52d5\uff09\u548c\u4e00\u7a2e\u6d3b\u5316\u4fee\u88dc\u7a0b\u5f0f\uff08\u6e2c\u91cf\u7dda\u7d22\u5c0d\u6a21\u578b\u9810\u6e2c\u7684\u5f71\u97ff\uff09\u3002\u6211\u5011\u767c\u73fe BERT \u50be\u5411\u65bc\u512a\u5148\u8003\u616e\u8108\u7d61\u4e2d\u7684\u7b2c\u4e00\u500b\u7dda\u7d22\uff0c\u4ee5\u5f62\u6210\u76ee\u6a19\u5b57\u8a5e\u8868\u5fb5\u548c\u6a21\u578b\u9810\u6e2c\uff0c\u800c GPT-2 \u5247\u66f4\u591a\u4f9d\u8cf4\u6700\u5f8c\u4e00\u500b\u7dda\u7d22\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u57fa\u65bc\u7de8\u78bc\u5668\u548c\u57fa\u65bc\u89e3\u78bc\u5668\u7684\u6a21\u578b\u5728\u512a\u5148\u9806\u5e8f\u548c\u4f7f\u7528\u8108\u7d61\u8cc7\u8a0a\u9032\u884c\u9810\u6e2c\u65b9\u9762\u5b58\u5728\u986f\u8457\u5dee\u7570\u3002", "author": "Hamidreza Amirzadeh et.al.", "authors": "Hamidreza Amirzadeh, Afra Alishahi, Hosein Mohebbi", "id": "2410.03447v1", "paper_url": "http://arxiv.org/abs/2410.03447v1", "repo": "null"}}