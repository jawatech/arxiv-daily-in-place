{"2502.02007": {"publish_time": "2025-02-04", "title": "Reasoning Bias of Next Token Prediction Training", "paper_summary": "Since the inception of Large Language Models (LLMs), the quest to efficiently\ntrain them for superior reasoning capabilities has been a pivotal challenge.\nThe dominant training paradigm for LLMs is based on next token prediction\n(NTP). Alternative methodologies, called Critical Token Prediction (CTP),\nfocused exclusively on specific critical tokens (such as the answer in Q\\&A\ndataset), aiming to reduce the overfitting of extraneous information and noise.\nContrary to initial assumptions, our research reveals that despite NTP's\nexposure to noise during training, it surpasses CTP in reasoning ability. We\nattribute this counterintuitive outcome to the regularizing influence of noise\non the training dynamics. Our empirical analysis shows that NTP-trained models\nexhibit enhanced generalization and robustness across various benchmark\nreasoning datasets, demonstrating greater resilience to perturbations and\nachieving flatter loss minima. These findings illuminate that NTP is\ninstrumental in fostering reasoning abilities during pretraining, whereas CTP\nis more effective for finetuning, thereby enriching our comprehension of\noptimal training strategies in LLM development.", "paper_summary_zh": "\u81ea\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8a95\u751f\u4ee5\u4f86\uff0c\u6709\u6548\u8a13\u7df4\u5b83\u5011\u4ee5\u7372\u5f97\u5353\u8d8a\u63a8\u7406\u80fd\u529b\u7684\u8ffd\u6c42\u4e00\u76f4\u662f\u4e00\u9805\u95dc\u9375\u6311\u6230\u3002\nLLM \u7684\u4e3b\u8981\u8a13\u7df4\u7bc4\u4f8b\u57fa\u65bc\u4e0b\u4e00\u500b\u7b26\u865f\u9810\u6e2c (NTP)\u3002\u7a31\u70ba\u95dc\u9375\u7b26\u865f\u9810\u6e2c (CTP) \u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5c08\u6ce8\u65bc\u7279\u5b9a\u95dc\u9375\u7b26\u865f\uff08\u4f8b\u5982\u554f\u7b54\u8cc7\u6599\u96c6\u4e2d\u7684\u7b54\u6848\uff09\uff0c\u65e8\u5728\u6e1b\u5c11\u904e\u5ea6\u64ec\u5408\u984d\u5916\u7684\u8cc7\u8a0a\u548c\u96dc\u8a0a\u3002\n\u8207\u6700\u521d\u7684\u5047\u8a2d\u76f8\u53cd\uff0c\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c\u5118\u7ba1 NTP \u5728\u8a13\u7df4\u671f\u9593\u6703\u63a5\u89f8\u5230\u96dc\u8a0a\uff0c\u4f46\u5b83\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8d85\u8d8a\u4e86 CTP\u3002\u6211\u5011\u5c07\u9019\u500b\u53cd\u76f4\u89ba\u7684\u7d50\u679c\u6b78\u56e0\u65bc\u96dc\u8a0a\u5c0d\u8a13\u7df4\u52d5\u614b\u7684\u898f\u7bc4\u5316\u5f71\u97ff\u3002\u6211\u5011\u7684\u5be6\u8b49\u5206\u6790\u8868\u660e\uff0cNTP \u8a13\u7df4\u7684\u6a21\u578b\u5728\u5404\u7a2e\u57fa\u6e96\u63a8\u7406\u8cc7\u6599\u96c6\u4e0a\u8868\u73fe\u51fa\u589e\u5f37\u7684\u6cdb\u5316\u6027\u548c\u7a69\u5065\u6027\uff0c\u8b49\u660e\u4e86\u5c0d\u64fe\u52d5\u7684\u66f4\u5f37\u97cc\u6027\uff0c\u4e26\u5be6\u73fe\u4e86\u66f4\u5e73\u5766\u7684\u640d\u5931\u6700\u5c0f\u503c\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0cNTP \u5728\u9810\u8a13\u7df4\u671f\u9593\u57f9\u990a\u63a8\u7406\u80fd\u529b\u81f3\u95dc\u91cd\u8981\uff0c\u800c CTP \u5c0d\u5fae\u8abf\u66f4\u6709\u6548\uff0c\u5f9e\u800c\u8c50\u5bcc\u4e86\u6211\u5011\u5c0d LLM \u958b\u767c\u4e2d\u6700\u4f73\u8a13\u7df4\u7b56\u7565\u7684\u7406\u89e3\u3002", "author": "Pengxiao Lin et.al.", "authors": "Pengxiao Lin, Zhongwang Zhang, Zhi-Qin John Xu", "id": "2502.02007v1", "paper_url": "http://arxiv.org/abs/2502.02007v1", "repo": "null"}}