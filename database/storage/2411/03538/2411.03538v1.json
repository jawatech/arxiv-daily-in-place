{"2411.03538": {"publish_time": "2024-11-05", "title": "Long Context RAG Performance of Large Language Models", "paper_summary": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\nexternal information. With the advent of LLMs that support increasingly longer\ncontext lengths, there is a growing interest in understanding how these models\nperform in RAG scenarios. Can these new long context models improve RAG\nperformance? This paper presents a comprehensive study of the impact of\nincreased context length on RAG performance across 20 popular open source and\ncommercial LLMs. We ran RAG workflows while varying the total context length\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\ndomain-specific datasets, and report key insights on the benefits and\nlimitations of long context in RAG applications. Our findings reveal that while\nretrieving more documents can improve performance, only a handful of the most\nrecent state of the art LLMs can maintain consistent accuracy at long context\nabove 64k tokens. We also identify distinct failure modes in long context\nscenarios, suggesting areas for future research.", "paper_summary_zh": "\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff08RAG\uff09\u5df2\u6210\u70ba\u4e00\u7a2e\u81f3\u95dc\u91cd\u8981\u7684\u6280\u8853\uff0c\u5b83\u900f\u904e\u7d0d\u5165\u5916\u90e8\u8cc7\u8a0a\u4f86\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6e96\u78ba\u6027\u3002\u96a8\u8457\u652f\u63f4\u8d8a\u4f86\u8d8a\u9577\u8108\u7d61\u9577\u5ea6\u7684 LLM \u7684\u51fa\u73fe\uff0c\u4eba\u5011\u8d8a\u4f86\u8d8a\u6709\u8208\u8da3\u4e86\u89e3\u9019\u4e9b\u6a21\u578b\u5728 RAG \u5834\u666f\u4e2d\u7684\u8868\u73fe\u3002\u9019\u4e9b\u65b0\u7684\u9577\u8108\u7d61\u6a21\u578b\u80fd\u5920\u63d0\u5347 RAG \u7684\u6548\u80fd\u55ce\uff1f\u672c\u6587\u91dd\u5c0d 20 \u500b\u6d41\u884c\u7684\u958b\u653e\u539f\u59cb\u78bc\u548c\u5546\u696d LLM\uff0c\u5c0d\u589e\u52a0\u8108\u7d61\u9577\u5ea6\u5c0d RAG \u6548\u80fd\u7684\u5f71\u97ff\u9032\u884c\u5168\u9762\u7684\u7814\u7a76\u3002\u6211\u5011\u5728\u4e09\u500b\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u96c6\u4e0a\u57f7\u884c RAG \u5de5\u4f5c\u6d41\u7a0b\uff0c\u540c\u6642\u5c07\u7e3d\u8108\u7d61\u9577\u5ea6\u5f9e 2,000 \u500b\u4ee3\u5e63\u8b8a\u66f4\u70ba 128,000 \u500b\u4ee3\u5e63\uff08\u5728\u53ef\u80fd\u7684\u60c5\u6cc1\u4e0b\u70ba 200 \u842c\u500b\u4ee3\u5e63\uff09\uff0c\u4e26\u91dd\u5c0d RAG \u61c9\u7528\u4e2d\u9577\u8108\u7d61\u7684\u512a\u9ede\u548c\u9650\u5236\u5831\u544a\u95dc\u9375\u898b\u89e3\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u96d6\u7136\u6aa2\u7d22\u66f4\u591a\u6587\u4ef6\u53ef\u4ee5\u63d0\u5347\u6548\u80fd\uff0c\u4f46\u53ea\u6709\u5c11\u6578\u6700\u5148\u9032\u7684 LLM \u80fd\u5920\u5728 64k \u500b\u4ee3\u5e63\u4ee5\u4e0a\u7684\u9577\u8108\u7d61\u4e2d\u7dad\u6301\u4e00\u81f4\u7684\u6e96\u78ba\u6027\u3002\u6211\u5011\u4e5f\u5728\u9577\u8108\u7d61\u5834\u666f\u4e2d\u627e\u51fa\u4e0d\u540c\u7684\u5931\u6557\u6a21\u5f0f\uff0c\u63d0\u51fa\u672a\u4f86\u7814\u7a76\u7684\u65b9\u5411\u3002", "author": "Quinn Leng et.al.", "authors": "Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin", "id": "2411.03538v1", "paper_url": "http://arxiv.org/abs/2411.03538v1", "repo": "null"}}