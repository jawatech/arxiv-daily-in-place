{"2411.02886": {"publish_time": "2024-11-05", "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection", "paper_summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u767c\u5c55\uff0c\u8655\u7406\u8f03\u9577\u80cc\u666f\u7684\u80fd\u529b\u5df2\u6210\u70ba\u7db2\u8def\u61c9\u7528\u7a0b\u5f0f\u7684\u95dc\u9375\u529f\u80fd\uff0c\u4f8b\u5982\u8de8\u6587\u4ef6\u7406\u89e3\u548c LLM \u9a45\u52d5\u7684\u641c\u5c0b\u7cfb\u7d71\u3002\u7136\u800c\uff0c\u9019\u500b\u9032\u5c55\u9762\u81e8\u5169\u500b\u4e3b\u8981\u6311\u6230\uff1a\u7531\u65bc\u5e8f\u5217\u9577\u5ea6\u8d85\u51fa\u5206\u4f48\u800c\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u4e8c\u6b21\u904b\u7b97\u8907\u96dc\u5ea6\u9020\u6210\u7684\u904e\u9577\u63a8\u8ad6\u6642\u9593\u3002\u9019\u4e9b\u554f\u984c\u963b\u7919\u4e86 LLM \u5728\u9577\u80cc\u666f\u5834\u666f\u4e2d\u7684\u61c9\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u52d5\u614b Token \u5c64\u7d1a KV \u5feb\u53d6\u9078\u64c7\uff08TokenSelect\uff09\uff0c\u4e00\u7a2e\u8207\u6a21\u578b\u7121\u95dc\u3001\u7121\u9700\u8a13\u7df4\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u6709\u6548\u4e14\u6e96\u78ba\u7684\u9577\u80cc\u666f\u63a8\u8ad6\u3002TokenSelect \u5efa\u7acb\u5728\u975e\u9023\u7e8c\u6ce8\u610f\u529b\u7a00\u758f\u6027\u7684\u89c0\u5bdf\u4e4b\u4e0a\uff0c\u4f7f\u7528\u67e5\u8a62\u9375\u9ede\u7a4d\u6e2c\u91cf\u6bcf\u500b\u982d\u90e8 KV \u5feb\u53d6\u5728 Token \u5c64\u7d1a\u7684\u91cd\u8981\u6027\u3002\u900f\u904e\u6bcf\u500b\u982d\u90e8\u7684\u8edf\u6027\u6295\u7968\u6a5f\u5236\uff0cTokenSelect \u9078\u64c7\u6027\u5730\u5c07\u5c11\u91cf\u95dc\u9375 KV \u5feb\u53d6 Token \u7d0d\u5165\u6ce8\u610f\u529b\u8a08\u7b97\u4e2d\uff0c\u540c\u6642\u4e0d\u72a7\u7272\u6e96\u78ba\u6027\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u52a0\u901f TokenSelect\uff0c\u6211\u5011\u6839\u64da\u9023\u7e8c\u67e5\u8a62\u76f8\u4f3c\u6027\u7684\u89c0\u5bdf\u8a2d\u8a08\u4e86\u9078\u64c7\u5feb\u53d6\uff0c\u4e26\u5be6\u4f5c\u4e86\u6709\u6548\u7684\u9ede\u7a4d\u6838\uff0c\u5927\u5e45\u6e1b\u5c11 Token \u9078\u64c7\u7684\u958b\u92b7\u3002\u5c0d TokenSelect \u7684\u5168\u9762\u8a55\u4f30\u986f\u793a\uff0c\u6ce8\u610f\u529b\u904b\u7b97\u901f\u5ea6\u63d0\u5347\u4e86 23.84 \u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u9072\u52a0\u901f\u4e86 2.28 \u500d\uff0c\u540c\u6642\u8207\u6700\u5148\u9032\u7684\u9577\u80cc\u666f\u63a8\u8ad6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6548\u80fd\u3002", "author": "Wei Wu et.al.", "authors": "Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, Hui Xiong", "id": "2411.02886v1", "paper_url": "http://arxiv.org/abs/2411.02886v1", "repo": "null"}}