{"2408.04138": {"publish_time": "2024-08-08", "title": "Enhancing Healthcare through Large Language Models: A Study on Medical Question Answering", "paper_summary": "In recent years, the application of Large Language Models (LLMs) in\nhealthcare has shown significant promise in improving the accessibility and\ndissemination of medical knowledge. This paper presents a detailed study of\nvarious LLMs trained on the MedQuAD medical question-answering dataset, with a\nfocus on identifying the most effective model for providing accurate medical\ninformation. Among the models tested, the Sentence-t5 combined with Mistral 7B\ndemonstrated superior performance, achieving a precision score of 0.762. This\nmodel's enhanced capabilities are attributed to its advanced pretraining\ntechniques, robust architecture, and effective prompt construction\nmethodologies. By leveraging these strengths, the Sentence-t5 + Mistral 7B\nmodel excels in understanding and generating precise medical answers. Our\nfindings highlight the potential of integrating sophisticated LLMs in medical\ncontexts to facilitate efficient and accurate medical knowledge retrieval, thus\nsignificantly enhancing patient education and support.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u7642\u4fdd\u5065\u4e2d\u7684\u61c9\u7528\u5df2\u5c55\u73fe\u51fa\u986f\u8457\u7684\u5e0c\u671b\uff0c\u53ef\u6539\u5584\u91ab\u7642\u77e5\u8b58\u7684\u53ef\u53ca\u6027\u548c\u50b3\u64ad\u3002\u672c\u6587\u91dd\u5c0d\u5728 MedQuAD \u91ab\u7642\u554f\u7b54\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u5404\u7a2e LLM \u9032\u884c\u8a73\u7d30\u7814\u7a76\uff0c\u91cd\u9ede\u5728\u65bc\u627e\u51fa\u63d0\u4f9b\u6e96\u78ba\u91ab\u7642\u8cc7\u8a0a\u6700\u6709\u6548\u7684\u6a21\u578b\u3002\u5728\u6e2c\u8a66\u7684\u6a21\u578b\u4e2d\uff0cSentence-t5 \u7d50\u5408 Mistral 7B \u8868\u73fe\u512a\u7570\uff0c\u9054\u5230 0.762 \u7684\u7cbe\u6e96\u5ea6\u5206\u6578\u3002\u6b64\u6a21\u578b\u7684\u589e\u5f37\u529f\u80fd\u6b78\u529f\u65bc\u5176\u5148\u9032\u7684\u9810\u8a13\u7df4\u6280\u8853\u3001\u5f37\u5927\u7684\u67b6\u69cb\u548c\u6709\u6548\u7684\u63d0\u793a\u5efa\u69cb\u65b9\u6cd5\u3002Sentence-t5 + Mistral 7B \u6a21\u578b\u85c9\u7531\u904b\u7528\u9019\u4e9b\u512a\u52e2\uff0c\u5728\u7406\u89e3\u548c\u7522\u751f\u7cbe\u78ba\u7684\u91ab\u7642\u7b54\u6848\u65b9\u9762\u8868\u73fe\u51fa\u8272\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u5c07\u8907\u96dc\u7684 LLM \u6574\u5408\u5230\u91ab\u7642\u80cc\u666f\u4e2d\u7684\u6f5b\u529b\uff0c\u4ee5\u4fc3\u9032\u6709\u6548\u7387\u4e14\u6e96\u78ba\u7684\u91ab\u7642\u77e5\u8b58\u64f7\u53d6\uff0c\u9032\u800c\u986f\u8457\u63d0\u5347\u75c5\u60a3\u6559\u80b2\u548c\u652f\u6301\u3002", "author": "Haoran Yu et.al.", "authors": "Haoran Yu, Chang Yu, Zihan Wang, Dongxian Zou, Hao Qin", "id": "2408.04138v1", "paper_url": "http://arxiv.org/abs/2408.04138v1", "repo": "null"}}