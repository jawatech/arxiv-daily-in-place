{"2408.08295": {"publish_time": "2024-08-15", "title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training", "paper_summary": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u6301\u7e8c\u5b78\u7fd2\u8207\u9810\u8a13\u7df4 (CLPT) \u53d7\u5230\u5ee3\u6cdb\u95dc\u6ce8\uff0c\u53d6\u4ee3\u50b3\u7d71\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u7684\u91cd\u9ede\u3002\u4f7f\u7528\u5f37\u5927\u7684\u9810\u8a13\u7df4\u6a21\u578b (PTM) \u53ef\u4ee5\u5927\u5e45\u4fc3\u9032\u77e5\u8b58\u8f49\u79fb\uff0c\u4e26\u6e1b\u8f15\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u4f46\u4e5f\u98fd\u53d7\u7279\u5b9a\u4e0b\u6e38\u4efb\u52d9\u4e2d\u9810\u8a13\u7df4\u77e5\u8b58\u7684\u6f38\u9032\u904e\u5ea6\u64ec\u5408\u6240\u82e6\u3002\u76ee\u524d\u5927\u591a\u6578\u65b9\u6cd5\u901a\u5e38\u4fdd\u6301 PTM \u51cd\u7d50\uff0c\u4e26\u7d50\u5408\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u63d0\u793a\uff0c\u4ee5\u6307\u5c0e\u8868\u5fb5\u5b78\u7fd2\uff0c\u518d\u642d\u914d\u63d0\u793a\u9078\u53d6\u7a0b\u5e8f\u9032\u884c\u63a8\u8ad6\u3002\u7136\u800c\uff0c\u7531\u65bc\u63d0\u793a\u53c3\u6578\u7684\u5bb9\u91cf\u6709\u9650\uff0c\u6b64\u7b56\u7565\u5728\u6301\u7e8c\u5b78\u7fd2\u4e2d\u50c5\u5c55\u73fe\u6b21\u4f73\u6548\u80fd\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u8abf\u6574 PTM \u7684\u6240\u6709\u53c3\u6578\u901a\u5e38\u80fd\u63d0\u4f9b\u8868\u5fb5\u5b78\u7fd2\u6700\u5927\u7684\u6f5b\u529b\uff0c\u8b93\u5faa\u5e8f\u6f38\u9032\u5fae\u8abf (Seq FT) \u6210\u70ba CLPT \u4e2d\u88ab\u5ffd\u7565\u7684\u57fa\u672c\u57fa\u6e96\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f9e Seq FT \u7684\u89d2\u5ea6\u6df1\u5165\u5206\u6790\u6f38\u9032\u904e\u5ea6\u64ec\u5408\u554f\u984c\u3002\u8003\u91cf\u5230\u904e\u65bc\u5feb\u901f\u8868\u5fb5\u5b78\u7fd2\u548c\u5e36\u6709\u504f\u5dee\u7684\u5206\u985e\u5c64\u69cb\u6210\u6b64\u7279\u5b9a\u554f\u984c\uff0c\u6211\u5011\u63a8\u51fa\u9032\u968e\u7684\u6162\u5b78\u7fd2\u5668\u8207\u5206\u985e\u5668\u6821\u6e96 (SLCA++) \u67b6\u69cb\uff0c\u4ee5\u767c\u63ee Seq FT \u7684\u5f37\u5927\u529f\u80fd\uff0c\u4f5c\u70ba CLPT \u7684\u5f37\u5927\u57fa\u6e96\u65b9\u6cd5\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u6162\u5b78\u7fd2\u5668\uff0c\u4ee5\u9078\u64c7\u6027\u964d\u4f4e\u4e3b\u5e79\u53c3\u6578\u7684\u5b78\u7fd2\u7387\uff0c\u4ee5\u53ca\u5206\u985e\u5668\u6821\u6e96\uff0c\u4ee5\u4e8b\u5f8c\u65b9\u5f0f\u6821\u6e96\u4e0d\u76f8\u4ea4\u7684\u5206\u985e\u5c64\u3002\u6211\u5011\u9032\u4e00\u6b65\u900f\u904e\u5c0d\u7a31\u4ea4\u53c9\u71b5\u640d\u5931\u589e\u5f37 SL \u7684\u6548\u80fd\uff0c\u4e26\u63a1\u7528\u53c3\u6578\u6709\u6548\u7387\u7684\u7b56\u7565\uff0c\u4ee5 SLCA++ \u5be6\u4f5c Seq FT\u3002\u5728\u5f71\u50cf\u5206\u985e\u57fa\u6e96\u4e0a\u7684\u5404\u7a2e\u6301\u7e8c\u5b78\u7fd2\u60c5\u5883\u4e2d\uff0c\u6211\u5011\u7684\u505a\u6cd5\u63d0\u4f9b\u4e86\u5927\u5e45\u6539\u5584\uff0c\u4e26\u4ee5\u6975\u5927\u5dee\u8ddd\u8d85\u8d8a\u73fe\u6709\u6280\u8853\u3002\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/GengDavid/SLCA\u3002", "author": "Gengwei Zhang et.al.", "authors": "Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei", "id": "2408.08295v1", "paper_url": "http://arxiv.org/abs/2408.08295v1", "repo": "https://github.com/gengdavid/slca"}}