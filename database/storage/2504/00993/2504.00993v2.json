{"2504.00993": {"publish_time": "2025-04-01", "title": "MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs", "paper_summary": "Medical tasks such as diagnosis and treatment planning require precise and\ncomplex reasoning, particularly in life-critical domains. Unlike mathematical\nreasoning, medical reasoning demands meticulous, verifiable thought processes\nto ensure reliability and accuracy. However, there is a notable lack of\ndatasets that provide transparent, step-by-step reasoning to validate and\nenhance the medical reasoning ability of AI models. To bridge this gap, we\nintroduce MedReason, a large-scale high-quality medical reasoning dataset\ndesigned to enable faithful and explainable medical problem-solving in large\nlanguage models (LLMs). We utilize a structured medical knowledge graph (KG) to\nconvert clinical QA pairs into logical chains of reasoning, or ``thinking\npaths'', which trace connections from question elements to answers via relevant\nKG entities. Each path is validated for consistency with clinical logic and\nevidence-based medicine. Our pipeline generates detailed reasoning for various\nmedical questions from 7 medical datasets, resulting in a dataset of 32,682\nquestion-answer pairs, each with detailed, step-by-step explanations.\nExperiments demonstrate that fine-tuning with our dataset consistently boosts\nmedical problem-solving capabilities, achieving significant gains of up to 7.7%\nfor DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the\nHuatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the\nclinical benchmark MedBullets. We also engage medical professionals from\ndiverse specialties to assess our dataset's quality, ensuring MedReason offers\naccurate and coherent medical reasoning. Our data, models, and code is\navailable at https://github.com/UCSC-VLAA/MedReason.", "paper_summary_zh": "\u91ab\u7642\u4efb\u52d9\uff0c\u4f8b\u5982\u8a3a\u65b7\u548c\u6cbb\u7642\u8a08\u5283\uff0c\u9700\u8981\u7cbe\u78ba\u4e14\u8907\u96dc\u7684\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u6538\u95dc\u751f\u547d\u7684\u9818\u57df\u3002\u8207\u6578\u5b78\u63a8\u7406\u4e0d\u540c\uff0c\u91ab\u7642\u63a8\u7406\u9700\u8981\u4e00\u7d72\u4e0d\u82df\u3001\u53ef\u9a57\u8b49\u7684\u601d\u7dad\u904e\u7a0b\u4ee5\u78ba\u4fdd\u53ef\u9760\u6027\u548c\u6e96\u78ba\u6027\u3002\u7136\u800c\uff0c\u76ee\u524d\u660e\u986f\u7f3a\u4e4f\u63d0\u4f9b\u900f\u660e\u3001\u9010\u6b65\u63a8\u7406\u7684\u6578\u64da\u96c6\u4f86\u9a57\u8b49\u548c\u589e\u5f37AI\u6a21\u578b\u7684\u91ab\u7642\u63a8\u7406\u80fd\u529b\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u63a8\u51fa\u4e86 MedReason\uff0c\u9019\u662f\u4e00\u500b\u5927\u898f\u6a21\u3001\u9ad8\u8cea\u91cf\u7684\u91ab\u7642\u63a8\u7406\u6578\u64da\u96c6\uff0c\u65e8\u5728\u4f7f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u5920\u9032\u884c\u5fe0\u5be6\u4e14\u53ef\u89e3\u91cb\u7684\u91ab\u7642\u554f\u984c\u89e3\u6c7a\u3002\u6211\u5011\u5229\u7528\u7d50\u69cb\u5316\u7684\u91ab\u5b78\u77e5\u8b58\u5716\u8b5c (KG) \u5c07\u81e8\u5e8a\u554f\u7b54\u5c0d\u8f49\u63db\u70ba\u908f\u8f2f\u63a8\u7406\u93c8\uff0c\u6216\u7a31\u70ba\u300c\u601d\u8003\u8def\u5f91\u300d\uff0c\u5176\u8ffd\u8e64\u5f9e\u554f\u984c\u8981\u7d20\u5230\u7b54\u6848\u4e4b\u9593\u901a\u904e\u76f8\u95dc KG \u5be6\u9ad4\u7684\u9023\u63a5\u3002\u6bcf\u689d\u8def\u5f91\u90fd\u7d93\u904e\u9a57\u8b49\uff0c\u4ee5\u78ba\u4fdd\u8207\u81e8\u5e8a\u908f\u8f2f\u548c\u5faa\u8b49\u91ab\u5b78\u7684\u4e00\u81f4\u6027\u3002\u6211\u5011\u7684\u6d41\u7a0b\u5f9e 7 \u500b\u91ab\u5b78\u6578\u64da\u96c6\u4e2d\u751f\u6210\u5404\u7a2e\u91ab\u5b78\u554f\u984c\u7684\u8a73\u7d30\u63a8\u7406\uff0c\u7522\u751f\u4e00\u500b\u5305\u542b 32,682 \u500b\u554f\u7b54\u5c0d\u7684\u6578\u64da\u96c6\uff0c\u6bcf\u500b\u554f\u7b54\u5c0d\u90fd\u5305\u542b\u8a73\u7d30\u7684\u9010\u6b65\u89e3\u91cb\u3002\u5be6\u9a57\u8868\u660e\uff0c\u4f7f\u7528\u6211\u5011\u7684\u6578\u64da\u96c6\u9032\u884c\u5fae\u8abf\u53ef\u4ee5\u6301\u7e8c\u63d0\u5347\u91ab\u7642\u554f\u984c\u89e3\u6c7a\u80fd\u529b\uff0c\u4f7f DeepSeek-Ditill-8B \u7684\u6027\u80fd\u63d0\u5347\u9ad8\u9054 7.7%\u3002\u6211\u5011\u6027\u80fd\u6700\u4f73\u7684\u6a21\u578b MedReason-8B \u5728\u81e8\u5e8a\u57fa\u6e96 MedBullets \u4e0a\u7684\u8868\u73fe\u6bd4\u6700\u5148\u9032\u7684\u91ab\u7642\u63a8\u7406\u6a21\u578b Huatuo-o1-8B \u9ad8\u51fa 4.2%\u3002\u6211\u5011\u9084\u9080\u8acb\u4e86\u4f86\u81ea\u4e0d\u540c\u5c08\u696d\u7684\u91ab\u7642\u5c08\u696d\u4eba\u54e1\u4f86\u8a55\u4f30\u6211\u5011\u6578\u64da\u96c6\u7684\u8cea\u91cf\uff0c\u4ee5\u78ba\u4fdd MedReason \u63d0\u4f9b\u6e96\u78ba\u4e14\u4e00\u81f4\u7684\u91ab\u7642\u63a8\u7406\u3002\u6211\u5011\u7684\u6578\u64da\u3001\u6a21\u578b\u548c\u4ee3\u78bc\u53ef\u5728 https://github.com/UCSC-VLAA/MedReason \u7372\u53d6\u3002\n", "author": "Juncheng Wu et.al.", "authors": "Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, Yuyin Zhou", "id": "2504.00993v2", "paper_url": "http://arxiv.org/abs/2504.00993v2", "repo": "https://github.com/UCSC-VLAA/MedReason"}}