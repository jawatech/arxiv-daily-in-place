{"2502.10389": {"publish_time": "2025-02-14", "title": "Region-Adaptive Sampling for Diffusion Transformers", "paper_summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.", "paper_summary_zh": "\u64f4\u6563\u6a21\u578b (DM) \u5df2\u6210\u70ba\u8de8\u9818\u57df\u751f\u6210\u4efb\u52d9\u7684\u9996\u9078\u3002\u7136\u800c\uff0c\u5b83\u5011\u4f9d\u8cf4\u65bc\u591a\u500b\u9806\u5e8f\u7684\u524d\u5411\u50b3\u905e\uff0c\u9019\u986f\u8457\u9650\u5236\u4e86\u5be6\u6642\u6548\u80fd\u3002\u5148\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u65bc\u6e1b\u5c11\u63a1\u6a23\u6b65\u9a5f\u7684\u6578\u91cf\u6216\u91cd\u8907\u4f7f\u7528\u4e2d\u9593\u7d50\u679c\uff0c\u7531\u65bc\u5377\u7a4d U-Net \u7d50\u69cb\u7684\u9650\u5236\uff0c\u7121\u6cd5\u5229\u7528\u5f71\u50cf\u4e2d\u5404\u7a7a\u9593\u5340\u57df\u7684\u8b8a\u5316\u3002\u900f\u904e\u5229\u7528\u64f4\u6563Transformer (DiT) \u5728\u8655\u7406\u53ef\u8b8a\u6578\u91cf\u6b0a\u6756\u6642\u7684\u9748\u6d3b\u6027\uff0c\u6211\u5011\u5f15\u5165\u4e86 RAS\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u3001\u7121\u9700\u8a13\u7df4\u7684\u63a1\u6a23\u7b56\u7565\uff0c\u53ef\u6839\u64da DiT \u6a21\u578b\u7684\u7126\u9ede\uff0c\u52d5\u614b\u5730\u5c07\u4e0d\u540c\u7684\u63a1\u6a23\u6bd4\u7387\u5206\u914d\u7d66\u5f71\u50cf\u4e2d\u7684\u5340\u57df\u3002\u6211\u5011\u7684\u95dc\u9375\u89c0\u5bdf\u662f\uff0c\u5728\u6bcf\u500b\u63a1\u6a23\u6b65\u9a5f\u4e2d\uff0c\u6a21\u578b\u90fd\u96c6\u4e2d\u5728\u8a9e\u7fa9\u4e0a\u6709\u610f\u7fa9\u7684\u5340\u57df\uff0c\u800c\u9019\u4e9b\u7126\u9ede\u5340\u57df\u5728\u9023\u7e8c\u7684\u6b65\u9a5f\u4e2d\u5c55\u73fe\u51fa\u5f37\u70c8\u7684\u9023\u7e8c\u6027\u3002\u5229\u7528\u9019\u500b\u898b\u89e3\uff0cRAS \u50c5\u66f4\u65b0\u76ee\u524d\u7126\u9ede\u6240\u5728\u7684\u5340\u57df\uff0c\u800c\u5176\u4ed6\u5340\u57df\u5247\u4f7f\u7528\u4f86\u81ea\u524d\u4e00\u6b65\u9a5f\u7684\u5feb\u53d6\u96dc\u8a0a\u9032\u884c\u66f4\u65b0\u3002\u8a72\u6a21\u578b\u7684\u7126\u9ede\u53d6\u6c7a\u65bc\u524d\u4e00\u6b65\u9a5f\u7684\u8f38\u51fa\uff0c\u4e26\u5229\u7528\u6211\u5011\u89c0\u5bdf\u5230\u7684\u6642\u9593\u4e00\u81f4\u6027\u3002\u6211\u5011\u5728 Stable Diffusion 3 \u548c Lumina-Next-T2I \u4e0a\u8a55\u4f30 RAS\uff0c\u5206\u5225\u9054\u5230 2.36 \u500d\u548c 2.51 \u500d\u7684\u52a0\u901f\uff0c\u800c\u751f\u6210\u54c1\u8cea\u7684\u964d\u4f4e\u5e45\u5ea6\u5f88\u5c0f\u3002\u6b64\u5916\uff0c\u4e00\u9805\u4f7f\u7528\u8005\u7814\u7a76\u986f\u793a\uff0cRAS \u5728\u4eba\u985e\u8a55\u4f30\u4e0b\u63d0\u4f9b\u4e86\u53ef\u6bd4\u8f03\u7684\u54c1\u8cea\uff0c\u540c\u6642\u5be6\u73fe\u4e86 1.6 \u500d\u7684\u52a0\u901f\u3002\u6211\u5011\u7684\u505a\u6cd5\u671d\u8457\u66f4\u6709\u6548\u7387\u7684\u64f4\u6563Transformer\u9081\u51fa\u4e86\u4e00\u5927\u6b65\uff0c\u63d0\u5347\u4e86\u5b83\u5011\u5728\u5be6\u6642\u61c9\u7528\u4e2d\u7684\u6f5b\u529b\u3002", "author": "Ziming Liu et.al.", "authors": "Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang", "id": "2502.10389v1", "paper_url": "http://arxiv.org/abs/2502.10389v1", "repo": "null"}}