{"2406.08035": {"publish_time": "2024-06-12", "title": "LVBench: An Extreme Long Video Understanding Benchmark", "paper_summary": "Recent progress in multimodal large language models has markedly enhanced the\nunderstanding of short videos (typically under one minute), and several\nevaluation datasets have emerged accordingly. However, these advancements fall\nshort of meeting the demands of real-world applications such as embodied\nintelligence for long-term decision-making, in-depth movie reviews and\ndiscussions, and live sports commentary, all of which require comprehension of\nlong videos spanning several hours. To address this gap, we introduce LVBench,\na benchmark specifically designed for long video understanding. Our dataset\ncomprises publicly sourced videos and encompasses a diverse set of tasks aimed\nat long video comprehension and information extraction. LVBench is designed to\nchallenge multimodal models to demonstrate long-term memory and extended\ncomprehension capabilities. Our extensive evaluations reveal that current\nmultimodal models still underperform on these demanding long video\nunderstanding tasks. Through LVBench, we aim to spur the development of more\nadvanced models capable of tackling the complexities of long video\ncomprehension. Our data and code are publicly available at:\nhttps://lvbench.github.io.", "paper_summary_zh": "\u6700\u8fd1\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u77ed\u89c6\u9891\uff08\u901a\u5e38\u5728 1 \u5206\u949f\u5185\uff09\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u51fa\u73b0\u4e86\u51e0\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u8fdb\u6b65\u672a\u80fd\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7a0b\u5e8f\u7684\u9700\u6c42\uff0c\u4f8b\u5982\u9762\u5411\u957f\u671f\u51b3\u7b56\u5236\u5b9a\u7684\u5177\u8eab\u667a\u80fd\u3001\u6df1\u5165\u7684\u7535\u5f71\u8bc4\u8bba\u548c\u8ba8\u8bba\u4ee5\u53ca\u73b0\u573a\u4f53\u80b2\u8bc4\u8bba\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u9700\u8981\u7406\u89e3\u8de8\u8d8a\u6570\u5c0f\u65f6\u7684\u957f\u89c6\u9891\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 LVBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u7406\u89e3\u957f\u89c6\u9891\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5305\u62ec\u516c\u5f00\u83b7\u53d6\u7684\u89c6\u9891\uff0c\u5e76\u5305\u542b\u4e00\u7ec4\u9488\u5bf9\u957f\u89c6\u9891\u7406\u89e3\u548c\u4fe1\u606f\u63d0\u53d6\u7684\u4efb\u52a1\u3002LVBench \u65e8\u5728\u6311\u6218\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ee5\u5c55\u793a\u957f\u671f\u8bb0\u5fc6\u548c\u6269\u5c55\u7406\u89e3\u80fd\u529b\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u8fd9\u4e9b\u8981\u6c42\u82db\u523b\u7684\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u4ecd\u7136\u8868\u73b0\u4e0d\u4f73\u3002\u901a\u8fc7 LVBench\uff0c\u6211\u4eec\u65e8\u5728\u4fc3\u8fdb\u80fd\u591f\u5e94\u5bf9\u957f\u89c6\u9891\u7406\u89e3\u7684\u590d\u6742\u6027\u7684\u66f4\u9ad8\u7ea7\u6a21\u578b\u7684\u5f00\u53d1\u3002\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u53ef\u516c\u5f00\u83b7\u53d6\uff1ahttps://lvbench.github.io\u3002", "author": "Weihan Wang et.al.", "authors": "Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang", "id": "2406.08035v1", "paper_url": "http://arxiv.org/abs/2406.08035v1", "repo": "null"}}