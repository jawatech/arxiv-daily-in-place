{"2502.14538": {"publish_time": "2025-02-20", "title": "LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization", "paper_summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing, but their full fine-tuning remains resource-intensive.\nParameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have emerged as a practical solution by approximating parameter updates\nwith low-rank matrices. However, LoRA often exhibits a \"double descent\"\nphenomenon during fine-tuning, where model performance degrades due to\noverfitting and limited expressiveness caused by low-rank constraints. To\naddress this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation\nOptimization), a novel method that leverages gradient and weight norms to\ngenerate targeted perturbations. By optimizing the sharpness of the loss\nlandscape, LoRA-GGPO guides the model toward flatter minima, mitigating the\ndouble descent problem and improving generalization. Extensive experiments on\nnatural language understanding (NLU) and generation (NLG) tasks demonstrate\nthat LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore,\nextended experiments specifically designed to analyze the double descent\nphenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing\nmore robust and generalizable models. Our work provides a robust and efficient\nsolution for fine-tuning LLMs, with broad applicability in real-world\nscenarios. The code is available at https://github.com/llm172/LoRA-GGPO.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6210\u529f\uff0c\u4f46\u5b83\u5011\u7684\u5b8c\u5168\u5fae\u8abf\u4ecd\u7136\u9700\u8981\u5927\u91cf\u8cc7\u6e90\u3002\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff08\u4f8b\u5982\u4f4e\u79e9\u9069\u61c9 (LoRA)\uff09\u5df2\u6210\u70ba\u4e00\u7a2e\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5b83\u901a\u904e\u4f4e\u79e9\u77e9\u9663\u8fd1\u4f3c\u53c3\u6578\u66f4\u65b0\u3002\u7136\u800c\uff0cLoRA \u5728\u5fae\u8abf\u904e\u7a0b\u4e2d\u7d93\u5e38\u8868\u73fe\u51fa\u300c\u96d9\u91cd\u4e0b\u964d\u300d\u73fe\u8c61\uff0c\u5176\u4e2d\u6a21\u578b\u6027\u80fd\u6703\u56e0\u904e\u5ea6\u64ec\u5408\u548c\u4f4e\u79e9\u7d04\u675f\u5c0e\u81f4\u7684\u8868\u9054\u80fd\u529b\u6709\u9650\u800c\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LoRA-GGPO\uff08\u68af\u5ea6\u5f15\u5c0e\u64fe\u52d5\u512a\u5316\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u5229\u7528\u68af\u5ea6\u548c\u6b0a\u91cd\u7bc4\u6578\u4f86\u7522\u751f\u76ee\u6a19\u64fe\u52d5\u7684\u65b0\u65b9\u6cd5\u3002\u901a\u904e\u512a\u5316\u640d\u5931\u51fd\u6578\u66f2\u9762\u7684\u9661\u5ea6\uff0cLoRA-GGPO \u5f15\u5c0e\u6a21\u578b\u671d\u5411\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u5f9e\u800c\u6e1b\u8f15\u96d9\u91cd\u4e0b\u964d\u554f\u984c\u4e26\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002\u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u548c\u751f\u6210 (NLG) \u4efb\u52d9\u4e2d\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cLoRA-GGPO \u512a\u65bc LoRA \u53ca\u5176\u6700\u5148\u9032\u7684\u8b8a\u9ad4\u3002\u6b64\u5916\uff0c\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u5206\u6790\u96d9\u91cd\u4e0b\u964d\u73fe\u8c61\u7684\u5ef6\u4f38\u5be6\u9a57\u8b49\u5be6\uff0cLoRA-GGPO \u6709\u6548\u5730\u7de9\u89e3\u4e86\u9019\u500b\u554f\u984c\uff0c\u7522\u751f\u4e86\u66f4\u5f37\u5927\u4e14\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u70ba\u5fae\u8abf LLM \u63d0\u4f9b\u4e86\u4e00\u500b\u5f37\u5927\u4e14\u9ad8\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5728\u73fe\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u5177\u6709\u5ee3\u6cdb\u7684\u9069\u7528\u6027\u3002\u4ee3\u78bc\u53ef\u5728 https://github.com/llm172/LoRA-GGPO \u7372\u5f97\u3002", "author": "Yupeng Chang et.al.", "authors": "Yupeng Chang, Chenlu Guo, Yi Chang, Yuan Wu", "id": "2502.14538v1", "paper_url": "http://arxiv.org/abs/2502.14538v1", "repo": "null"}}