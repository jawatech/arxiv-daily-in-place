{"2405.03425": {"publish_time": "2024-05-06", "title": "Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models", "paper_summary": "Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and\npoor calibration, particularly when fine-tuned on small datasets. To address\nthese challenges, we propose a simple combination of Low-Rank Adaptation (LoRA)\nwith Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate\nBayesian inference in LLMs. Through extensive testing across several Natural\nLanguage Processing (NLP) benchmarks, we demonstrate that our straightforward\nand computationally efficient approach improves model generalization and\ncalibration. We further show that our method exhibits greater robustness\nagainst distribution shift, as reflected in its performance on\nout-of-distribution tasks.", "paper_summary_zh": "", "author": "Emre Onal et.al.", "authors": "Emre Onal,Klemens Fl\u00f6ge,Emma Caldwell,Arsen Sheverdin,Vincent Fortuin", "id": "2405.03425v1", "paper_url": "http://arxiv.org/abs/2405.03425v1", "repo": "null"}}