{"2412.11937": {"publish_time": "2024-12-16", "title": "Precise Length Control in Large Language Models", "paper_summary": "Large Language Models (LLMs) are increasingly used in production systems,\npowering applications such as chatbots, summarization, and question answering.\nDespite their success, controlling the length of their response remains a\nsignificant challenge, particularly for tasks requiring structured outputs or\nspecific levels of detail. In this work, we propose a method to adapt\npre-trained decoder-only LLMs for precise control of response length. Our\napproach incorporates a secondary length-difference positional encoding (LDPE)\ninto the input embeddings, which counts down to a user-set response termination\nlength. Fine-tuning with LDPE allows the model to learn to terminate responses\ncoherently at the desired length, achieving mean token errors of less than 3\ntokens. We also introduce Max New Tokens++, an extension that enables flexible\nupper-bound length control, rather than an exact target. Experimental results\non tasks such as question answering and document summarization demonstrate that\nour method enables precise length control without compromising response\nquality.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)  zunehmend in Produktionssystemen eingesetzt werden, die Anwendungen wie Chatbots, Zusammenfassungen und Fragen und Antworten unterst\u00fctzen. Trotz ihres Erfolgs bleibt die Steuerung der L\u00e4nge ihrer Antwort eine gro\u00dfe Herausforderung, insbesondere f\u00fcr Aufgaben, die strukturierte Ausgaben oder bestimmte Detailstufen erfordern. In dieser Arbeit schlagen wir eine Methode vor, vortrainierte Decoder-only-LLMs f\u00fcr eine pr\u00e4zise Steuerung der Antwortl\u00e4nge anzupassen. Unser Ansatz integriert eine sekund\u00e4re Positionskodierung der L\u00e4ngenunterschiede (LDPE) in die Eingabeembeddings, die bis zu einer vom Benutzer festgelegten L\u00e4nge der Beendigung der Antwort herunterz\u00e4hlt. Die Feinabstimmung mit LDPE erm\u00f6glicht es dem Modell, zu lernen, Antworten zusammenh\u00e4ngend in der gew\u00fcnschten L\u00e4nge zu beenden und mittlere Tokenfehler von weniger als 3 Token zu erzielen. Wir stellen au\u00dferdem Max New Tokens++ vor, eine Erweiterung, die eine flexible Steuerung der Obergrenze der L\u00e4nge erm\u00f6glicht, anstatt ein genaues Ziel. Experimentelle Ergebnisse zu Aufgaben wie Fragen und Antworten und Dokumentzusammenfassung zeigen, dass unsere Methode eine pr\u00e4zise L\u00e4ngenkontrolle erm\u00f6glicht, ohne die Qualit\u00e4t der Antwort zu beeintr\u00e4chtigen.", "author": "Bradley Butcher et.al.", "authors": "Bradley Butcher, Michael O'Keefe, James Titchener", "id": "2412.11937v1", "paper_url": "http://arxiv.org/abs/2412.11937v1", "repo": "null"}}