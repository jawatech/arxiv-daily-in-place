{"2502.14752": {"publish_time": "2025-02-20", "title": "TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators", "paper_summary": "Triton, a high-level Python-like language designed for building efficient GPU\nkernels, is widely adopted in deep learning frameworks due to its portability,\nflexibility, and accessibility. However, programming and parallel optimization\nstill require considerable trial and error from Triton developers. Despite\nadvances in large language models (LLMs) for conventional code generation,\nthese models struggle to generate accurate, performance-optimized Triton code,\nas they lack awareness of its specifications and the complexities of GPU\nprogramming. More critically, there is an urgent need for systematic\nevaluations tailored to Triton. In this work, we introduce TritonBench, the\nfirst comprehensive benchmark for Triton operator generation. TritonBench\nfeatures two evaluation channels: a curated set of 184 real-world operators\nfrom GitHub and a collection of operators aligned with PyTorch interfaces.\nUnlike conventional code benchmarks prioritizing functional correctness,\nTritonBench also profiles efficiency performance on widely deployed GPUs\naligned with industry applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to generate efficient Triton operators,\nhighlighting a significant gap in high-performance code generation. TritonBench\nwill be available at https://github.com/thunlp/TritonBench.", "paper_summary_zh": "Triton \u662f\u4e00\u7a2e\u9ad8\u968e\u7684\u985e Python \u8a9e\u8a00\uff0c\u5c08\u9580\u7528\u65bc\u5efa\u69cb\u9ad8\u6548\u7684 GPU \u6838\u5fc3\uff0c\u7531\u65bc\u5176\u53ef\u79fb\u690d\u6027\u3001\u9748\u6d3b\u6027\u53ca\u53ef\u5b58\u53d6\u6027\uff0c\u5df2\u5ee3\u6cdb\u63a1\u7528\u65bc\u6df1\u5ea6\u5b78\u7fd2\u6846\u67b6\u4e2d\u3002\u7136\u800c\uff0c\u7de8\u7a0b\u548c\u4e26\u884c\u6700\u4f73\u5316\u4ecd\u9700\u8981 Triton \u958b\u767c\u4eba\u54e1\u9032\u884c\u5927\u91cf\u7684\u8a66\u9a57\u548c\u932f\u8aa4\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u50b3\u7d71\u7a0b\u5f0f\u78bc\u7522\u751f\u65b9\u9762\u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u4f46\u9019\u4e9b\u6a21\u578b\u5728\u7522\u751f\u6e96\u78ba\u4e14\u6548\u80fd\u6700\u4f73\u5316\u7684 Triton \u7a0b\u5f0f\u78bc\u6642\u4ecd\u9762\u81e8\u56f0\u96e3\uff0c\u56e0\u70ba\u5b83\u5011\u7f3a\u4e4f\u5c0d\u5176\u898f\u683c\u548c GPU \u7de8\u7a0b\u8907\u96dc\u6027\u7684\u8a8d\u8b58\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8feb\u5207\u9700\u8981\u91dd\u5c0d Triton \u91cf\u8eab\u6253\u9020\u7684\u7cfb\u7d71\u6027\u8a55\u4f30\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 TritonBench\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d Triton \u7b97\u5b50\u7522\u751f\u9032\u884c\u5168\u9762\u8a55\u6bd4\u7684\u57fa\u6e96\u3002TritonBench \u5177\u6709\u5169\u500b\u8a55\u4f30\u7ba1\u9053\uff1a\u4e00\u7d44\u4f86\u81ea GitHub \u7684 184 \u500b\u771f\u5be6\u4e16\u754c\u7b97\u5b50\uff0c\u4ee5\u53ca\u4e00\u7d44\u8207 PyTorch \u4ecb\u9762\u5c0d\u9f4a\u7684\u7b97\u5b50\u3002\u8207\u512a\u5148\u8003\u616e\u529f\u80fd\u6b63\u78ba\u6027\u7684\u50b3\u7d71\u7a0b\u5f0f\u78bc\u57fa\u6e96\u4e0d\u540c\uff0cTritonBench \u9084\u5256\u6790\u4e86\u8207\u7522\u696d\u61c9\u7528\u5c0d\u9f4a\u7684\u5ee3\u6cdb\u90e8\u7f72 GPU \u4e0a\u7684\u6548\u80fd\u8868\u73fe\u3002\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c\u76ee\u524d\u6700\u5148\u9032\u7684\u7a0b\u5f0f\u78bc LLM \u96e3\u4ee5\u7522\u751f\u9ad8\u6548\u7684 Triton \u7b97\u5b50\uff0c\u7a81\u986f\u4e86\u9ad8\u6027\u80fd\u7a0b\u5f0f\u78bc\u7522\u751f\u4e2d\u7684\u91cd\u5927\u5dee\u8ddd\u3002TritonBench \u5c07\u5728 https://github.com/thunlp/TritonBench \u63d0\u4f9b\u3002", "author": "Jianling Li et.al.", "authors": "Jianling Li, Shangzhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, Haojie Wang, Jianrong Wang, Xu Han, Zhiyuan Liu, Maosong Sun", "id": "2502.14752v1", "paper_url": "http://arxiv.org/abs/2502.14752v1", "repo": "null"}}