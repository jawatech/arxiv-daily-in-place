{"2410.11371": {"publish_time": "2024-10-15", "title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "paper_summary": "Large Language Models (LLMs) have shown promising performance in text-to-SQL,\nwhich involves translating natural language questions into SQL queries.\nHowever, current text-to-SQL LLMs are computationally expensive and challenging\nto deploy in real-world applications, highlighting the importance of\ncompressing them. To achieve this goal, knowledge distillation (KD) is a common\napproach, which aims to distill the larger teacher model into a smaller student\nmodel. While numerous KD methods for autoregressive LLMs have emerged recently,\nit is still under-explored whether they work well in complex text-to-SQL\nscenarios. To this end, we conduct a series of analyses and reveal that these\nKD methods generally fall short in balancing performance and efficiency. In\nresponse to this problem, we propose to improve the KD with Imperfect Data,\nnamely KID, which effectively boosts the performance without introducing much\ntraining budget. The core of KID is to efficiently mitigate the\ntraining-inference mismatch by simulating the cascading effect of inference in\nthe imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks\nshow that, KID can not only achieve consistent and significant performance\ngains (up to +5.83% average score) across all model types and sizes, but also\neffectively improve the training efficiency.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6587\u672c\u8f49 SQL \u4e2d\u5c55\u73fe\u4e86\u6709\u524d\u9014\u7684\u8868\u73fe\uff0c\u5176\u4e2d\u6d89\u53ca\u5c07\u81ea\u7136\u8a9e\u8a00\u554f\u984c\u8f49\u63db\u70ba SQL \u67e5\u8a62\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6587\u672c\u8f49 SQL LLM \u5728\u904b\u7b97\u4e0a\u5f88\u6602\u8cb4\uff0c\u4e14\u96e3\u4ee5\u90e8\u7f72\u5728\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u4e2d\uff0c\u9019\u51f8\u986f\u4e86\u58d3\u7e2e\u5b83\u5011\u7684\u91cd\u8981\u6027\u3002\u70ba\u4e86\u9054\u6210\u9019\u500b\u76ee\u6a19\uff0c\u77e5\u8b58\u84b8\u993e\uff08KD\uff09\u662f\u4e00\u7a2e\u5e38\u898b\u7684\u65b9\u6cd5\uff0c\u5176\u76ee\u6a19\u662f\u5c07\u8f03\u5927\u7684\u6559\u5e2b\u6a21\u578b\u84b8\u993e\u6210\u8f03\u5c0f\u7684\u5b78\u751f\u6a21\u578b\u3002\u5118\u7ba1\u6700\u8fd1\u51fa\u73fe\u4e86\u8a31\u591a\u91dd\u5c0d\u81ea\u8ff4\u6b78 LLM \u7684 KD \u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u662f\u5426\u9069\u7528\u65bc\u8907\u96dc\u7684\u6587\u672c\u8f49 SQL \u5834\u666f\u4ecd\u6709\u5f85\u63a2\u8a0e\u3002\u70ba\u6b64\uff0c\u6211\u5011\u9032\u884c\u4e86\u4e00\u7cfb\u5217\u5206\u6790\uff0c\u4e26\u63ed\u793a\u9019\u4e9b KD \u65b9\u6cd5\u901a\u5e38\u7121\u6cd5\u5728\u6548\u80fd\u548c\u6548\u7387\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u8b70\u4f7f\u7528\u4e0d\u5b8c\u7f8e\u8cc7\u6599\u6539\u5584 KD\uff0c\u5373 KID\uff0c\u5b83\u6709\u6548\u5730\u63d0\u5347\u4e86\u6548\u80fd\uff0c\u800c\u4e0d\u6703\u5f15\u5165\u592a\u591a\u8a13\u7df4\u9810\u7b97\u3002KID \u7684\u6838\u5fc3\u662f\u900f\u904e\u6a21\u64ec\u4e0d\u5b8c\u7f8e\u8a13\u7df4\u8cc7\u6599\u4e2d\u63a8\u8ad6\u7684\u5c64\u758a\u6548\u61c9\uff0c\u6709\u6548\u5730\u6e1b\u8f15\u8a13\u7df4\u63a8\u8ad6\u4e0d\u5339\u914d\u7684\u554f\u984c\u3002\u5728 5 \u500b\u6587\u672c\u8f49 SQL \u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0cKID \u4e0d\u50c5\u53ef\u4ee5\u5728\u6240\u6709\u6a21\u578b\u985e\u578b\u548c\u898f\u6a21\u4e0a\u5be6\u73fe\u4e00\u81f4\u4e14\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff08\u5e73\u5747\u5206\u6578\u63d0\u5347\u9054 +5.83%\uff09\uff0c\u9084\u80fd\u6709\u6548\u63d0\u5347\u8a13\u7df4\u6548\u7387\u3002", "author": "Qihuang Zhong et.al.", "authors": "Qihuang Zhong, Kunfeng Chen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao", "id": "2410.11371v1", "paper_url": "http://arxiv.org/abs/2410.11371v1", "repo": "null"}}