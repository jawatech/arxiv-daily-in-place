{"2404.17926": {"publish_time": "2024-04-27", "title": "Pre-training on High Definition X-ray Images: An Experimental Study", "paper_summary": "Existing X-ray based pre-trained vision models are usually conducted on a\nrelatively small-scale dataset (less than 500k samples) with limited resolution\n(e.g., 224 $\\times$ 224). However, the key to the success of self-supervised\npre-training large models lies in massive training data, and maintaining high\nresolution in the field of X-ray images is the guarantee of effective solutions\nto difficult miscellaneous diseases. In this paper, we address these issues by\nproposing the first high-definition (1280 $\\times$ 1280) X-ray based\npre-trained foundation vision model on our newly collected large-scale dataset\nwhich contains more than 1 million X-ray images. Our model follows the masked\nauto-encoder framework which takes the tokens after mask processing (with a\nhigh rate) is used as input, and the masked image patches are reconstructed by\nthe Transformer encoder-decoder network. More importantly, we introduce a novel\ncontext-aware masking strategy that utilizes the chest contour as a boundary\nfor adaptive masking operations. We validate the effectiveness of our model on\ntwo downstream tasks, including X-ray report generation and disease\nrecognition. Extensive experiments demonstrate that our pre-trained medical\nfoundation vision model achieves comparable or even new state-of-the-art\nperformance on downstream benchmark datasets. The source code and pre-trained\nmodels of this paper will be released on\nhttps://github.com/Event-AHU/Medical_Image_Analysis.", "paper_summary_zh": "", "author": "Xiao Wang et.al.", "authors": "Xiao Wang,Yuehang Li,Wentao Wu,Jiandong Jin,Yao Rong,Bo Jiang,Chuanfu Li,Jin Tang", "id": "2404.17926v1", "paper_url": "http://arxiv.org/abs/2404.17926v1", "repo": "https://github.com/event-ahu/medical_image_analysis"}}