{"2412.08174": {"publish_time": "2024-12-11", "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?", "paper_summary": "While great success has been achieved in building vision models with\nContrastive Language-Image Pre-training (CLIP) over Internet-scale image-text\npairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is\nchallenging because of three fundamental issues: the scarcity of labeled data\nand text supervision, different levels of downstream tasks, and the conceptual\ngaps between domains. In this work, to address these issues, we leverage\nmulti-modal prompt learning to effectively adapt pre-trained GNN to downstream\ntasks and data, given only a few semantically labeled samples, each with\nextremely weak text supervision. Our new paradigm embeds the graphs directly in\nthe same space as the Large Language Models (LLMs) by learning both graph\nprompts and text prompts simultaneously. To accomplish this, we improve\nstate-of-the-art graph prompt method, and then propose the first graph-language\nmulti-modal prompt learning approach for exploiting the knowledge in\npre-trained models. Notably, due to the insufficient supervision for\nfine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen,\nso the learnable parameters are much fewer than fine-tuning any pre-trained\nmodel. Through extensive experiments on real-world datasets, we demonstrate the\nsuperior performance of our paradigm in few-shot, multi-task-level, and\ncross-domain settings. Moreover, we build the first CLIP-style zero-shot\nclassification prototype that can generalize GNNs to unseen classes with\nextremely weak text supervision.", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u5728\u4f7f\u7528\u7db2\u969b\u7db2\u8def\u898f\u6a21\u7684\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u9032\u884c\u5c0d\u6bd4\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (CLIP) \u4f86\u5efa\u7acb\u8996\u89ba\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\uff0c\u4f46\u4f7f\u7528 CLIP \u7ba1\u7dda\u5efa\u7acb\u53ef\u8f49\u79fb\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u537b\u5f88\u5177\u6311\u6230\u6027\uff0c\u539f\u56e0\u5728\u65bc\u4e09\u500b\u6839\u672c\u554f\u984c\uff1a\u6a19\u8a18\u8cc7\u6599\u548c\u6587\u5b57\u76e3\u7763\u7684\u7a00\u5c11\u6027\u3001\u4e0d\u540c\u5c64\u7d1a\u7684\u4e0b\u6e38\u4efb\u52d9\uff0c\u4ee5\u53ca\u4e0d\u540c\u9818\u57df\u4e4b\u9593\u7684\u6982\u5ff5\u5dee\u8ddd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5229\u7528\u591a\u6a21\u614b\u63d0\u793a\u5b78\u7fd2\uff0c\u5728\u50c5\u6709\u5c11\u6578\u8a9e\u7fa9\u6a19\u8a18\u7bc4\u4f8b\u7684\u60c5\u6cc1\u4e0b\uff0c\u6709\u6548\u5730\u8abf\u6574\u9810\u8a13\u7df4\u7684 GNN \u4ee5\u9069\u7528\u65bc\u4e0b\u6e38\u4efb\u52d9\u548c\u8cc7\u6599\uff0c\u6bcf\u500b\u7bc4\u4f8b\u90fd\u5177\u6709\u6975\u5176\u8584\u5f31\u7684\u6587\u5b57\u76e3\u7763\u3002\u6211\u5011\u7684\u65b0\u7bc4\u4f8b\u5c07\u5716\u5f62\u76f4\u63a5\u5d4c\u5165\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u76f8\u540c\u7684\u7a7a\u9593\u4e2d\uff0c\u65b9\u6cd5\u662f\u540c\u6642\u5b78\u7fd2\u5716\u5f62\u63d0\u793a\u548c\u6587\u5b57\u63d0\u793a\u3002\u70ba\u4e86\u9054\u6210\u9019\u500b\u76ee\u6a19\uff0c\u6211\u5011\u6539\u9032\u4e86\u6700\u5148\u9032\u7684\u5716\u5f62\u63d0\u793a\u65b9\u6cd5\uff0c\u7136\u5f8c\u63d0\u51fa\u7b2c\u4e00\u500b\u5716\u5f62\u8a9e\u8a00\u591a\u6a21\u614b\u63d0\u793a\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u7684\u77e5\u8b58\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7531\u65bc\u5fae\u8abf\u7684\u76e3\u7763\u4e0d\u8db3\uff0c\u5728\u6211\u5011\u7684\u7bc4\u4f8b\u4e2d\uff0c\u9810\u8a13\u7df4\u7684 GNN \u548c LLM \u4fdd\u6301\u51cd\u7d50\u72c0\u614b\uff0c\u56e0\u6b64\u53ef\u5b78\u7fd2\u53c3\u6578\u9060\u5c11\u65bc\u5fae\u8abf\u4efb\u4f55\u9810\u8a13\u7df4\u6a21\u578b\u3002\u900f\u904e\u5c0d\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u7bc4\u4f8b\u5728\u5c11\u6a23\u672c\u3001\u591a\u4efb\u52d9\u5c64\u7d1a\u548c\u8de8\u9818\u57df\u8a2d\u5b9a\u4e2d\u7684\u5353\u8d8a\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u7b2c\u4e00\u500b CLIP \u98a8\u683c\u7684\u96f6\u6a23\u672c\u5206\u985e\u539f\u578b\uff0c\u5b83\u53ef\u4ee5\u5c07 GNN \u63a8\u5ee3\u5230\u5177\u6709\u6975\u5176\u8584\u5f31\u6587\u5b57\u76e3\u7763\u7684\u672a\u898b\u985e\u5225\u3002</paragraph>", "author": "Zihao Li et.al.", "authors": "Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han", "id": "2412.08174v2", "paper_url": "http://arxiv.org/abs/2412.08174v2", "repo": "null"}}