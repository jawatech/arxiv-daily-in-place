{"2405.21046": {"publish_time": "2024-05-31", "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF", "paper_summary": "Reinforcement learning from human feedback (RLHF) has emerged as a central\ntool for language model alignment. We consider online exploration in RLHF,\nwhich exploits interactive access to human or AI feedback by deliberately\nencouraging the model to produce diverse, maximally informative responses. By\nallowing RLHF to confidently stray from the pre-trained model, online\nexploration offers the possibility of novel, potentially super-human\ncapabilities, but its full potential as a paradigm for language model training\nhas yet to be realized, owing to computational and statistical bottlenecks in\ndirectly adapting existing reinforcement learning techniques. We propose a new\nalgorithm for online exploration in RLHF, Exploratory Preference Optimization\n(XPO), which is simple and practical -- a one-line change to (online) Direct\nPreference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the\nstrongest known provable guarantees and promising empirical performance. XPO\naugments the DPO objective with a novel and principled exploration bonus,\nempowering the algorithm to explore outside the support of the initial model\nand human feedback data. In theory, we show that XPO is provably\nsample-efficient and converges to a near-optimal language model policy under\nnatural exploration conditions, irrespective of whether the initial model has\ngood coverage. Our analysis, which builds on the observation that DPO\nimplicitly performs a form of $Q^{\\star}$-approximation (or, Bellman error\nminimization), combines previously disparate techniques from language modeling\nand theoretical reinforcement learning in a serendipitous fashion through the\nperspective of KL-regularized Markov decision processes. Empirically, we find\nthat XPO is more sample-efficient than non-exploratory DPO variants in a\npreliminary evaluation.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u6210\u70ba\u8a9e\u8a00\u6a21\u578b\u5c0d\u9f4a\u7684\u6838\u5fc3\u5de5\u5177\u3002\u6211\u5011\u8003\u616e RLHF \u4e2d\u7684\u7dda\u4e0a\u63a2\u7d22\uff0c\u5b83\u900f\u904e\u6545\u610f\u9f13\u52f5\u6a21\u578b\u7522\u751f\u591a\u6a23\u5316\u3001\u6975\u5177\u8cc7\u8a0a\u6027\u7684\u56de\u61c9\uff0c\u4f86\u5584\u7528\u8207\u4eba\u985e\u6216 AI \u56de\u994b\u7684\u4e92\u52d5\u5b58\u53d6\u3002\u900f\u904e\u5141\u8a31 RLHF \u81ea\u4fe1\u5730\u504f\u96e2\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u7dda\u4e0a\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u7a4e\u3001\u6f5b\u5728\u8d85\u8d8a\u4eba\u985e\u7684\u80fd\u529b\uff0c\u4f46\u7531\u65bc\u5728\u76f4\u63a5\u8abf\u6574\u73fe\u6709\u5f37\u5316\u5b78\u7fd2\u6280\u8853\u6642\u6703\u9047\u5230\u904b\u7b97\u548c\u7d71\u8a08\u74f6\u9838\uff0c\u56e0\u6b64\u5b83\u4f5c\u70ba\u8a9e\u8a00\u6a21\u578b\u8a13\u7df4\u5178\u7bc4\u7684\u5168\u90e8\u6f5b\u529b\u5c1a\u672a\u5be6\u73fe\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684 RLHF \u7dda\u4e0a\u63a2\u7d22\u6f14\u7b97\u6cd5\uff0c\u63a2\u7d22\u6027\u504f\u597d\u6700\u4f73\u5316 (XPO)\uff0c\u5b83\u7c21\u55ae\u4e14\u5be6\u7528\uff0c\u53ea\u9700\u5c0d (\u7dda\u4e0a) \u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO\uff1bRafailov \u7b49\u4eba\uff0c2023 \u5e74) \u9032\u884c\u4e00\u884c\u8b8a\u66f4\uff0c\u4f46\u537b\u4eab\u6709\u5df2\u77e5\u6700\u5f37\u5927\u7684\u53ef\u8b49\u660e\u4fdd\u8b49\u548c\u6709\u524d\u9014\u7684\u7d93\u9a57\u6548\u80fd\u3002XPO \u4ee5\u65b0\u7a4e\u4e14\u6709\u539f\u5247\u7684\u63a2\u7d22\u734e\u52f5\u4f86\u64f4\u5145 DPO \u76ee\u6a19\uff0c\u8ce6\u4e88\u6f14\u7b97\u6cd5\u5728\u521d\u59cb\u6a21\u578b\u548c\u4eba\u985e\u56de\u994b\u8cc7\u6599\u7684\u652f\u6301\u5916\u63a2\u7d22\u7684\u80fd\u529b\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u8b49\u660e XPO \u5728\u81ea\u7136\u63a2\u7d22\u689d\u4ef6\u4e0b\uff0c\u7121\u8ad6\u521d\u59cb\u6a21\u578b\u662f\u5426\u6709\u826f\u597d\u7684\u6db5\u84cb\u7bc4\u570d\uff0c\u90fd\u53ef\u8b49\u660e\u5177\u6709\u6a23\u672c\u6548\u7387\uff0c\u4e26\u6536\u6582\u5230\u63a5\u8fd1\u6700\u4f73\u7684\u8a9e\u8a00\u6a21\u578b\u653f\u7b56\u3002\u6211\u5011\u7684\u5206\u6790\u5efa\u7acb\u5728 DPO \u96b1\u542b\u57f7\u884c\u4e00\u7a2e\u5f62\u5f0f\u7684 $Q^{\\star}$-\u8fd1\u4f3c\uff08\u6216 Bellman \u8aa4\u5dee\u6700\u5c0f\u5316\uff09\u7684\u89c0\u5bdf\u4e0a\uff0c\u5b83\u900f\u904e KL \u6b63\u898f\u5316\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b\u7684\u89d2\u5ea6\uff0c\u4ee5\u4e00\u7a2e\u610f\u5916\u7684\u65b9\u5f0f\u7d50\u5408\u4e86\u8a9e\u8a00\u5efa\u6a21\u548c\u7406\u8ad6\u5f37\u5316\u5b78\u7fd2\u4e2d\u5148\u524d\u4e0d\u540c\u7684\u6280\u8853\u3002\u5728\u7d93\u9a57\u4e0a\uff0c\u6211\u5011\u767c\u73fe XPO \u5728\u521d\u6b65\u8a55\u4f30\u4e2d\u6bd4\u975e\u63a2\u7d22\u6027 DPO \u8b8a\u9ad4\u66f4\u5177\u6a23\u672c\u6548\u7387\u3002", "author": "Tengyang Xie et.al.", "authors": "Tengyang Xie, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, Alexander Rakhlin", "id": "2405.21046v1", "paper_url": "http://arxiv.org/abs/2405.21046v1", "repo": "null"}}