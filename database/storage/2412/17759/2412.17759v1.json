{"2412.17759": {"publish_time": "2024-12-23", "title": "Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy", "paper_summary": "Multimodal learning, a rapidly evolving field in artificial intelligence,\nseeks to construct more versatile and robust systems by integrating and\nanalyzing diverse types of data, including text, images, audio, and video.\nInspired by the human ability to assimilate information through many senses,\nthis method enables applications such as text-to-video conversion, visual\nquestion answering, and image captioning. Recent developments in datasets that\nsupport multimodal language models (MLLMs) are highlighted in this overview.\nLarge-scale multimodal datasets are essential because they allow for thorough\ntesting and training of these models. With an emphasis on their contributions\nto the discipline, the study examines a variety of datasets, including those\nfor training, domain-specific tasks, and real-world applications. It also\nemphasizes how crucial benchmark datasets are for assessing models' performance\nin a range of scenarios, scalability, and applicability. Since multimodal\nlearning is always changing, overcoming these obstacles will help AI research\nand applications reach new heights.", "paper_summary_zh": "\u591a\u6a21\u6001\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e2d\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\n\u5b83\u901a\u8fc7\u6574\u5408\u548c\u5206\u6790\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u5728\u5185\u7684\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\n\u65e8\u5728\u6784\u5efa\u66f4\u591a\u529f\u80fd\u5f3a\u5927\u4e14\u5065\u58ee\u7684\u7cfb\u7edf\u3002\n\u53d7\u4eba\u7c7b\u901a\u8fc7\u591a\u79cd\u611f\u5b98\u5438\u6536\u4fe1\u606f\u7684\u80fd\u529b\u7684\u542f\u53d1\uff0c\n\u8fd9\u79cd\u65b9\u6cd5\u652f\u6301\u6587\u672c\u8f6c\u89c6\u9891\u8f6c\u6362\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u56fe\u50cf\u6807\u9898\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002\n\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u652f\u6301\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u6570\u636e\u96c6\u7684\u6700\u65b0\u53d1\u5c55\u3002\n\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u5141\u8bb8\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5f7b\u5e95\u7684\u6d4b\u8bd5\u548c\u8bad\u7ec3\u3002\n\u672c\u7814\u7a76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5b83\u4eec\u5bf9\u8be5\u5b66\u79d1\u7684\u8d21\u732e\uff0c\u8003\u5bdf\u4e86\u5404\u79cd\u6570\u636e\u96c6\uff0c\u5305\u62ec\u7528\u4e8e\u8bad\u7ec3\u3001\u7279\u5b9a\u9886\u57df\u7684\u4efb\u52a1\u548c\u5b9e\u9645\u5e94\u7528\u7a0b\u5e8f\u7684\u6570\u636e\u96c6\u3002\n\u5b83\u8fd8\u5f3a\u8c03\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u5bf9\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u5404\u79cd\u573a\u666f\u3001\u53ef\u6269\u5c55\u6027\u548c\u9002\u7528\u6027\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\n\u7531\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\u603b\u662f\u5728\u53d8\u5316\uff0c\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u5c06\u6709\u52a9\u4e8e\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u548c\u5e94\u7528\u8fbe\u5230\u65b0\u7684\u9ad8\u5ea6\u3002", "author": "Priyaranjan Pattnayak et.al.", "authors": "Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar", "id": "2412.17759v1", "paper_url": "http://arxiv.org/abs/2412.17759v1", "repo": "null"}}