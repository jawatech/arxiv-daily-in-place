{"2411.05665": {"publish_time": "2024-11-08", "title": "Unmasking the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal", "paper_summary": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities.", "paper_summary_zh": "\u9019\u7bc7\u8ad6\u6587\u56b4\u683c\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8655\u7406\u906e\u853d\u6587\u5b57\u7684\u80fd\u529b\uff0c\u9032\u800c\u95e1\u660e\u5176\u9650\u5236\u3002\u6211\u5011\u5f15\u5165\u4e86\u5169\u9805\u65b0\u4efb\u52d9\uff1aMskQA\uff0c\u7528\u65bc\u8861\u91cf\u5728\u906e\u853d\u554f\u7b54\u8cc7\u6599\u96c6\uff08\u5982 RealtimeQA\uff09\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff1b\u4ee5\u53ca MskCal\uff0c\u7528\u65bc\u8a55\u4f30\u5728\u906e\u853d\u7b97\u8853\u554f\u984c\u4e0a\u7684\u6578\u503c\u63a8\u7406\u80fd\u529b\u3002\u6e2c\u8a66 GPT-4o \u548c 4o-mini \u986f\u793a\uff0c\u5118\u7ba1 LLM \u5c0d\u906e\u853d\u6587\u5b57\u5177\u6709\u4e00\u5b9a\u7684\u97cc\u6027\uff0c\u4f46\u5176\u6548\u80fd\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u906e\u853d\u7387\u548c\u8a9e\u7fa9\u7dda\u7d22\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u300c\u5b8c\u5168\u906e\u853d\u300d\uff08\u8a9e\u7fa9\u7dda\u7d22\u5b8c\u5168\u4e0d\u5b58\u5728\uff09\u6703\u5c0e\u81f4\u6548\u80fd\u986f\u8457\u4e0b\u964d\uff0c\u800c\u300c\u90e8\u5206\u89e3\u9664\u300d\uff08\u4fdd\u7559\u4e00\u4e9b\u8a9e\u7fa9\u8cc7\u8a0a\uff09\u5247\u4e0d\u6703\uff0c\u9019\u8868\u793a LLM \u4f9d\u8cf4\u65bc\u8868\u9762\u6a21\u5f0f\u3002\u6709\u8da3\u7684\u662f\uff0cGPT-4o \u7684\u8868\u73fe\u59cb\u7d42\u512a\u65bc 4o-mini\uff0c\u7279\u5225\u662f\u5728 MskCal \u4e2d\uff0c\u9019\u986f\u793a\u51fa\u5b83\u5728\u8655\u7406\u906e\u853d\u6587\u5b57\u6578\u503c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u66f4\u5f37\u3002\u9019\u7a81\u986f\u4e86\u8a9e\u7fa9\u7dda\u7d22\u5728 LLM \u63a8\u7406\u904e\u7a0b\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u6211\u5011\u7684\u7814\u7a76\u95e1\u660e\u4e86\u80cc\u666f\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\u5728\u906e\u853d\u6587\u5b57\u8655\u7406\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u70ba\u66f4\u6df1\u5165\u4e86\u89e3 LLM \u7684\u80fd\u529b\u548c\u9650\u5236\u92ea\u8def\uff0c\u4e26\u5f37\u8abf\u9700\u8981\u66f4\u5065\u5168\u7684\u8a55\u4f30\u65b9\u6cd5\u4f86\u6e96\u78ba\u8a55\u4f30\u5176\u771f\u6b63\u7684\u7406\u89e3\u80fd\u529b\u3002", "author": "Fuka Matsuzaki et.al.", "authors": "Fuka Matsuzaki, Haru-Tada Sato", "id": "2411.05665v1", "paper_url": "http://arxiv.org/abs/2411.05665v1", "repo": "https://github.com/isfhub/maskcode"}}