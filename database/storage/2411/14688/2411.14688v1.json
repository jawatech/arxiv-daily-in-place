{"2411.14688": {"publish_time": "2024-11-22", "title": "Whats in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning", "paper_summary": "Generating automatic dense captions for videos that accurately describe their\ncontents remains a challenging area of research. Most current models require\nprocessing the entire video at once. Instead, we propose an efficient, online\napproach which outputs frequent, detailed and temporally aligned captions,\nwithout access to future frames. Our model uses a novel autoregressive\nfactorized decoding architecture, which models the sequence of visual features\nfor each time segment, outputting localized descriptions and efficiently\nleverages the context from the previous video segments. This allows the model\nto output frequent, detailed captions to more comprehensively describe the\nvideo, according to its actual local content, rather than mimic the training\ndata. Second, we propose an optimization for efficient training and inference,\nwhich enables scaling to longer videos. Our approach shows excellent\nperformance compared to both offline and online methods, and uses 20\\% less\ncompute. The annotations produced are much more comprehensive and frequent, and\ncan further be utilized in automatic video tagging and in large-scale video\ndata harvesting.", "paper_summary_zh": "\u751f\u6210\u81ea\u52d5\u4e14\u8a73\u7d30\u7684\u5f71\u7247\u5b57\u5e55\uff0c\u4e26\u6e96\u78ba\u63cf\u8ff0\u5f71\u7247\u5167\u5bb9\uff0c\u4e00\u76f4\u662f\u7814\u7a76\u9818\u57df\u4e2d\u7684\u4e00\u5927\u6311\u6230\u3002\u76ee\u524d\u5927\u591a\u6578\u6a21\u578b\u90fd\u9700\u8981\u4e00\u6b21\u8655\u7406\u5b8c\u6574\u5f71\u7247\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u9ad8\u6548\u7684\u7dda\u4e0a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5b58\u53d6\u672a\u4f86\u5f71\u683c\u7684\u60c5\u6cc1\u4e0b\uff0c\u8f38\u51fa\u983b\u7e41\u3001\u8a73\u7d30\u4e14\u6642\u9593\u5c0d\u9f4a\u7684\u5b57\u5e55\u3002\u6211\u5011\u7684\u6a21\u578b\u4f7f\u7528\u4e00\u7a2e\u65b0\u7a4e\u7684\u81ea\u8ff4\u6b78\u5206\u89e3\u89e3\u78bc\u67b6\u69cb\uff0c\u5b83\u6703\u70ba\u6bcf\u500b\u6642\u9593\u5340\u6bb5\u5efa\u6a21\u8996\u89ba\u7279\u5fb5\u5e8f\u5217\uff0c\u8f38\u51fa\u5c40\u90e8\u63cf\u8ff0\uff0c\u4e26\u6709\u6548\u5229\u7528\u524d\u4e00\u500b\u5f71\u7247\u5340\u6bb5\u7684\u5167\u5bb9\u3002\u9019\u8b93\u6a21\u578b\u53ef\u4ee5\u8f38\u51fa\u983b\u7e41\u4e14\u8a73\u7d30\u7684\u5b57\u5e55\uff0c\u6839\u64da\u5be6\u969b\u7684\u5c40\u90e8\u5167\u5bb9\u66f4\u5168\u9762\u5730\u63cf\u8ff0\u5f71\u7247\uff0c\u800c\u4e0d\u662f\u6a21\u4eff\u8a13\u7df4\u8cc7\u6599\u3002\u5176\u6b21\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7528\u65bc\u9ad8\u6548\u8a13\u7df4\u548c\u63a8\u8ad6\u7684\u6700\u4f73\u5316\uff0c\u9019\u8b93\u6211\u5011\u80fd\u5920\u64f4\u5145\u5230\u66f4\u9577\u7684\u5f71\u7247\u3002\u6211\u5011\u7684\u505a\u6cd5\u8207\u96e2\u7dda\u548c\u7dda\u4e0a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c55\u73fe\u4e86\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u4e26\u6e1b\u5c11\u4e86 20% \u7684\u904b\u7b97\u3002\u7522\u751f\u7684\u8a3b\u89e3\u66f4\u5168\u9762\u4e14\u983b\u7e41\uff0c\u53ef\u7528\u65bc\u81ea\u52d5\u5f71\u7247\u6a19\u8a18\u548c\u5927\u578b\u5f71\u7247\u8cc7\u6599\u6536\u96c6\u3002", "author": "AJ Piergiovanni et.al.", "authors": "AJ Piergiovanni, Dahun Kim, Michael S. Ryoo, Isaac Noble, Anelia Angelova", "id": "2411.14688v1", "paper_url": "http://arxiv.org/abs/2411.14688v1", "repo": "null"}}