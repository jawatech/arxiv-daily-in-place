{"2406.05955": {"publish_time": "2024-06-10", "title": "Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters", "paper_summary": "Exploiting activation sparsity is a promising approach to significantly\naccelerating the inference process of large language models (LLMs) without\ncompromising performance. However, activation sparsity is determined by\nactivation functions, and commonly used ones like SwiGLU and GeGLU exhibit\nlimited sparsity. Simply replacing these functions with ReLU fails to achieve\nsufficient sparsity. Moreover, inadequate training data can further increase\nthe risk of performance degradation. To address these challenges, we propose a\nnovel dReLU function, which is designed to improve LLM activation sparsity,\nalong with a high-quality training data mixture ratio to facilitate effective\nsparsification. Additionally, we leverage sparse activation patterns within the\nFeed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to\nfurther boost efficiency. By applying our neuron sparsification method to the\nMistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are\nactivated per inference iteration, respectively, while achieving even more\npowerful model performance. Evaluation results demonstrate that this sparsity\nachieves a 2-5x decoding speedup. Remarkably, on mobile phones, our\nTurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second.\nOur models are available at \\url{https://huggingface.co/PowerInfer}", "paper_summary_zh": "\u5229\u7528\u6fc0\u6d3b\u7a00\u758f\u6027\u662f\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u7136\u800c\uff0c\u6fc0\u6d3b\u7a00\u758f\u6027\u662f\u7531\u6fc0\u6d3b\u51fd\u6570\u51b3\u5b9a\u7684\uff0c\u800c\u5e38\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u5982 SwiGLU \u548c GeGLU\uff09\u8868\u73b0\u51fa\u7684\u7a00\u758f\u6027\u6709\u9650\u3002\u7b80\u5355\u5730\u7528 ReLU \u66ff\u6362\u8fd9\u4e9b\u51fd\u6570\u65e0\u6cd5\u8fbe\u5230\u8db3\u591f\u7684\u7a00\u758f\u6027\u3002\u6b64\u5916\uff0c\u4e0d\u8db3\u7684\u8bad\u7ec3\u6570\u636e\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\u6027\u80fd\u4e0b\u964d\u7684\u98ce\u9669\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 dReLU \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u65e8\u5728\u63d0\u9ad8 LLM \u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u5e76\u91c7\u7528\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u6765\u4fc3\u8fdb\u6709\u6548\u7684\u7a00\u758f\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u6df7\u5408\u4e13\u5bb6 (MoE) \u6a21\u578b\u7684\u9988\u9001\u524d\u5411\u7f51\u7edc (FFN) \u4e13\u5bb6\u4e2d\u7684\u7a00\u758f\u6fc0\u6d3b\u6a21\u5f0f\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\u3002\u901a\u8fc7\u5c06\u6211\u4eec\u7684\u795e\u7ecf\u5143\u7a00\u758f\u5316\u65b9\u6cd5\u5e94\u7528\u4e8e Mistral \u548c Mixtral \u6a21\u578b\uff0c\u6bcf\u6b21\u63a8\u7406\u8fed\u4ee3\u4ec5\u6fc0\u6d3b 25 \u4ebf\u548c 43 \u4ebf\u4e2a\u53c2\u6570\uff0c\u540c\u65f6\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6027\u80fd\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u7a00\u758f\u6027\u5b9e\u73b0\u4e86 2-5 \u500d\u7684\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u79fb\u52a8\u7535\u8bdd\u4e0a\uff0c\u6211\u4eec\u7684 TurboSparse-Mixtral-47B \u7684\u63a8\u7406\u901f\u5ea6\u8fbe\u5230\u6bcf\u79d2 11 \u4e2a\u6807\u8bb0\u3002\u6211\u4eec\u7684\u6a21\u578b\u53ef\u5728 \\url{https://huggingface.co/PowerInfer} \u83b7\u5f97", "author": "Yixin Song et.al.", "authors": "Yixin Song, Haotong Xie, Zhengyan Zhang, Bo Wen, Li Ma, Zeyu Mi, Haibo Chen", "id": "2406.05955v2", "paper_url": "http://arxiv.org/abs/2406.05955v2", "repo": "null"}}