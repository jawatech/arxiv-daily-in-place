{"2501.16215": {"publish_time": "2025-01-27", "title": "Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models", "paper_summary": "Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u7642\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u7684\u8996\u89ba\u6aa2\u67e5\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u9054\u5230\u4e86\u8207\u4eba\u985e\u81e8\u5e8a\u91ab\u751f\u76f8\u7576\u7684\u719f\u7df4\u5ea6\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u5ee3\u6cdb\u7bc4\u570d\u9650\u5236\u4e86\u7279\u5b9a\u9818\u57df\u7684\u7cbe\u78ba\u5ea6\uff0c\u800c\u5c08\u6709\u6b0a\u91cd\u963b\u7919\u4e86\u91dd\u5c0d\u7279\u5b9a\u8cc7\u6599\u96c6\u7684\u5fae\u8abf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5c0f\u578b\u5c08\u7528\u6a21\u578b (SSM) \u5728\u76ee\u6a19\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u8907\u96dc\u81e8\u5e8a\u6c7a\u7b56\u5236\u5b9a\u6240\u9700\u7684\u80cc\u666f\u63a8\u7406\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 ConMIL\uff08\u5171\u5f62\u591a\u5be6\u4f8b\u5b78\u7fd2\uff09\uff0c\u9019\u662f\u4e00\u500b\u8207 LLM \u7121\u7e2b\u6574\u5408\u7684\u6c7a\u7b56\u652f\u63f4 SSM\u3002\u900f\u904e\u4f7f\u7528\u591a\u5be6\u4f8b\u5b78\u7fd2 (MIL) \u4f86\u8b58\u5225\u81e8\u5e8a\u986f\u8457\u8a0a\u865f\u5340\u6bb5\uff0c\u4e26\u5c0d\u6821\u6e96\u7684\u96c6\u5408\u503c\u8f38\u51fa\u9032\u884c\u5171\u5f62\u9810\u6e2c\uff0cConMIL \u589e\u5f37\u4e86 LLM \u5c0d\u91ab\u7642\u6642\u9593\u5e8f\u5217\u5206\u6790\u7684\u89e3\u91cb\u80fd\u529b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cConMIL \u660e\u986f\u6539\u5584\u4e86\u6700\u5148\u9032 LLM \u7684\u6548\u80fd\uff0c\u4f8b\u5982 ChatGPT4.0 \u548c Qwen2-VL-7B\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\\ConMIL{}- \u652f\u63f4\u7684 Qwen2-VL-7B \u5728\u5fc3\u5f8b\u4e0d\u6574\u5075\u6e2c\u548c\u7761\u7720\u5206\u671f\u4e2d\uff0c\u5c0d\u65bc\u6709\u4fe1\u5fc3\u7684\u6a23\u672c\u9054\u5230\u4e86 94.92% \u548c 96.82% \u7684\u7cbe\u78ba\u5ea6\uff0c\u800c\u7368\u7acb LLM \u7684\u6e96\u78ba\u5ea6\u50c5\u70ba 46.13% \u548c 13.16%\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86 ConMIL \u5728\u6a4b\u63a5\u7279\u5b9a\u4efb\u52d9\u7684\u7cbe\u78ba\u5ea6\u548c\u66f4\u5ee3\u6cdb\u7684\u80cc\u666f\u63a8\u7406\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u53ef\u9760\u4e14\u53ef\u89e3\u91cb\u7684 AI \u9a45\u52d5\u81e8\u5e8a\u6c7a\u7b56\u652f\u63f4\u3002", "author": "Huayu Li et.al.", "authors": "Huayu Li, Xiwen Chen, Ci Zhang, Stuart F. Quan, William D. S. Killgore, Shu-Fen Wung, Chen X. Chen, Geng Yuan, Jin Lu, Ao Li", "id": "2501.16215v1", "paper_url": "http://arxiv.org/abs/2501.16215v1", "repo": "null"}}