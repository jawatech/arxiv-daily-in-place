{"2412.09230": {"publish_time": "2024-12-12", "title": "Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering", "paper_summary": "This paper tackles the intricate challenge of video question-answering\n(VideoQA). Despite notable progress, current methods fall short of effectively\nintegrating questions with video frames and semantic object-level abstractions\nto create question-aware video representations. We introduce Local-Global\nQuestion Aware Video Embedding (LGQAVE), which incorporates three major\ninnovations to integrate multi-modal knowledge better and emphasize semantic\nvisual concepts relevant to specific questions. LGQAVE moves beyond traditional\nad-hoc frame sampling by utilizing a cross-attention mechanism that precisely\nidentifies the most relevant frames concerning the questions. It captures the\ndynamics of objects within these frames using distinct graphs, grounding them\nin question semantics with the miniGPT model. These graphs are processed by a\nquestion-aware dynamic graph transformer (Q-DGT), which refines the outputs to\ndevelop nuanced global and local video representations. An additional\ncross-attention module integrates these local and global embeddings to generate\nthe final video embeddings, which a language model uses to generate answers.\nExtensive evaluations across multiple benchmarks demonstrate that LGQAVE\nsignificantly outperforms existing models in delivering accurate multi-choice\nand open-ended answers.", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u4e86\u5f71\u7247\u554f\u7b54 (VideoQA) \u7684\u8907\u96dc\u6311\u6230\u3002\u5118\u7ba1\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u76ee\u524d\u7684\u6280\u8853\u4ecd\u7121\u6cd5\u6709\u6548\u7d50\u5408\u554f\u984c\u3001\u5f71\u7247\u756b\u9762\u548c\u8a9e\u7fa9\u7269\u4ef6\u5c64\u7d1a\u62bd\u8c61\uff0c\u4ee5\u5efa\u7acb\u554f\u984c\u611f\u77e5\u7684\u5f71\u7247\u8868\u5fb5\u3002\u6211\u5011\u5f15\u9032\u4e86\u5c40\u90e8-\u5168\u57df\u554f\u984c\u611f\u77e5\u5f71\u7247\u5d4c\u5165 (LGQAVE)\uff0c\u5b83\u5305\u542b\u4e09\u9805\u91cd\u5927\u5275\u65b0\uff0c\u4ee5\u66f4\u597d\u5730\u6574\u5408\u591a\u6a21\u5f0f\u77e5\u8b58\uff0c\u4e26\u5f37\u8abf\u8207\u7279\u5b9a\u554f\u984c\u76f8\u95dc\u7684\u8a9e\u7fa9\u8996\u89ba\u6982\u5ff5\u3002LGQAVE \u8d85\u8d8a\u4e86\u50b3\u7d71\u7684\u81e8\u6642\u756b\u9762\u53d6\u6a23\uff0c\u5229\u7528\u8de8\u6ce8\u610f\u529b\u6a5f\u5236\u7cbe\u78ba\u627e\u51fa\u8207\u554f\u984c\u6700\u76f8\u95dc\u7684\u756b\u9762\u3002\u5b83\u4f7f\u7528\u4e0d\u540c\u7684\u5716\u5f62\u6355\u6349\u9019\u4e9b\u756b\u9762\u4e2d\u7269\u4ef6\u7684\u52d5\u614b\uff0c\u4e26\u900f\u904e miniGPT \u6a21\u578b\u5c07\u5b83\u5011\u5960\u57fa\u65bc\u554f\u984c\u8a9e\u7fa9\u4e2d\u3002\u9019\u4e9b\u5716\u5f62\u7531\u554f\u984c\u611f\u77e5\u52d5\u614b\u5716\u5f62\u8f49\u63db\u5668 (Q-DGT) \u8655\u7406\uff0c\u5b83\u6703\u6539\u5584\u8f38\u51fa\uff0c\u4ee5\u958b\u767c\u7d30\u7dfb\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5f71\u7247\u8868\u5fb5\u3002\u984d\u5916\u7684\u8de8\u6ce8\u610f\u529b\u6a21\u7d44\u6574\u5408\u9019\u4e9b\u5c40\u90e8\u548c\u5168\u5c40\u5d4c\u5165\uff0c\u4ee5\u7522\u751f\u6700\u7d42\u7684\u5f71\u7247\u5d4c\u5165\uff0c\u8a9e\u8a00\u6a21\u578b\u4f7f\u7528\u9019\u4e9b\u5d4c\u5165\u4f86\u7522\u751f\u7b54\u6848\u3002\u8de8\u591a\u500b\u57fa\u6e96\u7684\u5ee3\u6cdb\u8a55\u4f30\u8b49\u660e\uff0cLGQAVE \u5728\u63d0\u4f9b\u6e96\u78ba\u7684\u591a\u9078\u548c\u958b\u653e\u5f0f\u7b54\u6848\u65b9\u9762\uff0c\u660e\u986f\u512a\u65bc\u73fe\u6709\u6a21\u578b\u3002", "author": "Sai Bhargav Rongali et.al.", "authors": "Sai Bhargav Rongali, Mohamad Hassan N C, Ankit Jha, Neha Bhargava, Saurabh Prasad, Biplab Banerjee", "id": "2412.09230v1", "paper_url": "http://arxiv.org/abs/2412.09230v1", "repo": "null"}}