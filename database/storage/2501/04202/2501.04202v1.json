{"2501.04202": {"publish_time": "2025-01-08", "title": "Generative Dataset Distillation Based on Self-knowledge Distillation", "paper_summary": "Dataset distillation is an effective technique for reducing the cost and\ncomplexity of model training while maintaining performance by compressing large\ndatasets into smaller, more efficient versions. In this paper, we present a\nnovel generative dataset distillation method that can improve the accuracy of\naligning prediction logits. Our approach integrates self-knowledge distillation\nto achieve more precise distribution matching between the synthetic and\noriginal data, thereby capturing the overall structure and relationships within\nthe data. To further improve the accuracy of alignment, we introduce a\nstandardization step on the logits before performing distribution matching,\nensuring consistency in the range of logits. Through extensive experiments, we\ndemonstrate that our method outperforms existing state-of-the-art methods,\nresulting in superior distillation performance.", "paper_summary_zh": "\u8cc7\u6599\u96c6\u84b8\u993e\u662f\u4e00\u7a2e\u6709\u6548\u6280\u8853\uff0c\u53ef\u7528\u65bc\u964d\u4f4e\u6a21\u578b\u8a13\u7df4\u7684\u6210\u672c\u548c\u8907\u96dc\u6027\uff0c\u540c\u6642\u900f\u904e\u5c07\u5927\u578b\u8cc7\u6599\u96c6\u58d3\u7e2e\u6210\u66f4\u5c0f\u3001\u66f4\u6709\u6548\u7387\u7684\u7248\u672c\u4f86\u7dad\u6301\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u751f\u6210\u8cc7\u6599\u96c6\u84b8\u993e\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6539\u5584\u5c0d\u9f4a\u9810\u6e2c logit \u7684\u6e96\u78ba\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u6574\u5408\u81ea\u6211\u77e5\u8b58\u84b8\u993e\uff0c\u4ee5\u5728\u5408\u6210\u8cc7\u6599\u548c\u539f\u59cb\u8cc7\u6599\u4e4b\u9593\u9054\u6210\u66f4\u7cbe\u78ba\u7684\u5206\u5e03\u6bd4\u5c0d\uff0c\u5f9e\u800c\u64f7\u53d6\u8cc7\u6599\u4e2d\u7684\u6574\u9ad4\u7d50\u69cb\u548c\u95dc\u4fc2\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u6539\u5584\u5c0d\u9f4a\u7684\u6e96\u78ba\u6027\uff0c\u6211\u5011\u5728\u57f7\u884c\u5206\u5e03\u6bd4\u5c0d\u4e4b\u524d\uff0c\u65bc logit \u4e0a\u5f15\u5165\u6a19\u6e96\u5316\u6b65\u9a5f\uff0c\u78ba\u4fdd logit \u7bc4\u570d\u7684\u4e00\u81f4\u6027\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u512a\u65bc\u73fe\u6709\u7684\u6700\u5148\u9032\u65b9\u6cd5\uff0c\u9032\u800c\u7522\u751f\u51fa\u8272\u7684\u84b8\u993e\u6548\u80fd\u3002", "author": "Longzhen Li et.al.", "authors": "Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama", "id": "2501.04202v1", "paper_url": "http://arxiv.org/abs/2501.04202v1", "repo": "null"}}