{"2410.23182": {"publish_time": "2024-10-30", "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm", "paper_summary": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u4f86\uff0c\u57fa\u65bc Transformer \u7684\u67b6\u69cb\u4e3b\u5c0e\u4e86\u6a5f\u5668\u5b78\u7fd2\u7684\u5404\u500b\u9818\u57df\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u4e14\u5f37\u5927\u7684\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u65e8\u5728\u589e\u5f37\u57fa\u65bc Transformer \u7684\u67b6\u69cb\u7684\u97cc\u6027\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6b64\u6280\u8853\u53ef\u4ee5\u4f5c\u70ba\u5373\u63d2\u5373\u7528\u7684\u5c64\u6574\u5408\u5230\u73fe\u6709\u7684 Transformer \u4e2d\uff0c\u5728\u7121\u9700\u984d\u5916\u8a13\u7df4\u6216\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\u63d0\u9ad8\u5176\u7a69\u5065\u6027\u3002\u901a\u904e\u5168\u9762\u7684\u5be6\u9a57\u548c\u6d88\u878d\u7814\u7a76\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684 ProTransformer \u5728\u5404\u7a2e\u9810\u6e2c\u4efb\u52d9\u3001\u653b\u64ca\u6a5f\u5236\u3001\u4e3b\u5e79\u67b6\u69cb\u548c\u6578\u64da\u9818\u57df\u4e2d\u986f\u8457\u589e\u5f37\u4e86 Transformer \u6a21\u578b\u7684\u7a69\u5065\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u4e0d\u9032\u4e00\u6b65\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\uff0cProTransformer \u5728\u7d93\u5178\u7684 TextFooler \u653b\u64ca\u4e0b\uff0c\u5206\u5225\u70ba BERT\u3001ALBERT\u3001DistilBERT \u548c RoBERTa \u63d0\u5347\u4e86 19.5%\u300128.3%\u300116.1% \u548c 11.4% \u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cProTransformer \u5728\u57fa\u65bc\u63d0\u793a\u7684\u653b\u64ca\u4e2d\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u986f\u793a\u51fa\u6709\u5e0c\u671b\u7684\u97cc\u6027\uff0c\u5206\u5225\u5c07 T5 \u548c LLaMA \u7684\u6027\u80fd\u63d0\u5347\u4e86 24.8% \u548c 17.8%\uff0c\u4e26\u5728\u8d8a\u7344\u653b\u64ca\u4e2d\u5c07 Vicuna \u7684\u6027\u80fd\u5e73\u5747\u63d0\u5347\u4e86 10.4%\u3002\u9664\u4e86\u8a9e\u8a00\u9818\u57df\u4e4b\u5916\uff0cProTransformer \u5728\u8996\u89ba\u548c\u5716\u5f62\u9818\u57df\u4e5f\u8868\u73fe\u51fa\u51fa\u8272\u7684\u7a69\u5065\u6027\u3002</paragraph>", "author": "Zhichao Hou et.al.", "authors": "Zhichao Hou, Weizhi Gao, Yuchen Shen, Feiyi Wang, Xiaorui Liu", "id": "2410.23182v1", "paper_url": "http://arxiv.org/abs/2410.23182v1", "repo": "null"}}