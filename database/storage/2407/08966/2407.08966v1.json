{"2407.08966": {"publish_time": "2024-07-12", "title": "LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models", "paper_summary": "Out-of-distribution (OOD) detection is crucial for model reliability, as it\nidentifies samples from unknown classes and reduces errors due to unexpected\ninputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful\ntools for OOD detection by integrating multi-modal information. However, the\npractical application of such systems is challenged by manual prompt\nengineering, which demands domain expertise and is sensitive to linguistic\nnuances. In this paper, we introduce Label-driven Automated Prompt Tuning\n(LAPT), a novel approach to OOD detection that reduces the need for manual\nprompt engineering. We develop distribution-aware prompts with in-distribution\n(ID) class names and negative labels mined automatically. Training samples\nlinked to these class labels are collected autonomously via image synthesis and\nretrieval methods, allowing for prompt learning without manual effort. We\nutilize a simple cross-entropy loss for prompt optimization, with cross-modal\nand cross-distribution mixing strategies to reduce image noise and explore the\nintermediate space between distributions, respectively. The LAPT framework\noperates autonomously, requiring only ID class names as input and eliminating\nthe need for manual intervention. With extensive experiments, LAPT consistently\noutperforms manually crafted prompts, setting a new standard for OOD detection.\nMoreover, LAPT not only enhances the distinction between ID and OOD samples,\nbut also improves the ID classification accuracy and strengthens the\ngeneralization robustness to covariate shifts, resulting in outstanding\nperformance in challenging full-spectrum OOD detection tasks. Codes are\navailable at \\url{https://github.com/YBZh/LAPT}.", "paper_summary_zh": "<paragraph>\u7570\u5206\u5e03 (OOD) \u5075\u6e2c\u5c0d\u65bc\u6a21\u578b\u53ef\u9760\u6027\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u5b83\u80fd\u8b58\u5225\u4f86\u81ea\u672a\u77e5\u985e\u5225\u7684\u6a23\u672c\uff0c\u4e26\u6e1b\u5c11\u7531\u65bc\u610f\u5916\u8f38\u5165\u9020\u6210\u7684\u932f\u8aa4\u3002\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\uff0c\u4f8b\u5982 CLIP\uff0c\u6b63\u900f\u904e\u6574\u5408\u591a\u6a21\u614b\u8cc7\u8a0a\uff0c\u6210\u70ba OOD \u5075\u6e2c\u7684\u5f37\u5927\u5de5\u5177\u3002\u7136\u800c\uff0c\u6b64\u985e\u7cfb\u7d71\u7684\u5be6\u969b\u61c9\u7528\u53d7\u5230\u624b\u52d5\u63d0\u793a\u5de5\u7a0b\u7684\u6311\u6230\uff0c\u9019\u9700\u8981\u9818\u57df\u5c08\u696d\u77e5\u8b58\uff0c\u800c\u4e14\u5c0d\u8a9e\u8a00\u5dee\u7570\u5f88\u654f\u611f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u6a19\u7c64\u9a45\u52d5\u81ea\u52d5\u5316\u63d0\u793a\u8abf\u6574 (LAPT)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684 OOD \u5075\u6e2c\u65b9\u6cd5\uff0c\u53ef\u6e1b\u5c11\u5c0d\u624b\u52d5\u63d0\u793a\u5de5\u7a0b\u7684\u9700\u6c42\u3002\u6211\u5011\u958b\u767c\u51fa\u5177\u6709\u5206\u4f48\u611f\u77e5\u7684\u63d0\u793a\uff0c\u5176\u4e2d\u5305\u542b\u81ea\u52d5\u6316\u6398\u7684\u540c\u5206\u4f48 (ID) \u985e\u5225\u540d\u7a31\u548c\u8ca0\u9762\u6a19\u7c64\u3002\u9023\u7d50\u5230\u9019\u4e9b\u985e\u5225\u6a19\u7c64\u7684\u8a13\u7df4\u6a23\u672c\u662f\u900f\u904e\u5f71\u50cf\u5408\u6210\u548c\u6aa2\u7d22\u65b9\u6cd5\u81ea\u4e3b\u6536\u96c6\u7684\uff0c\u5141\u8a31\u63d0\u793a\u5b78\u7fd2\u800c\u7121\u9700\u624b\u52d5\u64cd\u4f5c\u3002\u6211\u5011\u5229\u7528\u7c21\u55ae\u7684\u4ea4\u53c9\u71b5\u640d\u5931\u9032\u884c\u63d0\u793a\u6700\u4f73\u5316\uff0c\u4e26\u63a1\u7528\u8de8\u6a21\u614b\u548c\u8de8\u5206\u4f48\u6df7\u5408\u7b56\u7565\uff0c\u5206\u5225\u7528\u65bc\u6e1b\u5c11\u5f71\u50cf\u96dc\u8a0a\u548c\u63a2\u7d22\u5206\u4f48\u4e4b\u9593\u7684\u4e2d\u9593\u7a7a\u9593\u3002LAPT \u6846\u67b6\u81ea\u4e3b\u904b\u4f5c\uff0c\u50c5\u9700\u8981 ID \u985e\u5225\u540d\u7a31\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u6d88\u9664\u4e86\u624b\u52d5\u4ecb\u5165\u7684\u9700\u8981\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0cLAPT \u6301\u7e8c\u512a\u65bc\u624b\u52d5\u88fd\u4f5c\u7684\u63d0\u793a\uff0c\u70ba OOD \u5075\u6e2c\u8a2d\u5b9a\u4e86\u65b0\u6a19\u6e96\u3002\u6b64\u5916\uff0cLAPT \u4e0d\u50c5\u589e\u5f37\u4e86 ID \u548c OOD \u6a23\u672c\u4e4b\u9593\u7684\u5340\u5225\uff0c\u9084\u6539\u5584\u4e86 ID \u5206\u985e\u7684\u6e96\u78ba\u6027\uff0c\u4e26\u52a0\u5f37\u4e86\u5c0d\u5354\u8b8a\u6578\u8f49\u79fb\u7684\u6cdb\u5316\u7a69\u5065\u6027\uff0c\u5f9e\u800c\u7522\u751f\u4e86\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u5168\u5149\u8b5c OOD \u5075\u6e2c\u4efb\u52d9\u4e2d\u7684\u5091\u51fa\u8868\u73fe\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/YBZh/LAPT} \u53d6\u5f97\u3002</paragraph>", "author": "Yabin Zhang et.al.", "authors": "Yabin Zhang, Wenjie Zhu, Chenhang He, Lei Zhang", "id": "2407.08966v1", "paper_url": "http://arxiv.org/abs/2407.08966v1", "repo": "https://github.com/ybzh/lapt"}}