{"2502.10248": {"publish_time": "2025-02-14", "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model", "paper_summary": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa Step-Video-T2V\uff0c\u4e00\u500b\u6700\u5148\u9032\u7684\u6587\u5b57\u8f49\u5f71\u7247\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u5177\u6709 30B \u53c3\u6578\uff0c\u4e26\u80fd\u5920\u7522\u751f\u9577\u9054 204 \u5e40\u7684\u5f71\u7247\u3002\u4e00\u500b\u6df1\u5ea6\u58d3\u7e2e\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668 Video-VAE\uff0c\u5c08\u70ba\u5f71\u7247\u7522\u751f\u4efb\u52d9\u800c\u8a2d\u8a08\uff0c\u53ef\u5be6\u73fe 16x16 \u7a7a\u9593\u548c 8x \u6642\u9593\u58d3\u7e2e\u6bd4\uff0c\u540c\u6642\u7dad\u6301\u51fa\u8272\u7684\u5f71\u7247\u91cd\u5efa\u54c1\u8cea\u3002\u4f7f\u7528\u5169\u500b\u96d9\u8a9e\u6587\u5b57\u7de8\u78bc\u5668\u5c0d\u4f7f\u7528\u8005\u63d0\u793a\u9032\u884c\u7de8\u78bc\uff0c\u4ee5\u8655\u7406\u82f1\u6587\u548c\u4e2d\u6587\u3002\u4f7f\u7528\u6d41\u5339\u914d\u8a13\u7df4\u5177\u6709 3D \u5168\u6ce8\u610f\u529b\u7684 DiT\uff0c\u4e26\u7528\u65bc\u5c07\u8f38\u5165\u96dc\u8a0a\u53bb\u566a\u6210\u6f5b\u5728\u5e40\u3002\u61c9\u7528\u57fa\u65bc\u5f71\u7247\u7684 DPO \u65b9\u6cd5 Video-DPO\uff0c\u4ee5\u6e1b\u5c11\u4eba\u5de5\u88fd\u54c1\u4e26\u63d0\u5347\u7522\u751f\u5f71\u7247\u7684\u8996\u89ba\u54c1\u8cea\u3002\u6211\u5011\u4e5f\u8a73\u7d30\u8aaa\u660e\u6211\u5011\u7684\u8a13\u7df4\u7b56\u7565\uff0c\u4e26\u5206\u4eab\u95dc\u9375\u89c0\u5bdf\u548c\u898b\u89e3\u3002Step-Video-T2V \u7684\u6548\u80fd\u662f\u5728\u4e00\u500b\u65b0\u7a4e\u7684\u5f71\u7247\u7522\u751f\u57fa\u6e96 Step-Video-T2V-Eval \u4e0a\u8a55\u4f30\uff0c\u8207\u958b\u6e90\u548c\u5546\u696d\u5f15\u64ce\u76f8\u6bd4\uff0c\u5c55\u793a\u5176\u6700\u5148\u9032\u7684\u6587\u5b57\u8f49\u5f71\u7247\u54c1\u8cea\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u7576\u524d\u57fa\u65bc\u64f4\u6563\u6a21\u578b\u7bc4\u4f8b\u7684\u9650\u5236\uff0c\u4e26\u6982\u8ff0\u5f71\u7247\u57fa\u790e\u6a21\u578b\u7684\u672a\u4f86\u65b9\u5411\u3002\u6211\u5011\u5728 https://github.com/stepfun-ai/Step-Video-T2V \u4e0a\u63d0\u4f9b Step-Video-T2V \u548c Step-Video-T2V-Eval\u3002\u4e5f\u53ef\u4ee5\u5f9e https://yuewen.cn/videos \u8a2a\u554f\u7dda\u4e0a\u7248\u672c\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u52a0\u901f\u5f71\u7247\u57fa\u790e\u6a21\u578b\u7684\u5275\u65b0\uff0c\u4e26\u8ce6\u80fd\u5f71\u7247\u5167\u5bb9\u5275\u4f5c\u8005\u3002</paragraph>", "author": "Guoqing Ma et.al.", "authors": "Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang", "id": "2502.10248v1", "paper_url": "http://arxiv.org/abs/2502.10248v1", "repo": "null"}}