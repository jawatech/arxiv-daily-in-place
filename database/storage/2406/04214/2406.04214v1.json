{"2406.04214": {"publish_time": "2024-06-06", "title": "ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models", "paper_summary": "Large Language Models (LLMs) are transforming diverse fields and gaining\nincreasing influence as human proxies. This development underscores the urgent\nneed for evaluating value orientations and understanding of LLMs to ensure\ntheir responsible integration into public-facing applications. This work\nintroduces ValueBench, the first comprehensive psychometric benchmark for\nevaluating value orientations and value understanding in LLMs. ValueBench\ncollects data from 44 established psychometric inventories, encompassing 453\nmultifaceted value dimensions. We propose an evaluation pipeline grounded in\nrealistic human-AI interactions to probe value orientations, along with novel\ntasks for evaluating value understanding in an open-ended value space. With\nextensive experiments conducted on six representative LLMs, we unveil their\nshared and distinctive value orientations and exhibit their ability to\napproximate expert conclusions in value-related extraction and generation\ntasks. ValueBench is openly accessible at\nhttps://github.com/Value4AI/ValueBench.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6b63\u5728\u8f49\u8b8a\u5404\u7a2e\u9818\u57df\uff0c\u4e26\u4f5c\u70ba\u4eba\u985e\u4ee3\u7406\u4eba\u7372\u5f97\u8d8a\u4f86\u8d8a\u5927\u7684\u5f71\u97ff\u529b\u3002\u9019\u7a2e\u767c\u5c55\u5f37\u8abf\u4e86\u8a55\u4f30\u50f9\u503c\u53d6\u5411\u548c\u7406\u89e3 LLM \u7684\u8feb\u5207\u9700\u8981\uff0c\u4ee5\u78ba\u4fdd\u5b83\u5011\u8ca0\u8cac\u4efb\u5730\u6574\u5408\u5230\u9762\u5411\u516c\u773e\u7684\u61c9\u7528\u7a0b\u5f0f\u4e2d\u3002\u9019\u9805\u5de5\u4f5c\u5f15\u5165\u4e86 ValueBench\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5168\u9762\u7684\u5fc3\u7406\u6e2c\u91cf\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u4e2d\u7684\u50f9\u503c\u53d6\u5411\u548c\u50f9\u503c\u7406\u89e3\u3002ValueBench \u5f9e 44 \u4efd\u65e2\u5b9a\u7684\u5fc3\u7406\u6e2c\u91cf\u6e05\u55ae\u4e2d\u6536\u96c6\u8cc7\u6599\uff0c\u6db5\u84cb 453 \u500b\u591a\u9762\u5411\u7684\u50f9\u503c\u7dad\u5ea6\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8a55\u4f30\u7ba1\u9053\uff0c\u5b83\u4ee5\u73fe\u5be6\u7684\u4eba\u5de5\u667a\u6167\u4e92\u52d5\u70ba\u57fa\u790e\uff0c\u7528\u65bc\u63a2\u8a0e\u50f9\u503c\u53d6\u5411\uff0c\u4ee5\u53ca\u8a55\u4f30\u958b\u653e\u5f0f\u50f9\u503c\u7a7a\u9593\u4e2d\u50f9\u503c\u7406\u89e3\u7684\u65b0\u4efb\u52d9\u3002\u900f\u904e\u5c0d\u516d\u500b\u5177\u4ee3\u8868\u6027\u7684 LLM \u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u63ed\u793a\u4e86\u5b83\u5011\u7684\u5171\u540c\u548c\u7368\u7279\u50f9\u503c\u53d6\u5411\uff0c\u4e26\u5c55\u793a\u4e86\u5b83\u5011\u5728\u8207\u50f9\u503c\u76f8\u95dc\u7684\u8403\u53d6\u548c\u751f\u6210\u4efb\u52d9\u4e2d\u903c\u8fd1\u5c08\u5bb6\u7d50\u8ad6\u7684\u80fd\u529b\u3002ValueBench \u53ef\u5728 https://github.com/Value4AI/ValueBench \u516c\u958b\u53d6\u5f97\u3002", "author": "Yuanyi Ren et.al.", "authors": "Yuanyi Ren, Haoran Ye, Hanjun Fang, Xin Zhang, Guojie Song", "id": "2406.04214v1", "paper_url": "http://arxiv.org/abs/2406.04214v1", "repo": "https://github.com/value4ai/valuebench"}}