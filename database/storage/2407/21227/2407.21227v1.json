{"2407.21227": {"publish_time": "2024-07-30", "title": "Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models", "paper_summary": "Large Language Models (LLMs) show promising potential in Software\nEngineering, especially for code-related tasks like code completion and code\ngeneration. LLMs' evaluation is generally centred around general metrics\ncomputed over benchmarks. While painting a macroscopic view of the benchmarks\nand of the LLMs' capacity, it is unclear how each programming task in these\nbenchmarks assesses the capabilities of the LLMs. In particular, the difficulty\nlevel of the tasks in the benchmarks is not reflected in the score used to\nreport the performance of the model. Yet, a model achieving a 90% score on a\nbenchmark of predominantly easy tasks is likely less capable than a model\nachieving a 90% score on a benchmark containing predominantly difficult tasks.\nThis paper devises a framework, HardEval, for assessing task difficulty for\nLLMs and crafting new tasks based on identified hard tasks. The framework uses\na diverse array of prompts for a single task across multiple LLMs to obtain a\ndifficulty score for each task of a benchmark. Using two code generation\nbenchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably\nidentify the hard tasks within those benchmarks, highlighting that only 21% of\nHumanEval+ and 27% of ClassEval tasks are hard for LLMs. Through our analysis\nof task difficulty, we also characterize 6 practical hard task topics which we\nused to generate new hard tasks. Orthogonal to current benchmarking evaluation\nefforts, HardEval can assist researchers and practitioners in fostering better\nassessments of LLMs. The difficulty score can be used to identify hard tasks\nwithin existing benchmarks. This, in turn, can be leveraged to generate more\nhard tasks centred around specific topics either for evaluation or improvement\nof LLMs. HardEval generalistic approach can be applied to other domains such as\ncode completion or Q/A.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8edf\u9ad4\u5de5\u7a0b\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u671f\u5f85\u7684\u6f5b\u529b\uff0c\u7279\u5225\u662f\u5728\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u7684\u4efb\u52d9\u4e2d\uff0c\u4f8b\u5982\u7a0b\u5f0f\u78bc\u5b8c\u6210\u548c\u7a0b\u5f0f\u78bc\u751f\u6210\u3002LLM \u7684\u8a55\u4f30\u901a\u5e38\u96c6\u4e2d\u5728\u57fa\u6e96\u4e0a\u8a08\u7b97\u51fa\u7684\u901a\u7528\u6307\u6a19\u3002\u96d6\u7136\u63cf\u7e6a\u51fa\u57fa\u6e96\u548c LLM \u80fd\u529b\u7684\u5de8\u89c0\u89c0\u9ede\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u9019\u4e9b\u57fa\u6e96\u4e2d\u7684\u6bcf\u500b\u7a0b\u5f0f\u8a2d\u8a08\u4efb\u52d9\u5982\u4f55\u8a55\u4f30 LLM \u7684\u80fd\u529b\u3002\u7279\u5225\u662f\uff0c\u57fa\u6e96\u4e2d\u4efb\u52d9\u7684\u96e3\u5ea6\u7b49\u7d1a\u4e26\u672a\u53cd\u6620\u5728\u7528\u65bc\u5831\u544a\u6a21\u578b\u6548\u80fd\u7684\u5206\u6578\u4e2d\u3002\u7136\u800c\uff0c\u5728\u4e3b\u8981\u7531\u5bb9\u6613\u4efb\u52d9\u7d44\u6210\u7684\u57fa\u6e96\u4e0a\u9054\u5230 90% \u5206\u6578\u7684\u6a21\u578b\uff0c\u5176\u80fd\u529b\u53ef\u80fd\u4f4e\u65bc\u5728\u4e3b\u8981\u7531\u56f0\u96e3\u4efb\u52d9\u7d44\u6210\u7684\u57fa\u6e96\u4e0a\u9054\u5230 90% \u5206\u6578\u7684\u6a21\u578b\u3002\u672c\u6587\u8a2d\u8a08\u4e86\u4e00\u500b\u67b6\u69cb HardEval\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u7684\u4efb\u52d9\u96e3\u5ea6\uff0c\u4e26\u6839\u64da\u8b58\u5225\u51fa\u7684\u56f0\u96e3\u4efb\u52d9\u5efa\u7acb\u65b0\u4efb\u52d9\u3002\u8a72\u67b6\u69cb\u91dd\u5c0d\u55ae\u4e00\u4efb\u52d9\u4f7f\u7528\u591a\u500b LLM \u7684\u5404\u7a2e\u63d0\u793a\uff0c\u4ee5\u53d6\u5f97\u57fa\u6e96\u4e2d\u6bcf\u500b\u4efb\u52d9\u7684\u96e3\u5ea6\u5206\u6578\u3002\u4f7f\u7528\u5169\u500b\u7a0b\u5f0f\u78bc\u751f\u6210\u57fa\u6e96 HumanEval+ \u548c ClassEval\uff0c\u6211\u5011\u5c55\u793a HardEval \u53ef\u4ee5\u53ef\u9760\u5730\u8b58\u5225\u9019\u4e9b\u57fa\u6e96\u4e2d\u7684\u56f0\u96e3\u4efb\u52d9\uff0c\u4e26\u5f37\u8abf\u53ea\u6709 21% \u7684 HumanEval+ \u548c 27% \u7684 ClassEval \u4efb\u52d9\u5c0d LLM \u4f86\u8aaa\u662f\u56f0\u96e3\u7684\u3002\u900f\u904e\u5c0d\u4efb\u52d9\u96e3\u5ea6\u7684\u5206\u6790\uff0c\u6211\u5011\u9084\u63cf\u8ff0\u4e86 6 \u500b\u5be6\u969b\u7684\u56f0\u96e3\u4efb\u52d9\u4e3b\u984c\uff0c\u6211\u5011\u7528\u9019\u4e9b\u4e3b\u984c\u4f86\u7522\u751f\u65b0\u7684\u56f0\u96e3\u4efb\u52d9\u3002\u8207\u76ee\u524d\u7684\u57fa\u6e96\u8a55\u4f30\u5de5\u4f5c\u6b63\u4ea4\uff0cHardEval \u53ef\u4ee5\u5354\u52a9\u7814\u7a76\u4eba\u54e1\u548c\u5f9e\u696d\u4eba\u54e1\u4fc3\u9032\u5c0d LLM \u7684\u66f4\u4f73\u8a55\u4f30\u3002\u96e3\u5ea6\u5206\u6578\u53ef\u7528\u65bc\u8b58\u5225\u73fe\u6709\u57fa\u6e96\u4e2d\u7684\u56f0\u96e3\u4efb\u52d9\u3002\u53cd\u904e\u4f86\uff0c\u9019\u53ef\u4ee5\u7528\u65bc\u7522\u751f\u66f4\u591a\u570d\u7e5e\u7279\u5b9a\u4e3b\u984c\u7684\u56f0\u96e3\u4efb\u52d9\uff0c\u4ee5\u7528\u65bc\u8a55\u4f30\u6216\u6539\u9032 LLM\u3002HardEval \u7684\u901a\u7528\u65b9\u6cd5\u53ef\u4ee5\u61c9\u7528\u65bc\u5176\u4ed6\u9818\u57df\uff0c\u4f8b\u5982\u7a0b\u5f0f\u78bc\u5b8c\u6210\u6216\u554f\u7b54\u3002", "author": "Florian Tambon et.al.", "authors": "Florian Tambon, Amin Nikanjam, Foutse Khomh, Giuliano Antoniol", "id": "2407.21227v1", "paper_url": "http://arxiv.org/abs/2407.21227v1", "repo": "null"}}