{"2408.12742": {"publish_time": "2024-08-22", "title": "TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based Computing", "paper_summary": "Due to the high computation overhead of Vision Transformers (ViTs), In-memory\nComputing architectures are being researched towards energy-efficient\ndeployment in edge-computing scenarios. Prior works have proposed efficient\nalgorithm-hardware co-design and IMC-architectural improvements to improve the\nenergy-efficiency of IMC-implemented ViTs. However, all prior works have\nneglected the overhead and co-depencence of attention blocks on the\naccuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose\nTReX- an attention-reuse-driven ViT optimization framework that effectively\nperforms attention reuse in ViT models to achieve optimal\naccuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer\nencoders for attention reuse to achieve near iso-accuracy performance while\nmeeting the user-specified delay requirement. Based on our analysis on the\nImagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and\n1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S\n(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP\nreduction compared to state-of-the-art token pruning and weight sharing\napproaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal\naccuracy compared to baseline at 1.6x lower EDAP.", "paper_summary_zh": "\u7531\u65bc\u8996\u89ba\u8f49\u63db\u5668 (ViT) \u7684\u9ad8\u904b\u7b97\u8ca0\u64d4\uff0c\u5167\u5b58\u4e2d\u904b\u7b97\u67b6\u69cb\u6b63\u88ab\u7814\u7a76\u7528\u65bc\u908a\u7de3\u904b\u7b97\u5834\u666f\u4e2d\u7684\u7bc0\u80fd\u90e8\u7f72\u3002\u5148\u524d\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u6f14\u7b97\u6cd5\u786c\u9ad4\u5354\u540c\u8a2d\u8a08\u548c IMC \u67b6\u69cb\u6539\u9032\uff0c\u4ee5\u63d0\u9ad8 IMC \u5be6\u4f5c\u7684 ViT \u7684\u80fd\u6548\u3002\u7136\u800c\uff0c\u6240\u6709\u5148\u524d\u7684\u7814\u7a76\u90fd\u5ffd\u7565\u4e86 IMC \u5be6\u4f5c\u7684 ViT \u7684\u6e96\u78ba\u5ea6\u3001\u80fd\u8017\u3001\u5ef6\u9072\u548c\u9762\u7a4d\u4e0a\u7684\u6ce8\u610f\u529b\u5340\u584a\u7684\u8ca0\u64d4\u548c\u76f8\u4e92\u4f9d\u8cf4\u6027\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TReX\uff0c\u4e00\u500b\u7531\u6ce8\u610f\u529b\u91cd\u7528\u9a45\u52d5\u7684 ViT \u6700\u4f73\u5316\u67b6\u69cb\uff0c\u5b83\u6709\u6548\u5730\u5728 ViT \u6a21\u578b\u4e2d\u57f7\u884c\u6ce8\u610f\u529b\u91cd\u7528\uff0c\u4ee5\u9054\u6210\u6700\u4f73\u7684\u6e96\u78ba\u5ea6\u3001\u80fd\u8017\u3001\u5ef6\u9072\u548c\u9762\u7a4d\u6298\u8877\u3002TReX \u6700\u4f73\u5316\u5730\u9078\u64c7Transformer\u7de8\u78bc\u5668\u9032\u884c\u6ce8\u610f\u529b\u91cd\u7528\uff0c\u4ee5\u9054\u6210\u8fd1\u4f3c\u7b49\u6e96\u78ba\u5ea6\u7684\u6548\u80fd\uff0c\u540c\u6642\u7b26\u5408\u4f7f\u7528\u8005\u6307\u5b9a\u7684\u5ef6\u9072\u9700\u6c42\u3002\u6839\u64da\u6211\u5011\u5c0d ImageNet-1k \u8cc7\u6599\u96c6\u7684\u5206\u6790\uff0c\u6211\u5011\u767c\u73fe TReX \u5728 DeiT-S (LV-ViT-S) ViT \u6a21\u578b\u4e2d\u9054\u5230\u4e86 2.3 \u500d (2.19 \u500d) \u7684 EDAP \u6e1b\u5c11\u548c 1.86 \u500d (1.79 \u500d) \u7684 TOPS/mm2 \u6539\u9032\uff0c\u540c\u6642\u6e96\u78ba\u5ea6\u4e0b\u964d\u7d04 1%\u3002\u6b64\u5916\uff0c\u8207\u6700\u5148\u9032\u7684\u4ee3\u5e63\u526a\u679d\u548c\u6b0a\u91cd\u5171\u4eab\u65b9\u6cd5\u76f8\u6bd4\uff0cTReX \u5728\u9ad8 EDAP \u6e1b\u5c11\u4e0b\u5be6\u73fe\u4e86\u9ad8\u6e96\u78ba\u5ea6\u3002\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\uff08\u4f8b\u5982 CoLA\uff09\u4e0a\uff0cTReX \u5728 EDAP \u964d\u4f4e 1.6 \u500d\u7684\u60c5\u6cc1\u4e0b\uff0c\u6bd4\u57fa\u6e96\u9ad8\u51fa 2% \u7684\u975e\u7406\u60f3\u6e96\u78ba\u5ea6\u3002", "author": "Abhishek Moitra et.al.", "authors": "Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda", "id": "2408.12742v1", "paper_url": "http://arxiv.org/abs/2408.12742v1", "repo": "null"}}