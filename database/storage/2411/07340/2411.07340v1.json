{"2411.07340": {"publish_time": "2024-11-11", "title": "Warmstarting for Scaling Language Models", "paper_summary": "Scaling model sizes to scale performance has worked remarkably well for the\ncurrent large language models paradigm. The research and empirical findings of\nvarious scaling studies led to novel scaling results and laws that guides\nsubsequent research. High training costs for contemporary scales of data and\nmodels result in a lack of thorough understanding of how to tune and arrive at\nsuch training setups. One direction to ameliorate the cost of pretraining large\nmodels is to warmstart the large-scale training from smaller models that are\ncheaper to tune. In this work, we attempt to understand if the behavior of\noptimal hyperparameters can be retained under warmstarting for scaling. We\nexplore simple operations that allow the application of theoretically motivated\nmethods of zero-shot transfer of optimal hyperparameters using {\\mu}Transfer.\nWe investigate the aspects that contribute to the speedup in convergence and\nthe preservation of stable training dynamics under warmstarting with\n{\\mu}Transfer. We find that shrinking smaller model weights, zero-padding, and\nperturbing the resulting larger model with scaled initialization from {\\mu}P\nenables effective warmstarting of $\\mut{}$.", "paper_summary_zh": "\u5c07\u6a21\u578b\u898f\u6a21\u64f4\u5c55\u5230\u64f4\u5c55\u6548\u80fd\u5c0d\u65bc\u76ee\u524d\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b\u7bc4\u4f8b\u800c\u8a00\u904b\u4f5c\u5f97\u975e\u5e38\u597d\u3002\u5404\u7a2e\u898f\u6a21\u7814\u7a76\u7684\u7814\u7a76\u548c\u7d93\u9a57\u7d50\u679c\u5c0e\u81f4\u65b0\u7a4e\u7684\u898f\u6a21\u7d50\u679c\u548c\u5b9a\u5f8b\uff0c\u9019\u4e9b\u5b9a\u5f8b\u6307\u5c0e\u5f8c\u7e8c\u7684\u7814\u7a76\u3002\u7576\u4ee3\u8cc7\u6599\u548c\u6a21\u578b\u7684\u9ad8\u8a13\u7df4\u6210\u672c\u5c0e\u81f4\u7f3a\u4e4f\u5c0d\u5982\u4f55\u8abf\u6574\u548c\u9054\u6210\u6b64\u985e\u8a13\u7df4\u8a2d\u5b9a\u7684\u900f\u5fb9\u7406\u89e3\u3002\u6539\u5584\u5927\u578b\u6a21\u578b\u9810\u8a13\u7df4\u6210\u672c\u7684\u4e00\u500b\u65b9\u5411\u662f\u5f9e\u8f03\u5c0f\u7684\u6a21\u578b\u958b\u59cb\u9032\u884c\u5927\u898f\u6a21\u8a13\u7df4\uff0c\u8f03\u5c0f\u7684\u6a21\u578b\u8abf\u6574\u6210\u672c\u8f03\u4f4e\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5617\u8a66\u4e86\u89e3\u6700\u4f73\u8d85\u53c3\u6578\u7684\u884c\u70ba\u662f\u5426\u53ef\u4ee5\u5728\u64f4\u5c55\u7684\u71b1\u555f\u52d5\u4e0b\u4fdd\u7559\u3002\u6211\u5011\u63a2\u7d22\u5141\u8a31\u61c9\u7528\u7406\u8ad6\u6fc0\u52f5\u7684\u6700\u4f73\u8d85\u53c3\u6578\u96f6\u6b21\u8f49\u79fb\u65b9\u6cd5\u7684\u7c21\u55ae\u64cd\u4f5c\uff0c\u4f7f\u7528 {\\mu}Transfer\u3002\u6211\u5011\u7814\u7a76\u4e86\u6709\u52a9\u65bc\u52a0\u901f\u6536\u6582\u548c\u5728\u4f7f\u7528 {\\mu}Transfer \u71b1\u555f\u52d5\u6642\u7dad\u6301\u7a69\u5b9a\u8a13\u7df4\u52d5\u614b\u7684\u65b9\u9762\u3002\u6211\u5011\u767c\u73fe\u7e2e\u5c0f\u8f03\u5c0f\u7684\u6a21\u578b\u6b0a\u91cd\u3001\u96f6\u586b\u5145\u4ee5\u53ca\u4f7f\u7528\u4f86\u81ea {\\mu}P \u7684\u7e2e\u653e\u521d\u59cb\u5316\u64fe\u52d5\u7522\u751f\u7684\u8f03\u5927\u6a21\u578b\uff0c\u80fd\u5920\u6709\u6548\u5730\u71b1\u555f\u52d5 $\\mut{}$\u3002", "author": "Neeratyoy Mallik et.al.", "authors": "Neeratyoy Mallik, Maciej Janowski, Johannes Hog, Herilalaina Rakotoarison, Aaron Klein, Josif Grabocka, Frank Hutter", "id": "2411.07340v1", "paper_url": "http://arxiv.org/abs/2411.07340v1", "repo": "null"}}