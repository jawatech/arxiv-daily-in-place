{"2407.06153": {"publish_time": "2024-07-08", "title": "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study", "paper_summary": "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u9818\u57df\u7684\u767c\u5c55\u65e5\u76ca\u84ec\u52c3\uff0c\u5f15\u8d77\u7814\u7a76\u4eba\u54e1\u7684\u9ad8\u5ea6\u95dc\u6ce8\u3002\u70ba\u4e86\u63d0\u5347 LLM \u7684\u7a0b\u5f0f\u78bc\u751f\u6210\u80fd\u529b\uff0c\u76ee\u524d\u7684\u7814\u7a76\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u6536\u96c6\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\u96c6\u548c\u5229\u7528\u591a\u5143\u7684\u8a13\u7df4\u6280\u8853\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u73fe\u6709\u65b9\u6cd5\u7684\u9650\u5236\u548c\u754c\u7dda\uff0c\u537b\u9bae\u5c11\u6709\u5168\u9762\u7684\u7814\u7a76\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u7f3a\u53e3\uff0c\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u5ee3\u6cdb\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u8a55\u4f30\u4e86\u4e09\u7a2e\u9818\u5148\u7684\u9589\u6e90 LLM \u548c\u56db\u7a2e\u6d41\u884c\u7684\u958b\u6e90 LLM \u5728\u4e09\u500b\u5e38\u7528\u7684\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7814\u7a76\u8a55\u4f30\u4e86\u751f\u6210\u7a0b\u5f0f\u78bc\u7684\u9577\u5ea6\u3001\u5708\u8907\u5ea6\u548c API \u6578\u91cf\uff0c\u7d50\u679c\u986f\u793a\uff0c\u9019\u4e9b LLM \u5728\u70ba\u66f4\u8907\u96dc\u7684\u554f\u984c\u751f\u6210\u6210\u529f\u7a0b\u5f0f\u78bc\u6642\u9762\u81e8\u6311\u6230\uff0c\u4e26\u4e14\u50be\u5411\u65bc\u7522\u751f\u6bd4\u6a19\u6e96\u89e3\u6cd5\u66f4\u77ed\u4f46\u66f4\u8907\u96dc\u7684\u7a0b\u5f0f\u78bc\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u70ba\u4e0d\u6b63\u78ba\u7684\u7a0b\u5f0f\u78bc\u958b\u767c\u4e86\u4e00\u5957\u932f\u8aa4\u5206\u985e\u6cd5\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u500b\u985e\u5225\u548c 12 \u500b\u5b50\u985e\u5225\uff0c\u4e26\u5206\u6790\u4e86\u5e38\u898b\u932f\u8aa4\u985e\u578b\u7684\u6839\u672c\u539f\u56e0\u3002\u9032\u4e00\u6b65\u5730\uff0c\u70ba\u4e86\u66f4\u597d\u5730\u4e86\u89e3 LLM \u5728\u5be6\u969b\u5c08\u6848\u4e2d\u7684\u6548\u80fd\uff0c\u6211\u5011\u624b\u52d5\u5efa\u7acb\u4e86\u4e00\u500b\u5305\u542b 140 \u500b\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\u7684\u771f\u5be6\u4e16\u754c\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u7684\u5206\u6790\u7a81\u986f\u4e86\u5be6\u969b\u5834\u666f\u548c\u73fe\u6709\u57fa\u6e96\u6e2c\u8a66\u4e4b\u9593\u932f\u8aa4\u5206\u4f48\u7684\u986f\u8457\u5dee\u7570\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u7121\u8a13\u7df4\u53cd\u8986\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u81ea\u6211\u6279\u5224\uff0c\u4f7f LLM \u80fd\u5920\u6839\u64da\u932f\u8aa4\u985e\u578b\u548c\u7de8\u8b6f\u5668\u56de\u994b\u4f86\u6279\u5224\u548c\u4fee\u6b63\u5176\u751f\u6210\u7684\u7a0b\u5f0f\u78bc\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u5728\u5169\u6b21\u53cd\u8986\u904b\u7b97\u5f8c\u5927\u5e45\u6e1b\u5c11\u932f\u8aa4\uff0c\u4e26\u5c07\u901a\u904e\u7387\u63d0\u9ad8 29.2%\uff0c\u9019\u8868\u793a LLM \u8655\u7406\u66f4\u8907\u96dc\u554f\u984c\u7684\u6f5b\u529b\u5de8\u5927\u3002", "author": "Shihan Dou et.al.", "authors": "Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang", "id": "2407.06153v1", "paper_url": "http://arxiv.org/abs/2407.06153v1", "repo": "null"}}