{"2407.08454": {"publish_time": "2024-07-11", "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks", "paper_summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.", "paper_summary_zh": "<paragraph>\u5982\u4f55\u6709\u6548\u5730\u63d0\u4f9b\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u4e3a\u4e00\u9879\u7d27\u8feb\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u5de8\u5927\u3002\u4e3a\u4e86\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0cLLM \u7ecf\u5e38\u91c7\u7528 KV \u7f13\u5b58\u6280\u672f\u6765\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u3002\u867d\u7136\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4f46 KV \u7f13\u5b58\u7684\u5b58\u50a8\u9700\u6c42\u5f88\u5927\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\uff0c\u4ece\u800c\u5bfc\u81f4\u4e86\u5927\u91cf\u7684\u5185\u5b58\u6d88\u8017\u3002\u73b0\u6709\u7684 KV \u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\u901a\u5e38\u4f1a\u56e0\u9a71\u9010\u5f15\u5165\u7684\u4fe1\u606f\u4e22\u5931\u800c\u964d\u4f4e LLM \u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 KV \u7f13\u5b58\u5408\u5e76\u65b9\u6cd5\uff0c\u79f0\u4e3a KVMerger\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94 KV \u7f13\u5b58\u538b\u7f29\uff0c\u7528\u4e8e\u5728\u53d7\u9650\u5185\u5b58\u9884\u7b97\u4e0b\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\uff0c\u800c\u4e0d\u4f1a\u663e\u7740\u964d\u4f4e\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53d7\u5230\u4ee5\u4e0b\u6709\u8da3\u89c2\u5bdf\u7684\u542f\u53d1\uff1a\u5173\u952e\u72b6\u6001\u5728\u5355\u4e2a\u5e8f\u5217\u4e2d\u7684\u6807\u8bb0\u7ea7\u522b\u8868\u73b0\u51fa\u9ad8\u5ea6\u76f8\u4f3c\u6027\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5408\u5e76\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u76f4\u63a5\u7684\u5408\u5e76\u96c6\u8bc6\u522b\u7b97\u6cd5\uff0c\u4ee5\u8bc6\u522b\u9002\u5408\u5408\u5e76\u7684 KV \u72b6\u6001\u3002\u6211\u4eec\u7684\u5408\u5e76\u96c6\u8bc6\u522b\u7b97\u6cd5\u6fc0\u53d1\u4e86\u7b2c\u4e8c\u4e2a\u89c2\u5bdf\u7ed3\u679c\uff0c\u5373\u4ece\u76f8\u4f3c\u6027\u7684\u89d2\u5ea6\u6765\u770b\uff0cKV \u7f13\u5b58\u7a00\u758f\u6027\u4e0e\u6570\u636e\u96c6\u65e0\u5173\uff0c\u5e76\u4e14\u5728\u6a21\u578b\u7ea7\u522b\u4fdd\u6301\u4e0d\u53d8\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u65af\u6838\u52a0\u6743\u5408\u5e76\u7b97\u6cd5\uff0c\u4ee5\u9009\u62e9\u6027\u5730\u5408\u5e76\u6bcf\u4e2a\u5408\u5e76\u96c6\u4e2d\u7684\u6240\u6709\u72b6\u6001\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e KVMerger \u5728\u53d7\u9650\u5185\u5b58\u9884\u7b97\u4e0b\u5bf9\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u5305\u62ec Llama2-7B-chat \u548c Llama2-13B-chat \u5728\u5185\u7684\u6a21\u578b\u3002\u4f7f\u7528 LongBench \u548c ZeroScroll \u57fa\u51c6\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u5176\u4ed6 KV \u7f13\u5b58\u538b\u7f29\u6280\u672f\uff08\u5305\u62ec H2O \u548c CaM\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728 KV \u7f13\u5b58\u9884\u7b97\u4e3a 50% \u548c 35% \u7684\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002</paragraph>", "author": "Zheng Wang et.al.", "authors": "Zheng Wang, Boxiao Jin, Zhongzhi Yu, Minjia Zhang", "id": "2407.08454v1", "paper_url": "http://arxiv.org/abs/2407.08454v1", "repo": "null"}}