{"2501.18895": {"publish_time": "2025-01-31", "title": "Efficient Supernet Training with Orthogonal Softmax for Scalable ASR Model Compression", "paper_summary": "ASR systems are deployed across diverse environments, each with specific\nhardware constraints. We use supernet training to jointly train multiple\nencoders of varying sizes, enabling dynamic model size adjustment to fit\nhardware constraints without redundant training. Moreover, we introduce a novel\nmethod called OrthoSoftmax, which applies multiple orthogonal softmax functions\nto efficiently identify optimal subnets within the supernet, avoiding\nresource-intensive search. This approach also enables more flexible and precise\nsubnet selection by allowing selection based on various criteria and levels of\ngranularity. Our results with CTC on Librispeech and TED-LIUM-v2 show that\nFLOPs-aware component-wise selection achieves the best overall performance.\nWith the same number of training updates from one single job, WERs for all\nmodel sizes are comparable to or slightly better than those of individually\ntrained models. Furthermore, we analyze patterns in the selected components and\nreveal interesting insights.", "paper_summary_zh": "ASR \u7cfb\u7d71\u90e8\u7f72\u5728\u5404\u7a2e\u74b0\u5883\u4e2d\uff0c\u6bcf\u500b\u74b0\u5883\u90fd\u6709\u7279\u5b9a\u7684\u786c\u9ad4\u9650\u5236\u3002\u6211\u5011\u4f7f\u7528\u8d85\u7db2\u8def\u8a13\u7df4\u4f86\u806f\u5408\u8a13\u7df4\u5404\u7a2e\u5927\u5c0f\u7684\u7de8\u78bc\u5668\uff0c\u5f9e\u800c\u80fd\u5920\u6839\u64da\u786c\u9ad4\u9650\u5236\u52d5\u614b\u8abf\u6574\u6a21\u578b\u5927\u5c0f\uff0c\u800c\u7121\u9700\u9032\u884c\u5197\u9918\u8a13\u7df4\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u7a31\u70ba OrthoSoftmax \u7684\u65b0\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u61c9\u7528\u591a\u500b\u6b63\u4ea4 softmax \u51fd\u6578\u4f86\u6709\u6548\u8b58\u5225\u8d85\u7db2\u8def\u4e2d\u7684\u6700\u4f73\u5b50\u7db2\u8def\uff0c\u5f9e\u800c\u907f\u514d\u8cc7\u6e90\u5bc6\u96c6\u578b\u641c\u7d22\u3002\u9019\u7a2e\u65b9\u6cd5\u9084\u5141\u8a31\u6839\u64da\u5404\u7a2e\u6a19\u6e96\u548c\u7c92\u5ea6\u7d1a\u5225\u9032\u884c\u9078\u64c7\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u9748\u6d3b\u548c\u7cbe\u78ba\u7684\u5b50\u7db2\u8def\u9078\u64c7\u3002\u6211\u5011\u5728 Librispeech \u548c TED-LIUM-v2 \u4e0a\u4f7f\u7528 CTC \u7372\u5f97\u7684\u7d50\u679c\u8868\u660e\uff0c\u8003\u616e FLOP \u7684\u7d44\u4ef6\u7d1a\u5225\u9078\u64c7\u53ef\u5be6\u73fe\u6700\u4f73\u6574\u9ad4\u6548\u80fd\u3002\u5728\u540c\u4e00\u500b\u5de5\u4f5c\u4e2d\u9032\u884c\u76f8\u540c\u6b21\u6578\u7684\u8a13\u7df4\u66f4\u65b0\uff0c\u6240\u6709\u6a21\u578b\u5927\u5c0f\u7684 WER \u90fd\u8207\u55ae\u7368\u8a13\u7df4\u7684\u6a21\u578b\u76f8\u7576\u6216\u7565\u597d\u3002\u6b64\u5916\uff0c\u6211\u5011\u5206\u6790\u4e86\u6240\u9078\u7d44\u4ef6\u4e2d\u7684\u6a21\u5f0f\uff0c\u4e26\u63ed\u793a\u4e86\u6709\u8da3\u7684\u898b\u89e3\u3002", "author": "Jingjing Xu et.al.", "authors": "Jingjing Xu, Eugen Beck, Zijian Yang, Ralf Schl\u00fcter", "id": "2501.18895v1", "paper_url": "http://arxiv.org/abs/2501.18895v1", "repo": "null"}}