{"2410.13708": {"publish_time": "2024-10-17", "title": "On the Role of Attention Heads in Large Language Model Safety", "paper_summary": "Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u591a\u7a2e\u8a9e\u8a00\u4efb\u52d9\u4e0a\u5be6\u73fe\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4f46\u5176\u5b89\u5168\u9632\u8b77\u63aa\u65bd\u53ef\u80fd\u6703\u88ab\u898f\u907f\uff0c\u5c0e\u81f4\u6709\u5bb3\u7684\u751f\u6210\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6700\u8fd1\u95dc\u65bc\u5b89\u5168\u6a5f\u5236\u7684\u7684\u7814\u7a76\u61c9\u904b\u800c\u751f\uff0c\u63ed\u793a\u4e86\u7576\u5b89\u5168\u8868\u793a\u6216\u7d44\u6210\u53d7\u5230\u6291\u5236\u6642\uff0cLLM \u7684\u5b89\u5168\u80fd\u529b\u6703\u53d7\u5230\u640d\u5bb3\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u7814\u7a76\u50be\u5411\u65bc\u5ffd\u7565\u591a\u982d\u6ce8\u610f\u529b\u6a5f\u5236\u5c0d\u5b89\u5168\u6027\u7684\u5f71\u97ff\uff0c\u5118\u7ba1\u5b83\u5011\u5728\u5404\u7a2e\u6a21\u578b\u529f\u80fd\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u63a2\u8a0e\u6a19\u6e96\u6ce8\u610f\u529b\u6a5f\u5236\u8207\u5b89\u5168\u80fd\u529b\u4e4b\u9593\u7684\u95dc\u806f\uff0c\u4ee5\u586b\u88dc\u8207\u5b89\u5168\u76f8\u95dc\u7684\u6a5f\u5236\u53ef\u89e3\u91cb\u6027\u4e2d\u7684\u9019\u9805\u7a7a\u767d\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u91dd\u5c0d\u591a\u982d\u6ce8\u610f\u529b\u91cf\u8eab\u6253\u9020\u7684\u65b0\u6307\u6a19\uff0c\u7a31\u70ba\u5b89\u5168\u982d\u90e8\u91cd\u8981\u5206\u6578 (Ships)\uff0c\u4ee5\u8a55\u4f30\u5404\u500b\u982d\u90e8\u5c0d\u6a21\u578b\u5b89\u5168\u6027\u7684\u8ca2\u737b\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u5c07 Ships \u63a8\u5ee3\u5230\u8cc7\u6599\u96c6\u5c64\u7d1a\uff0c\u4e26\u9032\u4e00\u6b65\u5f15\u5165\u5b89\u5168\u6ce8\u610f\u529b\u982d\u90e8\u6b78\u56e0\u6f14\u7b97\u6cd5 (Sahara)\uff0c\u4ee5\u6b78\u56e0\u6a21\u578b\u5167\u90e8\u95dc\u9375\u7684\u5b89\u5168\u6ce8\u610f\u529b\u982d\u90e8\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u7279\u6b8a\u7684\u6ce8\u610f\u529b\u982d\u90e8\u5c0d\u5b89\u5168\u6027\u6709\u986f\u8457\u7684\u5f71\u97ff\u3002\u6d88\u878d\u55ae\u4e00\u5b89\u5168\u982d\u90e8\u5141\u8a31\u5c0d\u9f4a\u6a21\u578b\uff08\u4f8b\u5982 Llama-2-7b-chat\uff09\u56de\u61c9\u591a\u51fa 16 \u500d\u7684\u6709\u5bb3\u67e5\u8a62\uff0c\u540c\u6642\u53ea\u4fee\u6539\u4e86 0.006% \u7684\u53c3\u6578\uff0c\u9019\u8207\u5148\u524d\u7684\u7814\u7a76\u4e2d\u6240\u9700\u7d04 5% \u7684\u4fee\u6539\u5f62\u6210\u5c0d\u6bd4\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u8b49\u660e\u6ce8\u610f\u529b\u982d\u90e8\u4e3b\u8981\u5145\u7576\u5b89\u5168\u6027\u7684\u7279\u5fb5\u8403\u53d6\u5668\uff0c\u4e26\u4e14\u5f9e\u540c\u4e00\u500b\u57fa\u790e\u6a21\u578b\u5fae\u8abf\u7684\u6a21\u578b\u900f\u904e\u5168\u9762\u7684\u5be6\u9a57\u8868\u73fe\u51fa\u91cd\u758a\u7684\u5b89\u5168\u982d\u90e8\u3002\u6211\u5011\u7684\u6b78\u56e0\u65b9\u6cd5\u548c\u7814\u7a76\u7d50\u679c\u5171\u540c\u70ba\u89e3\u958b\u5927\u578b\u6a21\u578b\u4e2d\u5b89\u5168\u6a5f\u5236\u7684\u9ed1\u7bb1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c0\u9ede\u3002", "author": "Zhenhong Zhou et.al.", "authors": "Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li", "id": "2410.13708v1", "paper_url": "http://arxiv.org/abs/2410.13708v1", "repo": "null"}}