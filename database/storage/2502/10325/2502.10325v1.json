{"2502.10325": {"publish_time": "2025-02-14", "title": "Process Reward Models for LLM Agents: Practical Framework and Directions", "paper_summary": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39\u4e86\u4ee3\u7406\u7a0b\u5e8f\u734e\u52f5\u6a21\u578b (AgentPRM)\uff0c\u9019\u662f\u4e00\u500b\u7c21\u55ae\u4e14\u53ef\u64f4\u5145\u7684\u6846\u67b6\uff0c\u7528\u65bc\u8a13\u7df4 LLM \u4ee3\u7406\u7a0b\u5e8f\u900f\u904e\u4e92\u52d5\u6301\u7e8c\u6539\u9032\u3002AgentPRM \u9075\u5faa\u8f15\u91cf\u7d1a\u7684 Actor-Critic \u7bc4\u4f8b\uff0c\u4f7f\u7528\u8499\u5730\u5361\u7f85\u6efe\u52d5\u8a08\u7b97\u734e\u52f5\u76ee\u6a19\u4e26\u6700\u4f73\u5316\u7b56\u7565\u3002\u5b83\u53ea\u9700\u8981\u5c0d\u73fe\u6709\u7684 RLHF \u7ba1\u7dda\u9032\u884c\u6700\u5c0f\u7684\u4fee\u6539\uff0c\u4f7f\u5176\u6613\u65bc\u5927\u898f\u6a21\u6574\u5408\u3002\u9664\u4e86 AgentPRM\uff0c\u6211\u5011\u63d0\u51fa\u4e86 InversePRM\uff0c\u5b83\u76f4\u63a5\u5f9e\u793a\u7bc4\u4e2d\u5b78\u7fd2\u904e\u7a0b\u734e\u52f5\uff0c\u800c\u7121\u9700\u660e\u78ba\u7684\u7d50\u679c\u76e3\u7763\u3002\u6211\u5011\u9084\u63a2\u8a0e\u4e86\u95dc\u9375\u6311\u6230\u548c\u6a5f\u6703\uff0c\u5305\u62ec\u63a2\u7d22\u3001\u904e\u7a0b\u734e\u52f5\u5851\u9020\u548c\u6a21\u578b\u9810\u6e2c\u63a8\u7406\u3002\u6211\u5011\u5728 ALFWorld \u57fa\u6e96\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u8868\u660e\u4f7f\u7528 AgentPRM \u548c InversePRM \u8a13\u7df4\u7684\u5c0f\u578b 3B \u6a21\u578b\u512a\u65bc\u5f37\u5927\u7684 GPT-4o \u57fa\u6e96\uff0c\u4e26\u5206\u6790\u6e2c\u8a66\u6642\u9593\u7e2e\u653e\u3001\u734e\u52f5\u7834\u89e3\u7b49\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u65bc\u6b64\u8655\u53d6\u5f97\uff1ahttps://github.com/sanjibanc/agent_prm\u3002", "author": "Sanjiban Choudhury et.al.", "authors": "Sanjiban Choudhury", "id": "2502.10325v1", "paper_url": "http://arxiv.org/abs/2502.10325v1", "repo": "null"}}