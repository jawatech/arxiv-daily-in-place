{"2407.09121": {"publish_time": "2024-07-12", "title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training", "paper_summary": "This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.", "paper_summary_zh": "\u672c\u7814\u7a76\u900f\u904e\u8fa8\u8b58\u4e26\u8655\u7406\u5b89\u5168\u8abf\u6574\u8cc7\u6599\u4e2d\u7684\u62d2\u7d55\u7acb\u5834\u504f\u5dee\uff0c\u4f86\u89e3\u6c7a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5b89\u5168\u8abf\u6574\u5be6\u52d9\u4e2d\u7684\u91cd\u5927\u7f3a\u53e3\uff0c\u8a72\u504f\u5dee\u6703\u640d\u5bb3\u6a21\u578b\u9069\u7576\u62d2\u7d55\u7522\u751f\u4e0d\u5b89\u5168\u5167\u5bb9\u7684\u80fd\u529b\u3002\u6211\u5011\u5f15\u9032\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba\u89e3\u8026\u62d2\u7d55\u8a13\u7df4 (DeRTa)\uff0c\u65e8\u5728\u8b93 LLM \u80fd\u5920\u5728\u4efb\u4f55\u56de\u61c9\u4f4d\u7f6e\u62d2\u7d55\u9075\u5b88\u6709\u5bb3\u63d0\u793a\uff0c\u5927\u5e45\u63d0\u5347\u5176\u5b89\u5168\u80fd\u529b\u3002DeRTa \u5305\u542b\u5169\u500b\u5275\u65b0\u7684\u7d44\u6210\u90e8\u5206\uff1a(1) \u5e36\u6709\u6709\u5bb3\u56de\u61c9\u524d\u7db4\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8a08 (MLE)\uff0c\u8a13\u7df4\u6a21\u578b\u900f\u904e\u5728\u5b89\u5168\u56de\u61c9\u958b\u982d\u9644\u52a0\u4e00\u6bb5\u6709\u5bb3\u56de\u61c9\uff0c\u4f86\u8fa8\u8b58\u548c\u907f\u514d\u4e0d\u5b89\u5168\u5167\u5bb9\uff0c\u4ee5\u53ca (2) \u589e\u5f37\u904e\u6e21\u6700\u4f73\u5316 (RTO)\uff0c\u8b93\u6a21\u578b\u5177\u5099\u5728\u6574\u500b\u6709\u5bb3\u56de\u61c9\u5e8f\u5217\u4e2d\uff0c\u5f9e\u6f5b\u5728\u5371\u5bb3\u904e\u6e21\u5230\u5b89\u5168\u62d2\u7d55\u7684\u80fd\u529b\u3002\u6211\u5011\u4f7f\u7528 LLaMA3 \u548c Mistral \u6a21\u578b\u5bb6\u65cf\u5728\u516d\u7a2e\u653b\u64ca\u60c5\u5883\u4e2d\u9032\u884c\u5be6\u8b49\u8a55\u4f30\uff0c\u8b49\u660e\u6211\u5011\u7684\u65b9\u6cd5\u4e0d\u50c5\u80fd\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\uff0c\u800c\u4e0d\u640d\u5bb3\u6548\u80fd\uff0c\u9084\u80fd\u8d85\u8d8a GPT-4 \u7b49\u77e5\u540d\u6a21\u578b\uff0c\u62b5\u79a6\u653b\u64ca\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6210\u529f\u62b5\u79a6\u6700\u8fd1\u9032\u968e\u7684\u653b\u64ca\u65b9\u6cd5 (\u4f8b\u5982 CodeAttack)\uff0c\u9019\u4e9b\u65b9\u6cd5\u5df2\u7834\u89e3 GPT-4 \u548c LLaMA3-70B-Instruct\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u4ee5\u5728 https://github.com/RobustNLP/DeRTa \u627e\u5230\u3002", "author": "Youliang Yuan et.al.", "authors": "Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu", "id": "2407.09121v1", "paper_url": "http://arxiv.org/abs/2407.09121v1", "repo": "https://github.com/robustnlp/derta"}}