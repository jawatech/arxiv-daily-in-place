{"2407.13377": {"publish_time": "2024-07-18", "title": "Linear-Complexity Self-Supervised Learning for Speech Processing", "paper_summary": "Self-supervised learning (SSL) models usually require weeks of pre-training\nwith dozens of high-end GPUs. These models typically have a multi-headed\nself-attention (MHSA) context encoder. However, MHSA takes quadratic time and\nspace in the input length, contributing to the high pre-training cost.\nLinear-complexity alternatives to MHSA have been proposed. For instance, in\nsupervised training, the SummaryMixing model is the first to outperform MHSA\nacross multiple speech processing tasks. However, these cheaper alternatives\nhave not been explored for SSL yet. This paper studies a linear-complexity\ncontext encoder for SSL for the first time. With better or equivalent\nperformance for the downstream tasks of the MP3S benchmark, SummaryMixing\nreduces the pre-training time and peak VRAM of wav2vec 2.0 model by 18% and by\n23%, respectively, leading to the pre-training of a 155M wav2vec 2.0 model\nfinished within one week with 4 Tesla A100 GPUs. Code is available at\nhttps://github.com/SamsungLabs/SummaryMixing.", "paper_summary_zh": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u901a\u5e38\u9700\u8981\u6570\u5468\u7684\u9884\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528\u6570\u5341\u4e2a\u9ad8\u7aef GPU\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u5177\u6709\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08MHSA\uff09\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u3002\u7136\u800c\uff0cMHSA \u5728\u8f93\u5165\u957f\u5ea6\u4e0a\u5448\u4e8c\u6b21\u65f6\u95f4\u548c\u7a7a\u95f4\uff0c\u8fd9\u5bfc\u81f4\u4e86\u8f83\u9ad8\u7684\u9884\u8bad\u7ec3\u6210\u672c\u3002\u5df2\u7ecf\u63d0\u51fa\u4e86 MHSA \u7684\u7ebf\u6027\u590d\u6742\u5ea6\u66ff\u4ee3\u65b9\u6848\u3002\u4f8b\u5982\uff0c\u5728\u76d1\u7763\u8bad\u7ec3\u4e2d\uff0cSummaryMixing \u6a21\u578b\u662f\u7b2c\u4e00\u4e2a\u5728\u591a\u4e2a\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e MHSA \u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u66f4\u4fbf\u5b9c\u7684\u66ff\u4ee3\u65b9\u6848\u5c1a\u672a\u7528\u4e8e SSL\u3002\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86 SSL \u7684\u7ebf\u6027\u590d\u6742\u5ea6\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u3002\u5bf9\u4e8e MP3S \u57fa\u51c6\u7684\u4e0b\u6e38\u4efb\u52a1\uff0cSummaryMixing \u5177\u6709\u66f4\u597d\u6216\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5b83\u5c06 wav2vec 2.0 \u6a21\u578b\u7684\u9884\u8bad\u7ec3\u65f6\u95f4\u548c\u5cf0\u503c VRAM \u5206\u522b\u51cf\u5c11\u4e86 18% \u548c 23%\uff0c\u4ece\u800c\u5728\u4e0d\u5230\u4e00\u5468\u7684\u65f6\u95f4\u5185\u4f7f\u7528 4 \u4e2a Tesla A100 GPU \u5b8c\u6210\u4e86 155M wav2vec 2.0 \u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/SamsungLabs/SummaryMixing \u83b7\u5f97\u3002", "author": "Shucong Zhang et.al.", "authors": "Shucong Zhang, Titouan Parcollet, Rogier van Dalen, Sourav Bhattacharya", "id": "2407.13377v1", "paper_url": "http://arxiv.org/abs/2407.13377v1", "repo": "null"}}