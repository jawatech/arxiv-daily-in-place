{"2501.09265": {"publish_time": "2025-01-16", "title": "Perspective Transition of Large Language Models for Solving Subjective Tasks", "paper_summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u9818\u57df\uff0c\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5be6\u73fe\u4e86\u986f\u8457\u9032\u5c55\u3002\u8207\u5e38\u8b58\u63a8\u7406\u548c\u7b97\u8853\u554f\u7b54\u7b49\u5ba2\u89c0\u4efb\u52d9\u4e0d\u540c\uff0cLLM \u5728\u4e3b\u89c0\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u4ecd\u7136\u6709\u9650\uff0c\u5176\u4e2d\u5c0d\u7279\u5b9a\u554f\u984c\u7684\u89c0\u9ede\u5c0d\u65bc\u66f4\u597d\u5730\u8a6e\u91cb\u8108\u7d61\u548c\u7d66\u51fa\u9069\u7576\u7684\u56de\u61c9\u81f3\u95dc\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0cLLM \u5728\u5f9e\u5c08\u5bb6\u89d2\u8272\u7684\u89d2\u5ea6\u56de\u7b54\u554f\u984c\u6642\u53ef\u80fd\u6703\u8868\u73fe\u5f97\u66f4\u597d\uff0c\u5f9e\u800c\u6f5b\u5728\u5730\u5f15\u767c\u4ed6\u5011\u76f8\u95dc\u7684\u9818\u57df\u77e5\u8b58\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0cLLM \u5728\u5f9e\u7b2c\u4e09\u4eba\u7a31\u89d2\u5ea6\u56de\u7b54\u554f\u984c\u6642\u53ef\u80fd\u6703\u63d0\u4f9b\u66f4\u6e96\u78ba\u7684\u56de\u61c9\uff0c\u5f9e\u800c\u80fd\u5920\u66f4\u5168\u9762\u5730\u7406\u89e3\u554f\u984c\u4e26\u6f5b\u5728\u5730\u6e1b\u8f15\u56fa\u6709\u504f\u898b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u57fa\u65bc\u60c5\u5883\u5b78\u7fd2\u7684\u63a8\u7406\u900f\u8996\u8f49\u63db (RPT) \u65b9\u6cd5\uff0c\u4f7f LLM \u80fd\u5920\u5728\u76f4\u63a5\u3001\u89d2\u8272\u548c\u7b2c\u4e09\u4eba\u7a31\u8996\u89d2\u4e2d\u52d5\u614b\u9078\u64c7\uff0c\u4ee5\u627e\u5230\u89e3\u6c7a\u76f8\u61c9\u4e3b\u89c0\u554f\u984c\u7684\u6700\u4f73\u65b9\u5f0f\u3002\u901a\u904e\u5728\u4f7f\u7528\u9589\u6e90\u548c\u958b\u6e90 LLM\uff08\u5305\u62ec GPT-4\u3001GPT-3.5\u3001Llama-3 \u548c Qwen-2\uff09\u7684 12 \u9805\u4e3b\u89c0\u4efb\u52d9\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u7684\u6a21\u578b\u512a\u65bc\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u65bc\u55ae\u4e00\u56fa\u5b9a\u8996\u89d2\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u601d\u60f3\u93c8\u63d0\u793a\u548c\u5c08\u5bb6\u63d0\u793a\uff0c\u7a81\u51fa\u4e86 LLM \u53ef\u4ee5\u8abf\u6574\u5176\u8996\u89d2\u4ee5\u91dd\u5c0d\u4e0d\u540c\u7684\u554f\u984c\u63d0\u4f9b\u7d30\u7dfb\u5165\u5fae\u4e14\u7b26\u5408\u60c5\u5883\u7684\u56de\u61c9\u7684\u8907\u96dc\u65b9\u5f0f\u3002", "author": "Xiaolong Wang et.al.", "authors": "Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu", "id": "2501.09265v1", "paper_url": "http://arxiv.org/abs/2501.09265v1", "repo": "null"}}