{"2405.16802": {"publish_time": "2024-05-27", "title": "AutoCV: Empowering Reasoning with Automated Process Labeling via Confidence Variation", "paper_summary": "In this work, we propose a novel method named \\textbf{Auto}mated Process\nLabeling via \\textbf{C}onfidence \\textbf{V}ariation (\\textbf{\\textsc{AutoCV}})\nto enhance the reasoning capabilities of large language models (LLMs) by\nautomatically annotating the reasoning steps. Our approach begins by training a\nverification model on the correctness of final answers, enabling it to generate\nautomatic process annotations. This verification model assigns a confidence\nscore to each reasoning step, indicating the probability of arriving at the\ncorrect final answer from that point onward. We detect relative changes in the\nverification's confidence scores across reasoning steps to automatically\nannotate the reasoning process. This alleviates the need for numerous manual\nannotations or the high computational costs associated with model-induced\nannotation approaches. We experimentally validate that the confidence\nvariations learned by the verification model trained on the final answer\ncorrectness can effectively identify errors in the reasoning steps.\nSubsequently, we demonstrate that the process annotations generated by\n\\textsc{AutoCV} can improve the accuracy of the verification model in selecting\nthe correct answer from multiple outputs generated by LLMs. Notably, we achieve\nsubstantial improvements across five datasets in mathematics and commonsense\nreasoning. The source code of \\textsc{AutoCV} is available at\n\\url{https://github.com/rookie-joe/AUTOCV}.", "paper_summary_zh": "\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u900f\u904e\u4fe1\u5fc3\u8b8a\u7570\u9032\u884c\u81ea\u52d5\u5316\u7a0b\u5e8f\u6a19\u8a18\uff08AutoCV\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u85c9\u7531\u81ea\u52d5\u8a3b\u89e3\u63a8\u7406\u6b65\u9a5f\u4f86\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u7684\u505a\u6cd5\u9996\u5148\u8a13\u7df4\u9a57\u8b49\u6a21\u578b\u4ee5\u9a57\u8b49\u6700\u7d42\u7b54\u6848\u7684\u6b63\u78ba\u6027\uff0c\u4f7f\u5176\u80fd\u5920\u7522\u751f\u81ea\u52d5\u7a0b\u5e8f\u8a3b\u89e3\u3002\u6b64\u9a57\u8b49\u6a21\u578b\u6703\u70ba\u6bcf\u500b\u63a8\u7406\u6b65\u9a5f\u6307\u5b9a\u4e00\u500b\u4fe1\u5fc3\u5206\u6578\uff0c\u8868\u793a\u5f9e\u8a72\u9ede\u958b\u59cb\u5f97\u51fa\u6b63\u78ba\u6700\u7d42\u7b54\u6848\u7684\u6a5f\u7387\u3002\u6211\u5011\u5075\u6e2c\u9a57\u8b49\u4fe1\u5fc3\u5206\u6578\u5728\u63a8\u7406\u6b65\u9a5f\u4e2d\u7684\u76f8\u5c0d\u8b8a\u5316\uff0c\u4ee5\u81ea\u52d5\u8a3b\u89e3\u63a8\u7406\u904e\u7a0b\u3002\u9019\u6e1b\u8f15\u4e86\u5927\u91cf\u624b\u52d5\u8a3b\u89e3\u6216\u8207\u6a21\u578b\u8a98\u5c0e\u8a3b\u89e3\u65b9\u6cd5\u76f8\u95dc\u7684\u9ad8\u904b\u7b97\u6210\u672c\u7684\u9700\u6c42\u3002\u6211\u5011\u900f\u904e\u5be6\u9a57\u9a57\u8b49\uff0c\u5728\u6700\u7d42\u7b54\u6848\u6b63\u78ba\u6027\u4e0a\u8a13\u7df4\u7684\u9a57\u8b49\u6a21\u578b\u6240\u5b78\u7fd2\u7684\u4fe1\u5fc3\u8b8a\u7570\u53ef\u4ee5\u6709\u6548\u627e\u51fa\u63a8\u7406\u6b65\u9a5f\u4e2d\u7684\u932f\u8aa4\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u8b49\u660e\u4e86 AutoCV \u6240\u7522\u751f\u7684\u7a0b\u5e8f\u8a3b\u89e3\u53ef\u4ee5\u63d0\u9ad8\u9a57\u8b49\u6a21\u578b\u5f9e LLM \u6240\u7522\u751f\u7684\u591a\u500b\u8f38\u51fa\u4e2d\u9078\u64c7\u6b63\u78ba\u7b54\u6848\u7684\u6e96\u78ba\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u5728\u6578\u5b78\u548c\u5e38\u8b58\u63a8\u7406\u7684\u4e94\u500b\u8cc7\u6599\u96c6\u4e2d\u90fd\u53d6\u5f97\u4e86\u986f\u8457\u7684\u9032\u6b65\u3002AutoCV \u7684\u539f\u59cb\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/rookie-joe/AUTOCV \u53d6\u5f97\u3002", "author": "Jianqiao Lu et.al.", "authors": "Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, Zhijiang Guo", "id": "2405.16802v2", "paper_url": "http://arxiv.org/abs/2405.16802v2", "repo": "https://github.com/rookie-joe/autocv"}}