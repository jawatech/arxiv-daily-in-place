{"2501.18475": {"publish_time": "2025-01-30", "title": "CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA Initialization", "paper_summary": "Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has\nbecome a highly efficient approach for downstream tasks, particularly in\nscenarios with limited computational resources. However, applying LoRA\ntechniques to quantized LLMs poses unique challenges due to the reduced\nrepresentational precision of quantized weights. In this paper, we introduce\nCLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic\ninitialization strategy designed to overcome these challenges. Our approach\nfocuses on minimizing the layer-wise discrepancy between the original LLM and\nits quantized counterpart with LoRA components during initialization. By\nleveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and\ndetermines the optimal LoRA components for each layer, ensuring a strong\nfoundation for subsequent fine-tuning. A key contribution of this work is a\nnovel theoretical result that enables the accurate and closed-form construction\nof these optimal LoRA components. We validate the efficacy of CLoQ across\nmultiple tasks such as language generation, arithmetic reasoning, and\ncommonsense reasoning, demonstrating that it consistently outperforms existing\nLoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit\nwidths.", "paper_summary_zh": "\u4f7f\u7528\u4f4e\u79e9\u9069\u61c9 (LoRA) \u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u4e0b\u6e38\u4efb\u52d9\u7684\u9ad8\u5ea6\u6709\u6548\u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728\u8a08\u7b97\u8cc7\u6e90\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\u3002\u7136\u800c\uff0c\u7531\u65bc\u91cf\u5316\u6b0a\u91cd\u7684\u8868\u793a\u7cbe\u5ea6\u964d\u4f4e\uff0c\u5c07 LoRA \u6280\u8853\u61c9\u7528\u65bc\u91cf\u5316 LLM \u6703\u5e36\u4f86\u7368\u7279\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CLoQ\uff08\u91cf\u5316 LLM \u7684\u6821\u6e96 LoRA \u521d\u59cb\u5316\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65e8\u5728\u514b\u670d\u9019\u4e9b\u6311\u6230\u7684\u7c21\u5316\u521d\u59cb\u5316\u7b56\u7565\u3002\u6211\u5011\u7684\u505a\u6cd5\u8457\u91cd\u65bc\u5728\u521d\u59cb\u5316\u671f\u9593\u6700\u5c0f\u5316\u539f\u59cb LLM \u548c\u5176\u91cf\u5316\u5c0d\u61c9\u9805\u8207 LoRA \u7d44\u4ef6\u4e4b\u9593\u7684\u9010\u5c64\u5dee\u7570\u3002\u901a\u904e\u5229\u7528\u4e00\u500b\u5c0f\u578b\u6821\u6e96\u6578\u64da\u96c6\uff0cCLoQ \u91cf\u5316\u9810\u8a13\u7df4\u7684 LLM \u4e26\u78ba\u5b9a\u6bcf\u5c64\u7684\u6700\u4f73 LoRA \u7d44\u4ef6\uff0c\u5f9e\u800c\u78ba\u4fdd\u5f8c\u7e8c\u5fae\u8abf\u7684\u5805\u5be6\u57fa\u790e\u3002\u9019\u9805\u5de5\u4f5c\u7684\u95dc\u9375\u8ca2\u737b\u662f\u5275\u65b0\u7684\u7406\u8ad6\u7d50\u679c\uff0c\u5b83\u80fd\u6e96\u78ba\u4e14\u5c01\u9589\u5730\u69cb\u9020\u9019\u4e9b\u6700\u4f73 LoRA \u7d44\u4ef6\u3002\u6211\u5011\u9a57\u8b49\u4e86 CLoQ \u5728\u8a9e\u8a00\u751f\u6210\u3001\u7b97\u8853\u63a8\u7406\u548c\u5e38\u8b58\u63a8\u7406\u7b49\u591a\u9805\u4efb\u52d9\u4e2d\u7684\u529f\u6548\uff0c\u8b49\u660e\u5b83\u59cb\u7d42\u512a\u65bc\u73fe\u6709\u7684\u91cf\u5316 LLM \u7684 LoRA \u5fae\u8abf\u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728\u6975\u4f4e\u4f4d\u5bec\u5ea6\u4e0b\u3002", "author": "Yanxia Deng et.al.", "authors": "Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, Penghang Yin", "id": "2501.18475v1", "paper_url": "http://arxiv.org/abs/2501.18475v1", "repo": "https://github.com/AozhongZhang/CLoQ"}}