{"2502.15336": {"publish_time": "2025-02-21", "title": "Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions", "paper_summary": "Embodied multimodal large models (EMLMs) have gained significant attention in\nrecent years due to their potential to bridge the gap between perception,\ncognition, and action in complex, real-world environments. This comprehensive\nreview explores the development of such models, including Large Language Models\n(LLMs), Large Vision Models (LVMs), and other models, while also examining\nother emerging architectures. We discuss the evolution of EMLMs, with a focus\non embodied perception, navigation, interaction, and simulation. Furthermore,\nthe review provides a detailed analysis of the datasets used for training and\nevaluating these models, highlighting the importance of diverse, high-quality\ndata for effective learning. The paper also identifies key challenges faced by\nEMLMs, including issues of scalability, generalization, and real-time\ndecision-making. Finally, we outline future directions, emphasizing the\nintegration of multimodal sensing, reasoning, and action to advance the\ndevelopment of increasingly autonomous systems. By providing an in-depth\nanalysis of state-of-the-art methods and identifying critical gaps, this paper\naims to inspire future advancements in EMLMs and their applications across\ndiverse domains.", "paper_summary_zh": "\u5177\u8eab\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b (EMLM) \u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\uff0c\u56e0\u4e3a\u5b83\u4eec\u6709\u53ef\u80fd\u5f25\u5408\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u611f\u77e5\u3001\u8ba4\u77e5\u548c\u884c\u52a8\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8fd9\u9879\u5168\u9762\u7684\u8bc4\u8bba\u63a2\u8ba8\u4e86\u6b64\u7c7b\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3001\u5927\u578b\u89c6\u89c9\u6a21\u578b (LVM) \u548c\u5176\u4ed6\u6a21\u578b\uff0c\u540c\u65f6\u8fd8\u8003\u5bdf\u4e86\u5176\u4ed6\u65b0\u5174\u67b6\u6784\u3002\u6211\u4eec\u8ba8\u8bba\u4e86 EMLM \u7684\u6f14\u53d8\uff0c\u91cd\u70b9\u5173\u6ce8\u5177\u8eab\u611f\u77e5\u3001\u5bfc\u822a\u3001\u4ea4\u4e92\u548c\u6a21\u62df\u3002\u6b64\u5916\uff0c\u8be5\u8bc4\u8bba\u5bf9\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u5f3a\u8c03\u4e86\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9\u4e8e\u6709\u6548\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u8fd8\u6307\u51fa\u4e86 EMLM \u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u6cdb\u5316\u548c\u5b9e\u65f6\u51b3\u7b56\u7684\u95ee\u9898\u3002\u6700\u540e\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u672a\u6765\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7684\u6574\u5408\uff0c\u4ee5\u63a8\u8fdb\u65e5\u76ca\u81ea\u4e3b\u7684\u7cfb\u7edf\u7684\u5f00\u53d1\u3002\u901a\u8fc7\u5bf9\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6df1\u5165\u5206\u6790\u5e76\u627e\u51fa\u5173\u952e\u5dee\u8ddd\uff0c\u672c\u6587\u65e8\u5728\u6fc0\u53d1 EMLM \u53ca\u5176\u5728\u4e0d\u540c\u9886\u57df\u5e94\u7528\u7684\u672a\u6765\u8fdb\u6b65\u3002", "author": "Shoubin Chen et.al.", "authors": "Shoubin Chen, Zehao Wu, Kai Zhang, Chunyu Li, Baiyang Zhang, Fei Ma, Fei Richard Yu, Qingquan Li", "id": "2502.15336v1", "paper_url": "http://arxiv.org/abs/2502.15336v1", "repo": "null"}}