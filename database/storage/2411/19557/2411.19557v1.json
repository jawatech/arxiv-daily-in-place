{"2411.19557": {"publish_time": "2024-11-29", "title": "Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning", "paper_summary": "Low-rank adapters have become a standard approach for efficiently fine-tuning\nlarge language models (LLMs), but they often fall short of achieving the\nperformance of full fine-tuning. We propose a method, LoRA Silver Bullet or\nLoRA-SB, that approximates full fine-tuning within low-rank subspaces using a\ncarefully designed initialization strategy. We theoretically demonstrate that\nthe architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B\nand A while keeping other matrices fixed, provides the precise conditions\nneeded for this approximation. We leverage its constrained update space to\nachieve optimal scaling for high-rank gradient updates while removing the need\nfor hyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using 27-90x fewer\nparameters, and comprehensively outperforms LoRA-XS. Our findings establish\nthat it is possible to simulate full fine-tuning in low-rank subspaces, and\nachieve significant efficiency gains without sacrificing performance. Our code\nis publicly available at https://github.com/RaghavSinghal10/lora-sb.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u914d\u5668\u5df2\u6210\u70ba\u6709\u6548\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6a19\u6e96\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u7121\u6cd5\u9054\u5230\u5b8c\u5168\u5fae\u8abf\u7684\u6548\u80fd\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u7a31\u70ba LoRA \u9280\u5f48\u6216 LoRA-SB\uff0c\u5b83\u4f7f\u7528\u7cbe\u5fc3\u8a2d\u8a08\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5728\u4f4e\u79e9\u5b50\u7a7a\u9593\u4e2d\u8fd1\u4f3c\u5b8c\u5168\u5fae\u8abf\u3002\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u8b49\u660e\u4e86 LoRA-XS \u7684\u67b6\u69cb\uff0c\u5b83\u5728 B \u548c A \u4e4b\u9593\u63d2\u5165\u4e00\u500b\u53ef\u8a13\u7df4\u7684 (r x r) \u77e9\u9663\uff0c\u540c\u6642\u4fdd\u6301\u5176\u4ed6\u77e9\u9663\u56fa\u5b9a\uff0c\u63d0\u4f9b\u4e86\u6b64\u8fd1\u4f3c\u6240\u9700\u7684\u7cbe\u78ba\u689d\u4ef6\u3002\u6211\u5011\u5229\u7528\u5176\u53d7\u9650\u7684\u66f4\u65b0\u7a7a\u9593\uff0c\u5728\u79fb\u9664\u8d85\u53c3\u6578\u8abf\u6574\u9700\u6c42\u7684\u540c\u6642\uff0c\u5be6\u73fe\u9ad8\u79e9\u68af\u5ea6\u66f4\u65b0\u7684\u6700\u4f73\u7e2e\u653e\u3002\u6211\u5011\u8b49\u660e\u6211\u5011\u7684\u521d\u59cb\u5316\u63d0\u4f9b\u4e86\u521d\u59cb\u68af\u5ea6\u7684\u6700\u4f73\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u4e26\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u4fdd\u7559\u66f4\u65b0\u65b9\u5411\u3002\u8de8\u8d8a\u6578\u5b78\u63a8\u7406\u3001\u5e38\u8b58\u63a8\u7406\u548c\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u8d85\u904e\u4e86\u6a19\u6e96 LoRA \u7684\u6548\u80fd\uff0c\u540c\u6642\u4f7f\u7528\u5c11 27-90 \u500d\u7684\u53c3\u6578\uff0c\u4e26\u5168\u9762\u512a\u65bc LoRA-XS\u3002\u6211\u5011\u7684\u767c\u73fe\u8b49\u5be6\u4e86\u5728\u4f4e\u79e9\u5b50\u7a7a\u9593\u4e2d\u6a21\u64ec\u5b8c\u5168\u5fae\u8abf\u662f\u53ef\u884c\u7684\uff0c\u800c\u4e14\u53ef\u4ee5\u5728\u4e0d\u72a7\u7272\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u986f\u8457\u7684\u6548\u7387\u63d0\u5347\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/RaghavSinghal10/lora-sb \u516c\u958b\u53d6\u5f97\u3002", "author": "Kaustubh Ponkshe et.al.", "authors": "Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma", "id": "2411.19557v1", "paper_url": "http://arxiv.org/abs/2411.19557v1", "repo": "https://github.com/raghavsinghal10/lora-sb"}}