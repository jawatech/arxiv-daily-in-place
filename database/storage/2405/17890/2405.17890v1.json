{"2405.17890": {"publish_time": "2024-05-28", "title": "SLMRec: Empowering Small Language Models for Sequential Recommendation", "paper_summary": "The sequential Recommendation (SR) task involves predicting the next item a\nuser is likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, we discover that most intermediate layers of LLMs are\nredundant. Motivated by this insight, we empower small language models for SR,\nnamely SLMRec, which adopt a simple yet effective knowledge distillation\nmethod. Moreover, SLMRec is orthogonal to other post-training efficiency\ntechniques, such as quantization and pruning, so that they can be leveraged in\ncombination. Comprehensive experimental results illustrate that the proposed\nSLMRec model attains the best performance using only 13% of the parameters\nfound in LLM-based recommendation models, while simultaneously achieving up to\n6.6x and 8.0x speedups in training and inference time costs, respectively.", "paper_summary_zh": "\u5e8f\u5217\u63a8\u85a6 (SR) \u4efb\u52d9\u6d89\u53ca\u9810\u6e2c\u4f7f\u7528\u8005\u6839\u64da\u904e\u53bb\u4e92\u52d5\uff0c\u63a5\u4e0b\u4f86\u53ef\u80fd\u6703\u4e92\u52d5\u7684\u9805\u76ee\u3002SR \u6a21\u578b\u6703\u6aa2\u8996\u4f7f\u7528\u8005\u7684\u52d5\u4f5c\u5e8f\u5217\uff0c\u4ee5\u8fa8\u5225\u66f4\u8907\u96dc\u7684\u884c\u70ba\u6a21\u5f0f\u548c\u6642\u9593\u52d5\u614b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0cLLM \u5c0d\u5e8f\u5217\u63a8\u85a6\u7cfb\u7d71\u6709\u5f88\u5927\u7684\u5f71\u97ff\uff0c\u7121\u8ad6\u662f\u5c07\u5e8f\u5217\u63a8\u85a6\u8996\u70ba\u8a9e\u8a00\u6a21\u578b\uff0c\u6216\u4f5c\u70ba\u4f7f\u7528\u8005\u8868\u5fb5\u7684\u9aa8\u5e79\u3002\u5118\u7ba1\u9019\u4e9b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5091\u51fa\u7684\u6548\u80fd\uff0c\u4f46\u9bae\u5c11\u8b49\u64da\u986f\u793a\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u53ca\u9700\u8981\u591a\u5927\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u7279\u5225\u662f\u5728\u5e8f\u5217\u63a8\u85a6\u5834\u666f\u4e2d\u3002\u540c\u6642\uff0c\u7531\u65bc LLM \u9f90\u5927\uff0c\u5728\u73fe\u5be6\u4e16\u754c\u5e73\u53f0\u4e2d\u61c9\u7528 LLM \u70ba\u57fa\u790e\u7684\u6a21\u578b\u65e2\u4f4e\u6548\u53c8\u4e0d\u5207\u5be6\u969b\uff0c\u56e0\u70ba\u9019\u4e9b\u5e73\u53f0\u901a\u5e38\u9700\u8981\u6bcf\u5929\u8655\u7406\u6578\u5341\u5104\u500b\u6d41\u91cf\u8a18\u9304\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5728\u5927\u578b\u7522\u696d\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u63a2\u8a0e LLM \u6df1\u5ea6\u7684\u5f71\u97ff\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe LLM \u7684\u5927\u591a\u6578\u4e2d\u9593\u5c64\u90fd\u662f\u591a\u9918\u7684\u3002\u53d7\u5230\u9019\u500b\u898b\u89e3\u7684\u555f\u767c\uff0c\u6211\u5011\u70ba SR \u8ce6\u80fd\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u5373 SLMRec\uff0c\u5b83\u63a1\u7528\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u77e5\u8b58\u63d0\u7149\u65b9\u6cd5\u3002\u6b64\u5916\uff0cSLMRec \u8207\u5176\u4ed6\u8a13\u7df4\u5f8c\u6548\u7387\u6280\u8853\u6b63\u4ea4\uff0c\u4f8b\u5982\u91cf\u5316\u548c\u526a\u679d\uff0c\u56e0\u6b64\u53ef\u4ee5\u7d50\u5408\u4f7f\u7528\u3002\u5168\u9762\u7684\u5be6\u9a57\u7d50\u679c\u8aaa\u660e\uff0c\u5efa\u8b70\u7684 SLMRec \u6a21\u578b\u53ea\u4f7f\u7528 LLM \u70ba\u57fa\u790e\u7684\u63a8\u85a6\u6a21\u578b\u4e2d 13% \u7684\u53c3\u6578\uff0c\u5c31\u80fd\u7372\u5f97\u6700\u4f73\u6548\u80fd\uff0c\u540c\u6642\u5728\u8a13\u7df4\u548c\u63a8\u7406\u6642\u9593\u6210\u672c\u5206\u5225\u9054\u5230\u6700\u9ad8 6.6 \u500d\u548c 8.0 \u500d\u7684\u52a0\u901f\u3002", "author": "Wujiang Xu et.al.", "authors": "Wujiang Xu, Zujie Liang, Jiaojiao Han, Xuying Ning, Wenfang Lin, Linxun Chen, Feng Wei, Yongfeng Zhang", "id": "2405.17890v1", "paper_url": "http://arxiv.org/abs/2405.17890v1", "repo": "null"}}