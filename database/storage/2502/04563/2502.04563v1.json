{"2502.04563": {"publish_time": "2025-02-06", "title": "WaferLLM: A Wafer-Scale LLM Inference System", "paper_summary": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing\ntechnologies, integrating hundreds of thousands of AI cores in a mesh-based\narchitecture with large distributed on-chip memory (tens of GB in total) and\nultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM\ninference systems, optimized for shared memory architectures like GPUs, fail to\nfully exploit these accelerators. We introduce WaferLLM, the first wafer-scale\nLLM inference system. WaferLLM is guided by a novel PLMR device model that\ncaptures the unique hardware characteristics of wafer-scale architectures.\nLeveraging this model, WaferLLM pioneers wafer-scale LLM parallelism,\noptimizing the utilization of hundreds of thousands of on-chip cores. It also\nintroduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations\ndesigned to scale effectively on wafer-scale accelerators. Evaluations show\nthat WaferLLM achieves 200$\\times$ better wafer-scale accelerator utilization\nthan state-of-the-art systems. On a commodity wafer-scale accelerator, WaferLLM\ndelivers 606$\\times$ faster and 22$\\times$ more energy-efficient GEMV compared\nto an advanced GPU. For LLMs, WaferLLM enables 39$\\times$ faster decoding with\n1.7$\\times$ better energy efficiency. We anticipate these numbers will grow\nsignificantly as wafer-scale AI models, software, and hardware continue to\nmature.", "paper_summary_zh": "\u65b0\u8208\u7684 AI \u52a0\u901f\u5668\u6b63\u65e5\u76ca\u63a1\u7528\u6676\u5713\u7d1a\u88fd\u9020\u6280\u8853\uff0c\u5c07\u6578\u5341\u842c\u500b AI \u6838\u5fc3\u6574\u5408\u5230\u57fa\u65bc\u7db2\u683c\u7684\u67b6\u69cb\u4e2d\uff0c\u4e26\u5177\u5099\u5927\u578b\u5206\u5e03\u5f0f\u6676\u7247\u8a18\u61b6\u9ad4\uff08\u7e3d\u8a08\u6578\u5341 GB\uff09\u548c\u8d85\u9ad8\u6676\u7247\u8a18\u61b6\u9ad4\u983b\u5bec\uff08\u6578\u5341 PB/s\uff09\u3002\u7136\u800c\uff0c\u76ee\u524d\u91dd\u5c0d\u5171\u4eab\u8a18\u61b6\u9ad4\u67b6\u69cb\uff08\u5982 GPU\uff09\u9032\u884c\u6700\u4f73\u5316\u7684 LLM \u63a8\u8ad6\u7cfb\u7d71\uff0c\u7121\u6cd5\u5145\u5206\u5229\u7528\u9019\u4e9b\u52a0\u901f\u5668\u3002\u6211\u5011\u63a8\u51fa WaferLLM\uff0c\u9019\u662f\u6709\u53f2\u4ee5\u4f86\u7b2c\u4e00\u500b\u6676\u5713\u7d1a LLM \u63a8\u8ad6\u7cfb\u7d71\u3002WaferLLM \u7531\u4e00\u7a2e\u65b0\u7a4e\u7684 PLMR \u88dd\u7f6e\u6a21\u578b\u5f15\u5c0e\uff0c\u8a72\u6a21\u578b\u64f7\u53d6\u4e86\u6676\u5713\u7d1a\u67b6\u69cb\u7684\u7368\u7279\u786c\u9ad4\u7279\u6027\u3002\u85c9\u7531\u5229\u7528\u6b64\u6a21\u578b\uff0cWaferLLM \u958b\u5275\u4e86\u6676\u5713\u7d1a LLM \u5e73\u884c\u8655\u7406\uff0c\u6700\u4f73\u5316\u4e86\u6578\u5341\u842c\u500b\u6676\u7247\u6838\u5fc3\u7684\u4f7f\u7528\u7387\u3002\u5b83\u9084\u5f15\u5165\u4e86 MeshGEMM \u548c MeshGEMV\uff0c\u9019\u662f\u7b2c\u4e00\u500b GEMM \u548c GEMV \u5be6\u4f5c\uff0c\u65e8\u5728\u6709\u6548\u5730\u64f4\u5145\u6676\u5713\u7d1a\u52a0\u901f\u5668\u3002\u8a55\u4f30\u7d50\u679c\u986f\u793a\uff0c\u8207\u73fe\u6709\u6700\u5148\u9032\u7684\u7cfb\u7d71\u76f8\u6bd4\uff0cWaferLLM \u9054\u5230\u4e86 200 \u500d\u66f4\u597d\u7684\u6676\u5713\u7d1a\u52a0\u901f\u5668\u4f7f\u7528\u7387\u3002\u5728\u4e00\u500b\u5546\u7528\u6676\u5713\u7d1a\u52a0\u901f\u5668\u4e0a\uff0c\u8207\u5148\u9032\u7684 GPU \u76f8\u6bd4\uff0cWaferLLM \u63d0\u4f9b\u4e86\u5feb 606 \u500d\u3001\u80fd\u6548\u63d0\u9ad8 22 \u500d\u7684 GEMV\u3002\u5c0d\u65bc LLM\uff0cWaferLLM \u80fd\u5920\u4ee5 1.7 \u500d\u66f4\u597d\u7684\u80fd\u6548\u9032\u884c\u5feb 39 \u500d\u7684\u89e3\u78bc\u3002\u6211\u5011\u9810\u671f\u96a8\u8457\u6676\u5713\u7d1a AI \u6a21\u578b\u3001\u8edf\u9ad4\u548c\u786c\u9ad4\u6301\u7e8c\u6210\u719f\uff0c\u9019\u4e9b\u6578\u5b57\u5c07\u5927\u5e45\u6210\u9577\u3002", "author": "Congjie He et.al.", "authors": "Congjie He, Yeqi Huang, Pei Mu, Ziming Miao, Jilong Xue, Lingxiao Ma, Fan Yang, Luo Mai", "id": "2502.04563v1", "paper_url": "http://arxiv.org/abs/2502.04563v1", "repo": "null"}}