{"2408.03047": {"publish_time": "2024-08-06", "title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents", "paper_summary": "Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.", "paper_summary_zh": "\u591a\u6a21\u6001\u5bf9\u8bdd\u4ee3\u7406\u6781\u5177\u5438\u5f15\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u63d0\u4f9b\u81ea\u7136\u4e14\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4e92\u52a8\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u534f\u4f5c\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002\u867d\u7136\u50cf GPT-4o \u548c Gemini \u8fd9\u6837\u7684\u4e13\u6709\u7cfb\u7edf\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u97f3\u9891\u3001\u89c6\u9891\u548c\u6587\u672c\u96c6\u6210\uff0c\u54cd\u5e94\u65f6\u95f4\u4e3a 200-250 \u6beb\u79d2\uff0c\u4f46\u5728\u5e73\u8861\u5ef6\u8fdf\u3001\u51c6\u786e\u6027\u3001\u6210\u672c\u548c\u6570\u636e\u9690\u79c1\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u91cf\u5316\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 OpenOmni\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u7aef\u5230\u7aef\u7ba1\u9053\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u5b83\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5982\u8bed\u97f3\u8f6c\u6587\u672c\u3001\u60c5\u611f\u68c0\u6d4b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u96c6\u6210\u81ea\u5b9a\u4e49\u6a21\u578b\u7684\u80fd\u529b\u3002OpenOmni \u652f\u6301\u672c\u5730\u548c\u4e91\u90e8\u7f72\uff0c\u786e\u4fdd\u6570\u636e\u9690\u79c1\u5e76\u652f\u6301\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002\u8fd9\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u81ea\u5b9a\u4e49\u7ba1\u9053\uff0c\u4e13\u6ce8\u4e8e\u771f\u6b63\u7684\u74f6\u9888\u5e76\u4fc3\u8fdb\u5feb\u901f\u7684\u6982\u5ff5\u9a8c\u8bc1\u5f00\u53d1\u3002OpenOmni \u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u5e94\u7528\u7a0b\u5e8f\uff0c\u5982\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u5ba4\u5185\u5e2e\u52a9\uff0c\u63a8\u8fdb\u4eba\u673a\u4ea4\u4e92\u3002\u6211\u4eec\u7684\u6f14\u793a\u89c6\u9891\u53ef\u5728 https://www.youtube.com/watch?v=zaSiT3clWqY \u83b7\u5f97\uff0c\u6f14\u793a\u53ef\u5728 https://openomni.ai4wa.com \u83b7\u5f97\uff0c\u4ee3\u7801\u53ef\u5728 https://github.com/AI4WA/OpenOmniFramework \u83b7\u5f97\u3002", "author": "Qiang Sun et.al.", "authors": "Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, Wei Liu", "id": "2408.03047v1", "paper_url": "http://arxiv.org/abs/2408.03047v1", "repo": "null"}}