{"2405.03372": {"publish_time": "2024-05-06", "title": "Snake Learning: A Communication- and Computation-Efficient Distributed Learning Framework for 6G", "paper_summary": "In the evolution towards 6G, integrating Artificial Intelligence (AI) with\nadvanced network infrastructure emerges as a pivotal strategy for enhancing\nnetwork intelligence and resource utilization. Existing distributed learning\nframeworks like Federated Learning and Split Learning often struggle with\nsignificant challenges in dynamic network environments including high\nsynchronization demands, costly communication overheads, severe computing\nresource consumption, and data heterogeneity across network nodes. These\nobstacles hinder the applications of ubiquitous computing capabilities of 6G\nnetworks, especially in light of the trend of escalating model parameters and\ntraining data volumes. To address these challenges effectively, this paper\nintroduces \"Snake Learning\", a cost-effective distributed learning framework.\nSpecifically, Snake Learning respects the heterogeneity of inter-node computing\ncapability and local data distribution in 6G networks, and sequentially trains\nthe designated part of model layers on individual nodes. This layer-by-layer\nserpentine update mechanism contributes to significantly reducing the\nrequirements for storage, memory and communication during the model training\nphase, and demonstrates superior adaptability and efficiency for both Computer\nVision (CV) training and Large Language Model (LLM) fine-tuning tasks across\nhomogeneous and heterogeneous data distributions.", "paper_summary_zh": "", "author": "Xiaoxue Yu et.al.", "authors": "Xiaoxue Yu,Xingfu Yi,Rongpeng Li,Fei Wang,Chenghui Peng,Zhifeng Zhao,Honggang Zhang", "id": "2405.03372v1", "paper_url": "http://arxiv.org/abs/2405.03372v1", "repo": "null"}}