{"2409.04249": {"publish_time": "2024-09-06", "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices", "paper_summary": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u57fa\u65bc Transformer \u7684\u5927\u578b\u6a21\u578b\u61c9\u7528\u5df2\u53d6\u5f97\u8a31\u591a\u6210\u529f\u3002\u7136\u800c\uff0c\u5927\u578b\u6a21\u578b\u5f15\u6578\u7684\u6307\u6578\u6210\u9577\u70ba\u908a\u7de3\u90e8\u7f72\u5e36\u4f86\u4e86\u56b4\u5cfb\u7684\u8a18\u61b6\u9ad4\u6311\u6230\u3002\u5148\u524d\u91dd\u5c0d\u6b64\u6311\u6230\u7684\u89e3\u6c7a\u65b9\u6848\u4e3b\u8981\u5c08\u6ce8\u65bc\u6700\u4f73\u5316\u6a21\u578b\u7d50\u69cb\u548c\u63a1\u7528\u8a18\u61b6\u9ad4\u4ea4\u63db\u65b9\u6cd5\u3002\u4f46\u662f\uff0c\u524d\u8005\u6703\u964d\u4f4e\u63a8\u8ad6\u6e96\u78ba\u5ea6\uff0c\u800c\u5f8c\u8005\u6703\u589e\u52a0\u63a8\u8ad6\u5ef6\u9072\u3002\u672c\u6587\u4ecb\u7d39 PIPELOAD\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u7701\u8a18\u61b6\u9ad4\u7ba1\u9053\u57f7\u884c\u6a5f\u5236\u3002\u5b83\u900f\u904e\u6574\u5408\u52d5\u614b\u8a18\u61b6\u9ad4\u7ba1\u7406\u4f86\u6e1b\u5c11\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u4e26\u900f\u904e\u63a1\u7528\u5e73\u884c\u6a21\u578b\u8f09\u5165\u4f86\u6700\u5c0f\u5316\u63a8\u8ad6\u5ef6\u9072\u3002\u57fa\u65bc PIPELOAD \u6a5f\u5236\uff0c\u6211\u5011\u63d0\u51fa Hermes\uff0c\u4e00\u500b\u91dd\u5c0d\u908a\u7de3\u88dd\u7f6e\u4e0a\u7684\u5927\u578b\u6a21\u578b\u63a8\u8ad6\u6700\u4f73\u5316\u7684\u6846\u67b6\u3002\u6211\u5011\u5728\u4e0d\u540c\u5927\u5c0f\u7684\u57fa\u65bc Transformer \u7684\u6a21\u578b\u4e0a\u8a55\u4f30 Hermes\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5c0d\u65bc BERT \u548c ViT \u6a21\u578b\uff0cHermes \u7684\u63a8\u8ad6\u901f\u5ea6\u63d0\u9ad8\u4e86 4.24 \u500d\uff0c\u8a18\u61b6\u9ad4\u6d88\u8017\u964d\u4f4e\u4e86 86.7%\uff0c\u800c\u5c0d\u65bc GPT \u985e\u578b\u7684\u6a21\u578b\uff0c\u63a8\u8ad6\u901f\u5ea6\u63d0\u9ad8\u4e86 2.58 \u500d\uff0c\u8a18\u61b6\u9ad4\u6d88\u8017\u964d\u4f4e\u4e86 90.3%\uff0c\u512a\u65bc\u73fe\u6709\u6700\u5148\u9032\u7684\u7ba1\u9053\u6a5f\u5236\u3002", "author": "Xueyuan Han et.al.", "authors": "Xueyuan Han, Zinuo Cai, Yichu Zhang, Chongxin Fan, Junhan Liu, Ruhui Ma, Rajkumar Buyya", "id": "2409.04249v1", "paper_url": "http://arxiv.org/abs/2409.04249v1", "repo": "null"}}