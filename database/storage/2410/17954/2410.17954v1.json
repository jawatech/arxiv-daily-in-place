{"2410.17954": {"publish_time": "2024-10-23", "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference", "paper_summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.", "paper_summary_zh": "\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u578b\u867d\u7136\u5728\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u7a20\u5bc6\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7531\u4e8e\u5176\u9ad8\u5185\u5b58\u9700\u6c42\u800c\u9762\u4e34\u7740\u91cd\u5927\u7684\u90e8\u7f72\u6311\u6218\u3002\u73b0\u6709\u7684\u5378\u8f7d\u6280\u672f\u6d89\u53ca\u5728 GPU \u548c CPU \u4e4b\u95f4\u4ea4\u6362\u6fc0\u6d3b\u7684\u548c\u7a7a\u95f2\u7684\u4e13\u5bb6\uff0c\u901a\u5e38\u4f1a\u53d7\u5230\u50f5\u5316\u7684\u4e13\u5bb6\u7f13\u5b58\u673a\u5236\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u673a\u5236\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u8def\u7531\uff0c\u5bfc\u81f4\u7f13\u5b58\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u6216\u5bfc\u81f4\u9884\u6d4b\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u7279\u5b9a\u4e8e\u63a8\u7406\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ExpertFlow\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u901a\u8fc7\u9002\u5e94\u7075\u6d3b\u8def\u7531\u548c\u5728 CPU \u548c GPU \u4e4b\u95f4\u5b9e\u73b0\u9ad8\u6548\u4e13\u5bb6\u8c03\u5ea6\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u7684\u7efc\u5408\u7cfb\u7edf\u3002\u8fd9\u51cf\u5c11\u4e86\u5f00\u9500\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u6d4b\u8def\u7531\u8def\u5f84\u7684\u5378\u8f7d\u673a\u5236\uff0c\u8be5\u673a\u5236\u5229\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\u5728\u8ba1\u7b97\u5f00\u59cb\u4e4b\u524d\u51c6\u786e\u9884\u6d4b\u8def\u7531\u8def\u5f84\u3002\u8fd9\u79cd\u4e3b\u52a8\u7b56\u7565\u5141\u8bb8\u5728\u4e13\u5bb6\u7f13\u5b58\u4e2d\u8fdb\u884c\u5b9e\u65f6\u9519\u8bef\u6821\u6b63\uff0c\u663e\u8457\u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d\u7387\u5e76\u964d\u4f4e\u4e13\u5bb6\u4f20\u8f93\u7684\u9891\u7387\uff0c\u4ece\u800c\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11 I/O \u5f00\u9500\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5b9e\u65bd\u4e86\u4e00\u4e2a\u52a8\u6001\u4ee4\u724c\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u7684\u6279\u6b21\u4e4b\u95f4\u91cd\u65b0\u6392\u5217\u8f93\u5165\u4ee4\u724c\u6765\u4f18\u5316 MoE \u63a8\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u51cf\u5c11\u4e86\u6bcf\u4e2a\u6279\u6b21\u4e2d\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cExpertFlow \u53ef\u8282\u7701\u9ad8\u8fbe 93.72% \u7684 GPU \u5185\u5b58\uff0c\u5e76\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8 2 \u5230 10 \u500d\uff0c\u7a81\u51fa\u4e86\u5176\u4f5c\u4e3a\u8d44\u6e90\u53d7\u9650\u63a8\u7406\u573a\u666f\u7684\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "author": "Xin He et.al.", "authors": "Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon", "id": "2410.17954v1", "paper_url": "http://arxiv.org/abs/2410.17954v1", "repo": "null"}}