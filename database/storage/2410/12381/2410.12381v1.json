{"2410.12381": {"publish_time": "2024-10-16", "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks", "paper_summary": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.", "paper_summary_zh": "<paragraph>\u7de8\u78bc\u4efb\u52d9\u5c0d\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f88\u6709\u50f9\u503c\uff0c\n\u56e0\u70ba\u5b83\u5011\u9700\u8981\u7406\u89e3\u9ad8\u5c64\u7d1a\u6307\u4ee4\u3001\u8907\u96dc\u63a8\u7406\uff0c\n\u4ee5\u53ca\u5be6\u4f5c\u51fd\u5f0f\u7a0b\u5f0f -- \u4eba\u5de5\u901a\u7528\u667a\u6167\u9032\u6b65\u7684\u6838\u5fc3\u80fd\u529b\u3002\u5118\u7ba1\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u6709\u9032\u5c55\uff0c\u5b83\u64f4\u5145\u4e86 LLM \u7684\u8996\u89ba\u611f\u77e5\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4ecd\u7136\u986f\u8457\u7f3a\u4e4f\u7de8\u78bc\u57fa\u6e96\uff0c\u56b4\u683c\u8a55\u4f30\u9019\u4e9b\u6a21\u578b\uff0c\u7279\u5225\u662f\u5728\u5f37\u8abf\u8996\u89ba\u63a8\u7406\u7684\u4efb\u52d9\u4e2d\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u7f3a\u53e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 HumanEval-V\uff0c\u4e00\u500b\u65b0\u7a4e\u4e14\u8f15\u91cf\u5316\u7684\u57fa\u6e96\uff0c\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u900f\u904e\u7a0b\u5f0f\u78bc\u7522\u751f\u4f86\u8a55\u4f30 LMM \u7684\u8996\u89ba\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002HumanEval-V \u5305\u542b 108 \u500b\u7cbe\u5fc3\u88fd\u4f5c\u7684\u5165\u9580\u7d1a Python \u7de8\u78bc\u4efb\u52d9\uff0c\u884d\u751f\u81ea CodeForces \u548c Stack Overflow \u7b49\u5e73\u53f0\u3002\u6bcf\u500b\u4efb\u52d9\u900f\u904e\u4fee\u6539\u539f\u59cb\u554f\u984c\u7684\u80cc\u666f\u548c\u6f14\u7b97\u6cd5\u6a21\u5f0f\u4f86\u6539\u7de8\uff0c\u4e26\u91cd\u65b0\u7e6a\u88fd\u8996\u89ba\u5143\u7d20\u4ee5\u78ba\u4fdd\u8207\u4f86\u6e90\u5340\u5206\uff0c\u9632\u6b62\u6f5b\u5728\u8cc7\u6599\u5916\u6d29\u3002LMM \u9700\u8981\u6839\u64da\u63d0\u4f9b\u7684\u8996\u89ba\u80cc\u666f\u548c\u9810\u5148\u5b9a\u7fa9\u7684 Python \u51fd\u5f0f\u7c3d\u7ae0\u4f86\u5b8c\u6210\u7a0b\u5f0f\u78bc\u89e3\u6c7a\u65b9\u6848\uff0c\u6982\u8ff0\u4efb\u52d9\u9700\u6c42\u3002\u6bcf\u500b\u4efb\u52d9\u90fd\u914d\u5099\u4e86\u7cbe\u5fc3\u88fd\u4f5c\u7684\u6e2c\u8a66\u6848\u4f8b\uff0c\u4ee5\u78ba\u4fdd\u5c0d\u6a21\u578b\u7522\u751f\u7684\u89e3\u6c7a\u65b9\u6848\u9032\u884c\u5fb9\u5e95\u4e14\u53ef\u9760\u7684\u8a55\u4f30\u3002\u6211\u5011\u4f7f\u7528 HumanEval-V \u8a55\u4f30\u4e86 19 \u500b\u6700\u5148\u9032\u7684 LMM\uff0c\u767c\u73fe\u4e86\u91cd\u5927\u6311\u6230\u3002GPT-4o \u7b49\u5c08\u6709\u6a21\u578b\u50c5\u9054\u5230 13% \u7684 pass@1 \u548c 36.4% \u7684 pass@10\uff0c\u800c\u5177\u6709 70B \u53c3\u6578\u7684\u958b\u653e\u6b0a\u91cd\u6a21\u578b\u5f97\u5206\u4f4e\u65bc 4% \u7684 pass@1\u3002\u6d88\u878d\u7814\u7a76\u9032\u4e00\u6b65\u63ed\u793a\u4e86\u7576\u524d LMM \u5728\u8996\u89ba\u63a8\u7406\u548c\u7de8\u78bc\u80fd\u529b\u65b9\u9762\u7684\u9650\u5236\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86\u672a\u4f86\u7814\u7a76\u4ee5\u589e\u5f37 LMM \u80fd\u529b\u7684\u4e3b\u8981\u9818\u57df\u3002\u6211\u5011\u5df2\u5728 https://github.com/HumanEval-V/HumanEval-V-Benchmark \u958b\u653e\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u57fa\u6e96\u3002</paragraph>", "author": "Fengji Zhang et.al.", "authors": "Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, Jacky Keung", "id": "2410.12381v1", "paper_url": "http://arxiv.org/abs/2410.12381v1", "repo": "null"}}