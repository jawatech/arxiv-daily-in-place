{"2408.03511": {"publish_time": "2024-08-07", "title": "MoExtend: Tuning New Experts for Modality and Task Extension", "paper_summary": "Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u4e3b\u8981\u8a13\u7df4\n\u6587\u672c\u8cc7\u6599\uff0c\u9650\u5236\u4e86\u5176\u61c9\u7528\u7bc4\u570d\u3002\u64f4\u5c55 LLM \u80fd\u529b\u4ee5\n\u5305\u542b\u8996\u89ba\u8a9e\u8a00\u7406\u89e3\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u5b83\u5011\u591a\u6a21\u614b\n\u8cc7\u6599\u5177\u6709\u6311\u6230\u6027\u4e14\u6210\u672c\u9ad8\u6602\u3002\u73fe\u6709\u7684\u6307\u4ee4\u8abf\u6574\n\u65b9\u6cd5\uff0c\u4f8b\u5982 LLAVA\uff0c\u901a\u5e38\u901a\u904e\u5b8c\u5168\u5fae\u8abf LLM \u4f86\u9023\u63a5\u9810\u8a13\u7df4\u7684 CLIP \u8996\u89ba\u7de8\u78bc\u5668\u548c LLM\n\u5f4c\u5408\u6a21\u614b\u5dee\u8ddd\u3002\u7136\u800c\uff0c\u5b8c\u5168\u5fae\u8abf\u6703\u53d7\u5230\u707d\u96e3\u6027\u907a\u5fd8\u7684\u56f0\u64fe\uff0c\u5373\u5fd8\u8a18\u4e4b\u524d\u7684\n\u77e5\u8b58\uff0c\u7279\u5225\u662f\u5728\u4efb\u52d9\u548c\u6a21\u614b\u589e\u52a0\u7684\u6642\u4ee3\uff0c\u8a13\u7df4\u6210\u672c\u5f88\u9ad8\n\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 MoExtend\uff0c\u4e00\u500b\u6709\u6548\u7684\n\u67b6\u69cb\u8a2d\u8a08\u7528\u65bc\u7c21\u5316\u6a21\u614b\u9069\u61c9\u548c\u64f4\u5c55\n\u6df7\u5408\u5c08\u5bb6 (MoE) \u6a21\u578b\u3002MoExtend \u5c07\u65b0\u7684\u5c08\u5bb6\u7121\u7e2b\u96c6\u6210\u5230\u9810\u8a13\u7df4\u7684 MoE \u6a21\u578b\u4e2d\uff0c\u8ce6\u4e88\u5b83\u5011\u65b0\u7684\u77e5\u8b58\uff0c\u800c\u7121\u9700\u8abf\u6574\u9810\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u4f8b\u5982 MoE \u548c\u8996\u89ba\u7de8\u78bc\u5668\u3002\u9019\u7a2e\u65b9\u6cd5\n\u80fd\u5920\u5feb\u901f\u9069\u61c9\u548c\u64f4\u5c55\u5230\u65b0\u7684\u6a21\u614b\u8cc7\u6599\u6216\u4efb\u52d9\uff0c\u6709\u6548\u5730\n\u89e3\u6c7a\u4e86\u5728 LLM \u4e2d\u5bb9\u7d0d\u65b0\u6a21\u614b\u7684\u6311\u6230\u3002\n\u6b64\u5916\uff0cMoExtend \u907f\u514d\u8abf\u6574\u9810\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u5f9e\u800c\u964d\u4f4e\u4e86\u98a8\u96aa\n\u707d\u96e3\u6027\u907a\u5fd8\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 MoExtend \u5728\u589e\u5f37 LLM \u7684\u591a\u6a21\u614b\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\n\u4fc3\u9032\u591a\u6a21\u614b AI \u7814\u7a76\u7684\u9032\u6b65\u3002\u7a0b\u5f0f\u78bc\uff1a\nhttps://github.com/zhongshsh/MoExtend\u3002", "author": "Shanshan Zhong et.al.", "authors": "Shanshan Zhong, Shanghua Gao, Zhongzhan Huang, Wushao Wen, Marinka Zitnik, Pan Zhou", "id": "2408.03511v1", "paper_url": "http://arxiv.org/abs/2408.03511v1", "repo": "null"}}