{"2410.18541": {"publish_time": "2024-10-24", "title": "On Explaining with Attention Matrices", "paper_summary": "This paper explores the much discussed, possible explanatory link between\nattention weights (AW) in transformer models and predicted output. Contrary to\nintuition and early research on attention, more recent prior research has\nprovided formal arguments and empirical evidence that AW are not explanatorily\nrelevant. We show that the formal arguments are incorrect. We introduce and\neffectively compute efficient attention, which isolates the effective\ncomponents of attention matrices in tasks and models in which AW play an\nexplanatory role. We show that efficient attention has a causal role (provides\nminimally necessary and sufficient conditions) for predicting model output in\nNLP tasks requiring contextual information, and we show, contrary to [7], that\nefficient attention matrices are probability distributions and are effectively\ncalculable. Thus, they should play an important part in the explanation of\nattention based model behavior. We offer empirical experiments in support of\nour method illustrating various properties of efficient attention with various\nmetrics on four datasets.", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u4e86Transformer\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u6b0a\u91cd (AW) \u8207\u9810\u6e2c\u8f38\u51fa\u4e4b\u9593\u5ee3\u6cdb\u8a0e\u8ad6\u7684\u53ef\u80fd\u89e3\u91cb\u6027\u9023\u7d50\u3002\u8207\u76f4\u89ba\u548c\u65e9\u671f\u6ce8\u610f\u529b\u7814\u7a76\u76f8\u53cd\uff0c\u6700\u8fd1\u8f03\u65e9\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6b63\u5f0f\u8ad6\u8b49\u548c\u7d93\u9a57\u8b49\u64da\uff0c\u8868\u660e AW \u6c92\u6709\u89e3\u91cb\u6027\u76f8\u95dc\u6027\u3002\u6211\u5011\u8868\u660e\u6b63\u5f0f\u8ad6\u8b49\u662f\u4e0d\u6b63\u78ba\u7684\u3002\u6211\u5011\u5f15\u5165\u4e26\u6709\u6548\u8a08\u7b97\u6709\u6548\u6ce8\u610f\u529b\uff0c\u5b83\u6703\u5206\u96e2\u4efb\u52d9\u548c\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u77e9\u9663\u7684\u6709\u6548\u7d44\u6210\uff0c\u5728\u9019\u4e9b\u4efb\u52d9\u548c\u6a21\u578b\u4e2d\uff0cAW \u626e\u6f14\u4e86\u89e3\u91cb\u6027\u89d2\u8272\u3002\u6211\u5011\u8868\u660e\u6709\u6548\u6ce8\u610f\u529b\u5728\u9810\u6e2c\u9700\u8981\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u7684 NLP \u4efb\u52d9\u4e2d\u7684\u6a21\u578b\u8f38\u51fa\u6642\u6709\u56e0\u679c\u95dc\u4fc2\uff08\u63d0\u4f9b\u6700\u5c0f\u5fc5\u8981\u4e14\u5145\u5206\u7684\u689d\u4ef6\uff09\uff0c\u800c\u4e14\u6211\u5011\u8868\u660e\u8207 [7] \u76f8\u53cd\uff0c\u6709\u6548\u6ce8\u610f\u529b\u77e9\u9663\u662f\u6a5f\u7387\u5206\u4f48\uff0c\u4e14\u53ef\u4ee5\u6709\u6548\u8a08\u7b97\u3002\u56e0\u6b64\uff0c\u5b83\u5011\u61c9\u5728\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u6a21\u578b\u884c\u70ba\u7684\u89e3\u91cb\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002\u6211\u5011\u63d0\u4f9b\u7d93\u9a57\u5be6\u9a57\u4ee5\u652f\u6301\u6211\u5011\u7684\u5404\u7a2e\u6307\u6a19\u5728\u56db\u500b\u8cc7\u6599\u96c6\u4e0a\u8aaa\u660e\u6709\u6548\u6ce8\u610f\u529b\u7684\u5404\u7a2e\u5c6c\u6027\u7684\u65b9\u6cd5\u3002", "author": "Omar Naim et.al.", "authors": "Omar Naim, Nicholas Asher", "id": "2410.18541v1", "paper_url": "http://arxiv.org/abs/2410.18541v1", "repo": "https://github.com/omyokun/on-explaining-with-attention-matrices"}}