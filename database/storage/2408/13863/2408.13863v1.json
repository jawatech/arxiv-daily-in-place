{"2408.13863": {"publish_time": "2024-08-25", "title": "CodeGraph: Enhancing Graph Reasoning of LLMs with Code", "paper_summary": "With the increasing popularity of large language models (LLMs), reasoning on\nbasic graph algorithm problems is an essential intermediate step in assessing\ntheir abilities to process and infer complex graph reasoning tasks. Existing\nmethods usually convert graph-structured data to textual descriptions and then\nuse LLMs for reasoning and computation. However, LLMs often produce computation\nerrors on arithmetic parts in basic graph algorithm problems, such as counting\nnumber of edges. In addition, they struggle to control or understand the output\nof the reasoning process, raising concerns about whether LLMs are simply\nguessing. In this paper, we introduce CodeGraph, a method that encodes graph\nproblem solutions as code. The methods solve new graph problems by learning\nfrom exemplars, generating programs, and executing them via a program\ninterpreter. Using the few-shot setting, we evaluate CodeGraph with the base\nLLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and\nMixtral-8x7B Instruct. Experimental results on six tasks with six graph\nencoding methods in the GraphQA dataset demonstrate that CodeGraph can boost\nperformance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on\nthe task. Compared to the existing methods, CodeGraph demonstrates strong\nperformance on arithmetic problems in graph tasks and offers a more\ncontrollable and interpretable approach to the reasoning process.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u65e5\u6f38\u666e\u53ca\uff0c\u5c0d\u57fa\u672c\u5716\u5f62\u6f14\u7b97\u6cd5\u554f\u984c\u9032\u884c\u63a8\u7406\u662f\u8a55\u4f30\u5b83\u5011\u8655\u7406\u548c\u63a8\u8ad6\u8907\u96dc\u5716\u5f62\u63a8\u7406\u4efb\u52d9\u7684\u80fd\u529b\u4e2d\u4e00\u500b\u91cd\u8981\u7684\u4e2d\u9593\u6b65\u9a5f\u3002\u73fe\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u6703\u5c07\u5716\u5f62\u7d50\u69cb\u5316\u7684\u8cc7\u6599\u8f49\u63db\u6210\u6587\u5b57\u63cf\u8ff0\uff0c\u7136\u5f8c\u4f7f\u7528 LLM \u9032\u884c\u63a8\u7406\u548c\u904b\u7b97\u3002\u7136\u800c\uff0cLLM \u901a\u5e38\u6703\u5728\u57fa\u672c\u5716\u5f62\u6f14\u7b97\u6cd5\u554f\u984c\u4e2d\uff0c\u4f8b\u5982\u8a08\u7b97\u908a\u7de3\u6578\u91cf\uff0c\u5c0d\u7b97\u8853\u90e8\u5206\u7522\u751f\u904b\u7b97\u932f\u8aa4\u3002\u6b64\u5916\uff0c\u5b83\u5011\u96e3\u4ee5\u63a7\u5236\u6216\u7406\u89e3\u63a8\u7406\u904e\u7a0b\u7684\u8f38\u51fa\uff0c\u9019\u5f15\u767c\u4e86 LLM \u662f\u5426\u53ea\u662f\u5728\u731c\u6e2c\u7684\u7591\u616e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CodeGraph\uff0c\u9019\u662f\u4e00\u7a2e\u5c07\u5716\u5f62\u554f\u984c\u89e3\u6c7a\u65b9\u6848\u7de8\u78bc\u70ba\u7a0b\u5f0f\u78bc\u7684\u65b9\u6cd5\u3002\u9019\u4e9b\u65b9\u6cd5\u900f\u904e\u5b78\u7fd2\u7bc4\u4f8b\u3001\u7522\u751f\u7a0b\u5f0f\uff0c\u4e26\u900f\u904e\u7a0b\u5f0f\u78bc\u76f4\u8b6f\u5668\u57f7\u884c\u5b83\u5011\u4f86\u89e3\u6c7a\u65b0\u7684\u5716\u5f62\u554f\u984c\u3002\u4f7f\u7528\u5c11\u6b21\u5617\u8a66\u8a2d\u5b9a\uff0c\u6211\u5011\u4f7f\u7528\u57fa\u790e LLM \u70ba GPT-3.5 Turbo\u3001Llama3-70B Instruct\u3001Mixtral-8x22B Instruct \u548c Mixtral-8x7B Instruct \u4f86\u8a55\u4f30 CodeGraph\u3002\u5728 GraphQA \u8cc7\u6599\u96c6\u4e2d\u4f7f\u7528\u516d\u7a2e\u5716\u5f62\u7de8\u78bc\u65b9\u6cd5\u5c0d\u516d\u9805\u4efb\u52d9\u9032\u884c\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cCodeGraph \u53ef\u4ee5\u5c07 LLM \u4e2d\u7684\u5716\u5f62\u63a8\u7406\u4efb\u52d9\u7684\u6548\u80fd\u63d0\u5347 1.3% \u5230 58.6%\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u4efb\u52d9\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cCodeGraph \u5728\u5716\u5f62\u4efb\u52d9\u4e2d\u7684\u7b97\u8853\u554f\u984c\u4e0a\u8868\u73fe\u51fa\u5f37\u52c1\u7684\u6548\u80fd\uff0c\u4e26\u70ba\u63a8\u7406\u904e\u7a0b\u63d0\u4f9b\u66f4\u5177\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91cb\u6027\u7684\u65b9\u6cd5\u3002", "author": "Qiaolong Cai et.al.", "authors": "Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song", "id": "2408.13863v1", "paper_url": "http://arxiv.org/abs/2408.13863v1", "repo": "null"}}