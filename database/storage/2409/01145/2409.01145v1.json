{"2409.01145": {"publish_time": "2024-09-02", "title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning", "paper_summary": "Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised\ngraph learning that has attracted attention across various application\nscenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet\nto be explored. Because conventional augmentation techniques like feature\nembedding masking cannot directly process textual attributes on TAGs. A naive\nstrategy for applying GCL to TAGs is to encode the textual attributes into\nfeature embeddings via a language model and then feed the embeddings into the\nfollowing GCL module for processing. Such a strategy faces three key\nchallenges: I) failure to avoid information loss, II) semantic loss during the\ntext encoding phase, and III) implicit augmentation constraints that lead to\nuncontrollable and incomprehensible results. In this paper, we propose a novel\nGCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to\nproduce textual augmentations and LLMs' powerful natural language processing\n(NLP) abilities to address the three limitations aforementioned to pave the way\nfor applying GCL to TAG tasks. Extensive experiments on four high-quality TAG\ndatasets illustrate the superiority of the proposed LATEX-GCL method. The\nsource codes and datasets are released to ease the reproducibility, which can\nbe accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.", "paper_summary_zh": "\u5716\u5f62\u5c0d\u6bd4\u5b78\u7fd2 (GCL) \u662f\u81ea\u76e3\u7763\u5716\u5f62\u5b78\u7fd2\u7684\u5f37\u5927\u7bc4\u4f8b\uff0c\u5df2\u5728\u5404\u7a2e\u61c9\u7528\u5834\u666f\u4e2d\u5f15\u8d77\u95dc\u6ce8\u3002\u7136\u800c\uff0cGCL \u5c0d\u65bc\u5728\u6587\u672c\u8a3b\u89e3\u5716\u5f62 (TAG) \u4e0a\u5b78\u7fd2\u5c1a\u672a\u88ab\u63a2\u8a0e\u3002\u56e0\u70ba\u7279\u5fb5\u5d4c\u5165\u906e\u7f69\u7b49\u50b3\u7d71\u64f4\u5145\u6280\u8853\u7121\u6cd5\u76f4\u63a5\u8655\u7406 TAG \u4e0a\u7684\u6587\u672c\u5c6c\u6027\u3002\u5c07 GCL \u61c9\u7528\u65bc TAG \u7684\u4e00\u7a2e\u5929\u771f\u7b56\u7565\u662f\u901a\u904e\u8a9e\u8a00\u6a21\u578b\u5c07\u6587\u672c\u5c6c\u6027\u7de8\u78bc\u5230\u7279\u5fb5\u5d4c\u5165\u4e2d\uff0c\u7136\u5f8c\u5c07\u5d4c\u5165\u8f38\u5165\u5f8c\u7e8c\u7684 GCL \u6a21\u7d44\u9032\u884c\u8655\u7406\u3002\u9019\u7a2e\u7b56\u7565\u9762\u81e8\u4e09\u500b\u95dc\u9375\u6311\u6230\uff1aI) \u7121\u6cd5\u907f\u514d\u8cc7\u8a0a\u907a\u5931\uff0cII) \u5728\u6587\u672c\u7de8\u78bc\u968e\u6bb5\u767c\u751f\u8a9e\u7fa9\u907a\u5931\uff0c\u4ee5\u53ca III) \u5c0e\u81f4\u7121\u6cd5\u63a7\u5236\u4e14\u96e3\u4ee5\u7406\u89e3\u7d50\u679c\u7684\u96b1\u5f0f\u64f4\u5145\u7d04\u675f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u540d\u70ba LATEX-GCL \u7684\u65b0\u7a4e GCL \u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u7522\u751f\u6587\u672c\u64f4\u5145\uff0c\u4ee5\u53ca LLM \u5f37\u5927\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u80fd\u529b\u4f86\u89e3\u6c7a\u4e0a\u8ff0\u4e09\u500b\u9650\u5236\uff0c\u70ba\u5c07 GCL \u61c9\u7528\u65bc TAG \u4efb\u52d9\u92ea\u5e73\u9053\u8def\u3002\u5728\u56db\u500b\u9ad8\u54c1\u8cea TAG \u8cc7\u6599\u96c6\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8aaa\u660e\u4e86\u6240\u63d0\u51fa\u7684 LATEX-GCL \u65b9\u6cd5\u7684\u512a\u8d8a\u6027\u3002\u539f\u59cb\u78bc\u548c\u8cc7\u6599\u96c6\u5df2\u767c\u5e03\u4ee5\u7c21\u5316\u53ef\u91cd\u88fd\u6027\uff0c\u53ef\u900f\u904e\u6b64\u9023\u7d50\u5b58\u53d6\uff1ahttps://anonymous.4open.science/r/LATEX-GCL-0712\u3002", "author": "Haoran Yang et.al.", "authors": "Haoran Yang, Xiangyu Zhao, Sirui Huang, Qing Li, Guandong Xu", "id": "2409.01145v1", "paper_url": "http://arxiv.org/abs/2409.01145v1", "repo": "null"}}