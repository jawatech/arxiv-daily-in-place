{"2412.19289": {"publish_time": "2024-12-26", "title": "ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image Captioning", "paper_summary": "Recent lightweight image captioning models using retrieved data mainly focus\non text prompts. However, previous works only utilize the retrieved text as\ntext prompts, and the visual information relies only on the CLIP visual\nembedding. Because of this issue, there is a limitation that the image\ndescriptions inherent in the prompt are not sufficiently reflected in the\nvisual embedding space. To tackle this issue, we propose ViPCap, a novel\nretrieval text-based visual prompt for lightweight image captioning. ViPCap\nleverages the retrieved text with image information as visual prompts to\nenhance the ability of the model to capture relevant visual information. By\nmapping text prompts into the CLIP space and generating multiple randomized\nGaussian distributions, our method leverages sampling to explore randomly\naugmented distributions and effectively retrieves the semantic features that\ncontain image information. These retrieved features are integrated into the\nimage and designated as the visual prompt, leading to performance improvements\non the datasets such as COCO, Flickr30k, and NoCaps. Experimental results\ndemonstrate that ViPCap significantly outperforms prior lightweight captioning\nmodels in efficiency and effectiveness, demonstrating the potential for a\nplug-and-play solution.", "paper_summary_zh": "\u8fd1\u671f\u4f7f\u7528\u64f7\u53d6\u8cc7\u6599\u7684\u8f15\u91cf\u7d1a\u5f71\u50cf\u6a19\u984c\u6a21\u578b\uff0c\u4e3b\u8981\u8457\u91cd\u65bc\u6587\u5b57\u63d0\u793a\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u4f5c\u54c1\u50c5\u5c07\u64f7\u53d6\u7684\u6587\u5b57\u4f5c\u70ba\u6587\u5b57\u63d0\u793a\u4f7f\u7528\uff0c\u800c\u8996\u89ba\u8cc7\u8a0a\u50c5\u4f9d\u8cf4\u65bc CLIP \u8996\u89ba\u5d4c\u5165\u3002\u7531\u65bc\u9019\u500b\u554f\u984c\uff0c\u5b58\u5728\u4e00\u500b\u9650\u5236\uff0c\u5373\u63d0\u793a\u4e2d\u56fa\u6709\u7684\u5f71\u50cf\u63cf\u8ff0\u4e26\u672a\u5145\u5206\u53cd\u6620\u5728\u8996\u89ba\u5d4c\u5165\u7a7a\u9593\u4e2d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa ViPCap\uff0c\u9019\u662f\u4e00\u7a2e\u7528\u65bc\u8f15\u91cf\u7d1a\u5f71\u50cf\u6a19\u984c\u7684\u65b0\u7a4e\u64f7\u53d6\u6587\u5b57\u70ba\u57fa\u790e\u7684\u8996\u89ba\u63d0\u793a\u3002ViPCap \u5229\u7528\u64f7\u53d6\u7684\u6587\u5b57\u548c\u5f71\u50cf\u8cc7\u8a0a\u4f5c\u70ba\u8996\u89ba\u63d0\u793a\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u64f7\u53d6\u76f8\u95dc\u8996\u89ba\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u900f\u904e\u5c07\u6587\u5b57\u63d0\u793a\u5c0d\u61c9\u5230 CLIP \u7a7a\u9593\u4e26\u7522\u751f\u591a\u500b\u96a8\u6a5f\u7684\u9ad8\u65af\u5206\u4f48\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u53d6\u6a23\u4f86\u63a2\u7d22\u96a8\u6a5f\u64f4\u5145\u7684\u5206\u4f48\uff0c\u4e26\u6709\u6548\u64f7\u53d6\u5305\u542b\u5f71\u50cf\u8cc7\u8a0a\u7684\u8a9e\u610f\u7279\u5fb5\u3002\u9019\u4e9b\u64f7\u53d6\u7684\u7279\u5fb5\u6703\u6574\u5408\u5230\u5f71\u50cf\u4e2d\uff0c\u4e26\u6307\u5b9a\u70ba\u8996\u89ba\u63d0\u793a\uff0c\u9032\u800c\u63d0\u5347\u5728 COCO\u3001Flickr30k \u548c NoCaps \u7b49\u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cViPCap \u5728\u6548\u7387\u548c\u6548\u80fd\u4e0a\u90fd\u660e\u986f\u512a\u65bc\u5148\u524d\u7684\u8f15\u91cf\u7d1a\u6a19\u984c\u6a21\u578b\uff0c\u8b49\u660e\u4e86\u5373\u63d2\u5373\u7528\u89e3\u6c7a\u65b9\u6848\u7684\u6f5b\u529b\u3002", "author": "Taewhan Kim et.al.", "authors": "Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim", "id": "2412.19289v1", "paper_url": "http://arxiv.org/abs/2412.19289v1", "repo": "https://github.com/taewhankim/vipcap"}}