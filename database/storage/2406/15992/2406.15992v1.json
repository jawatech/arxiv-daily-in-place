{"2406.15992": {"publish_time": "2024-06-23", "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "paper_summary": "Large language models (LLMs) demonstrate great potential for problems with\nimplicit graphical structures, while recent works seek to enhance the graph\nreasoning capabilities of LLMs through specialized instruction tuning. The\nresulting 'graph LLMs' are evaluated with in-distribution settings only, thus\nit remains underexplored whether LLMs are learning generalizable graph\nreasoning skills or merely memorizing patterns in the synthetic training data.\nTo this end, we propose the NLGift benchmark, an evaluation suite of LLM graph\nreasoning generalization: whether LLMs could go beyond semantic, numeric,\nstructural, reasoning patterns in the synthetic training data and improve\nutility on real-world graph-based tasks. Extensive experiments with two LLMs\nacross four graph reasoning tasks demonstrate that while generalization on\nsimple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to\ngeneralize across reasoning and real-world patterns, casting doubt on the\nbenefit of synthetic graph tuning for real-world tasks with underlying network\nstructures. We explore three strategies to improve LLM graph reasoning\ngeneralization, and we find that while post-training alignment is most\npromising for real-world tasks, empowering LLM graph reasoning to go beyond\npattern memorization remains an open research question.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u65bc\u5177\u6709\u96b1\u5f0f\u5716\u5f62\u7d50\u69cb\u7684\u554f\u984c\u5c55\u73fe\u51fa\u5de8\u5927\u7684\u6f5b\u529b\uff0c\u800c\u8fd1\u671f\u7814\u7a76\u5247\u900f\u904e\u5c08\u696d\u6307\u4ee4\u8abf\u6574\u4f86\u589e\u5f37 LLM \u7684\u5716\u5f62\u63a8\u7406\u80fd\u529b\u3002\u7531\u6b64\u7522\u751f\u7684\u300c\u5716\u5f62 LLM\u300d\u50c5\u5728\u5206\u5e03\u5167\u8a2d\u5b9a\u4e2d\u9032\u884c\u8a55\u4f30\uff0c\u56e0\u6b64 LLM \u662f\u5426\u5b78\u7fd2\u5230\u53ef\u6982\u62ec\u7684\u5716\u5f62\u63a8\u7406\u6280\u80fd\uff0c\u6216\u50c5\u50c5\u8a18\u61b6\u5408\u6210\u8a13\u7df4\u8cc7\u6599\u4e2d\u7684\u6a21\u5f0f\uff0c\u4ecd\u672a\u7372\u5f97\u5145\u5206\u63a2\u8a0e\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa NLGift \u57fa\u6e96\uff0c\u9019\u662f\u4e00\u500b LLM \u5716\u5f62\u63a8\u7406\u6982\u62ec\u8a55\u4f30\u5957\u4ef6\uff1aLLM \u662f\u5426\u53ef\u4ee5\u8d85\u8d8a\u5408\u6210\u8a13\u7df4\u8cc7\u6599\u4e2d\u7684\u8a9e\u7fa9\u3001\u6578\u503c\u3001\u7d50\u69cb\u63a8\u7406\u6a21\u5f0f\uff0c\u4e26\u63d0\u5347\u5728\u771f\u5be6\u4e16\u754c\u57fa\u65bc\u5716\u5f62\u7684\u4efb\u52d9\u4e2d\u7684\u6548\u7528\u3002\u900f\u904e\u5169\u500b LLM \u5728\u56db\u500b\u5716\u5f62\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u5118\u7ba1\u5728\u7c21\u55ae\u6a21\u5f0f\uff08\u8a9e\u7fa9\u3001\u6578\u503c\uff09\u4e0a\u7684\u6982\u62ec\u4ee4\u4eba\u6eff\u610f\uff0c\u4f46 LLM \u96e3\u4ee5\u5728\u63a8\u7406\u548c\u771f\u5be6\u4e16\u754c\u6a21\u5f0f\u4e2d\u6982\u62ec\uff0c\u5c0d\u5408\u6210\u5716\u5f62\u8abf\u6574\u5c0d\u65bc\u5177\u6709\u57fa\u790e\u7db2\u8def\u7d50\u69cb\u7684\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u7684\u76ca\u8655\u63d0\u51fa\u8cea\u7591\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u4e09\u7a2e\u7b56\u7565\u4f86\u6539\u5584 LLM \u5716\u5f62\u63a8\u7406\u6982\u62ec\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5118\u7ba1\u8a13\u7df4\u5f8c\u5c0d\u9f4a\u5c0d\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u6700\u6709\u5e0c\u671b\uff0c\u4f46\u8ce6\u80fd LLM \u5716\u5f62\u63a8\u7406\u4ee5\u8d85\u8d8a\u6a21\u5f0f\u8a18\u61b6\u4ecd\u7136\u662f\u4e00\u500b\u958b\u653e\u7684\u7814\u7a76\u554f\u984c\u3002", "author": "Yizhuo Zhang et.al.", "authors": "Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov", "id": "2406.15992v1", "paper_url": "http://arxiv.org/abs/2406.15992v1", "repo": "null"}}