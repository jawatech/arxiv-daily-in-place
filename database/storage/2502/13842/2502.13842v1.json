{"2502.13842": {"publish_time": "2025-02-19", "title": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking", "paper_summary": "Large language models (LLMs) face inherent performance bottlenecks under\nparameter constraints, particularly in processing critical tokens that demand\ncomplex reasoning. Empirical analysis reveals challenging tokens induce abrupt\ngradient spikes across layers, exposing architectural stress points in standard\nTransformers. Building on this insight, we propose Inner Thinking Transformer\n(ITT), which reimagines layer computations as implicit thinking steps. ITT\ndynamically allocates computation through Adaptive Token Routing, iteratively\nrefines representations via Residual Thinking Connections, and distinguishes\nreasoning phases using Thinking Step Encoding. ITT enables deeper processing of\ncritical tokens without parameter expansion. Evaluations across 162M-466M\nparameter models show ITT achieves 96.5\\% performance of a 466M Transformer\nusing only 162M parameters, reduces training data by 43.2\\%, and outperforms\nTransformer/Loop variants in 11 benchmarks. By enabling elastic computation\nallocation during inference, ITT balances performance and efficiency through\narchitecture-aware optimization of implicit thinking pathways.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u53c2\u6570\u9650\u5236\u4e0b\u4f1a\u9762\u4e34\u56fa\u6709\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u5173\u952e\u6807\u8bb0\u65f6\u3002\u7ecf\u9a8c\u5206\u6790\u8868\u660e\uff0c\u5177\u6709\u6311\u6218\u6027\u7684\u6807\u8bb0\u4f1a\u5f15\u8d77\u8de8\u5c42\u68af\u5ea6\u6025\u5267\u4e0a\u5347\uff0c\u66b4\u9732\u4e86\u6807\u51c6 Transformer \u4e2d\u7684\u67b6\u6784\u538b\u529b\u70b9\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Inner Thinking Transformer (ITT)\uff0c\u5b83\u5c06\u5c42\u8ba1\u7b97\u91cd\u65b0\u6784\u60f3\u4e3a\u9690\u5f0f\u601d\u8003\u6b65\u9aa4\u3002ITT \u901a\u8fc7\u81ea\u9002\u5e94\u6807\u8bb0\u8def\u7531\u52a8\u6001\u5206\u914d\u8ba1\u7b97\uff0c\u901a\u8fc7\u6b8b\u5dee\u601d\u8003\u8fde\u63a5\u8fed\u4ee3\u4f18\u5316\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u601d\u8003\u6b65\u9aa4\u7f16\u7801\u533a\u5206\u63a8\u7406\u9636\u6bb5\u3002ITT \u53ef\u4ee5\u5728\u4e0d\u6269\u5c55\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5bf9\u5173\u952e\u6807\u8bb0\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u5904\u7406\u3002\u5bf9 162M-466M \u53c2\u6570\u6a21\u578b\u7684\u8bc4\u4f30\u8868\u660e\uff0cITT \u4f7f\u7528\u4ec5 162M \u53c2\u6570\u5c31\u5b9e\u73b0\u4e86 466M Transformer \u7684 96.5% \u6027\u80fd\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u4e86 43.2%\uff0c\u5e76\u5728 11 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e Transformer/Loop \u53d8\u4f53\u3002\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u542f\u7528\u5f39\u6027\u8ba1\u7b97\u5206\u914d\uff0cITT \u901a\u8fc7\u9690\u5f0f\u601d\u8003\u8def\u5f84\u7684\u67b6\u6784\u611f\u77e5\u4f18\u5316\u6765\u5e73\u8861\u6027\u80fd\u548c\u6548\u7387\u3002", "author": "Yilong Chen et.al.", "authors": "Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang", "id": "2502.13842v1", "paper_url": "http://arxiv.org/abs/2502.13842v1", "repo": "null"}}