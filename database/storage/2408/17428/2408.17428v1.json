{"2408.17428": {"publish_time": "2024-08-30", "title": "CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models", "paper_summary": "The digitisation of historical print media archives is crucial for increasing\naccessibility to contemporary records. However, the process of Optical\nCharacter Recognition (OCR) used to convert physical records to digital text is\nprone to errors, particularly in the case of newspapers and periodicals due to\ntheir complex layouts. This paper introduces Context Leveraging OCR Correction\n(CLOCR-C), which utilises the infilling and context-adaptive abilities of\ntransformer-based language models (LMs) to improve OCR quality. The study aims\nto determine if LMs can perform post-OCR correction, improve downstream NLP\ntasks, and the value of providing the socio-cultural context as part of the\ncorrection process. Experiments were conducted using seven LMs on three\ndatasets: the 19th Century Serials Edition (NCSE) and two datasets from the\nOverproof collection. The results demonstrate that some LMs can significantly\nreduce error rates, with the top-performing model achieving over a 60%\nreduction in character error rate on the NCSE dataset. The OCR improvements\nextend to downstream tasks, such as Named Entity Recognition, with increased\nCosine Named Entity Similarity. Furthermore, the study shows that providing\nsocio-cultural context in the prompts improves performance, while misleading\nprompts lower performance. In addition to the findings, this study releases a\ndataset of 91 transcribed articles from the NCSE, containing a total of 40\nthousand words, to support further research in this area. The findings suggest\nthat CLOCR-C is a promising approach for enhancing the quality of existing\ndigital archives by leveraging the socio-cultural information embedded in the\nLMs and the text requiring correction.", "paper_summary_zh": "<paragraph>\u6b77\u53f2\u5370\u5237\u5a92\u9ad4\u6a94\u6848\u7684\u6578\u4f4d\u5316\u5c0d\u65bc\u589e\u52a0\u5c0d\u7576\u4ee3\u8a18\u9304\u7684\u53ef\u5b58\u53d6\u6027\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u7528\u65bc\u5c07\u5be6\u9ad4\u8a18\u9304\u8f49\u63db\u70ba\u6578\u4f4d\u6587\u5b57\u7684\u5149\u5b78\u5b57\u5143\u8fa8\u8b58 (OCR) \u8655\u7406\u5bb9\u6613\u51fa\u932f\uff0c\u7279\u5225\u662f\u5831\u7d19\u548c\u671f\u520a\u7531\u65bc\u5176\u8907\u96dc\u7684\u7248\u9762\u8a2d\u8a08\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u60c5\u5883\u69d3\u687f OCR \u6821\u6b63 (CLOCR-C)\uff0c\u5b83\u5229\u7528\u57fa\u65bc\u8f49\u63db\u5668\u7684\u8a9e\u8a00\u6a21\u578b (LM) \u7684\u586b\u88dc\u548c\u60c5\u5883\u9069\u61c9\u80fd\u529b\u4f86\u63d0\u5347 OCR \u54c1\u8cea\u3002\u672c\u7814\u7a76\u65e8\u5728\u78ba\u5b9a LM \u662f\u5426\u53ef\u4ee5\u57f7\u884c OCR \u5f8c\u6821\u6b63\u3001\u6539\u5584\u4e0b\u6e38 NLP \u4efb\u52d9\uff0c\u4ee5\u53ca\u5728\u6821\u6b63\u904e\u7a0b\u4e2d\u63d0\u4f9b\u793e\u6703\u6587\u5316\u60c5\u5883\u7684\u50f9\u503c\u3002\u4f7f\u7528\u4e03\u500b LM \u5c0d\u4e09\u500b\u8cc7\u6599\u96c6\u9032\u884c\u4e86\u5be6\u9a57\uff1a19 \u4e16\u7d00\u9023\u7e8c\u51fa\u7248\u54c1\u7248\u672c (NCSE) \u548c Overproof \u8490\u85cf\u4e2d\u7684\u5169\u500b\u8cc7\u6599\u96c6\u3002\u7d50\u679c\u8868\u660e\uff0c\u67d0\u4e9b LM \u53ef\u4ee5\u986f\u8457\u964d\u4f4e\u932f\u8aa4\u7387\uff0c\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b\u5728 NCSE \u8cc7\u6599\u96c6\u4e0a\u7684\u5b57\u5143\u932f\u8aa4\u7387\u964d\u4f4e\u4e86 60% \u4ee5\u4e0a\u3002OCR \u6539\u9032\u5ef6\u4f38\u5230\u4e0b\u6e38\u4efb\u52d9\uff0c\u4f8b\u5982\u547d\u540d\u5be6\u9ad4\u8fa8\u8b58\uff0c\u4e26\u63d0\u9ad8\u4e86\u9918\u5f26\u547d\u540d\u5be6\u9ad4\u76f8\u4f3c\u5ea6\u3002\u6b64\u5916\uff0c\u672c\u7814\u7a76\u8868\u660e\u5728\u63d0\u793a\u4e2d\u63d0\u4f9b\u793e\u6703\u6587\u5316\u60c5\u5883\u6703\u63d0\u5347\u6548\u80fd\uff0c\u800c\u8aa4\u5c0e\u6027\u63d0\u793a\u5247\u6703\u964d\u4f4e\u6548\u80fd\u3002\u9664\u4e86\u9019\u4e9b\u767c\u73fe\u5916\uff0c\u672c\u7814\u7a76\u9084\u767c\u5e03\u4e86\u4e00\u500b\u5305\u542b 91 \u7bc7\u4f86\u81ea NCSE \u7684\u8f49\u9304\u6587\u7ae0\u7684\u8cc7\u6599\u96c6\uff0c\u7e3d\u5171\u5305\u542b 40,000 \u500b\u5b57\uff0c\u4ee5\u652f\u6301\u6b64\u9818\u57df\u7684\u9032\u4e00\u6b65\u7814\u7a76\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0cCLOCR-C \u662f\u4e00\u9805\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u900f\u904e\u5229\u7528 LM \u4e2d\u5d4c\u5165\u7684\u793e\u6703\u6587\u5316\u8cc7\u8a0a\u548c\u9700\u8981\u6821\u6b63\u7684\u6587\u5b57\u4f86\u63d0\u5347\u73fe\u6709\u6578\u4f4d\u6a94\u6848\u7684\u54c1\u8cea\u3002</paragraph>", "author": "Jonathan Bourne et.al.", "authors": "Jonathan Bourne", "id": "2408.17428v1", "paper_url": "http://arxiv.org/abs/2408.17428v1", "repo": "null"}}