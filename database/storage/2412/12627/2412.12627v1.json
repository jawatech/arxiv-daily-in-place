{"2412.12627": {"publish_time": "2024-12-17", "title": "Make Imagination Clearer! Stable Diffusion-based Visual Imagination for Multimodal Machine Translation", "paper_summary": "Visual information has been introduced for enhancing machine translation\n(MT), and its effectiveness heavily relies on the availability of large amounts\nof bilingual parallel sentence pairs with manual image annotations. In this\npaper, we introduce a stable diffusion-based imagination network into a\nmultimodal large language model (MLLM) to explicitly generate an image for each\nsource sentence, thereby advancing the multimodel MT. Particularly, we build\nheuristic human feedback with reinforcement learning to ensure the consistency\nof the generated image with the source sentence without the supervision of\nimage annotation, which breaks the bottleneck of using visual information in\nMT. Furthermore, the proposed method enables imaginative visual information to\nbe integrated into large-scale text-only MT in addition to multimodal MT.\nExperimental results show that our model significantly outperforms existing\nmultimodal MT and text-only MT, especially achieving an average improvement of\nmore than 14 BLEU points on Multi30K multimodal MT benchmarks.", "paper_summary_zh": "\u8996\u89ba\u8cc7\u8a0a\u5df2\u5c0e\u5165\u4ee5\u589e\u5f37\u6a5f\u5668\u7ffb\u8b6f (MT)\uff0c\u5176\u6709\u6548\u6027\u6975\u5ea6\u4ef0\u8cf4\u5927\u91cf\u5177\u624b\u52d5\u5f71\u50cf\u8a3b\u89e3\u7684\u96d9\u8a9e\u5e73\u884c\u53e5\u5b50\u5c0d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u4e00\u500b\u7a69\u5b9a\u7684\u57fa\u65bc\u64f4\u6563\u7684\u60f3\u50cf\u7db2\u8def\u5c0e\u5165\u4e00\u500b\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4e2d\uff0c\u4ee5\u660e\u78ba\u5730\u70ba\u6bcf\u500b\u539f\u59cb\u53e5\u5b50\u7522\u751f\u4e00\u500b\u5f71\u50cf\uff0c\u5f9e\u800c\u63a8\u9032\u591a\u6a21\u614b MT\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u5efa\u69cb\u4e00\u500b\u5177\u5099\u5f37\u5316\u5b78\u7fd2\u7684\u555f\u767c\u5f0f\u4eba\u985e\u56de\u994b\uff0c\u4ee5\u78ba\u4fdd\u7522\u751f\u7684\u5f71\u50cf\u8207\u539f\u59cb\u53e5\u5b50\u7684\u4e00\u81f4\u6027\uff0c\u7121\u9700\u5f71\u50cf\u8a3b\u89e3\u7684\u76e3\u7763\uff0c\u9019\u6253\u7834\u4e86\u5728 MT \u4e2d\u4f7f\u7528\u8996\u89ba\u8cc7\u8a0a\u7684\u74f6\u9838\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8b93\u5bcc\u60f3\u50cf\u529b\u7684\u8996\u89ba\u8cc7\u8a0a\u80fd\u5920\u6574\u5408\u5230\u5927\u578b\u7d14\u6587\u5b57 MT \u4e2d\uff0c\u9664\u4e86\u591a\u6a21\u614b MT \u4e4b\u5916\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u8457\u512a\u65bc\u73fe\u6709\u7684\u591a\u6a21\u614b MT \u548c\u7d14\u6587\u5b57 MT\uff0c\u7279\u5225\u662f\u5728 Multi30K \u591a\u6a21\u614b MT \u57fa\u6e96\u4e0a\u5e73\u5747\u63d0\u5347\u4e86 14 \u500b BLEU \u9ede\u4ee5\u4e0a\u3002", "author": "Andong Chen et.al.", "authors": "Andong Chen, Yuchen Song, Kehai Chen, Muyun Yang, Tiejun Zhao, Min Zhang", "id": "2412.12627v1", "paper_url": "http://arxiv.org/abs/2412.12627v1", "repo": "null"}}