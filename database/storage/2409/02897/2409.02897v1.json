{"2409.02897": {"publish_time": "2024-09-04", "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA", "paper_summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.", "paper_summary_zh": "\u5118\u7ba1\u76ee\u524d\u7684\u9577\u8a9e\u5883\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6839\u64da\u5ee3\u6cdb\u6587\u672c\u56de\u7b54\u4f7f\u7528\u8005\u554f\u984c\u65b9\u9762\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u5176\u56de\u61c9\u4e2d\u7f3a\u4e4f\u5f15\u6587\uff0c\u4f7f\u5f97\u4f7f\u7528\u8005\u96e3\u4ee5\u9a57\u8b49\uff0c\u9032\u800c\u5f15\u767c\u5c0d\u5176\u53ef\u4fe1\u5ea6\u7684\u7591\u616e\uff0c\u56e0\u70ba\u5b83\u5011\u53ef\u80fd\u6703\u51fa\u73fe\u5e7b\u89ba\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u8b93\u9577\u8a9e\u5883 LLM \u80fd\u5920\u7522\u751f\u5177\u6709\u7d30\u7dfb\u53e5\u5b50\u5c64\u7d1a\u5f15\u6587\u7684\u56de\u61c9\uff0c\u9032\u800c\u63d0\u5347\u5176\u5fe0\u5be6\u5ea6\u548c\u53ef\u9a57\u8b49\u6027\u3002\u6211\u5011\u9996\u5148\u4ecb\u7d39 LongBench-Cite\uff0c\u9019\u662f\u4e00\u500b\u81ea\u52d5\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u76ee\u524d LLM \u5728\u5e36\u5f15\u6587\u9577\u8a9e\u5883\u554f\u7b54 (LQAC) \u4e2d\u7684\u8868\u73fe\uff0c\u4e26\u63ed\u9732\u6709\u76f8\u7576\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa CoF (\u7c97\u5230\u7d30)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u7ba1\u7dda\uff0c\u5229\u7528\u73fe\u6210\u7684 LLM \u81ea\u52d5\u7522\u751f\u5177\u6709\u7cbe\u78ba\u53e5\u5b50\u5c64\u7d1a\u5f15\u6587\u7684\u9577\u8a9e\u5883 QA \u5be6\u4f8b\uff0c\u4e26\u5229\u7528\u6b64\u7ba1\u7dda\u5efa\u69cb LongCite-45k\uff0c\u9019\u662f\u4e00\u500b\u7528\u65bc LQAC \u7684\u5927\u578b SFT \u8cc7\u6599\u96c6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4f7f\u7528 LongCite-45k \u8cc7\u6599\u96c6\u8a13\u7df4 LongCite-8B \u548c LongCite-9B\uff0c\u6210\u529f\u8b93\u5b83\u5011\u80fd\u5920\u5728\u55ae\u4e00\u8f38\u51fa\u4e2d\u7522\u751f\u7cbe\u6e96\u7684\u56de\u61c9\u548c\u7d30\u7dfb\u7684\u53e5\u5b50\u5c64\u7d1a\u5f15\u6587\u3002\u5728 LongBench-Cite \u4e0a\u7684\u8a55\u4f30\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u8a13\u7df4\u7684\u6a21\u578b\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u5f15\u6587\u54c1\u8cea\uff0c\u8d85\u8d8a\u4e86\u5305\u62ec GPT-4o \u5728\u5167\u7684\u9032\u968e\u5c08\u6709\u6a21\u578b\u3002", "author": "jiajie Zhang et.al.", "authors": "jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li", "id": "2409.02897v1", "paper_url": "http://arxiv.org/abs/2409.02897v1", "repo": "null"}}