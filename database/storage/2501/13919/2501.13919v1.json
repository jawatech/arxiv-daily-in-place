{"2501.13919": {"publish_time": "2025-01-23", "title": "Temporal Preference Optimization for Long-Form Video Understanding", "paper_summary": "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.", "paper_summary_zh": "\u5118\u7ba1\u5728\u8996\u8a0a\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\uff08video-LMMs\uff09\u4e2d\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u5728\u9577\u7bc7\u5f71\u7247\u4e2d\u5be6\u73fe\u6709\u6548\u7684\u6642\u9593\u57fa\u790e\u4ecd\u662f\u73fe\u6709\u6a21\u578b\u7684\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u6642\u9593\u504f\u597d\u6700\u4f73\u5316\uff08TPO\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u5f8c\u8a13\u7df4\u67b6\u69cb\uff0c\u65e8\u5728\u900f\u904e\u504f\u597d\u5b78\u7fd2\u589e\u5f37 video-LMMs \u7684\u6642\u9593\u57fa\u790e\u80fd\u529b\u3002TPO \u63a1\u7528\u81ea\u8a13\u7df4\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u900f\u904e\u5229\u7528\u5169\u500b\u7c92\u5ea6\u5c64\u7d1a\u7684\u7cbe\u9078\u504f\u597d\u8cc7\u6599\u96c6\u4f86\u5340\u5206\u57fa\u790e\u826f\u597d\u7684\u6642\u9593\u56de\u61c9\u8207\u8f03\u4e0d\u6e96\u78ba\u7684\u6642\u9593\u56de\u61c9\uff1a\u5c40\u90e8\u6642\u9593\u57fa\u790e\uff0c\u5c08\u6ce8\u65bc\u7279\u5b9a\u5f71\u7247\u7247\u6bb5\uff0c\u4ee5\u53ca\u5168\u9762\u6642\u9593\u57fa\u790e\uff0c\u64f7\u53d6\u6574\u500b\u5f71\u7247\u5e8f\u5217\u4e2d\u5ef6\u4f38\u7684\u6642\u9593\u4f9d\u8cf4\u6027\u3002\u900f\u904e\u6700\u4f73\u5316\u9019\u4e9b\u504f\u597d\u8cc7\u6599\u96c6\uff0cTPO \u5927\u5e45\u589e\u5f37\u6642\u9593\u7406\u89e3\uff0c\u540c\u6642\u6e1b\u5c11\u5c0d\u624b\u52d5\u8a3b\u89e3\u8cc7\u6599\u7684\u4f9d\u8cf4\u3002\u5728\u4e09\u500b\u9577\u7bc7\u5f71\u7247\u7406\u89e3\u57fa\u6e96\u6e2c\u8a66\uff08LongVideoBench\u3001MLVU \u548c Video-MME\uff09\u4e0a\u9032\u884c\u7684\u5927\u91cf\u5be6\u9a57\u8b49\u660e\u4e86 TPO \u5728\u5169\u500b\u6700\u5148\u9032\u7684 video-LMMs \u4e2d\u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLLaVA-Video-TPO \u5728 Video-MME \u57fa\u6e96\u6e2c\u8a66\u4e2d\u78ba\u7acb\u4e86\u81ea\u5df1\u4f5c\u70ba\u9818\u5148\u7684 7B \u6a21\u578b\uff0c\u7a81\u986f\u4e86 TPO \u4f5c\u70ba\u53ef\u64f4\u5145\u4e14\u6709\u6548\u89e3\u6c7a\u65b9\u6848\u7684\u6f5b\u529b\uff0c\u53ef\u4fc3\u9032\u9577\u7bc7\u5f71\u7247\u7406\u89e3\u4e2d\u7684\u6642\u9593\u63a8\u7406\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://ruili33.github.io/tpo_website\u3002", "author": "Rui Li et.al.", "authors": "Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy", "id": "2501.13919v1", "paper_url": "http://arxiv.org/abs/2501.13919v1", "repo": "null"}}