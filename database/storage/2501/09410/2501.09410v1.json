{"2501.09410": {"publish_time": "2025-01-16", "title": "MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models", "paper_summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u5229\u7528\u908a\u7de3 LLM \u7684\u7570\u8cea\u80fd\u529b\u5c0d\u65bc\u4e0d\u540c\u7684\u65b0\u8208\u61c9\u7528\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u5b83\u80fd\u63d0\u9ad8\u6210\u672c\u6548\u76ca\u4e26\u964d\u4f4e\u5ef6\u9072\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 \\textit{\u6df7\u5408\u908a\u7de3\u5c08\u5bb6 (MoE$^2$)}\uff0c\u9019\u662f\u4e00\u7a2e\u908a\u7de3 LLM \u7684\u65b0\u578b\u5354\u4f5c\u63a8\u8ad6\u6846\u67b6\u3002\u6211\u5011\u5236\u5b9a\u4e86\u806f\u5408\u9598\u63a7\u548c\u5c08\u5bb6\u9078\u64c7\u554f\u984c\uff0c\u4ee5\u5728\u80fd\u6e90\u548c\u5ef6\u9072\u9650\u5236\u4e0b\u6700\u4f73\u5316\u63a8\u8ad6\u6548\u80fd\u3002\u8207\u50b3\u7d71\u7684 MoE \u554f\u984c\u4e0d\u540c\uff0cLLM \u5c08\u5bb6\u9078\u64c7\u7531\u65bc\u7d44\u5408\u6027\u8cea\u548c\u908a\u7de3 LLM \u5728\u5404\u7a2e\u5c6c\u6027\u4e0a\u7684\u7570\u8cea\u6027\u800c\u5177\u6709\u66f4\u5927\u7684\u6311\u6230\u6027\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5169\u7d1a\u5c08\u5bb6\u9078\u64c7\u6a5f\u5236\uff0c\u900f\u904e\u8a72\u6a5f\u5236\u6211\u5011\u767c\u73fe\u4e86\u9598\u63a7\u53c3\u6578\u5728\u5c08\u5bb6\u9078\u64c7\u4e2d\u5177\u6709\u4fdd\u512a\u6027\u8cea\u3002\u6b64\u6027\u8cea\u4f7f\u5f97\u8a13\u7df4\u548c\u9078\u64c7\u904e\u7a0b\u5f97\u4ee5\u5206\u89e3\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8907\u96dc\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528\u76ee\u6a19\u51fd\u6578\u7684\u55ae\u8abf\u6027\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u7a2e\u96e2\u6563\u55ae\u8abf\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\uff0c\u4ee5\u9032\u884c\u6700\u4f73\u5c08\u5bb6\u9078\u64c7\u3002\u6211\u5011\u4f7f\u7528 NVIDIA Jetson AGX Orins \u548c NVIDIA RTX 4090 GPU \u5be6\u4f5c\u908a\u7de3\u4f3a\u670d\u5668\uff0c\u4e26\u57f7\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u6211\u5011\u7684\u7d50\u679c\u9a57\u8b49\u4e86\u5404\u7a2e LLM \u6a21\u578b\u7684\u6548\u80fd\u63d0\u5347\uff0c\u4e26\u986f\u793a\u6211\u5011\u7684 MoE$^2$ \u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u5ef6\u9072\u548c\u80fd\u6e90\u9810\u7b97\u4e4b\u9593\u9054\u6210\u6700\u4f73\u6298\u8877\uff0c\u4e26\u5728\u5404\u7a2e\u7cfb\u7d71\u8cc7\u6e90\u9650\u5236\u4e0b\u512a\u65bc\u57fa\u6e96\u3002", "author": "Lyudong Jin et.al.", "authors": "Lyudong Jin, Yanning Zhang, Yanhan Li, Shurong Wang, Howard H. Yang, Jian Wu, Meng Zhang", "id": "2501.09410v1", "paper_url": "http://arxiv.org/abs/2501.09410v1", "repo": "null"}}