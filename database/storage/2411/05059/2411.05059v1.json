{"2411.05059": {"publish_time": "2024-11-07", "title": "FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?", "paper_summary": "There is great interest in fine-tuning frontier large language models (LLMs)\nto inject new information and update existing knowledge. While commercial LLM\nfine-tuning APIs from providers such as OpenAI and Google promise flexible\nadaptation for various applications, the efficacy of fine-tuning remains\nunclear. In this study, we introduce FineTuneBench, an evaluation framework and\ndataset for understanding how well commercial fine-tuning APIs can successfully\nlearn new and updated knowledge. We analyze five frontier LLMs with\ncommercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,\non their effectiveness in two settings: (1) ingesting novel information, such\nas recent news events and new people profiles, and (2) updating existing\nknowledge, such as updated medical guidelines and code frameworks. Our results\nreveal substantial shortcomings in all the models' abilities to effectively\nlearn new information through fine-tuning, with an average generalization\naccuracy of 37% across all models. When updating existing knowledge, such as\nincorporating medical guideline updates, commercial fine-tuning APIs show even\nmore limited capability (average generalization accuracy of 19%). Overall,\nfine-tuning GPT-4o mini is the most effective for infusing new knowledge and\nupdating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs\nfor Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or\nupdate existing knowledge. These findings underscore a major shortcoming in\nusing current commercial fine-tuning services to achieve reliable knowledge\ninfusion in common scenarios. We open source the FineTuneBench dataset at\nhttps://github.com/kevinwu23/StanfordFineTuneBench.", "paper_summary_zh": "<paragraph>\u5c0d\u65bc\u5fae\u8abf\u524d\u6cbf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u6ce8\u5165\u65b0\u8cc7\u8a0a\u548c\u66f4\u65b0\u73fe\u6709\u77e5\u8b58\uff0c\u5b58\u5728\u6975\u5927\u7684\u8208\u8da3\u3002\u96d6\u7136\u4f86\u81ea OpenAI \u548c Google \u7b49\u4f9b\u61c9\u5546\u7684\u5546\u7528 LLM \u5fae\u8abf API \u627f\u8afe\u9748\u6d3b\u9069\u61c9\u5404\u7a2e\u61c9\u7528\uff0c\u4f46\u5fae\u8abf\u7684\u6548\u80fd\u4ecd\u4e0d\u660e\u78ba\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 FineTuneBench\uff0c\u9019\u662f\u4e00\u500b\u8a55\u4f30\u67b6\u69cb\u548c\u8cc7\u6599\u96c6\uff0c\u7528\u65bc\u4e86\u89e3\u5546\u7528\u5fae\u8abf API \u5982\u4f55\u6210\u529f\u5b78\u7fd2\u65b0\u7684\u548c\u66f4\u65b0\u7684\u77e5\u8b58\u3002\u6211\u5011\u5206\u6790\u4e86\u4e94\u500b\u5177\u6709\u5546\u7528\u5fae\u8abf API \u7684\u524d\u6cbf LLM\uff0c\u5305\u62ec GPT-4o \u548c Gemini 1.5 Pro\uff0c\u5728\u5169\u7a2e\u8a2d\u5b9a\u4e2d\u7684\u6548\u80fd\uff1a(1) \u5438\u6536\u65b0\u8cc7\u8a0a\uff0c\u4f8b\u5982\u6700\u8fd1\u7684\u65b0\u805e\u4e8b\u4ef6\u548c\u65b0\u4eba\u7269\u7c21\u4ecb\uff0c\u4ee5\u53ca (2) \u66f4\u65b0\u73fe\u6709\u77e5\u8b58\uff0c\u4f8b\u5982\u66f4\u65b0\u7684\u91ab\u7642\u6307\u5357\u548c\u7a0b\u5f0f\u78bc\u67b6\u69cb\u3002\u6211\u5011\u7684\u7d50\u679c\u63ed\u793a\u4e86\u6240\u6709\u6a21\u578b\u5728\u900f\u904e\u5fae\u8abf\u6709\u6548\u5b78\u7fd2\u65b0\u8cc7\u8a0a\u7684\u80fd\u529b\u65b9\u9762\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u6240\u6709\u6a21\u578b\u7684\u5e73\u5747\u6982\u5316\u6e96\u78ba\u5ea6\u70ba 37%\u3002\u5728\u66f4\u65b0\u73fe\u6709\u77e5\u8b58\uff08\u4f8b\u5982\u7d0d\u5165\u91ab\u7642\u6307\u5357\u66f4\u65b0\uff09\u6642\uff0c\u5546\u7528\u5fae\u8abf API \u986f\u793a\u51fa\u66f4\u6709\u9650\u7684\u80fd\u529b\uff08\u5e73\u5747\u6982\u5316\u6e96\u78ba\u5ea6\u70ba 19%\uff09\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u5fae\u8abf GPT-4o mini \u5728\u704c\u8f38\u65b0\u77e5\u8b58\u548c\u66f4\u65b0\u77e5\u8b58\u65b9\u9762\u6700\u6709\u6548\uff0c\u5176\u6b21\u662f GPT-3.5 Turbo \u548c GPT-4o\u3002Gemini 1.5 Flesh \u548c Gemini 1.5 Pro \u7684\u5fae\u8abf API \u7121\u6cd5\u5b78\u7fd2\u65b0\u77e5\u8b58\u6216\u66f4\u65b0\u73fe\u6709\u77e5\u8b58\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86\u5728\u5e38\u898b\u5834\u666f\u4e2d\u4f7f\u7528\u76ee\u524d\u7684\u5546\u7528\u5fae\u8abf\u670d\u52d9\u4f86\u5be6\u73fe\u53ef\u9760\u77e5\u8b58\u704c\u8f38\u7684\u4e3b\u8981\u7f3a\u9ede\u3002\u6211\u5011\u5728 https://github.com/kevinwu23/StanfordFineTuneBench \u958b\u6e90\u4e86 FineTuneBench \u8cc7\u6599\u96c6\u3002</paragraph>", "author": "Eric Wu et.al.", "authors": "Eric Wu, Kevin Wu, James Zou", "id": "2411.05059v1", "paper_url": "http://arxiv.org/abs/2411.05059v1", "repo": "null"}}