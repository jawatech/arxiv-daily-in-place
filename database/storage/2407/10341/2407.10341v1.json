{"2407.10341": {"publish_time": "2024-07-14", "title": "Affordance-Guided Reinforcement Learning via Visual Prompting", "paper_summary": "Robots equipped with reinforcement learning (RL) have the potential to learn\na wide range of skills solely from a reward signal. However, obtaining a robust\nand dense reward signal for general manipulation tasks remains a challenge.\nExisting learning-based approaches require significant data, such as\ndemonstrations or examples of success and failure, to learn task-specific\nreward functions. Recently, there is also a growing adoption of large\nmulti-modal foundation models for robotics. These models can perform visual\nreasoning in physical contexts and generate coarse robot motions for various\nmanipulation tasks. Motivated by this range of capability, in this work, we\npropose and study rewards shaped by vision-language models (VLMs).\nState-of-the-art VLMs have demonstrated an impressive ability to reason about\naffordances through keypoints in zero-shot, and we leverage this to define\ndense rewards for robotic learning. On a real-world manipulation task specified\nby natural language description, we find that these rewards improve the sample\nefficiency of autonomous RL and enable successful completion of the task in 20K\nonline finetuning steps. Additionally, we demonstrate the robustness of the\napproach to reductions in the number of in-domain demonstrations used for\npretraining, reaching comparable performance in 35K online finetuning steps.", "paper_summary_zh": "\u914d\u5099\u5f37\u5316\u5b78\u7fd2 (RL) \u7684\u6a5f\u5668\u4eba\u6709\u6f5b\u529b\u50c5\u5f9e\u56de\u994b\u8a0a\u865f\u4e2d\u5b78\u7fd2\u5ee3\u6cdb\u7684\u6280\u80fd\u3002\u7136\u800c\uff0c\u70ba\u4e00\u822c\u64cd\u4f5c\u4efb\u52d9\u53d6\u5f97\u7a69\u5065\u4e14\u5bc6\u96c6\u7684\u56de\u994b\u8a0a\u865f\u4ecd\u7136\u662f\u4e00\u9805\u6311\u6230\u3002\u73fe\u6709\u7684\u57fa\u65bc\u5b78\u7fd2\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u8cc7\u6599\uff0c\u4f8b\u5982\u5c55\u793a\u6216\u6210\u529f\u8207\u5931\u6557\u7684\u7bc4\u4f8b\uff0c\u624d\u80fd\u5b78\u7fd2\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u56de\u994b\u51fd\u6578\u3002\u6700\u8fd1\uff0c\u5927\u578b\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u5728\u6a5f\u5668\u4eba\u6280\u8853\u7684\u63a1\u7528\u4e5f\u65e5\u76ca\u589e\u52a0\u3002\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u5728\u7269\u7406\u80cc\u666f\u4e2d\u57f7\u884c\u8996\u89ba\u63a8\u7406\uff0c\u4e26\u70ba\u5404\u7a2e\u64cd\u4f5c\u4efb\u52d9\u7522\u751f\u7c97\u7565\u7684\u6a5f\u5668\u4eba\u52d5\u4f5c\u3002\u53d7\u6b64\u80fd\u529b\u7bc4\u570d\u7684\u555f\u767c\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e26\u7814\u7a76\u7531\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5851\u9020\u7684\u56de\u994b\u3002\u6700\u5148\u9032\u7684 VLM \u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u900f\u904e\u96f6\u6b21\u5b78\u7fd2\u4e2d\u7684\u95dc\u9375\u9ede\u4f86\u63a8\u8ad6\u53ef\u8ca0\u64d4\u6027\uff0c\u6211\u5011\u5229\u7528\u9019\u4e00\u9ede\u4f86\u5b9a\u7fa9\u6a5f\u5668\u4eba\u5b78\u7fd2\u7684\u5bc6\u96c6\u56de\u994b\u3002\u5728\u7531\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u6307\u5b9a\u7684\u771f\u5be6\u4e16\u754c\u64cd\u4f5c\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u9019\u4e9b\u56de\u994b\u63d0\u9ad8\u4e86\u81ea\u4e3b RL \u7684\u53d6\u6a23\u6548\u7387\uff0c\u4e26\u80fd\u5728 20K \u7dda\u4e0a\u5fae\u8abf\u6b65\u9a5f\u4e2d\u6210\u529f\u5b8c\u6210\u4efb\u52d9\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u8a72\u65b9\u6cd5\u5c0d\u7528\u65bc\u9810\u8a13\u7df4\u7684\u9818\u57df\u5167\u793a\u7bc4\u6b21\u6578\u6e1b\u5c11\u7684\u7a69\u5065\u6027\uff0c\u5728 35K \u7dda\u4e0a\u5fae\u8abf\u6b65\u9a5f\u4e2d\u9054\u5230\u76f8\u7576\u7684\u6548\u80fd\u3002", "author": "Olivia Y. Lee et.al.", "authors": "Olivia Y. Lee, Annie Xie, Kuan Fang, Karl Pertsch, Chelsea Finn", "id": "2407.10341v1", "paper_url": "http://arxiv.org/abs/2407.10341v1", "repo": "null"}}