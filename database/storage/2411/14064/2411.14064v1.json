{"2411.14064": {"publish_time": "2024-11-21", "title": "Multi LoRA Meets Vision: Merging multiple adapters to create a multi task model", "paper_summary": "Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets.", "paper_summary_zh": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u65b9\u6cd5\u5e7f\u6cdb\u7528\u4e8e\u8bed\u8a00\u5927\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u751f\u6210\u6a21\u578b\u3002\u7279\u522b\u662f\u5728\u63a8\u7406\u671f\u95f4\uff0c\u53ef\u4ee5\u5229\u7528\u5176\u4e2d\u591a\u4e2a\u65b9\u6cd5\u6765\u6539\u53d8\u57fa\u7840\u6a21\u578b\u7684\u884c\u4e3a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u662f\u5426\u53ef\u4ee5\u5c06\u9488\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u8bad\u7ec3\u7684\u591a\u4e2a LoRA \u9002\u914d\u5668\u5408\u5e76\u5728\u4e00\u8d77\u5e76\u5728\u63a8\u7406\u671f\u95f4\u4f7f\u7528\uff0c\u800c\u4e0d\u4f1a\u635f\u5931\u6027\u80fd\u3002\u901a\u8fc7\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u53ea\u9700\u5408\u5e76\u4e0d\u540c\u7684 LoRA \u5373\u53ef\u521b\u5efa\u591a\u4efb\u52a1\u6a21\u578b\u3002\u5408\u5e76\u8fd9\u4e9b\u6a21\u578b\u5c06\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u989d\u5916\u7684\u91cd\u65b0\u8bad\u7ec3\u3002\u6211\u4eec\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u8bad\u7ec3\u4e86\u9002\u914d\u5668\uff0c\u5e76\u5728\u5408\u5e76\u540e\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u8fdb\u884c\u6bd4\u8f83\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u51bb\u7ed3\u4e3b\u5e72\u5e76\u5bf9\u5176\u5934\u90e8\u8fdb\u884c\u5fae\u8c03\u7684\u6a21\u578b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u7684\u5408\u5e76\u6280\u672f\uff0c\u901a\u8fc7\u5408\u5e76\u9002\u914d\u5668\u521b\u5efa\u591a\u4efb\u52a1\u6a21\u578b\u4e5f\u662f\u53ef\u884c\u7684\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u7565\u5fae\u964d\u4f4e\u6027\u80fd\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u5c06\u591a\u8fbe\u4e09\u4e2a\u9002\u914d\u5668\u5408\u5e76\u5728\u4e00\u8d77\u3002\u6839\u636e\u4efb\u52a1\u548c\u8bad\u7ec3\u9002\u914d\u5668\u7684\u6570\u636e\u7684\u76f8\u4f3c\u6027\uff0c\u5408\u5e76\u53ef\u4ee5\u4f18\u4e8e\u5934\u90e8\u5fae\u8c03\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u4f7f\u7528\u4e0d\u540c\u6570\u636e\u96c6\u8bad\u7ec3\u7684 LoRA \u4e0e\u5728\u7c7b\u4f3c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u5f80\u5f80\u8868\u73b0\u5f97\u66f4\u597d\u3002", "author": "Ege Kesim et.al.", "authors": "Ege Kesim, Selahattin Serdar Helli", "id": "2411.14064v1", "paper_url": "http://arxiv.org/abs/2411.14064v1", "repo": "null"}}