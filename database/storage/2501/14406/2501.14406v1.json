{"2501.14406": {"publish_time": "2025-01-24", "title": "Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models", "paper_summary": "Pre-trained Language Models (PLMs) have demonstrated their superiority and\nversatility in modern Natural Language Processing (NLP), effectively adapting\nto various downstream tasks through further fine-tuning. Federated\nParameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution\nto address privacy and efficiency challenges in distributed training for PLMs\non mobile devices. However, our measurements reveal two key limitations of\nFedPEFT: heterogeneous data leads to significant performance degradation, and a\nfixed parameter configuration results in communication inefficiency. To\novercome these limitations, we propose FedARA, a novel Federated Adaptive Rank\nAllocation for parameter-efficient fine-tuning of language models.\nSpecifically, FedARA employs truncated singular value decomposition (SVD)\nadaptation to enhance flexibility and expressiveness, significantly mitigating\nthe adverse effects of data heterogeneity. Subsequently, it utilizes dynamic\nrank allocation to progressively identify critical ranks, effectively improving\ncommunication efficiency. Lastly, it leverages rank-based module pruning to\nremove inactive modules, steadily reducing local training time and peak memory\nusage in each round. Extensive experiments show that FedARA consistently\noutperforms weak baselines by an average of 8.49\\% and strong baselines by\n6.95\\% across various datasets under data heterogeneity while significantly\nimproving communication efficiency by 2.40\\(\\times\\). Moreover, experiments on\nAGX Orin, Orin Nano and Raspberry Pi 5 devices demonstrate substantial\ndecreases in total training time and energy consumption by up to 48.90\\% and\n46.95\\%, respectively.", "paper_summary_zh": "<paragraph>\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u5df2\u5c55\u73fe\u5176\u5728\u73fe\u4ee3\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u7684\u512a\u8d8a\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u900f\u904e\u9032\u4e00\u6b65\u7684\u5fae\u8abf\uff0c\u6709\u6548\u5730\u9069\u61c9\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u3002\u806f\u90a6\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (FedPEFT) \u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u89e3\u6c7a\u884c\u52d5\u88dd\u7f6e\u4e0a PLM \u5206\u6563\u5f0f\u8a13\u7df4\u7684\u96b1\u79c1\u548c\u6548\u7387\u6311\u6230\u3002\u7136\u800c\uff0c\u6211\u5011\u7684\u6e2c\u91cf\u7d50\u679c\u63ed\u793a\u4e86 FedPEFT \u7684\u5169\u500b\u4e3b\u8981\u9650\u5236\uff1a\u7570\u8cea\u8cc7\u6599\u6703\u5c0e\u81f4\u6548\u80fd\u986f\u8457\u4e0b\u964d\uff0c\u800c\u56fa\u5b9a\u7684\u53c3\u6578\u7d44\u614b\u6703\u5c0e\u81f4\u901a\u8a0a\u6548\u7387\u4f4e\u4e0b\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa FedARA\uff0c\u4e00\u7a2e\u7528\u65bc\u8a9e\u8a00\u6a21\u578b\u53c3\u6578\u9ad8\u6548\u5fae\u8abf\u7684\u65b0\u578b\u806f\u90a6\u81ea\u9069\u61c9\u79e9\u5206\u914d\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cFedARA \u63a1\u7528\u622a\u65b7\u5947\u7570\u503c\u5206\u89e3 (SVD) \u9069\u61c9\u4f86\u589e\u5f37\u9748\u6d3b\u6027\u8207\u8868\u9054\u529b\uff0c\u5927\u5e45\u6e1b\u8f15\u8cc7\u6599\u7570\u8cea\u6027\u7684\u8ca0\u9762\u5f71\u97ff\u3002\u96a8\u5f8c\uff0c\u5b83\u5229\u7528\u52d5\u614b\u79e9\u5206\u914d\u4f86\u9010\u6b65\u8b58\u5225\u95dc\u9375\u79e9\uff0c\u6709\u6548\u6539\u5584\u901a\u8a0a\u6548\u7387\u3002\u6700\u5f8c\uff0c\u5b83\u5229\u7528\u57fa\u65bc\u79e9\u7684\u6a21\u7d44\u526a\u679d\u4f86\u79fb\u9664\u975e\u6d3b\u52d5\u6a21\u7d44\uff0c\u5728\u6bcf\u4e00\u8f2a\u4e2d\u7a69\u5b9a\u7684\u6e1b\u5c11\u5340\u57df\u8a13\u7df4\u6642\u9593\u548c\u5cf0\u503c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u986f\u793a\uff0cFedARA \u5728\u8cc7\u6599\u7570\u8cea\u6027\u4e0b\uff0c\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\uff0c\u5e73\u5747\u512a\u65bc\u5f31\u57fa\u7dda 8.49%\uff0c\u512a\u65bc\u5f37\u57fa\u7dda 6.95%\uff0c\u540c\u6642\u5c07\u901a\u8a0a\u6548\u7387\u986f\u8457\u63d0\u9ad8\u4e86 2.40 \u500d\u3002\u6b64\u5916\uff0c\u5728 AGX Orin\u3001Orin Nano \u548c Raspberry Pi 5 \u88dd\u7f6e\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u7e3d\u8a13\u7df4\u6642\u9593\u548c\u80fd\u6e90\u6d88\u8017\u5206\u5225\u5927\u5e45\u6e1b\u5c11\u4e86 48.90% \u548c 46.95%\u3002</paragraph>", "author": "Fei Wu et.al.", "authors": "Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang", "id": "2501.14406v1", "paper_url": "http://arxiv.org/abs/2501.14406v1", "repo": "null"}}