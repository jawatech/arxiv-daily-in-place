{"2412.05243": {"publish_time": "2024-12-06", "title": "CompCap: Improving Multimodal Large Language Models with Composite Captions", "paper_summary": "How well can Multimodal Large Language Models (MLLMs) understand composite\nimages? Composite images (CIs) are synthetic visuals created by merging\nmultiple visual elements, such as charts, posters, or screenshots, rather than\nbeing captured directly by a camera. While CIs are prevalent in real-world\napplications, recent MLLM developments have primarily focused on interpreting\nnatural images (NIs). Our research reveals that current MLLMs face significant\nchallenges in accurately understanding CIs, often struggling to extract\ninformation or perform complex reasoning based on these images. We find that\nexisting training data for CIs are mostly formatted for question-answer tasks\n(e.g., in datasets like ChartQA and ScienceQA), while high-quality\nimage-caption datasets, critical for robust vision-language alignment, are only\navailable for NIs. To bridge this gap, we introduce Composite Captions\n(CompCap), a flexible framework that leverages Large Language Models (LLMs) and\nautomation tools to synthesize CIs with accurate and detailed captions. Using\nCompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs\nacross six CI types. We validate the effectiveness of CompCap-118K by\nsupervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and\nLLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K\nsignificantly enhances MLLMs' understanding of CIs, yielding average gains of\n1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.", "paper_summary_zh": "\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u80fd\u6709\u591a\u597d\u5730\u7406\u89e3\u5408\u6210\u5f71\u50cf\uff1f\u5408\u6210\u5f71\u50cf (CI) \u662f\u900f\u904e\u5408\u4f75\u591a\u500b\u8996\u89ba\u5143\u7d20\uff08\u4f8b\u5982\u5716\u8868\u3001\u6d77\u5831\u6216\u87a2\u5e55\u622a\u5716\uff09\u6240\u5efa\u7acb\u7684\u5408\u6210\u8996\u89ba\u6548\u679c\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u7531\u76f8\u6a5f\u62cd\u651d\u3002\u5118\u7ba1 CI \u5728\u73fe\u5be6\u4e16\u754c\u7684\u61c9\u7528\u4e2d\u5f88\u666e\u904d\uff0c\u4f46\u6700\u8fd1\u7684 MLLM \u767c\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u8a6e\u91cb\u81ea\u7136\u5f71\u50cf (NI)\u3002\u6211\u5011\u7684\u7814\u7a76\u986f\u793a\uff0c\u73fe\u6709\u7684 MLLM \u5728\u6e96\u78ba\u7406\u89e3 CI \u6642\u9762\u81e8\u91cd\u5927\u6311\u6230\uff0c\u7d93\u5e38\u96e3\u4ee5\u6839\u64da\u9019\u4e9b\u5f71\u50cf\u64f7\u53d6\u8cc7\u8a0a\u6216\u57f7\u884c\u8907\u96dc\u7684\u63a8\u7406\u3002\u6211\u5011\u767c\u73fe\uff0cCI \u7684\u73fe\u6709\u8a13\u7df4\u8cc7\u6599\u5927\u591a\u4ee5\u554f\u7b54\u4efb\u52d9\u7684\u683c\u5f0f\u5448\u73fe\uff08\u4f8b\u5982 ChartQA \u548c ScienceQA \u7b49\u8cc7\u6599\u96c6\uff09\uff0c\u800c\u5c0d\u65bc\u7a69\u5065\u7684\u8996\u89ba\u8a9e\u8a00\u5c0d\u9f4a\u81f3\u95dc\u91cd\u8981\u7684\u512a\u8cea\u5f71\u50cf\u6a19\u984c\u8cc7\u6599\u96c6\u50c5\u9069\u7528\u65bc NI\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5408\u6210\u6a19\u984c (CompCap)\uff0c\u9019\u662f\u4e00\u500b\u9748\u6d3b\u7684\u67b6\u69cb\uff0c\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u81ea\u52d5\u5316\u5de5\u5177\u4f86\u5408\u6210\u5177\u6709\u6e96\u78ba\u4e14\u8a73\u7d30\u6a19\u984c\u7684 CI\u3002\u4f7f\u7528 CompCap\uff0c\u6211\u5011\u6574\u7406\u4e86 CompCap-118K\uff0c\u4e00\u500b\u5305\u542b 118K \u5f71\u50cf\u6a19\u984c\u5c0d\u61c9\u7684\u8cc7\u6599\u96c6\uff0c\u6db5\u84cb\u516d\u7a2e\u985e\u578b\u7684 CI\u3002\u6211\u5011\u900f\u904e\u76e3\u7763\u5fae\u8abf\u4e09\u7a2e\u5c3a\u5bf8\u7684 MLLM \u4f86\u9a57\u8b49 CompCap-118K \u7684\u6709\u6548\u6027\uff1axGen-MM-inst.-4B \u548c LLaVA-NeXT-Vicuna-7B/13B\u3002\u7d93\u9a57\u7d50\u679c\u986f\u793a\uff0cCompCap-118K \u5927\u5e45\u63d0\u5347\u4e86 MLLM \u5c0d CI \u7684\u7406\u89e3\uff0c\u5728 11 \u500b\u57fa\u6e96\u4e2d\u5206\u5225\u7522\u751f\u4e86\u5e73\u5747 1.7%\u30012.0% \u548c 2.9% \u7684\u589e\u76ca\u3002", "author": "Xiaohui Chen et.al.", "authors": "Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang, David Yang, ShengYun Peng, Hanchao Yu, Shen Yan, Xuewen Zhang, Baosheng He", "id": "2412.05243v1", "paper_url": "http://arxiv.org/abs/2412.05243v1", "repo": "null"}}