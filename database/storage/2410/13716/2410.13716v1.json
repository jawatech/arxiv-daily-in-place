{"2410.13716": {"publish_time": "2024-10-17", "title": "MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems", "paper_summary": "Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different\nheuristic-based metrics for evaluation, but these require human preferences as\nground truth for reference. In contrast, arena-based benchmarks, where two\nmodels compete each other, require an expensive Large Language Model (LLM) as a\njudge for a reliable evaluation. We present an easy and efficient technique to\nget the best of both worlds. The idea is to train a learning to rank model as a\n\"surrogate\" judge using RAG-based evaluation heuristics as input, to produce a\nsynthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a\nstandardized arena-based multilingual RAG benchmark for 18 diverse languages on\nWikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and\nextended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG\nextensively coupling both heuristic features and LLM as a judge evaluator. In\nour work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high\ncorrelation (Kendall Tau ($\\tau$) = 0.909) using our surrogate judge learned\nusing heuristic features with pairwise evaluations and between GPT-4o as a\nteacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We\nobserve proprietary and large open-source LLMs currently dominate in\nmultilingual RAG. MIRAGE-Bench is available at:\nhttps://github.com/vectara/mirage-bench.", "paper_summary_zh": "\u50b3\u7d71\u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff08RAG\uff09\u57fa\u6e96\u4f9d\u8cf4\u65bc\u4e0d\u540c\u7684\u57fa\u65bc\u555f\u767c\u5f0f\u7684\u6307\u6a19\u9032\u884c\u8a55\u4f30\uff0c\u4f46\u9019\u4e9b\u6307\u6a19\u9700\u8981\u4eba\u985e\u504f\u597d\u4f5c\u70ba\u53c3\u8003\u7684\u771f\u5be6\u4f9d\u64da\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7af6\u6280\u5834\u57fa\u6e96\uff0c\u5176\u4e2d\u5169\u500b\u6a21\u578b\u76f8\u4e92\u7af6\u722d\uff0c\u9700\u8981\u4e00\u500b\u6602\u8cb4\u7684\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u70ba\u4e00\u500b\u8a55\u59d4\uff0c\u4ee5\u9032\u884c\u53ef\u9760\u7684\u8a55\u4f30\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u8f15\u9b06\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u7372\u5f97\u5169\u5168\u5176\u7f8e\u7684\u6548\u679c\u3002\u9019\u500b\u60f3\u6cd5\u662f\u8a13\u7df4\u4e00\u500b\u5b78\u7fd2\u5c0d\u6a21\u578b\u9032\u884c\u6392\u540d\uff0c\u4f5c\u70ba\u4e00\u500b\u300c\u4ee3\u7406\u300d\u8a55\u59d4\uff0c\u4f7f\u7528\u57fa\u65bc RAG \u7684\u8a55\u4f30\u555f\u767c\u5f0f\u65b9\u6cd5\u4f5c\u70ba\u8f38\u5165\uff0c\u4ee5\u7522\u751f\u4e00\u500b\u7d9c\u5408\u7684\u7af6\u6280\u5834\u6392\u884c\u699c\u3002\u5229\u7528\u9019\u500b\u60f3\u6cd5\uff0c\u6211\u5011\u958b\u767c\u4e86 MIRAGE-Bench\uff0c\u4e00\u500b\u6a19\u6e96\u5316\u7684\u57fa\u65bc\u7af6\u6280\u5834\u7684\u591a\u8a9e\u8a00 RAG \u57fa\u6e96\uff0c\u9069\u7528\u65bc\u7dad\u57fa\u767e\u79d1\u4e0a\u7684 18 \u7a2e\u4e0d\u540c\u7684\u8a9e\u8a00\u3002\u8a72\u57fa\u6e96\u662f\u4f7f\u7528\u6aa2\u7d22\u6578\u64da\u96c6 MIRACL \u69cb\u5efa\u7684\uff0c\u4e26\u64f4\u5c55\u7528\u65bc\u591a\u8a9e\u8a00\u751f\u6210\u8a55\u4f30\u3002MIRAGE-Bench \u5c0d RAG \u9032\u884c\u5ee3\u6cdb\u8a55\u4f30\uff0c\u540c\u6642\u5c07\u555f\u767c\u5f0f\u7279\u5fb5\u548c LLM \u4f5c\u70ba\u8a55\u59d4\u8a55\u4f30\u5668\u7d50\u5408\u8d77\u4f86\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c0d 19 \u500b\u4e0d\u540c\u7684\u591a\u8a9e\u8a00 LLM \u9032\u884c\u4e86\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u4f7f\u7528\u6211\u5011\u7684\u4ee3\u7406\u8a55\u59d4\u5b78\u7fd2\u7684\u555f\u767c\u5f0f\u7279\u5fb5\u548c\u6210\u5c0d\u8a55\u4f30\uff0c\u4ee5\u53ca\u4f7f\u7528 Bradley-Terry \u6846\u67b6\u5728 MIRAGE-Bench \u6392\u884c\u699c\u4e0a\u5c07 GPT-4o \u4f5c\u70ba\u6559\u5e2b\uff0c\u5be6\u73fe\u4e86\u5f88\u9ad8\u7684\u76f8\u95dc\u6027\uff08Kendall Tau\uff08\u03c4\uff09= 0.909\uff09\u3002\u6211\u5011\u89c0\u5bdf\u5230\u5c08\u6709\u7684\u5927\u578b\u958b\u6e90 LLM \u76ee\u524d\u5728\u591a\u8a9e\u8a00 RAG \u4e2d\u4f54\u64da\u4e3b\u5c0e\u5730\u4f4d\u3002MIRAGE-Bench \u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u7372\u5f97\uff1ahttps://github.com/vectara/mirage-bench\u3002", "author": "Nandan Thakur et.al.", "authors": "Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, Amin Ahmad", "id": "2410.13716v1", "paper_url": "http://arxiv.org/abs/2410.13716v1", "repo": "https://github.com/vectara/mirage-bench"}}