{"2405.10529": {"publish_time": "2024-05-17", "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors", "paper_summary": "Large language models have become increasingly prominent, also signaling a\nshift towards multimodality as the next frontier in artificial intelligence,\nwhere their embeddings are harnessed as prompts to generate textual content.\nVision-language models (VLMs) stand at the forefront of this advancement,\noffering innovative ways to combine visual and textual data for enhanced\nunderstanding and interaction. However, this integration also enlarges the\nattack surface. Patch-based adversarial attack is considered the most realistic\nthreat model in physical vision applications, as demonstrated in many existing\nliterature. In this paper, we propose to address patched visual prompt\ninjection, where adversaries exploit adversarial patches to generate target\ncontent in VLMs. Our investigation reveals that patched adversarial prompts\nexhibit sensitivity to pixel-wise randomization, a trait that remains robust\neven against adaptive attacks designed to counteract such defenses. Leveraging\nthis insight, we introduce SmoothVLM, a defense mechanism rooted in smoothing\ntechniques, specifically tailored to protect VLMs from the threat of patched\nvisual prompt injectors. Our framework significantly lowers the attack success\nrate to a range between 0% and 5.0% on two leading VLMs, while achieving around\n67.3% to 95.0% context recovery of the benign images, demonstrating a balance\nbetween security and usability.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\uff0c\u4e5f\u6a19\u8a8c\u8457\u4eba\u5de5\u667a\u6167\u7684\u4e0b\u4e00\u524d\u6cbf\u5c07\u8f49\u5411\u591a\u6a21\u614b\uff0c\u5176\u4e2d\u5176\u5d4c\u5165\u88ab\u5229\u7528\u70ba\u63d0\u793a\u4f86\u7522\u751f\u6587\u5b57\u5167\u5bb9\u3002\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7ad9\u5728\u9019\u9805\u9032\u5c55\u7684\u6700\u524d\u7dda\uff0c\u63d0\u4f9b\u5275\u65b0\u7684\u65b9\u5f0f\u4f86\u7d50\u5408\u8996\u89ba\u548c\u6587\u5b57\u8cc7\u6599\uff0c\u4ee5\u589e\u5f37\u7406\u89e3\u548c\u4e92\u52d5\u3002\u7136\u800c\uff0c\u9019\u7a2e\u6574\u5408\u4e5f\u64f4\u5927\u4e86\u653b\u64ca\u9762\u3002\u57fa\u65bc\u8cbc\u7247\u7684\u5c0d\u6297\u6027\u653b\u64ca\u88ab\u8a8d\u70ba\u662f\u7269\u7406\u8996\u89ba\u61c9\u7528\u4e2d\u6700\u903c\u771f\u7684\u5a01\u8105\u6a21\u578b\uff0c\u5982\u8a31\u591a\u73fe\u6709\u6587\u737b\u4e2d\u6240\u5c55\u793a\u7684\u90a3\u6a23\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u8b70\u89e3\u6c7a\u8cbc\u7247\u8996\u89ba\u63d0\u793a\u6ce8\u5165\u7684\u554f\u984c\uff0c\u5176\u4e2d\u5c0d\u624b\u5229\u7528\u5c0d\u6297\u6027\u8cbc\u7247\u5728 VLM \u4e2d\u7522\u751f\u76ee\u6a19\u5167\u5bb9\u3002\u6211\u5011\u7684\u8abf\u67e5\u986f\u793a\uff0c\u8cbc\u7247\u7684\u5c0d\u6297\u6027\u63d0\u793a\u5c0d\u9010\u50cf\u7d20\u96a8\u6a5f\u5316\u8868\u73fe\u51fa\u654f\u611f\u6027\uff0c\u5373\u4f7f\u91dd\u5c0d\u65e8\u5728\u5c0d\u6297\u6b64\u985e\u9632\u79a6\u7684\u9069\u61c9\u6027\u653b\u64ca\uff0c\u9019\u7a2e\u7279\u5fb5\u4ecd\u7136\u5f88\u5f37\u5927\u3002\u5229\u7528\u9019\u4e00\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 SmoothVLM\uff0c\u9019\u662f\u4e00\u7a2e\u6839\u690d\u65bc\u5e73\u6ed1\u6280\u8853\u7684\u9632\u79a6\u6a5f\u5236\uff0c\u5c08\u9580\u7528\u65bc\u4fdd\u8b77 VLM \u514d\u53d7\u8cbc\u7247\u8996\u89ba\u63d0\u793a\u6ce8\u5165\u5668\u7684\u5a01\u8105\u3002\u6211\u5011\u7684\u6846\u67b6\u5c07\u653b\u64ca\u6210\u529f\u7387\u986f\u8457\u964d\u4f4e\u5230\u5169\u500b\u9818\u5148\u7684 VLM \u4e0a\u7684 0% \u5230 5.0% \u4e4b\u9593\uff0c\u540c\u6642\u5be6\u73fe\u4e86\u826f\u6027\u5716\u50cf\u7684 67.3% \u5230 95.0% \u7684\u5167\u5bb9\u6062\u5fa9\uff0c\u5c55\u793a\u4e86\u5b89\u5168\u6027\u8207\u53ef\u7528\u6027\u4e4b\u9593\u7684\u5e73\u8861\u3002", "author": "Jiachen Sun et.al.", "authors": "Jiachen Sun, Changsheng Wang, Jiongxiao Wang, Yiwei Zhang, Chaowei Xiao", "id": "2405.10529v1", "paper_url": "http://arxiv.org/abs/2405.10529v1", "repo": "null"}}