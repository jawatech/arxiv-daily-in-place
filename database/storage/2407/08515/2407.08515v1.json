{"2407.08515": {"publish_time": "2024-07-11", "title": "15M Multimodal Facial Image-Text Dataset", "paper_summary": "Currently, image-text-driven multi-modal deep learning models have\ndemonstrated their outstanding potential in many fields. In practice, tasks\ncentered around facial images have broad application prospects. This paper\npresents \\textbf{FaceCaption-15M}, a large-scale, diverse, and high-quality\ndataset of facial images accompanied by their natural language descriptions\n(facial image-to-text). This dataset aims to facilitate a study on\nface-centered tasks. FaceCaption-15M comprises over 15 million pairs of facial\nimages and their corresponding natural language descriptions of facial\nfeatures, making it the largest facial image-caption dataset to date. We\nconducted a comprehensive analysis of image quality, text naturalness, text\ncomplexity, and text-image relevance to demonstrate the superiority of\nFaceCaption-15M. To validate the effectiveness of FaceCaption-15M, we first\ntrained a facial language-image pre-training model (FLIP, similar to CLIP) to\nalign facial image with its corresponding captions in feature space.\nSubsequently, using both image and text encoders and fine-tuning only the\nlinear layer, our FLIP-based models achieved state-of-the-art results on two\nchallenging face-centered tasks. The purpose is to promote research in the\nfield of face-related tasks through the availability of the proposed\nFaceCaption-15M dataset. All data, codes, and models are publicly available.\nhttps://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M", "paper_summary_zh": "<paragraph>\u76ee\u524d\uff0c\u56fe\u50cf\u6587\u672c\u9a71\u52a8\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u5728\u4f17\u591a\u9886\u57df\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u6f5c\u529b\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u4ee5\u9762\u90e8\u56fe\u50cf\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002\u672c\u6587\u63d0\u51fa\u4e86\\textbf{FaceCaption-15M}\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u9644\u6709\u5176\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff08\u9762\u90e8\u56fe\u50cf\u5230\u6587\u672c\uff09\u3002\u6b64\u6570\u636e\u96c6\u65e8\u5728\u4fc3\u8fdb\u5bf9\u4ee5\u9762\u90e8\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u7684\u7814\u7a76\u3002FaceCaption-15M \u5305\u542b\u8d85\u8fc7 1500 \u4e07\u5bf9\u9762\u90e8\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u9762\u90e8\u7279\u5f81\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u4f7f\u5176\u6210\u4e3a\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u9762\u90e8\u56fe\u50cf\u6807\u9898\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9\u56fe\u50cf\u8d28\u91cf\u3001\u6587\u672c\u81ea\u7136\u6027\u3001\u6587\u672c\u590d\u6742\u6027\u548c\u6587\u672c\u56fe\u50cf\u76f8\u5173\u6027\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u4ee5\u8bc1\u660e FaceCaption-15M \u7684\u4f18\u8d8a\u6027\u3002\u4e3a\u4e86\u9a8c\u8bc1 FaceCaption-15M \u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u9996\u5148\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9762\u90e8\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b (FLIP\uff0c\u7c7b\u4f3c\u4e8e CLIP)\uff0c\u4ee5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5c06\u9762\u90e8\u56fe\u50cf\u4e0e\u5176\u5bf9\u5e94\u7684\u6807\u9898\u5bf9\u9f50\u3002\u968f\u540e\uff0c\u4f7f\u7528\u56fe\u50cf\u548c\u6587\u672c\u7f16\u7801\u5668\u5e76\u4ec5\u5fae\u8c03\u7ebf\u6027\u5c42\uff0c\u6211\u4eec\u57fa\u4e8e FLIP \u7684\u6a21\u578b\u5728\u4e24\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4ee5\u9762\u90e8\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u76ee\u7684\u662f\u901a\u8fc7\u63d0\u4f9b\u5efa\u8bae\u7684 FaceCaption-15M \u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u9762\u90e8\u76f8\u5173\u4efb\u52a1\u9886\u57df\u7684\u7814\u7a76\u3002\u6240\u6709\u6570\u636e\u3001\u4ee3\u7801\u548c\u6a21\u578b\u5747\u516c\u5f00\u53ef\u7528\u3002https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M</paragraph>", "author": "Dawei Dai et.al.", "authors": "Dawei Dai, YuTang Li, YingGe Liu, Mingming Jia, Zhang YuanHui, Guoyin Wang", "id": "2407.08515v1", "paper_url": "http://arxiv.org/abs/2407.08515v1", "repo": "null"}}