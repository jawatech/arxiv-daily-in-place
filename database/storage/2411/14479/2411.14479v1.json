{"2411.14479": {"publish_time": "2024-11-19", "title": "GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via Reinforcement Learning", "paper_summary": "Large language models (LLMs) have demonstrated impressive success in a wide\nrange of natural language processing (NLP) tasks due to their extensive general\nknowledge of the world. Recent works discovered that the performance of LLMs is\nheavily dependent on the input prompt. However, prompt engineering is usually\ndone manually in a trial-and-error fashion, which can be labor-intensive and\nchallenging in order to find the optimal prompts. To address these problems and\nunleash the utmost potential of LLMs, we propose a novel LLMs-agnostic\nframework for prompt optimization, namely GRL-Prompt, which aims to\nautomatically construct optimal prompts via reinforcement learning (RL) in an\nend-to-end manner. To provide structured action/state representation for\noptimizing prompts, we construct a knowledge graph (KG) that better encodes the\ncorrelation between the user query and candidate in-context examples.\nFurthermore, a policy network is formulated to generate the optimal action by\nselecting a set of in-context examples in a rewardable order to construct the\nprompt. Additionally, the embedding-based reward shaping is utilized to\nstabilize the RL training process. The experimental results show that\nGRL-Prompt outperforms recent state-of-the-art methods, achieving an average\nincrease of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in\nBLEU.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5ee3\u6cdb\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u529f\uff0c\u9019\u6b78\u529f\u65bc\u5b83\u5011\u5c0d\u4e16\u754c\u7684\u5ee3\u6cdb\u4e00\u822c\u77e5\u8b58\u3002\u6700\u8fd1\u7684\u7814\u7a76\u767c\u73fe\uff0cLLM \u7684\u6548\u80fd\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u8f38\u5165\u63d0\u793a\u3002\u7136\u800c\uff0c\u63d0\u793a\u5de5\u7a0b\u901a\u5e38\u4ee5\u8a66\u932f\u7684\u65b9\u5f0f\u624b\u52d5\u5b8c\u6210\uff0c\u9019\u5728\u5c0b\u627e\u6700\u4f73\u63d0\u793a\u6642\u53ef\u80fd\u6703\u8017\u8cbb\u5927\u91cf\u4eba\u529b\u4e14\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u4e26\u767c\u63ee LLM \u7684\u6700\u5927\u6f5b\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684 LLM \u4e0d\u53ef\u77e5\u6846\u67b6\uff0c\u7528\u65bc\u63d0\u793a\u6700\u4f73\u5316\uff0c\u5373 GRL-Prompt\uff0c\u5176\u65e8\u5728\u900f\u904e\u5f37\u5316\u5b78\u7fd2 (RL) \u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u81ea\u52d5\u5efa\u69cb\u6700\u4f73\u63d0\u793a\u3002\u70ba\u4e86\u63d0\u4f9b\u7d50\u69cb\u5316\u7684\u52d5\u4f5c/\u72c0\u614b\u8868\u793a\u4ee5\u6700\u4f73\u5316\u63d0\u793a\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u77e5\u8b58\u5716\u8b5c (KG)\uff0c\u5b83\u80fd\u66f4\u597d\u5730\u7de8\u78bc\u4f7f\u7528\u8005\u67e5\u8a62\u8207\u5019\u9078\u60c5\u5883\u7bc4\u4f8b\u4e4b\u9593\u7684\u95dc\u806f\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5236\u5b9a\u4e86\u4e00\u500b\u7b56\u7565\u7db2\u8def\uff0c\u900f\u904e\u4ee5\u53ef\u734e\u52f5\u7684\u9806\u5e8f\u9078\u64c7\u4e00\u7d44\u60c5\u5883\u7bc4\u4f8b\u4f86\u5efa\u69cb\u63d0\u793a\uff0c\u4ee5\u7522\u751f\u6700\u4f73\u52d5\u4f5c\u3002\u6b64\u5916\uff0c\u6211\u5011\u5229\u7528\u57fa\u65bc\u5d4c\u5165\u7684\u734e\u52f5\u5851\u9020\u4f86\u7a69\u5b9a RL \u8a13\u7df4\u904e\u7a0b\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cGRL-Prompt \u512a\u65bc\u6700\u8fd1\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u5728 ROUGE-1 \u4e2d\u5e73\u5747\u589e\u52a0 0.10\uff0c\u5728 ROUGE-2 \u4e2d\u589e\u52a0 0.07\uff0c\u5728 ROUGE-L \u4e2d\u589e\u52a0 0.07\uff0c\u5728 BLEU \u4e2d\u589e\u52a0 0.05\u3002", "author": "Yuze Liu et.al.", "authors": "Yuze Liu, Tingjie Liu, Tiehua Zhang, Youhua Xia, Jinze Wang, Zhishu Shen, Jiong Jin, Fei Richard Yu", "id": "2411.14479v1", "paper_url": "http://arxiv.org/abs/2411.14479v1", "repo": "null"}}