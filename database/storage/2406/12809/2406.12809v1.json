{"2406.12809": {"publish_time": "2024-06-18", "title": "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "paper_summary": "Large language models (LLMs) have demonstrated impressive capabilities, but\nstill suffer from inconsistency issues (e.g. LLMs can react differently to\ndisturbances like rephrasing or inconsequential order change). In addition to\nthese inconsistencies, we also observe that LLMs, while capable of solving hard\nproblems, can paradoxically fail at easier ones. To evaluate this hard-to-easy\ninconsistency, we develop the ConsisEval benchmark, where each entry comprises\na pair of questions with a strict order of difficulty. Furthermore, we\nintroduce the concept of consistency score to quantitatively measure this\ninconsistency and analyze the potential for improvement in consistency by\nrelative consistency score. Based on comprehensive experiments across a variety\nof existing models, we find: (1) GPT-4 achieves the highest consistency score\nof 92.2\\% but is still inconsistent to specific questions due to distraction by\nredundant information, misinterpretation of questions, etc.; (2) models with\nstronger capabilities typically exhibit higher consistency, but exceptions also\nexist; (3) hard data enhances consistency for both fine-tuning and in-context\nlearning. Our data and code will be publicly available on GitHub.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\n\u4ecd\u6709\u524d\u5f8c\u4e0d\u4e00\u81f4\u7684\u554f\u984c\uff08\u4f8b\u5982\uff0cLLM \u5c0d\u6539\u5beb\u6216\u7121\u95dc\u9806\u5e8f\u8b8a\u66f4\u7b49\u5e72\u64fe\u7684\u53cd\u61c9\u53ef\u80fd\u4e0d\u540c\uff09\u3002\u9664\u4e86\n\u9019\u4e9b\u4e0d\u4e00\u81f4\u4e4b\u5916\uff0c\u6211\u5011\u9084\u89c0\u5bdf\u5230\uff0cLLM \u5728\u89e3\u6c7a\u56f0\u96e3\u554f\u984c\u7684\u540c\u6642\uff0c\u537b\u53ef\u80fd\u5728\u8f03\u5bb9\u6613\u7684\u554f\u984c\u4e0a\u51fa\u73fe\u77db\u76fe\u7684\u5931\u6557\u3002\u70ba\u4e86\u8a55\u4f30\u9019\u7a2e\u96e3\u984c\u6613\u984c\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u6211\u5011\u958b\u767c\u4e86 ConsisEval \u57fa\u6e96\uff0c\u5176\u4e2d\u7684\u6bcf\u4e00\u9805\u90fd\u5305\u542b\u4e00\u5c0d\u96e3\u5ea6\u9806\u5e8f\u56b4\u683c\u7684\u554f\u984c\u3002\u6b64\u5916\uff0c\u6211\u5011\n\u5f15\u5165\u4e86\u524d\u5f8c\u4e00\u81f4\u6027\u8a55\u5206\u9019\u500b\u6982\u5ff5\uff0c\u4ee5\u91cf\u5316\u6e2c\u91cf\u9019\u7a2e\u4e0d\u4e00\u81f4\u6027\uff0c\u4e26\u900f\u904e\u76f8\u5c0d\u524d\u5f8c\u4e00\u81f4\u6027\u8a55\u5206\u5206\u6790\u524d\u5f8c\u4e00\u81f4\u6027\u6539\u9032\u7684\u53ef\u80fd\u6027\u3002\u6839\u64da\u5c0d\u5404\u7a2e\u73fe\u6709\u6a21\u578b\u9032\u884c\u7684\u7d9c\u5408\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe\uff1a(1) GPT-4 \u9054\u5230\u4e86\u6700\u9ad8\u7684\u524d\u5f8c\u4e00\u81f4\u6027\u8a55\u5206 92.2%\uff0c\u4f46\u7531\u65bc\u5197\u9918\u8cc7\u8a0a\u7684\u5e72\u64fe\u3001\u5c0d\u554f\u984c\u7684\u8aa4\u89e3\u7b49\u539f\u56e0\uff0c\u5728\u7279\u5b9a\u554f\u984c\u4e0a\u4ecd\u524d\u5f8c\u4e0d\u4e00\u81f4\uff1b(2) \u80fd\u529b\u8f03\u5f37\u7684\u6a21\u578b\u901a\u5e38\u8868\u73fe\u51fa\u8f03\u9ad8\u7684\u524d\u5f8c\u4e00\u81f4\u6027\uff0c\u4f46\u4e5f\u5b58\u5728\u4f8b\u5916\uff1b(3) \u56f0\u96e3\u7684\u8cc7\u6599\u589e\u5f37\u4e86\u5fae\u8abf\u548c\u60c5\u5883\u5b78\u7fd2\u7684\u4e00\u81f4\u6027\u3002\u6211\u5011\u7684\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u5c07\u5728 GitHub \u4e0a\u516c\u958b\u3002", "author": "Zhe Yang et.al.", "authors": "Zhe Yang, Yichang Zhang, Tianyu Liu, Jian Yang, Junyang Lin, Chang Zhou, Zhifang Sui", "id": "2406.12809v1", "paper_url": "http://arxiv.org/abs/2406.12809v1", "repo": "null"}}