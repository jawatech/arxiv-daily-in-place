{"2412.19531": {"publish_time": "2024-12-27", "title": "Is Your Text-to-Image Model Robust to Caption Noise?", "paper_summary": "In text-to-image (T2I) generation, a prevalent training technique involves\nutilizing Vision Language Models (VLMs) for image re-captioning. Even though\nVLMs are known to exhibit hallucination, generating descriptive content that\ndeviates from the visual reality, the ramifications of such caption\nhallucinations on T2I generation performance remain under-explored. Through our\nempirical investigation, we first establish a comprehensive dataset comprising\nVLM-generated captions, and then systematically analyze how caption\nhallucination influences generation outcomes. Our findings reveal that (1) the\ndisparities in caption quality persistently impact model outputs during\nfine-tuning. (2) VLMs confidence scores serve as reliable indicators for\ndetecting and characterizing noise-related patterns in the data distribution.\n(3) even subtle variations in caption fidelity have significant effects on the\nquality of learned representations. These findings collectively emphasize the\nprofound impact of caption quality on model performance and highlight the need\nfor more sophisticated robust training algorithm in T2I. In response to these\nobservations, we propose a approach leveraging VLM confidence score to mitigate\ncaption noise, thereby enhancing the robustness of T2I models against\nhallucination in caption.", "paper_summary_zh": "<paragraph>\u5728\u6587\u5b57\u8f49\u5716\u50cf (T2I) \u751f\u6210\u4e2d\uff0c\u4e00\u7a2e\u666e\u904d\u7684\u8a13\u7df4\u6280\u8853\u6d89\u53ca\u5229\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u9032\u884c\u5716\u50cf\u91cd\u65b0\u6a19\u984c\u3002\u5118\u7ba1\u5df2\u77e5 VLM \u6703\u51fa\u73fe\u5e7b\u89ba\uff0c\u7522\u751f\u504f\u96e2\u8996\u89ba\u73fe\u5be6\u7684\u63cf\u8ff0\u6027\u5167\u5bb9\uff0c\u4f46\u6b64\u985e\u6a19\u984c\u5e7b\u89ba\u5c0d T2I \u751f\u6210\u6548\u80fd\u7684\u5f71\u97ff\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u900f\u904e\u6211\u5011\u7684\u5be6\u8b49\u8abf\u67e5\uff0c\u6211\u5011\u9996\u5148\u5efa\u7acb\u4e00\u500b\u5305\u542b VLM \u751f\u6210\u7684\u6a19\u984c\u7684\u7d9c\u5408\u8cc7\u6599\u96c6\uff0c\u7136\u5f8c\u7cfb\u7d71\u6027\u5730\u5206\u6790\u6a19\u984c\u5e7b\u89ba\u5982\u4f55\u5f71\u97ff\u751f\u6210\u7d50\u679c\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff1a(1) \u6a19\u984c\u54c1\u8cea\u7684\u5dee\u7570\u5728\u5fae\u8abf\u671f\u9593\u6301\u7e8c\u5f71\u97ff\u6a21\u578b\u8f38\u51fa\u3002(2) VLM \u4fe1\u5fc3\u5206\u6578\u53ef\u7528\u4f5c\u5075\u6e2c\u548c\u8868\u5fb5\u8cc7\u6599\u5206\u4f48\u4e2d\u8207\u96dc\u8a0a\u76f8\u95dc\u7684\u6a21\u5f0f\u7684\u53ef\u9760\u6307\u6a19\u3002(3) \u5373\u4f7f\u6a19\u984c\u4fdd\u771f\u5ea6\u6709\u7d30\u5fae\u7684\u8b8a\u5316\uff0c\u4e5f\u6703\u5c0d\u5b78\u7fd2\u8868\u5fb5\u7684\u54c1\u8cea\u7522\u751f\u986f\u8457\u7684\u5f71\u97ff\u3002\u9019\u4e9b\u767c\u73fe\u5171\u540c\u5f37\u8abf\u6a19\u984c\u54c1\u8cea\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u6df1\u9060\u5f71\u97ff\uff0c\u4e26\u5f37\u8abf\u5728 T2I \u4e2d\u9700\u8981\u66f4\u7cbe\u5bc6\u7684\u7a69\u5065\u8a13\u7df4\u6f14\u7b97\u6cd5\u3002\u91dd\u5c0d\u9019\u4e9b\u89c0\u5bdf\u7d50\u679c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5229\u7528 VLM \u4fe1\u5fc3\u5206\u6578\u4f86\u6e1b\u8f15\u6a19\u984c\u96dc\u8a0a\u7684\u65b9\u6cd5\uff0c\u5f9e\u800c\u589e\u5f37 T2I \u6a21\u578b\u5c0d\u6a19\u984c\u4e2d\u5e7b\u89ba\u7684\u7a69\u5065\u6027\u3002</paragraph>", "author": "Weichen Yu et.al.", "authors": "Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang", "id": "2412.19531v1", "paper_url": "http://arxiv.org/abs/2412.19531v1", "repo": "null"}}