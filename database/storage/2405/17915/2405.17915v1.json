{"2405.17915": {"publish_time": "2024-05-28", "title": "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models", "paper_summary": "Long-context modeling capabilities are important for large language models\n(LLMs) in various applications. However, directly training LLMs with long\ncontext windows is insufficient to enhance this capability since some training\nsamples do not exhibit strong semantic dependencies across long contexts. In\nthis study, we propose a data mining framework \\textbf{ProLong} that can assign\neach training sample with a long dependency score, which can be used to rank\nand filter samples that are more advantageous for enhancing long-context\nmodeling abilities in LLM training. Specifically, we first use delta perplexity\nscores to measure the \\textit{Dependency Strength} between text segments in a\ngiven document. Then we refine this metric based on the \\textit{Dependency\nDistance} of these segments to incorporate spatial relationships across\nlong-contexts. Final results are calibrated with a \\textit{Dependency\nSpecificity} metric to prevent trivial dependencies introduced by repetitive\npatterns. Moreover, a random sampling approach is proposed to optimize the\ncomputational efficiency of ProLong. Comprehensive experiments on multiple\nbenchmarks indicate that ProLong effectively identifies documents that carry\nlong dependencies and LLMs trained on these documents exhibit significantly\nenhanced long-context modeling capabilities.", "paper_summary_zh": "\u9577\u8a9e\u5883\u5efa\u6a21\u80fd\u529b\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u76f4\u63a5\u4f7f\u7528\u9577\u8a9e\u5883\u7a97\u53e3\u8a13\u7df4 LLM \u7121\u6cd5\u589e\u5f37\u6b64\u80fd\u529b\uff0c\u56e0\u70ba\u6709\u4e9b\u8a13\u7df4\u7bc4\u4f8b\u4e26\u672a\u5728\u9577\u8a9e\u5883\u4e2d\u5c55\u73fe\u5f37\u70c8\u7684\u8a9e\u7fa9\u4f9d\u8cf4\u6027\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8cc7\u6599\u63a2\u52d8\u67b6\u69cb\u300cProLong\u300d\uff0c\u5b83\u53ef\u4ee5\u70ba\u6bcf\u500b\u8a13\u7df4\u7bc4\u4f8b\u5206\u914d\u4e00\u500b\u9577\u4f9d\u8cf4\u6027\u5206\u6578\uff0c\u53ef\u7528\u4f86\u5c0d\u7bc4\u4f8b\u9032\u884c\u6392\u5e8f\u548c\u7be9\u9078\uff0c\u4ee5\u5f37\u5316 LLM \u8a13\u7df4\u4e2d\u589e\u5f37\u9577\u8a9e\u5883\u5efa\u6a21\u80fd\u529b\u7684\u512a\u52e2\u3002\u5177\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528 delta \u56f0\u60d1\u5ea6\u5206\u6578\u6e2c\u91cf\u7d66\u5b9a\u6587\u4ef6\u4e2d\u7684\u6587\u5b57\u5340\u6bb5\u4e4b\u9593\u7684\u300c\u4f9d\u8cf4\u6027\u5f37\u5ea6\u300d\u3002\u7136\u5f8c\uff0c\u6211\u5011\u6839\u64da\u9019\u4e9b\u5340\u6bb5\u7684\u300c\u4f9d\u8cf4\u6027\u8ddd\u96e2\u300d\u8abf\u6574\u6b64\u6307\u6a19\uff0c\u4ee5\u7d0d\u5165\u9577\u8a9e\u5883\u4e2d\u7684\u7a7a\u9593\u95dc\u4fc2\u3002\u6700\u5f8c\u7684\u7d50\u679c\u4f7f\u7528\u300c\u4f9d\u8cf4\u6027\u7279\u7570\u6027\u300d\u6307\u6a19\u9032\u884c\u6821\u6b63\uff0c\u4ee5\u9632\u6b62\u91cd\u8907\u6a21\u5f0f\u6240\u7522\u751f\u7684\u7463\u788e\u4f9d\u8cf4\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u96a8\u6a5f\u62bd\u6a23\u65b9\u6cd5\u4f86\u6700\u4f73\u5316 ProLong \u7684\u904b\u7b97\u6548\u7387\u3002\u5728\u591a\u500b\u57fa\u6e96\u4e0a\u7684\u5168\u9762\u5be6\u9a57\u986f\u793a\uff0cProLong \u53ef\u4ee5\u6709\u6548\u8b58\u5225\u5177\u6709\u9577\u4f9d\u8cf4\u6027\u7684\u6587\u4ef6\uff0c\u800c\u91dd\u5c0d\u9019\u4e9b\u6587\u4ef6\u8a13\u7df4\u7684 LLM \u5247\u5c55\u73fe\u51fa\u986f\u8457\u589e\u5f37\u7684\u9577\u8a9e\u5883\u5efa\u6a21\u80fd\u529b\u3002", "author": "Longze Chen et.al.", "authors": "Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo, Min Yang", "id": "2405.17915v1", "paper_url": "http://arxiv.org/abs/2405.17915v1", "repo": "null"}}