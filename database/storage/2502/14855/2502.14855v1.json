{"2502.14855": {"publish_time": "2025-02-20", "title": "Prompt-to-Leaderboard", "paper_summary": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8a55\u4f30\u901a\u5e38\u4f9d\u8cf4\u65bc\u5f59\u7e3d\u7684\u6307\u6a19\uff0c\u4f8b\u5982\u6e96\u78ba\u6027\u6216\u4eba\u985e\u504f\u597d\uff0c\u5e73\u5747\u503c\u8de8\u4f7f\u7528\u8005\u548c\u63d0\u793a\u3002\u6b64\u5e73\u5747\u503c\u6a21\u7cca\u4e86\u4f7f\u7528\u8005\u548c\u63d0\u793a\u7279\u5b9a\u7684\u6a21\u578b\u6548\u80fd\u8b8a\u7570\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u63d0\u793a\u5230\u6392\u884c\u699c (P2L)\uff0c\u4e00\u7a2e\u7522\u751f\u7279\u5b9a\u65bc\u63d0\u793a\u7684\u6392\u884c\u699c\u7684\u65b9\u6cd5\u3002\u6838\u5fc3\u6982\u5ff5\u662f\u8a13\u7df4 LLM\uff0c\u5c07\u81ea\u7136\u8a9e\u8a00\u63d0\u793a\u4f5c\u70ba\u8f38\u5165\uff0c\u4ee5\u8f38\u51fa Bradley-Terry \u4fc2\u6578\u5411\u91cf\uff0c\u7136\u5f8c\u7528\u65bc\u9810\u6e2c\u4eba\u985e\u504f\u597d\u6295\u7968\u3002\u7522\u751f\u7684\u63d0\u793a\u76f8\u95dc\u6392\u884c\u699c\u5141\u8a31\u7121\u76e3\u7763\u4efb\u52d9\u7279\u5b9a\u8a55\u4f30\u3001\u6700\u4f73\u67e5\u8a62\u8def\u7531\u81f3\u6a21\u578b\u3001\u500b\u4eba\u5316\u4ee5\u53ca\u6a21\u578b\u512a\u7f3a\u9ede\u7684\u81ea\u52d5\u5316\u8a55\u4f30\u3002\u4f86\u81ea Chatbot Arena \u7684\u8cc7\u6599\u8868\u660e\uff0cP2L \u6bd4\u5e73\u5747\u6392\u884c\u699c\u66f4\u80fd\u6355\u6349\u8a9e\u8a00\u6a21\u578b\u6548\u80fd\u7684\u7d30\u5fae\u8b8a\u5316\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0cP2L \u7522\u751f\u63d0\u793a\u7279\u5b9a\u8a55\u4f30\u7684\u80fd\u529b\u9075\u5faa\u985e\u4f3c\u65bc LLM \u672c\u8eab\u89c0\u5bdf\u5230\u7684\u51aa\u5f8b\u7e2e\u653e\u30022025 \u5e74 1 \u6708\uff0c\u6211\u5011\u6839\u64da\u6b64\u65b9\u6cd5\u8a13\u7df4\u7684\u8def\u7531\u5668\u5728 Chatbot Arena \u6392\u884c\u699c\u4e2d\u7372\u5f97\u4e86\u7b2c\u4e00\u540d\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 GitHub \u9023\u7d50\u53d6\u5f97\uff1ahttps://github.com/lmarena/p2l\u3002", "author": "Evan Frick et.al.", "authors": "Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N. Angelopoulos, Ion Stoica", "id": "2502.14855v1", "paper_url": "http://arxiv.org/abs/2502.14855v1", "repo": "null"}}